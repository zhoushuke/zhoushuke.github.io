<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>再见2015,2016再见</title>
    <url>/2015/12/31/2015/</url>
    <content><![CDATA[<p><strong>再见, 2015</strong></p>
<p><strong>2016, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2015"><a href="#再见2015" class="headerlink" title="再见2015"></a><strong>再见2015</strong></h3><h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>正式成为<code>北漂</code>, 希望自己不会住地下室.</p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>捣鼓了一个blog系统，记录一些自己在生活&#x2F;工作上发生的事, 好的坏的，也许都可以写一写.</p>
<h3 id="2016再见"><a href="#2016再见" class="headerlink" title="2016再见"></a><strong>2016再见</strong></h3><p><strong>努力，奋斗</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2015.jpg" alt="2015"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2018,2019再见</title>
    <url>/2018/12/31/2018/</url>
    <content><![CDATA[<p><strong>再见, 2018</strong></p>
<p><strong>2019, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2018"><a href="#再见2018" class="headerlink" title="再见2018"></a><strong>再见2018</strong></h3><p><strong>心情不错, 工作顺利</strong></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>嗯,非律宾风景还是很不错的</p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>还行吧，不出众，也不搞砸</p>
<h3 id="2019再见"><a href="#2019再见" class="headerlink" title="2019再见"></a><strong>2019再见</strong></h3><p><strong>努力，奋斗</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2018.jpg" alt="2018"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2017,2018再见</title>
    <url>/2017/12/31/2017/</url>
    <content><![CDATA[<p><strong>再见, 2017</strong></p>
<p><strong>2018, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2017"><a href="#再见2017" class="headerlink" title="再见2017"></a><strong>再见2017</strong></h3><p>今天一睁眼,猛然发现,今天已是2017的最后一天.<br>回想这一年,<code>浑浑沌沌</code>.</p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>生活嘛,不能总是向着我喜欢的样子出现,生活之所以称之为生活,就是<code>生下来,活下去,生下来我们改变不了,但是活下去却只能靠我们自己</code>.<br><code>2017,弓虽女干了我</code></p>
<p>2017的<code>出逃计划</code>去了菲律宾,景色还可以,人文就两个字:<code>太穷</code></p>
<p>是的,<code>跟我一样穷</code>…</p>
<p>明年要去哪里呢,还没想好.</p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>一年的工作量算是比较充实吧,明年换个环境,准备卖命,新气象,新目标.</p>
<blockquote>
<ul>
<li>学习Go,Docker…</li>
<li>打折买的几本书,怎么着也不能拿它们来当枕头吧,当成是2018的课外作业.</li>
</ul>
</blockquote>
<p><code>还是那句话:不断充实自己肯定没有错,总觉得自己还可以做的更多,只有让自己忙起来,才觉得时间过的快,昨天还是2017,转眼就2018</code></p>
<h3 id="2018再见"><a href="#2018再见" class="headerlink" title="2018再见"></a><strong>2018再见</strong></h3><p><strong>得不到的在骚动</strong></p>
<p><code>拥有的有持无恐</code></p>
<p><code>拥有的请珍惜</code></p>
<p><code>想要的请努力</code></p>
<p><code>2018再见</code></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20241231105726.png" alt="2017"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2021,2022再见</title>
    <url>/2021/12/27/2021/</url>
    <content><![CDATA[<p><strong>再见, 2021</strong></p>
<p><strong>2022, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2021"><a href="#再见2021" class="headerlink" title="再见2021"></a><strong>再见2021</strong></h3><p><strong>后知后觉</strong></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>这一年出门的次数都少了，各界都有着很大的压力，不添乱最理性的配合</p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>工作上没有什么突破, 感觉还是太年轻, 总是后知后觉, <strong>就像是听不懂话术的2b青年</strong><br>能力固然很重要, 人情事故也得懂</p>
<h3 id="2022再见"><a href="#2022再见" class="headerlink" title="2022再见"></a><strong>2022再见</strong></h3><p><strong>去适应</strong></p>
<p>适应是成长的必经之路, 不是要环境适应你，而是你去适应环境<br>生活&#x2F;工作亦是如此</p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2021.jpg"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2016,2017再见</title>
    <url>/2016/12/31/2016/</url>
    <content><![CDATA[<p><strong>再见, 2016</strong></p>
<p><strong>2017, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2016"><a href="#再见2016" class="headerlink" title="再见2016"></a><strong>再见2016</strong></h3><p>今天一睁眼,猛然发现,今天已是2016的最后一天,按着一如既往的屌丝心态,是时候给自己开个一年一度的总结大会.<br>回想这一年,其实过的磕磕碰碰,很多本在意料之中的事却朝着相反的轨道行驶,<br>本以为凭我的力量是可以把它拉回正轨,但偏偏讽刺的是,很多时候<code>&quot;我以为&quot;</code>这三个字让我跌了一次又一次.<br>真的,别那么多<code>我以为.</code></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>生活嘛,不能总是向着我喜欢的样子出现,生活之所以称之为生活,就是<code>生下来,活下去,生下来我们改变不了,但是活下去却只能靠我们自己</code>.<br>而,2016年,我把生活过成了这个样子:</p>
<blockquote>
<p>简直可以用两个字概括:<code>&quot;粗糙&quot;</code><br>简直可以用三个字概括:<code>&quot;很粗糙&quot;</code><br>简直可以用四个字概括:<code>&quot;无比粗糙&quot;</code></p>
</blockquote>
<p>以前从来不会把这几个字安在自己身上,总觉得自己生活过的还可以,直到有个人跟我说了句:<code>你的生活怎么过的这么粗糙</code>…<br><code>How Dare You?</code><br>后来想想,也许只是自己不想承认罢了,明明就是生活一直在强奸我..<br>也不知道是什么时候做了决定,每年必须留空来一次<code>出去看看世界</code>的旅程,如此美好的事,可总是有些不法分子理解成这样:<code>每年都要去鬼混</code><br>其实说真的,<code>世界这么大,我希望在我的有生之年全都去看看...</code><br><code>为什么要说有生之年?</code><br><code>还不是因为我!!!!!太!!!!!穷!!!!!,穷的连一个域名都买不起.</code><br><code>斯里兰卡</code>的淳朴、<code>马来西亚的</code>风情,确实让我的2016新增一抹抹色彩<br><code>额,还有个不能说的秘密...</code><br><code>总体上,嗯,确实很粗糙...</code><br><code>2016,再!见!</code></p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>生活不如意,用工作来凑,好在还算是充实,也让我涨了不少知识.<br>写到这,突然我略有点不爽:<br>为什么总有甲方的同事会认为我刚毕业?<br>我拿着镜子照照自己,尼玛,这明明是张40岁男人沧桑的脸<br>不是我自黑,我确实显老对吧<br>不是颜值问题,那么只有一个完美的解释了,概括起来也是三个让我挺害怕的事:<br><code>不成熟</code><br>有木有?这是真的<br>不管是在生活上还是工作上,我一直都在试图让自己成熟起来.这样,让人比较有安全感.<br>以前总以为穿上西装就是成熟,其实我也挺害怕我自己穿西装的样子<br>吓的我赶紧Google下<code>何谓 成熟</code>压压惊<br><code>成熟,在于你处理事情的态度,反应的是一种能力.</code><br>还好真正的成熟跟年纪、颜值不成正比关系.<br>某些<code>正义的能力</code>,我觉得我还可以<br><code>说到态度</code>:我有一个问题很疑惑:<br>比如平时小明都是早上7点出门上班,上班路上会经过3个偶尔拥堵地,正常情况下8:15可以到公司,8:30刷脸,15分钟你还可以吃个早饭.公司规定一个月不能迟到4次.</p>
<blockquote>
<p>星期一,小明本来高高兴兴7点出门,抽风堵车了,到达公司迟到了.<br>星期二,小明本来高兴7点出门,抽风堵车了,到达公司迟到了.<br>星期三,小明7点出门,他很倔,抽风又堵车了,到达公司迟到了.<br>星期四,小明7点出门,他不信这个邪,抽风堵车了,到达公司迟到了.<br>星期五,小明7点出门,路上挺顺,8点到达公司没有迟到.</p>
</blockquote>
<p>恰好一切都看在老板眼里.老板也很难堪,毕竟老板头上还有大老板.<br>这种情况,大多数人一定会说小明的态度有问题!!!早起一点去公司不就可以了嘛!<br>可是对于习惯晚睡无法改善的单身小明来说,早上多睡一分钟,世界美好一整天.<br><code>这不是态度问题,运气差罢了</code><br>这就是跟买彩票,<code>你不会为了中一张500W的彩票而花1000W买下整个彩票池.</code><br>但是<code>无规矩不成方圆</code>,这是世界通行的准则,小明也必须遵守.</p>
<blockquote>
<ul>
<li>今年也时断时续的学了点<code>python</code>皮毛,写了点实用的脚本避免重复劳动力.</li>
<li>这段时间又想把博客搞起来,把之前收藏的好的文章从EverNote迁移到新博客上,<br>但是因为是轻博客使用<code>MarkDown</code>,暂时没有想到很好办的法批量格式化格式,特别是图片问题,有好想法的同学可以告诉我,感激不尽.</li>
<li>一直都各种APP关注行内技术趋势,多看看技术文章,因为公交车上实时是没座,眼睛也不知道往哪放,特别是夏天…好尴尬的…</li>
<li>双11贪便宜打折买的几本书,怎么着也不能拿它们来当枕头吧,当成是2017的课外作业.</li>
</ul>
</blockquote>
<p><code>不断充实自己肯定没有错,总觉得自己还可以做的更多,只有让自己忙起来,才觉得时间过的快,这可不,昨天还是2016,转眼就2017</code></p>
<h3 id="2017再见"><a href="#2017再见" class="headerlink" title="2017再见"></a><strong>2017再见</strong></h3><p><strong>顶你个肺</strong></p>
<p>说些什么呢？顶你个肺啊,2017年再见吧!</p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20241231105520.png" alt="2016"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2024,2025再见</title>
    <url>/2024/12/25/2024/</url>
    <content><![CDATA[<p><strong>再见, 2024</strong></p>
<p><strong>2025, 再见</strong></p>
<p><strong>seek logic, seek technology, seek detail</strong></p>
<span id="more"></span>

<h3 id="再见2024"><a href="#再见2024" class="headerlink" title="再见2024"></a><strong>再见2024</strong></h3><h4 id="西藏行"><a href="#西藏行" class="headerlink" title="西藏行"></a>西藏行</h4><p><strong>每年一次的背重远行,是生活最好的调节器</strong> </p>
<p>本来是号称最美西藏的7月份打算去，一拖再拖延就到10月份才走成(<strong>打工人果然没办法说走就走</strong>)，在西藏呆了半个月, 想起那句话: <strong>在西藏, 可以缺氧, 但不缺信仰</strong>.<br>我跟同事调侃道: <strong>花钱去高原拉练</strong>, 名其曰是去度假, 却选了条比上班还累的路，但远离城市的喧嚣, 身处大自然, 无比放松, <strong>祖国的大好河山, 是值得花时间去品尝, 西藏，我想我还会再去N+1(N&gt;0)次</strong></p>
<h4 id="打工人"><a href="#打工人" class="headerlink" title="打工人"></a>打工人</h4><p>2024整年我负责的算力规模已是年初规模的x倍有余，目前还处于扩容状态，几个数据：</p>
<ol>
<li>年底将达xxx核心,2025Q1达xxx核心, 2025年将再次翻倍, GPU可能达xxx规模</li>
<li>集群每天承载的任务数（按容器算）大约在x万+，全集群资源申请率可达90%</li>
<li>全年没有发生过P1故障（因集群宕机导致的ROI损失）</li>
</ol>
<h5 id="算力"><a href="#算力" class="headerlink" title="算力"></a>算力</h5><p>因为没有多余的人力维持多个集群，因此最开始规划的就是一个大的集群（我们不是没有考虑隔离不隔离的问题，但我还是比较有信心可以做到业务层隔离），这个大的集群里面混部着各类工作负载：生产态任务，研究态任务，训练&#x2F;推理任务，微服务等等</p>
<p>由于业务对时间非常敏感，敏感体现在两个方面</p>
<ol>
<li>上下游任务必须在规定时间内完成，任何一环任务delay都将影响全局产物交付，但凡有一环delay，都将对下游产生数量级的delay。</li>
<li>生产推理在毫秒级完成，这就要求在混部场景下集群各类agent组件不能有性能问题</li>
</ol>
<p>2024规划&#x2F;完成了以下的生产保障:</p>
<ol>
<li><p>控制面</p>
<blockquote>
<ul>
<li>扩容：集群目标承载能力在xx万核心左右，任务容器数xx万+</li>
<li>优化：全面对控制面组件做了优化</li>
<li>隔离：容器数5w的规模下，会产生数倍的事件数据，这些数据又在很多地方都有嵌套，这导致了某些操作变得缓慢，所以先将非核心object从etcd集群中剥离出来，如果仍有必要，下一步将剥离更核心object</li>
</ul>
</blockquote>
</li>
<li><p>数据面</p>
<blockquote>
<ul>
<li>优化：节点也做了相关的优化，主要聚焦在OS及kubelet</li>
</ul>
</blockquote>
</li>
<li><p>网络</p>
<blockquote>
<ul>
<li>容器网络：cilium做了必要优化</li>
<li>物理网络：IDC全量切换至spine leaf网络，集群节点的网卡都是context X5+级网卡，但是我们一直都是用的以太网模式，有点暴殄天物，训练在网络传输上也确实是遇到了瓶颈，近期做了ROCE相关的调研准备落地，</li>
</ul>
</blockquote>
</li>
</ol>
<h5 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h5><p>开发方面主要做了以下几件事，涉及golang,python,shell：</p>
<ol>
<li>更符合业务的多租户模型</li>
<li>所见即所得的监控体系</li>
<li>GPU工作负载按型号分配</li>
<li>kubelet cpu-manager cpu隔离逻辑</li>
<li>kube中resourcequota的统计逻辑</li>
<li>gitlab runner支持亲和性、自定义GPU调度逻辑</li>
<li>一些业务上的协同开发。。。</li>
</ol>
<h5 id="调研"><a href="#调研" class="headerlink" title="调研"></a>调研</h5><p>做了一些技术调研工作，这里只提几个未落地的topic，种种原因吧：</p>
<ol>
<li>集群ITTT（if this then that）</li>
<li>nvidia GDS</li>
<li>RoCEV2</li>
</ol>
<h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>回顾整个2024，还是有些不足：</p>
<ol>
<li>专注度不够：工作上总有一些时间花在<code>不必要</code>的事情上:，这里说的<code>不必要</code>是我认为的<code>没必要</code>，这类<code>不必要的事情</code>可能由很多原因导致，比如他人专业性不够，比如前期规划不足等等。但又不可避免地需要去配合，<strong>需求永远在变化</strong>，你不可能指望他人负责的领域完美无瑕，想起很经典的一句话：<code>每个打工的牛马多多少少会有些驴该干的活</code></li>
<li>拖延症显现：工作时心无旁物的状态不在，多了些懒散，应该可以有更多&#x2F;更多的产出，可能这是年过33的人的必经之路吧</li>
<li>其他客观因素。。。</li>
</ol>
<h4 id="规划展望"><a href="#规划展望" class="headerlink" title="规划展望"></a>规划展望</h4><p>本来想写规划来着，想想还是改成展望吧。<br>规划是一个很清晰的蓝图，而展望概念上相对模糊一些，这样包袱没那么重，以四象限原则：</p>
<ol>
<li><strong>重要且紧急</strong>：保障运动量，发量不可少，发际线不可移</li>
<li><strong>重要不紧急</strong>：持续可以拿到结果</li>
<li><strong>紧急不重要</strong>：拥抱AI, 深入prompt</li>
<li><strong>不重要不紧急</strong>：反向管理，PUA老板至少一次</li>
</ol>
<p><strong>身心健康永远放第一</strong></p>
<p>以上涉及的技术细节我都点到为止不细说。目前所有由我既定的技术选型与业务都很匹配，可能是因为我的过往经验很贴合目前的工作方向。</p>
<p>2024，75分。</p>
<h3 id="博客"><a href="#博客" class="headerlink" title="博客"></a><strong>博客</strong></h3><p>最后说一说我的博客，今年也没有更新几篇，开了一个新的系列<a href="https://izsk.me/2023/08/31/volcano-introduction/">volcano如何应对大规模任务</a>，其实还有几篇一直处于草稿阶段一拖就是几个月没有整理，羞愧。<br>好些人通过博客加上了我的微信，有人探讨技术，有人催更，有人希望我开通rss，还有一个同学想付费让我给他培训，这出乎了我的意外，不过我婉拒了他，我没有准备好。<br>关于为什么我的博客一直保持着级简单风格，原因就：<strong>Less is More</strong><br>博客内容大多数是<strong>我的生产&#x2F;实战经验，由点及面的分析</strong>，我觉得这是它唯一可以吸引别人的地方，实战嘛，本也不需要太过华丽的辞藻，<code>本质上我的工作并不具有创造性</code>，大多是踩在巨人的肩膀，如果我的经验可以当成肩膀让别人踩一踩，通过我的博客内容让他人少踩坑，那么这就是它的价值，也是我的初衷，其他徒有其表的东西不值一提。<br>我非常乐意能与大家做深入的技术探讨。<br>最后感谢那些给我打赏的宝子们。</p>
<h3 id="2025再见"><a href="#2025再见" class="headerlink" title="2025再见"></a><strong>2025再见</strong></h3><p>随着<strong>世界格局倒向右翼发展</strong>, 我想2025也将是不确定的一年。</p>
<p>祝各位发量永存，Bug不在，专心搞钱。</p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2024.jpg"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2022,2023再见</title>
    <url>/2022/12/29/2022/</url>
    <content><![CDATA[<p><strong>再见, 2022</strong></p>
<p><strong>2023, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2022"><a href="#再见2022" class="headerlink" title="再见2022"></a><strong>再见2022</strong></h3><p><strong>一边Fxxk,一边丝滑</strong></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p><strong>Fxxk,就这样</strong></p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p><strong>丝滑</strong></p>
<p>今年换了一份工作，虽从互联网来到了金融, 业务确实有很多的不同, 不过还是聚焦在自己擅长的领域，希望能继续发光发热<br>大家都说互联网大潮日渐衰退, 又能怎么样呢？<br>没有哪个方向是永远安全的, 过去说的铁饭碗现在也都不再铁，这在之前很多人看来也是不可能发生</p>
<p>世界格局瞬息万变, 以不变应万变, 不管大环境如何，自身的素质一定要硬<br>你要做的最大事情是: <strong>向企业证明你的价值是能解决问题, 而且能解决好问题</strong></p>
<p>但也需要有对趋势判断的能力, <strong>打开视野，认清自己</strong></p>
<h3 id="2023再见"><a href="#2023再见" class="headerlink" title="2023再见"></a><strong>2023再见</strong></h3><p><strong>NOFxxk</strong></p>
<p>疫情有了彻底放开的迹象, 全民信心也终将回归<br>职场上向来是<strong>适者生存,越早地接受这个定律, 就能越早地掌握主动</strong>, 希望在新的岗位可以<strong>做大做强, 再创辉煌</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2022.jpg"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2020,2021再见</title>
    <url>/2020/12/30/2020/</url>
    <content><![CDATA[<p><strong>再见, 2020</strong></p>
<p><strong>2021, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2020"><a href="#再见2020" class="headerlink" title="再见2020"></a><strong>再见2020</strong></h3><p><strong>不喝鸡汤，负重前行</strong></p>
<h3 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h3><p>生活如旧，没有跌宕起伏，时间不够用, 对人有辜负<br>可能是因为疫情的关系，大家都不好过，朋友聚在一直的时间也很少</p>
<h3 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h3><p>2020的工作前半年比较W&#x2F;F balance，后面相对就比较忙了，</p>
<p>好在工作规划基本都实现了，<strong>个人Role上也开始向Manager转变</strong></p>
<p>在这个过程中，让我感触最深的事是:</p>
<p><strong>面了那么多的候选人，会发现中年危机在互联网这个行业更加地严重</strong></p>
<p>以前自己做为被面试者，体会没那么深</p>
<p>反而是做了面试官之后，这种体会越发明显，也可能是年纪大了，比较惆怅</p>
<p>再过几年，我会成为那个他，<strong>如果改变不了这种现状，那就只能改变自己</strong></p>
<ol>
<li>不管大环境如何，自身的硬素质一定要有</li>
<li>工作重要，但也需要尽量维持人际关系</li>
<li>保持核心竞争力，尽可能成为团队核心</li>
<li>凡事表明态度、照顾队员情绪</li>
</ol>
<p>说到面试，我想说的是，面试了那么多了，其实就只有两种人:</p>
<ol>
<li>虚</li>
</ol>
<p>有些候选人为了更加自己的竞争力，简历上做了修饰，我觉得这个可以理解，竞争这么激烈，为了混口饭吃都不容易，但是也别一问细节什么都不知道，这就有点减分的效果了，写在简历上的东西，还是搞深一点比较好</p>
<p><strong>并不是所有的厂面试，都是上来直接二道算法hard，没有意义而且带坏风气</strong></p>
<ol start="2">
<li>到底谁面谁</li>
</ol>
<p>面试官并非各方面都懂，大部分情况下他只会问他知道的，所以如果抓到机会，可以互相讨论，除非碰到面试官一根筋，大部分情况下可以加分.</p>
<p><strong>2020，就这样了</strong></p>
<h3 id="2021再见"><a href="#2021再见" class="headerlink" title="2021再见"></a><strong>2021再见</strong></h3><p><strong>再前行</strong></p>
<p><strong>希望2020年的负重, 可以在2021年很好的前行</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2020.jpg" alt="2020"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2019,2020再见</title>
    <url>/2019/12/31/2019/</url>
    <content><![CDATA[<p><strong>再见, 2019</strong></p>
<p><strong>2020, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2019"><a href="#再见2019" class="headerlink" title="再见2019"></a><strong>再见2019</strong></h3><p><strong>小清新</strong></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>按照惯例地每年走出去一次,又去了一趟厦门&#x2F;HK, 小清新的城市, 风景也nice </p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>又换了一份新工作, 离开了互联网行业来到了AI公司, 不一样的技术领域确实会带有很多新鲜感,这一波风口希望自己踩对<br>我很庆幸自己每份工作的montor都很nice,在我的职业生涯中都教会我重要的一课，<strong>不早不晚，时间刚刚好</strong></p>
<h3 id="2020再见"><a href="#2020再见" class="headerlink" title="2020再见"></a><strong>2020再见</strong></h3><p><strong>新工作,新天地</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2019.jpg" alt="2019"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph学习(背景)</title>
    <url>/2018/04/08/Ceph%E5%AD%A6%E4%B9%A0(%E8%83%8C%E6%99%AF)/</url>
    <content><![CDATA[<p>Ceph是统一分布式存储系统,具有优异的性能、可靠性、可扩展性.  </p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Ceph项目从2004年6月份启动,源于Sage Weil的博士论文,后由其创建的Inktank公司主导,目前已被Redhat收购.<br><a href="http://storage.jd.com/text/weil-thesis.pdf">Ceph 论文</a></p>
<span id="more"></span>

<p><em>最近一个月的提交</em><br><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c1-1.png" alt="ceph-c1-1"></p>
<p><em>项目整体活跃度</em><br><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c1-2.png" alt="ceph-c1-2"></p>
<p><em>左侧为合作伙伴,右侧为用户</em><br><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c1-3.png" alt="ceph-c1-3"></p>
<p><em>国内主要公司</em></p>
<ul>
<li>华为</li>
<li>UnitedStack</li>
<li>XSKY</li>
<li>携程</li>
<li>联通研究院</li>
</ul>
<h2 id="其他系统的支持"><a href="#其他系统的支持" class="headerlink" title="其他系统的支持"></a>其他系统的支持</h2><ul>
<li>OpenStack</li>
<li>CloudStack </li>
<li>OpenNebula</li>
<li>Hadoop</li>
</ul>
<p><strong>主要在云计算相关领域</strong></p>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>定时备份Kubernetes集群常用对象到Yaml文件</title>
    <url>/2020/02/02/Backup-Kubernetes-Object-ToYaml/</url>
    <content><![CDATA[<p>有时候我们经常需要用到kubernetes中常用对象的yaml文件, 搞了个简单的脚本来定时备份集群中常用的资源对象保存到yaml文件，以防万一, 毕竟机器的运维权限在别人手上, 指不定什么时候就出问题.</p>
<span id="more"></span>

<p>脚本保存的常用对象如下, 需求比较明确, 脚本比较简单，也没啥好说的, 有些为了去掉yaml中不必要的字段使用了正则匹配, 又臭又长.</p>
<ul>
<li>Secrets</li>
<li>Config Maps</li>
<li>Deployments</li>
<li>Services</li>
<li>Ingress</li>
<li>Persistent Volumes</li>
<li>Cronjobs</li>
</ul>
<p>shell脚本如下: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> kubernetes-object-toyaml.backup</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">###########</span></span><br><span class="line"><span class="comment"># Global Configurations</span></span><br><span class="line"><span class="comment">#======================</span></span><br><span class="line">BACKUP_DIR=/usr/local/src/kubernetes-objectyaml-backup</span><br><span class="line">BACKUP_ENCS=<span class="variable">$BACKUP_DIR</span>/backup_encs</span><br><span class="line">AWS_CMD=/usr/bin/aws</span><br><span class="line">S3_BUCKET=your-s3-bucket</span><br><span class="line">AWS_ACCESS_KEY_ID=your-aws-access-key</span><br><span class="line">AWS_SECRET_ACCESS_KEY=your-aws-secret-key </span><br><span class="line">TIME_STAMP=$(<span class="built_in">date</span> +%Y-%m-%d_%H-%M)</span><br><span class="line">CLUSTER_NAME=your-cluster</span><br><span class="line"><span class="comment">#tar secret</span></span><br><span class="line">KUBE_ARCHIVE_PW=your-tar-password</span><br><span class="line"><span class="comment">######################</span></span><br><span class="line"><span class="keyword">function</span> get_secret &#123;</span><br><span class="line">  kubectl get secret -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> --field-selector <span class="built_in">type</span>!=kubernetes.io/service-account-token | sed -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_configmap &#123;</span><br><span class="line">  kubectl get configmap -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_ingress &#123;</span><br><span class="line">  kubectl get ing -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/status:/,+2d&#x27;</span> -e <span class="string">&#x27;/\- ip: \([0-9]\&#123;1,3\&#125;\.\)\&#123;3\&#125;[0-9]\&#123;1,3\&#125;/d&#x27;</span> -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span> -e <span class="string">&#x27;/generation: [0-9]\+/d&#x27;</span> -e <span class="string">&#x27;/field.cattle.io\/publicEndpoints/d&#x27;</span> -e <span class="string">&#x27;/field.cattle.io\/ingressState/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_service &#123;</span><br><span class="line">  kubectl get service -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/ownerReferences:/,+5d&#x27;</span> -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/clusterIP: \([0-9]\&#123;1,3\&#125;\.\)\&#123;3\&#125;[0-9]\&#123;1,3\&#125;/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_deployment &#123;</span><br><span class="line">  kubectl get deployment -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/deployment\.kubernetes\.io\/revision: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/status:/,+18d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span> -e <span class="string">&#x27;/generation: [0-9]\+/d&#x27;</span> -e <span class="string">&#x27;/field.cattle.io\/publicEndpoints/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_cronjob &#123;</span><br><span class="line">  kubectl get cronjob -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/status:/,+1d&#x27;</span> -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_pvc &#123;</span><br><span class="line">  kubectl get pvc -n <span class="variable">$&#123;1&#125;</span> -o=yaml --<span class="built_in">export</span> | sed -e <span class="string">&#x27;/control\-plane\.alpha\.kubernetes\.io\/leader\:/d&#x27;</span> -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> get_pv &#123;</span><br><span class="line">  <span class="keyword">for</span> pvolume <span class="keyword">in</span> `kubectl get pvc -n <span class="variable">$&#123;1&#125;</span> -o=custom-columns=:.spec.volumeName` </span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">     kubectl get pv -o=yaml --<span class="built_in">export</span> --field-selector metadata.name=<span class="variable">$&#123;pvolume&#125;</span> | sed -e <span class="string">&#x27;/resourceVersion: &quot;[0-9]\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/uid: [a-z0-9-]\+/d&#x27;</span> -e <span class="string">&#x27;/selfLink: [a-z0-9A-Z/]\+/d&#x27;</span> -e <span class="string">&#x27;/creationTimestamp: &quot;[0-9]\+-[0-9]\+-[0-9]\+T.\+&quot;/d&#x27;</span> -e <span class="string">&#x27;/kubectl.kubernetes.io\/last-applied-configuration/d&#x27;</span> -e <span class="string">&#x27;/&#123;&quot;apiVersion&quot;:&quot;.*&#125;/d&#x27;</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> export_ns &#123;</span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$&#123;BACKUP_DIR&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span>/</span><br><span class="line">  <span class="built_in">cd</span> <span class="variable">$&#123;BACKUP_DIR&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span>/</span><br><span class="line">  <span class="keyword">for</span> namespace <span class="keyword">in</span> `kubectl get namespaces --no-headers=<span class="literal">true</span> | awk <span class="string">&#x27;&#123; print $1 &#125;&#x27;</span> | grep -v -e <span class="string">&quot;cattle-prometheus&quot;</span> -e <span class="string">&quot;cattle-system&quot;</span> -e <span class="string">&quot;kube-system&quot;</span> -e <span class="string">&quot;kube-public&quot;</span>`</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">     <span class="built_in">echo</span> <span class="string">&quot;Namespace: <span class="variable">$namespace</span>&quot;</span></span><br><span class="line">     <span class="built_in">echo</span> <span class="string">&quot;+++++++++++++++++++++++++&quot;</span></span><br><span class="line">     <span class="built_in">mkdir</span> -p <span class="variable">$namespace</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> object_kind <span class="keyword">in</span> configmap ingress service secret deployment cronjob pvc</span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">       <span class="keyword">if</span> kubectl get <span class="variable">$&#123;object_kind&#125;</span> -n <span class="variable">$&#123;namespace&#125;</span> 2&gt;&amp;1 | grep <span class="string">&quot;No resources&quot;</span> &gt; /dev/null; <span class="keyword">then</span></span><br><span class="line">         <span class="built_in">echo</span> <span class="string">&quot;No resources found for <span class="variable">$&#123;object_kind&#125;</span> in <span class="variable">$&#123;namespace&#125;</span>&quot;</span></span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">         get_<span class="variable">$&#123;object_kind&#125;</span> <span class="variable">$&#123;namespace&#125;</span> &gt; <span class="variable">$&#123;namespace&#125;</span>/<span class="variable">$&#123;object_kind&#125;</span>.<span class="variable">$&#123;namespace&#125;</span>.yaml &amp;&amp;  <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;object_kind&#125;</span>.<span class="variable">$&#123;namespace&#125;</span>&quot;</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="keyword">if</span> [ <span class="variable">$&#123;object_kind&#125;</span> = <span class="string">&quot;pvc&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">           get_pv <span class="variable">$&#123;namespace&#125;</span> &gt; <span class="variable">$&#123;namespace&#125;</span>/pv.<span class="variable">$&#123;namespace&#125;</span>.yaml &amp;&amp;  <span class="built_in">echo</span> <span class="string">&quot;pv.<span class="variable">$&#123;namespace&#125;</span>&quot;</span>;</span><br><span class="line">         <span class="keyword">fi</span></span><br><span class="line">       <span class="keyword">fi</span></span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line">     <span class="built_in">echo</span> <span class="string">&quot;+++++++++++++++++++++++++&quot;</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="comment">## Archiving k8s data with password to upload it to AWS S3.</span></span><br><span class="line"><span class="comment">## This password is available on our password manager.</span></span><br><span class="line"><span class="comment">############################################################</span></span><br><span class="line"><span class="keyword">function</span> archive_ns &#123;</span><br><span class="line">  <span class="built_in">cd</span> <span class="variable">$&#123;BACKUP_DIR&#125;</span></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$&#123;BACKUP_ENCS&#125;</span></span><br><span class="line">  <span class="comment"># tar with openssl</span></span><br><span class="line">  tar cz <span class="variable">$&#123;CLUSTER_NAME&#125;</span> | openssl enc -aes-256-cbc -e -k <span class="variable">$&#123;KUBE_ARCHIVE_PW&#125;</span> &gt; <span class="variable">$&#123;BACKUP_ENCS&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span>-<span class="variable">$&#123;TIME_STAMP&#125;</span>.tar.gz.enc</span><br><span class="line">  <span class="comment"># untar with openssl</span></span><br><span class="line">  <span class="comment"># https://www.howtoing.com/encrypt-decrypt-files-tar-openssl-linux/</span></span><br><span class="line">  <span class="comment"># openssl enc -d -aes256-cbc -k $&#123;KUBE_ARCHIVE_PW&#125; -in xxx.ooo.tar.gz.enc | tar xz -C test</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload Backups</span></span><br><span class="line"><span class="comment">#===============</span></span><br><span class="line"><span class="keyword">function</span> upload_backup_to_s3 &#123;</span><br><span class="line">  <span class="variable">$&#123;AWS_CMD&#125;</span> s3 <span class="built_in">cp</span> <span class="variable">$&#123;BACKUP_DIR&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span>-<span class="variable">$&#123;TIME_STAMP&#125;</span>.tar.gz.enc s3://<span class="variable">$&#123;S3_BUCKET&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span>/</span><br><span class="line">  <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CLUSTER_NAME&#125;</span>-<span class="variable">$&#123;TIME_STAMP&#125;</span>.tar.gz.enc is successfully uploaded&quot;</span></span><br><span class="line">    <span class="built_in">rm</span> -rf <span class="variable">$&#123;BACKUP_DIR&#125;</span>/<span class="variable">$&#123;CLUSTER_NAME&#125;</span> <span class="variable">$&#123;BACKUP_DIR&#125;</span>/k8s-data-<span class="variable">$&#123;TIME_STAMP&#125;</span>.tar.gz.enc</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CLUSTER_NAME&#125;</span>-<span class="variable">$&#123;TIME_STAMP&#125;</span>.tar.gz.enc failed to be uploaded&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean 7 days ago</span></span><br><span class="line"><span class="keyword">function</span> clean_7days_ago &#123;</span><br><span class="line">  find <span class="variable">$&#123;BACKUP_ENCS&#125;</span> -mtime +7 -name <span class="string">&quot;*.tar.gz.enc&quot;</span> -<span class="built_in">exec</span> /bin/rm -rf &#123;&#125; \;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###########</span></span><br><span class="line">export_ns</span><br><span class="line">archive_ns</span><br><span class="line">upload_backup_to_s3</span><br><span class="line">clean_7days_ago</span><br></pre></td></tr></table></figure>

<p>在另一master节点下执行:</p>
<p><code>bash kubernetes-object-toyaml.backup</code></p>
<p>最后加入系统crontab即可</p>
<p><code>11 22 * * * bash /your/dir/kubernetes-object-toyaml.backup</code></p>
<p>再或者加个rsync同步到其它节点上，双保险.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>再见2023,2024再见</title>
    <url>/2023/12/29/2023/</url>
    <content><![CDATA[<p><strong>再见, 2023</strong></p>
<p><strong>2024, 再见</strong></p>
<span id="more"></span>

<h3 id="再见2023"><a href="#再见2023" class="headerlink" title="再见2023"></a><strong>再见2023</strong></h3><p><strong>不喝鸡汤，拒绝平庸</strong></p>
<h4 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h4><p>疫情放开, 又有了生活的气息<br>嗯, 这一年, 完成了几件<strong>很好的事</strong></p>
<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><p>职场从来不相信软弱, 当你需要规划某个方向一年或者未来几年的蓝图时, <strong>请把压力当成是无形的动力</strong><br>让实力去说服所有人,证明自己有独当一面的能力,这是一个职场菜场必修的课程</p>
<p>记住，你做不到的，一定有其它人可以做到</p>
<h3 id="2024再见"><a href="#2024再见" class="headerlink" title="2024再见"></a><strong>2024再见</strong></h3><p><strong>宛如初见, 一眼万年</strong></p>
<p>很多人说2024会比2023更最艰难, 也许吧, 但就个人而言, 我的2024不会比2023艰难</p>
<p>我也会以一个全新的角色进入2024</p>
<p>2023积压了一些技术一直没有追上, 一直在给自己加作业但却没有实现，表面说是一种鞭策，<strong>但更像是懒惰，不是个好习惯</strong></p>
<p>相信2024的挑战会更多, 希望自己将以更专业的姿态在2024带来更多的突破,<strong>多一些升华</strong></p>
<p><strong><code>终</code></strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2023.jpg"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>随淑笔记</category>
      </categories>
      <tags>
        <tag>随淑笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker之交叉编译</title>
    <url>/2021/04/05/Docker-buildx/</url>
    <content><![CDATA[<p>最近由于业务需要，在做一些关于国产化相关的事情，需要将平台从x86架构迁移到arm64上run起来，最重要的环节则在于镜像都需要rebuild一遍，在这个过程中还是碰到一些问题，拿出来分享一下，希望对其他人会有所帮助.</p>
<span id="more"></span>



<p>把所有的镜像都rebuild，这是个很辛苦且枯燥的事情，还好日常工作中都会良好的版本管理，每个应用的版本都对应有相应的commit号，这个可以直接使用稳定的版本进行rebuild,可以省去联调的工作</p>
<p>先简单介绍下binfmt_misc，后面docker交叉编译会用上.</p>
<h3 id="binfmt-misc"><a href="#binfmt-misc" class="headerlink" title="binfmt_misc"></a>binfmt_misc</h3><p>binfmt_misc(Miscellaneous Binary Format)是Linux内核从很早开始就引入的机制，可以通过要打开文件的特性来选择到底使用哪个程序来打开它，不光可以通过文件的扩展名来判断的，还可以通过文件开始位置的特殊的字节（Magic Byte）来判断</p>
<p>这里不过多介绍，简单来说说是通过注册一个“解释器”和一个文件识别方式，以达到运行文件的时候调用自定义解释器的目的，docker buildx即是通过这种方式</p>
<p>启用binfmt_misc也比较简单，可以使用现成的docker直接启动:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --<span class="built_in">rm</span> --privileged docker/binfmt:66f9012c56a8316f9244ffd7622d7c21c1f6f28d</span><br></pre></td></tr></table></figure>

<p>检查是否开启:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -al /proc/sys/fs/binfmt_misc/</span><br><span class="line">-rw-r--r-- 1 root root 0 11月 18 00:12 qemu-aarch64</span><br><span class="line">-rw-r--r-- 1 root root 0 11月 18 00:12 qemu-arm</span><br><span class="line">-rw-r--r-- 1 root root 0 11月 18 00:12 qemu-ppc64le</span><br><span class="line">-rw-r--r-- 1 root root 0 11月 18 00:12 qemu-s390x</span><br><span class="line">--w------- 1 root root 0 11月 18 00:09 register</span><br><span class="line">-rw-r--r-- 1 root root 0 11月 18 00:12 status</span><br></pre></td></tr></table></figure>

<p>注意: 如果是centos的话，建议升级内核到4+以上</p>
<h3 id="编译方法"><a href="#编译方法" class="headerlink" title="编译方法"></a>编译方法</h3><p>大致有3种编译方法:</p>
<ol>
<li>直接在对应架构的机器上进行</li>
<li>通过模拟器的方式，最常用的模拟器是开源的 <a href="https://www.wikiwand.com/zh-hans/QEMU">QEMU</a></li>
<li>通过binfmt_misc模拟目标硬件的用户空间</li>
<li>通过交叉编译，像andriod程序，一般就是通过交叉编译而来，还有如go语言等，本身就有交叉编译器</li>
</ol>
<p>这里要重点说的是docker的交叉编译</p>
<h3 id="docker-buildx"><a href="#docker-buildx" class="headerlink" title="docker buildx"></a>docker buildx</h3><p>docker 在19.03的版本支持了一个实验性的功能 - - buildx，大大地减少了编译工作，但也还是有些坑，慢慢道来</p>
<p>利用 Docker 19.03 引入的插件 <a href="https://github.com/docker/buildx">buildx</a>，可以很轻松地构建多平台 Docker 镜像。buildx 是 <code>docker build ...</code> 命令的下一代替代品，它利用 <a href="https://github.com/moby/buildkit">BuildKit</a> 的全部功能扩展了 <code>docker build</code> 的功能</p>
<p>这里不详细介绍该如此开启buildx了，大家可参考官方的handbook，这里假设开启了”experimental”: true</p>
<p>使用以下命令确认开启了:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker info | grep -i exp</span><br><span class="line">Experimental: <span class="literal">true</span>  </span><br></pre></td></tr></table></figure>

<p>Docker buildx需要开启binfmt_misc， 开启方法如下</p>
<p>验证是否启用了相应的处理器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/sys/fs/binfmt_misc/qemu-aarch64</span><br><span class="line">enabled</span><br><span class="line">interpreter /usr/bin/qemu-aarch64</span><br><span class="line">flags: OCF</span><br><span class="line">offset 0</span><br><span class="line">magic 7f454c460201010000000000000000000200b7mask </span><br><span class="line">ffffffffffffff00fffffffffffffffffeffff</span><br></pre></td></tr></table></figure>

<p>Docker 默认会使用不支持多 CPU 架构的构建器，我们需要手动切换。</p>
<p>先创建一个新的构建器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新建构建器</span></span><br><span class="line">docker buildx create --use --name mybuilder </span><br><span class="line"><span class="comment"># 启动构建器</span></span><br><span class="line">docker buildx inspect mybuilder --bootstrap</span><br></pre></td></tr></table></figure>

<p>准备工作就绪后，下面以例子来说明，既然是交叉编译，那就得先有一个初始的x86 的Dockerfile，然后将其编译出arm64的镜像</p>
<p><strong>下文说到的体系架构指的就是x86跟arm64</strong>.</p>
<h3 id="Docker-Manifests"><a href="#Docker-Manifests" class="headerlink" title="Docker Manifests"></a>Docker Manifests</h3><p>这里先解释下docker的manifests特性，因为下面会用到</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM golang:alpine AS builder</span><br></pre></td></tr></table></figure>

<p>相信大家在编写Dockerfile的时候会经常这么用，正常来说，这么写的镜像都是从docker hub拉取的镜像，docker hub上的镜像支持docker的manifests特性，简单来说就是一个镜像地址，当我们docker pull的时候，它会根据当前机器体系架构来拉取对应于这个体系架构的镜像，比如你是x86的，则给你拉取的是x86的镜像，是arm64的机器拉取的则是arm64的，原理就在于每个镜像都维护着一个manifest(清单),这个清单上记录着每个体系架构对应用镜像层，在docker pull的时候，docker这时做为一个client端，与docker hub的服务端通信的时候，会先看这个镜像存不存在manifests，如果存在则拉取匹配当前机器体系架构的镜像，如果不存在，则就直接拉取.</p>
<p>了解这个对下面的实践很重要</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>这里以go语言的应用为例，直接在<strong>x86的CentOS</strong>机器上，如果不做特别说明，以下的所有操作都是在该机器上执行</p>
<p>dockerfile如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM golang:alpine AS builder</span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">ADD . /app/</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o hello .  <span class="comment"># 注意这里</span></span><br><span class="line"></span><br><span class="line">FROM alpine</span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY --from=builder /app/hello .</span><br><span class="line">CMD [<span class="string">&quot;./hello&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>这里要注意go build命令，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o hello .</span><br></pre></td></tr></table></figure>

<p>简单说一下go的交叉编译:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build main.go .</span><br><span class="line">CGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build main.go .</span><br></pre></td></tr></table></figure>

<p>GOOS指定的是目标机器的操作系统</p>
<p>GOARCH指定的是体系架构（i386、amd64、arm64）等</p>
<p><strong>由于go是编译型语言，编译出来的都是二进制文件，这个二进制文件的运行环境一定是要跟编译时指的是编译参数一致，就好比，windows下的exe文件无法在linux上执行一样，如果指定了go编译的参数为amd64，则它就不能在arm64上运行,反过来同理</strong></p>
<h4 id="X86机器编译X86镜像"><a href="#X86机器编译X86镜像" class="headerlink" title="X86机器编译X86镜像"></a>X86机器编译X86镜像</h4><p>所以，x86正常编译的build命令为:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker build -t xxx/yyy/zzz:latest .</span><br></pre></td></tr></table></figure>

<p>由于是直接在x86的linux机器上，不存在交叉编译，所以可以直接使用build命令，最终产生的镜像就是一个可以运行在X86的机器上，如果运行在arm64的机器上，则会提示以下错误:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210408224334.png"></p>
<p>以上的错误基本都是运行的镜像与体系架构不匹配导致的</p>
<h4 id="X86机器上编译arm64镜像"><a href="#X86机器上编译arm64镜像" class="headerlink" title="X86机器上编译arm64镜像"></a>X86机器上编译arm64镜像</h4><p>编译x86非常方便，也是我们最常规的操作，如果将上面的Dockerfile使用交叉编译编译出arm64的镜像呢</p>
<p>这里要注意的是，由于go本身就支持交叉编译，因此基础镜像是可以不用调整的,修改如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM golang:alpine AS builder <span class="comment"># 注意这里</span></span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">ADD . /app/</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build -o hello .  <span class="comment"># 修改的这里</span></span><br><span class="line"></span><br><span class="line">FROM alpine  <span class="comment"># 注意这里</span></span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY --from=builder /app/hello .</span><br><span class="line">CMD [<span class="string">&quot;./hello&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>使用以下的命令编译出来的镜像就是arm64的了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker buildx build -t xxx/yyy/zzz:latest --platform=linux/arm64 . --push</span><br></pre></td></tr></table></figure>

<p>–push 表示构建之后直接将该镜像推送到镜像仓库中</p>
<p>可以看到，Dockerfile文件修改只修改了1行，已经在注释中标注出来了</p>
<p>二个From 为何都不需要改呢? 这个放到下文解析–platform中再说</p>
<p><code>go build</code>那行的命令GOARCH已经修改为arm64了，表明目标体系架构为arm64，那最终生成的二进制在arm64是可以运行的.</p>
<p>这里要重点说一下<code>docker buildx --platform</code>这个参数, 顺便解释一下为何两个From的基础镜像都不需要修改的原因</p>
<p>首先–platform这个参数决定了Dockerfile文件中所有From后面的镜像的体系架构以及最终生成的镜像的体系架构，像上面那样，直接是</p>
<p><code>FROM golang:alpine AS builder</code> 相当于<code>FROM --platform=arm64 golang:alpine AS builder</code>，而</p>
<p><code>FROM alpine 相当于</code>FROM –platform&#x3D;arm64 alpine&#96;, 拉取下来的就是arm64的镜像，因此上面是可以不用修改的</p>
<p>这里说的是From中没有指定<code>--platform</code>的情况则会自动添加上<code>docker buildx --platform</code>中的参数，如果本身就存在的话，则是以From本身的<code>--platform决定</code></p>
<p><strong>但是一定要确保，最后一个From的镜像体系架构一定要跟目标体系架构保持一致</strong>，也就是说，如果上面的例子中最后一个From改成</p>
<p><code>From alpine-amd64</code> (不一定有这个镜像，这里只是形象说明),docker buildx不会报错，会正常编译通过，在docker run 这个镜像的时候就会出现<code>exec format error</code></p>
<p>这里可能有人会问: 那为什么arm64的镜像可以在x86上run起来呢?</p>
<p><strong>这个其实就是binfmt_misc在起作用，模拟了目标体系架构</strong></p>
<p>但这有一个问题:</p>
<p>很多时候，由于各种不可明说的原因，我们并不会每次都直接去docker hub上拉取镜像，而是将镜像推送到公司内的镜像仓库，如果恰好镜像仓库还不支持manifest这个特性的话(比如我司)，那上面的例子就有坑了，因此，必须要这么写</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM xxx/yyy/golang:alpine AS builder <span class="comment"># 注意这里</span></span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">ADD . /app/</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build -o hello .  <span class="comment"># 注意这里</span></span><br><span class="line"></span><br><span class="line">FROM xxx/yyy/alpine-arm64  <span class="comment"># 注意这里</span></span><br><span class="line">RUN <span class="built_in">mkdir</span> /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY --from=builder /app/hello .</span><br><span class="line">CMD [<span class="string">&quot;./hello&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>最后的一个FROM一定要是arm64的，由于是在x86的机器上使用交叉编译，如果镜像仓库又不支持manifest，则只能显示地指定使用arm64的镜像，这样打出来的镜像还会是arm64的</p>
<p>那为什么第一个FROM没有修改呢? 因为go本身是支持交叉编译的，虽然拉下来的是x86的镜像， 但go build出来的是arm64的，最后塞到alpine-arm64镜像里去的也是arm64的，这个是不冲突的，当然，第一个FROM换成arm64的golang:alpine也是没问题的</p>
<h4 id="arm64机器编译arm64镜像"><a href="#arm64机器编译arm64镜像" class="headerlink" title="arm64机器编译arm64镜像"></a>arm64机器编译arm64镜像</h4><p>目标体系架构跟机器一致，这个就更没什么好说的了，直接使用最开始的Dockerfile就行</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个例子中使用的是go，由于go本身就支持交叉编译，因此还算是方便</p>
<p>对于一些前端js的编译，也比较方便，因为js编译出来的都是一堆js文件，跟机器体系架构关系不大，</p>
<p>java的也是如此，天然的是通过java虚拟机的方式，屏蔽了底层的差异，跟机器体系架构关系不大</p>
<p>因此总结来说</p>
<p><strong>对于交叉编译来说，保证最后一个FROM的镜像跟目标体系架构的一致即可</strong></p>
<p>另外，docker buildx还有很多其它的选项，值得深入研究一番，有时间下次更</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://blog.lyle.ac.cn/2020/04/14/transparently-running-binaries-from-any-architecture-in-linux-with-qemu-and-binfmt-misc/">https://blog.lyle.ac.cn/2020/04/14/transparently-running-binaries-from-any-architecture-in-linux-with-qemu-and-binfmt-misc/</a></li>
<li><a href="https://www.w3xue.com/exp/article/201912/66543.html">https://www.w3xue.com/exp/article/201912/66543.html</a></li>
<li><a href="https://engineering.docker.com/2019/04/multi-arch-images/">Building Multi-Arch Images for Arm and x86 with Docker Desktop</a></li>
<li><a href="https://gitee.com/windforce1981/buildx?_from=gitee_search#/windforce1981/buildx/blob/master/docs/reference/buildx_build.md">https://gitee.com/windforce1981/buildx?_from=gitee_search#/windforce1981/buildx/blob/master/docs/reference/buildx_build.md</a></li>
<li><a href="https://gitee.com/windforce1981/buildx/blob/master/docs/reference/buildx_build.md">https://gitee.com/windforce1981/buildx/blob/master/docs/reference/buildx_build.md</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(踩坑记)</title>
    <url>/2020/05/27/EFK-prombles/</url>
    <content><![CDATA[<p>各种各样的问题大部分都可以为是业务日志不规范的, 当然这个是不可避免的, 规范不是一天建立起来的.</p>
<p>在没有建立起来之前, 只能像打补丁一样修复</p>
<p><strong>在这里记录下运维EFK踩过的坑, 不定期更新</strong></p>
<span id="more"></span>



<h3 id="日志流"><a href="#日志流" class="headerlink" title="日志流"></a>日志流</h3><p><code>[Kubernetes/LB/...] --&gt; fluentd --&gt; kafka --&gt; logstash --&gt; es</code></p>
<h3 id="type不覆盖"><a href="#type不覆盖" class="headerlink" title="type不覆盖"></a>type不覆盖</h3><p>最近就发现存在小部分的日志丢失了, 经过层层debug, 终于发现原来是因为源日志中就存在type这个字段, </p>
<p>因为logstash环节一般都会根据业务添加一个type来进行区别, 然后根据不同的type输出到不同的存储，</p>
<p>平时很少会关注日志本身的字段, 导致这个问题花了差不多一天的时间定位.</p>
<p>如果源日志中就存在有type字段, 而在logstash中又指定了type, 那么这个type不会覆盖源日志中的type(后来想想其实也可以理解, 业务数据为王嘛)</p>
<p><a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html#plugins-inputs-kafka-type">官方</a>有说明, 这个有点巧, 刚好在页面的最下面, 都不怎么会看到</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200416163906.png"></p>
<p>因此在output中出现使用type来做为判断条件的那就会被丢弃掉, 因此此时的type是你源日志中的type.</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">output <span class="punctuation">&#123;</span></span><br><span class="line">  if <span class="punctuation">[</span>type<span class="punctuation">]</span> == <span class="string">&quot;xxx&quot;</span><span class="punctuation">&#123;</span></span><br><span class="line">    elasticsearch <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>找到问题解决办法也就容易多了, 直接在fluentd上把这个字段进行重命名, 就是add一个字段然后再把type字段remove</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line">  <span class="string">@type</span> <span class="string">record_transformer</span></span><br><span class="line">  <span class="string">enable_ruby</span> <span class="literal">true</span></span><br><span class="line">  <span class="string">&lt;record&gt;</span></span><br><span class="line">    <span class="string">f_type</span> <span class="string">$&#123;record.dig(&quot;type&quot;)&#125;</span></span><br><span class="line">  <span class="string">&lt;/record&gt;</span></span><br><span class="line"><span class="string">&lt;/filter&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line">  <span class="string">@type</span> <span class="string">record_transformer</span></span><br><span class="line">  <span class="string">remove_keys</span> <span class="string">type</span></span><br><span class="line"><span class="string">&lt;/filter&gt;</span></span><br></pre></td></tr></table></figure>



<p>这个问题也反映出本人对logstash用的不是很熟.</p>
<h3 id="字段类型转换"><a href="#字段类型转换" class="headerlink" title="字段类型转换"></a>字段类型转换</h3><p>由于后端的存储使用的es, es中有个 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html">Dynamic Mapping</a>, 默认情况下是开启状态, 这个特性能够根据文档的字段动态地进行填充, 也就是说新增字段会自动地添加mapping， 但是无法对已经存在的字段进行修改</p>
<p>一般情况下, 索引都是按天进行分隔, 也就是在零点的时候会自动生成新的索引，然后这个索引的mapping以第一条进来的文档为准, 假如有个字段, 比如下面的<code>call_db_dynamic_response.score</code>这个field, 这个表示对比的一个得分(代码中的float64),假如第一个文档中的这个<code>score</code>为1, 那么这个在es中的类型为<code>long</code>类型, 那么就不能改变,</p>
<p>但是<code>score</code>可能会再传过来<code>0.99</code>,那么这个时候就会出现以下报错, 这条数据是无法进入es的,<strong>es中long与float是不一样的类型</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200416164017.png"></p>
<p>那么解决办法, 有以下几种方式:</p>
<ol>
<li>直接从源代码中修改, 代码中本身就是float类型, 而且这个值是底层SDK传过来的, 在业务层做判断不是很友好, 毕竟每次都将多一层判断, 影响性能</li>
<li>可以在logstash中对改字段进行类型转换</li>
<li>在创建索引时使用自定义的template</li>
</ol>
<p>这里我采取的是第2种办法， 没有采用第1种办法是因为代码中本身就是float类型, 而且这个值是底层SDK传过来的, 在业务层做判断不是很友好, 毕竟每次都将多一层该值判断, 影响性能，把这个操作移步到日志系统中，毕竟日志系统只是做为一个排查工具, 不需要那么实时</p>
<p>在logstash中进行类型转换用到了<code>filter的mutate</code>,当然这里最好也对索引做个判断</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">if <span class="punctuation">[</span>tag<span class="punctuation">]</span> == <span class="string">&quot;yourindex&quot;</span> <span class="punctuation">&#123;</span></span><br><span class="line">  mutate <span class="punctuation">&#123;</span></span><br><span class="line">    convert =&gt; <span class="punctuation">[</span><span class="string">&quot;[call_db_dynamic_response][score]&quot;</span><span class="punctuation">,</span> <span class="string">&quot;float&quot;</span><span class="punctuation">]</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"> <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>这样的话, <code>score</code>就永远都是<code>float</code>类型了.</p>
<p>这里没有使用自定义的template，理论上这种方法是性能最高的, 因为不需要每条日志都判断一下是否存在这个<code>score</code>字段, 不过本人还是觉得第2种方法更容易, 效果还行.</p>
<h3 id="字段多种类型"><a href="#字段多种类型" class="headerlink" title="字段多种类型"></a>字段多种类型</h3><p>这个报错的原因跟上面的类似, 都是因为es对field的类型不支持多个, 如果是个object，那你就无法把一个值写进去,不然会提示以下错误</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200416164935.png"></p>
<p>这个问题曾经引起过一次线上es的启机, 有兴趣的可以看这里的<a href="https://izsk.me/2019/10/06/ES-FGC-Fix/">分析</a></p>
<h3 id="kafka版本不匹配"><a href="#kafka版本不匹配" class="headerlink" title="kafka版本不匹配"></a>kafka版本不匹配</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200416173539.png"></p>
<p>这是本人遇到过的最坑爹的问题</p>
<p>由于有个环境需要下线, 在迁移的过程中使用了一个新版本的fluentd, 但是却无法对接这个kafka</p>
<p>fluentd使用了一个<a href="https://github.com/fluent/fluent-plugin-kafka">fluent-plugin-kafka</a>, 比较坑的是里面有个ruby_kafka的client, 如果kafka的版本太旧而fluentd的版本又太新的话, 就会造成ruby_kafka_client无法适配旧版的kafka, 折腾半天无果后果断放弃, 由于这个kafka无法直接升级，最后只能重新搭建了一个新版本的kafka, 问题解决</p>
<h3 id="fluentd的tag"><a href="#fluentd的tag" class="headerlink" title="fluentd的tag"></a>fluentd的tag</h3><p>在使用fluentd中大家看fluentd的配置文件,开始的时候一定会对tag的使用有点懵</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">&lt;source&gt;</span></span><br><span class="line">  <span class="string">@id</span> <span class="string">fluentd-k8s-containers.log</span></span><br><span class="line">  <span class="string">@type</span> <span class="string">tail</span></span><br><span class="line">  <span class="string">path</span> <span class="string">/var/log/containers/*.log</span></span><br><span class="line">  <span class="string">pos_file</span> <span class="string">/var/log/fluentd-k8s-containers.log.pos</span></span><br><span class="line">  <span class="string">tag</span> <span class="string">kubernetes.*</span></span><br><span class="line">  <span class="string">&lt;parse&gt;</span></span><br><span class="line">    <span class="string">@type</span> <span class="string">multi_format</span></span><br><span class="line">    <span class="string">&lt;pattern&gt;</span></span><br><span class="line">      <span class="string">format</span> <span class="string">json</span></span><br><span class="line">      <span class="string">time_key</span> <span class="string">time</span></span><br><span class="line">      <span class="string">time_format</span> <span class="string">%Y-%m-%dT%H:%M:%S.%NZ</span></span><br><span class="line">    <span class="string">&lt;/pattern&gt;</span></span><br><span class="line">  <span class="string">&lt;/parse&gt;</span></span><br><span class="line"><span class="string">&lt;/source&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span> <span class="comment"># --</span></span><br><span class="line">  <span class="string">@id</span> <span class="string">filter_kubernetes_metadata</span></span><br><span class="line">  <span class="string">@type</span> <span class="string">kubernetes_metadata</span></span><br><span class="line">  <span class="string">skip_labels</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment">#skip_container_metadata true</span></span><br><span class="line">  <span class="string">skip_master_url</span> <span class="literal">true</span></span><br><span class="line">  <span class="string">skip_namespace_metadata</span> <span class="literal">true</span></span><br><span class="line"><span class="string">&lt;/filter&gt;</span></span><br></pre></td></tr></table></figure>

<p>这是一段收集kubernetes容器日志的经典配置</p>
<p>在<code>source</code>中使用了<code>tail</code>指定了<code>path,tag</code>, 那么这个时候的tag到底是什么?</p>
<p>先看下官方的<a href="https://docs.fluentd.org/input/tail#tag">说明</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#* can be used as a placeholder that expands to the actual file path, replacing &#x27;/&#x27; with &#x27;.&#x27;. For example, if you have the following configuration</span></span><br><span class="line">path /path/to/file</span><br><span class="line">tag foo.*</span><br><span class="line"><span class="comment"># in_tail emits the parsed events with the &#x27;foo.path.to.file&#x27; tag.</span></span><br></pre></td></tr></table></figure>

<p>意思就是说, tag是由<code>tag+path</code>一起组成，如上面, </p>
<p><code>path=/var/log/containers/*.log</code></p>
<p><code>tag=kubernetes.*</code></p>
<p>那么在<code>filter</code>时使用了<code>kubernetes.**</code>来表示匹配所有以<code>kubernetes开头的tag</code></p>
<p>每一个容器的tag为<code>kubernetes.var.log.containers.xxxxx.log</code></p>
<p><code>xxxxx</code>指&#x2F;var&#x2F;log&#x2F;containers&#x2F;目录下容器的信息</p>
<p>注意:</p>
<p><code>.*</code>跟<code>.**</code>是有区别的, 引入官方<a href="https://docs.fluentd.org/configuration/config-file">文档</a></p>
<p>The following match patterns can be used in <code>and</code> tags.</p>
<ul>
<li><p><code>*</code> matches a single tag part.</p>
<ul>
<li><p>For example, the pattern <code>a.*</code> matches <code>a.b</code>, but does not match</p>
<p><code>a</code> or <code>a.b.c</code></p>
</li>
</ul>
</li>
<li><p><code>**</code> matches zero or more tag parts.</p>
<ul>
<li>For example, the pattern <code>a.**</code> matches <code>a</code>, <code>a.b</code> and <code>a.b.c</code></li>
</ul>
</li>
<li><p><code>&#123;X,Y,Z&#125;</code> matches X, Y, or Z, where X, Y, and Z are match patterns.</p>
<ul>
<li>For example, the pattern <code>&#123;a,b&#125;</code> matches <code>a</code> and <code>b</code>, but does not match <code>c</code></li>
<li>This can be used in combination with the <code>*</code> or <code>**</code> patterns. Examples include <code>a.&#123;b,c&#125;.*</code> and <code>a.&#123;b,c.**&#125;</code></li>
</ul>
</li>
<li><p><code>#&#123;...&#125;</code> evaluates the string inside brackets as a Ruby expression (See the section “Embedding Ruby Expressions” below)</p>
</li>
<li><p>When multiple patterns are listed inside a single tag (delimited by one or more whitespaces), it matches any of the listed patterns. For example:</p>
<ul>
<li>The patterns &#96;&#96; match <code>a</code> and <code>b</code>.</li>
<li>The patterns &#96;&#96; match <code>a</code>, <code>a.b</code>, <code>a.b.c</code> (from the first pattern) and <code>b.d</code> (from the second pattern).</li>
</ul>
</li>
</ul>
<p>所以,这个tag还是非常长的, 因此最后都会将tag改成故名知义, 比如改成服务名.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">&lt;match</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line">  <span class="string">@type</span> <span class="string">rewrite_tag_filter</span></span><br><span class="line">  <span class="string">&lt;rule&gt;</span></span><br><span class="line">    <span class="string">key</span> <span class="string">request_method</span></span><br><span class="line">    <span class="string">pattern</span> <span class="string">/^HEAD$/</span></span><br><span class="line">    <span class="string">tag</span> <span class="string">clear</span></span><br><span class="line">  <span class="string">&lt;/rule&gt;</span></span><br><span class="line">  <span class="string">&lt;rule&gt;</span></span><br><span class="line">    <span class="string">key</span> <span class="string">service_name</span></span><br><span class="line">    <span class="string">pattern</span> <span class="string">^(.+)$</span></span><br><span class="line">    <span class="string">tag</span> <span class="string">$1</span></span><br><span class="line">  <span class="string">&lt;/rule&gt;</span></span><br><span class="line"><span class="string">&lt;/match&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="The-connection-might-have-been-closed-Sleeping-for-64-seconds"><a href="#The-connection-might-have-been-closed-Sleeping-for-64-seconds" class="headerlink" title="The connection might have been closed. Sleeping for 64 seconds"></a>The connection might have been closed. Sleeping for 64 seconds</h3><p>![image-20200519140723710](&#x2F;Users&#x2F;zhoushuke&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200519140723710.png)</p>
<p>原因: 从grafana上来看，fluentd的cpu会出现周期性的突升，同时，在apiserver会出现大量的watch pod调用</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200519150847.png"></p>
<p>解决: 增大fluentd的cpu使用，同时在配置文件中修改插件filter_kubernetes_metadata，</p>
<p>增加watch false,减少频繁地去请求apiserver</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Enriches records with Kubernetes metadata</span></span><br><span class="line"><span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line"><span class="string">@id</span> <span class="string">filter_kubernetes_metadata</span></span><br><span class="line"><span class="string">@type</span> <span class="string">kubernetes_metadata</span></span><br><span class="line"><span class="string">watch</span> <span class="literal">false</span> <span class="comment"># 这行</span></span><br><span class="line"><span class="string">skip_labels</span> <span class="literal">true</span></span><br><span class="line"><span class="comment">#skip_container_metadata true</span></span><br><span class="line"><span class="string">skip_master_url</span> <span class="literal">true</span></span><br><span class="line"><span class="string">skip_namespace_metadata</span> <span class="literal">true</span></span><br><span class="line"><span class="string">&lt;/filter&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="使用wildcard查询时无记录"><a href="#使用wildcard查询时无记录" class="headerlink" title="使用wildcard查询时无记录"></a>使用wildcard查询时无记录</h3><p>经常，会使用wildcard来查询某个时间段内的某个字段包含某个值的记录, 假如现在要筛选出15钟之内的msg中包含<code>unaryInterceptor</code>的记录</p>
<p>假如doc字段如下:</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;realtysense.viking-jarl-2020.05.27&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;RealtySense&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AXJVQxYRxgPqZboc0ExD&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;_score&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stdout&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;service_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;demo&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span> <span class="string">&quot;unaryInterceptor done&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;@version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;@timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2020-05-27T08:32:36.928Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;RealtySense&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;@timestamp&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="number">1590568356928</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;sort&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="number">1590568356928</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>那么相应的会使用以下的查询语句:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> -XGET <span class="string">&#x27;http://localhost:9200/demo-*/_search?pretty&amp;_source_include=%2A%2C%40timestamp&amp;ignore_unavailable=true&amp;scroll=30s&amp;size=10&#x27;</span> -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;query&quot;: &#123;</span></span><br><span class="line"><span class="string">    &quot;bool&quot;: &#123;</span></span><br><span class="line"><span class="string">      &quot;filter&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;bool&quot;: &#123;</span></span><br><span class="line"><span class="string">          &quot;must&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;range&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;@timestamp&quot;: &#123; # 指定时间段</span></span><br><span class="line"><span class="string">                  &quot;gt&quot;: &quot;2020-05-27T08:31:43.695209Z&quot;, </span></span><br><span class="line"><span class="string">                  &quot;lte&quot;: &quot;2020-05-27T08:33:15.189058Z&quot;</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">            &#125;,</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;wildcard&quot;: &#123; #使用通配符</span></span><br><span class="line"><span class="string">                &quot;msg&quot;: &quot;*unaryInterceptor*&quot; # 正确的写法: 1. &quot;msg.keyword&quot;: &quot;*unaryInterceptor*&quot;</span></span><br><span class="line"><span class="string">                														#						2. &quot;msg&quot;: &quot;*unaryinterceptor*&quot; 注意全都是小写.</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  &#125;,</span></span><br><span class="line"><span class="string">  &quot;sort&quot;: [</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;@timestamp&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;order&quot;: &quot;asc&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  ]</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>执行之后会发现，得到的结果为空,百思不得其解.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;took&quot;</span> : 4,</span><br><span class="line">  <span class="string">&quot;timed_out&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">&quot;_shards&quot;</span> : &#123;</span><br><span class="line">    <span class="string">&quot;total&quot;</span> : 20,</span><br><span class="line">    <span class="string">&quot;successful&quot;</span> : 20,</span><br><span class="line">    <span class="string">&quot;failed&quot;</span> : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;hits&quot;</span> : &#123;</span><br><span class="line">    <span class="string">&quot;total&quot;</span> : 0,</span><br><span class="line">    <span class="string">&quot;max_score&quot;</span> : null,</span><br><span class="line">    <span class="string">&quot;hits&quot;</span> : [ ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是从kibana上查看，在指定的时间段内是存在记录的, 原因就在于**<code>msg</code>字段本身是<code>analyzed</code>的，elasticsearch对于<code>analyzed</code>的字段会将字段按空格进行分隔, 而且会所有的大写字母转换成小写字母**, 因此，在上面使用的通配符中有大写字母就无法匹配上了,这也是为何结果无记录的原因,那么解决方案有两个</p>
<ol>
<li>通配符里直接使用小写</li>
<li>对于<code>analyzed</code>的字段可直接使用内置的keyword, 比如上面可改成 <code>&quot;msg.keyword&quot;: &quot;*unaryInterceptor*&quot;</code>,那么这样的话原始的字符就可以匹配上了,参考<a href="https://stackoverflow.com/questions/51849598/elasticsearch-wild-card-query-not-working">github</a></li>
</ol>
<h3 id="排查技巧"><a href="#排查技巧" class="headerlink" title="排查技巧"></a>排查技巧</h3><h4 id="fluentd"><a href="#fluentd" class="headerlink" title="fluentd"></a>fluentd</h4><p>fluentd的一个排查技巧就是debug模块, 只需要在fluentd的启动参数中指定级别</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200416174010.png"></p>
<p>当然在各类的output插件中, 也支持开启debug模块, 这个大家可以查看官方<a href="https://docs.fluentd.org/">文档</a></p>
<h4 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h4><p>kafka除了上次遇到过的版本问题也没出现过有问题是因为kafka造成的,但是有几条命令还是比较有用的, 毕竟数据经过它, 看看数据有时还是有必要</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看消费者</span></span><br><span class="line">./kafka-consumer-groups.sh --bootstrap-server localhost:9092 —list</span><br><span class="line"><span class="comment"># 查看数据消费情况</span></span><br><span class="line">./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic yourtopic</span><br></pre></td></tr></table></figure>

<p>后面那个命令可以看到数据进kafka是否符合预期, 当然这个数据可以直接在fluentd中看到, 是一样的</p>
<p>这里要注意一个问题就是, <strong>版本不同的kafka, 上面两条命令有所不同, 有时可能会需要使用–zookeeper.</strong></p>
<h4 id="logstash"><a href="#logstash" class="headerlink" title="logstash"></a>logstash</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">output</span> &#123;</span><br><span class="line">  <span class="string">if</span> [<span class="string">type</span>] <span class="string">==</span> <span class="string">&quot;RealtySense&quot;</span>&#123;</span><br><span class="line">    <span class="string">file</span> &#123;</span><br><span class="line">      <span class="string">path</span> <span class="string">=&gt;</span> <span class="string">&quot;/tmp/logstash/<span class="template-variable">%&#123;+yyyy&#125;</span>/<span class="template-variable">%&#123;+MM&#125;</span>/<span class="template-variable">%&#123;+dd-HH&#125;</span>.log&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>logstash定位日志时可以打印到console或者文件中</p>
<p>在个grok在线debug的[工具][<a href="https://grokdebug.herokuapp.com/">https://grokdebug.herokuapp.com/</a></p>
<h4 id="es"><a href="#es" class="headerlink" title="es"></a>es</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看索引 </span></span><br><span class="line">curl <span class="string">&#x27;http://elasticsearch:9200/_cat/indices?v&#x27;</span></span><br><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">curl <span class="string">&#x27;http://elasticsearch:9200/_cluster/health?pretty&#x27;</span></span><br><span class="line"><span class="comment"># 删除索引 </span></span><br><span class="line">curl -XDELETE http://elasticsearch:9200/yourindex</span><br><span class="line"><span class="comment"># 查看索引数据</span></span><br><span class="line">curl <span class="string">&#x27;http://elasticsearch:9200/logstash.index-2019.09.22/_doc/SjLHVG0BChdCCvDoTgAU?pretty&#x27;</span> </span><br><span class="line"><span class="comment"># 查看es集群占用磁盘资源</span></span><br><span class="line">curl -s <span class="string">&#x27;http://localhost:9200/_cat/indices?h=index,store.size&amp;bytes=m&#x27;</span>|grep yourindex|grep -v <span class="string">&#x27;close&#x27;</span>|<span class="built_in">head</span>|awk <span class="string">&#x27;&#123;sum+=$2&#125; END &#123;print &quot;yourindex_size_sum_total = &quot;, sum, &quot;mb&quot;&#125;&#x27;</span></span><br><span class="line"><span class="comment"># 关闭集群自平衡,这个在集群重启时有用</span></span><br><span class="line">curl -XPUT http://localhost::9200/_cluster/settings -d<span class="string">&#x27; &#123;&quot;transient&quot;: &#123;&quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;&#125;&#125;&#x27;</span></span><br><span class="line"><span class="comment"># 开启/关闭索引: </span></span><br><span class="line">curl -XPOST <span class="string">&#x27;localhost:9200/my_index/_close?pretty&#x27;</span>  curl -XPOST <span class="string">&#x27;localhost:9200/my_index/_open?pretty&#x27;</span></span><br><span class="line"><span class="comment"># 查询索引: </span></span><br><span class="line">GET /_cat/indices/senserealty.sensego-mingyuan-2019.11.20?pretty</span><br><span class="line"><span class="comment"># 查询每个节点的磁盘使用情况: </span></span><br><span class="line">curl -XGET <span class="string">&#x27;localhost:9200/_cat/allocation?v&amp;pretty&#x27;</span></span><br><span class="line"><span class="comment"># 查看unassigned shard: </span></span><br><span class="line">GET /_cat/shards?h=index,shard,prirep,state,unassigned.reason</span><br><span class="line"><span class="comment"># 查看某个文档</span></span><br><span class="line">GET /_cat/indices/realtysense.auth-2020.04.08?v</span><br><span class="line">GET /realtysense.auth-2020.04.08/RealtySense/AXFZK6wkxgPqZbocqveB</span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line">curl -XPOST <span class="string">&#x27;http://localhost:9200/realtysense.sensego-mingyuan-2020.04.15/RealtySense/&#x27;</span> -d <span class="string">&#x27;&#123;&quot;call_bk_url&quot;: &quot;3356&quot;&#125;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h3><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2019/10/06/ES-FGC-Fix/">https://izsk.me/2019/10/06/ES-FGC-Fix/</a></li>
<li><a href="https://docs.fluentd.org/">https://docs.fluentd.org/</a></li>
<li><a href="https://github.com/fluent/fluent-plugin-kafka">https://github.com/fluent/fluent-plugin-kafka</a></li>
<li><a href="https://grokdebug.herokuapp.com/">https://grokdebug.herokuapp.com/</a></li>
<li><a href="https://docs.fluentd.org/input/tail#tag">https://docs.fluentd.org/input/tail#tag</a></li>
<li><a href="https://docs.fluentd.org/configuration/config-file">https://docs.fluentd.org/configuration/config-file</a></li>
<li><a href="https://www.matviichuk.com/2016/07/08/nested-json-in-logstash">https://www.matviichuk.com/2016/07/08/nested-json-in-logstash</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html#plugins-inputs-kafka-type">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html#plugins-inputs-kafka-type</a></li>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html">https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html</a></li>
<li><a href="https://stackoverflow.com/questions/51849598/elasticsearch-wild-card-query-not-working">https://stackoverflow.com/questions/51849598/elasticsearch-wild-card-query-not-working</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>CockRoachDB学习(架构)</title>
    <url>/2019/10/06/CockroachDB-Struct/</url>
    <content><![CDATA[<p>项目中需要一个分布式数据库用于存储业务的KV数据, 在前期项目调研结合业务比对了业界常用的KV数据库模型后, 最后选择了CockRoachDB(CRDB).</p>
<span id="more"></span>

<p>选择CRDB主要基于以下几点考虑:</p>
<blockquote>
<ul>
<li>目前项目的主要开发语言为Golang,而CRDB就是Golang实现的</li>
<li>分布式, 高开发、高性能、高可用</li>
<li>运维成本简单,上手容易</li>
</ul>
</blockquote>
<p>CRDB开源以来, 业界很多大公司都在用, 百度还将CRDB的技术文档进行了翻译, 有兴趣的可以看<a href="http://www.cockroachchina.cn/">这里</a></p>
<p>目前CRDB的源码托管在<a href="https://github.com/cockroachdb/cockroach">github</a>上</p>
<h3 id="CRDB特性"><a href="#CRDB特性" class="headerlink" title="CRDB特性"></a>CRDB特性</h3><p>CockroachDB is a distributed SQL database. The primary design goals are scalability, strong consistency and survivability (hence the name). CockroachDB aims to tolerate disk, machine, rack, and even datacenter failures with minimal latency disruption and no manual intervention. CockroachDB nodes are symmetric; a design goal is homogeneous deployment (one binary) with minimal configuration and no required external dependencies.</p>
<p>CockroachDB是一个<strong>分布式</strong>的数据库，主要设计目标是<strong>可扩展，强一致和高可用</strong> 。CockroachDB旨在无人为干预情况下，以极短的中断时间容忍磁盘、主机、机架甚至整个数据中心的故障 。 CockroachDB采用完全<strong>去中心化</strong>架构，集群中各个节点的地位完全对等，同时所有功能封装在一个二进制文件中，可以做到尽量<strong>不依赖</strong>配置文件直接部署</p>
<p>从这段官文可以看到几个很显眼的字段: 分布式、可扩展、强一致、高可靠、去中心化、不依赖</p>
<p>从这些字眼来看，完全符合项目需要，但是也需要认真地学习下CRDB的原理</p>
<h4 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h4><p>现在搞个数据库, 不做成分布式都不好意思了, 这个没啥好说的.</p>
<h4 id="可扩展"><a href="#可扩展" class="headerlink" title="可扩展"></a>可扩展</h4><p>CRDB支持横向地增加节点到集群中来提升整个集群的存储容量, 理论上最大可以支撑4EB的数据存储, 理论上单集群支持10K节点规模，节点之间通过Gossip协议来同步状态.</p>
<p>查询以分布式任务的方式在各个数据节点并发执行，可以通过增加节点数来提升单个查询的性能</p>
<h4 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h4><p>CRDB Range的副本数据同步是基于Raft协议来保证强一致性的, 所有一致性状态都存储在RocksDB中</p>
<p>对range内的数据所做的任何更改都依赖于一致性算法，以确保其大多数副本同意提交后才能返回给客户端,.</p>
<p>当写入未达成共识时，转发程序将停止以保持集群内的一致性</p>
<p>副本同步复制跟异常复制:</p>
<p><strong>同步复制要求所有写入传播到法定数量的数据副本之后，才能提交。 为了确保与数据的一致性，这是CockroachDB使用的复制类型</strong></p>
<p><strong>异步复制只需要一个节点来接收写入来被认为已提交; 之后它会传播到每个数据副本。 这或多或少等同于NoSQL数据库推广的“最终一致性”。 这种复制方法可能会导致异常和数据丢失</strong></p>
<h4 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h4><p>将Range副本分布在一个数据中心，可以确保低延迟复制，同时能容忍磁盘或机器故障。如果将副本分布在不同机架，即使某些网络交换机故障，CockroachDB仍可提供服务</p>
<p>Range副本可以跨数据中心和跨地域分布，以应对来自数据中心电源中断或网络中断，以及区域电力故障等问题</p>
<p>例如，一个Range包含三个副本，每个副本可以位于不同的位置：</p>
<ul>
<li>如果副本分布于同一台服务器上的多块磁盘，可以容忍单块磁盘故障。</li>
<li>如果副本分布于同一机架上的不同服务器，可以容忍单台服务器故障。</li>
<li>如果副本分布于同一个数据中心不同机架，可以容忍单个机架电源和网络故障。</li>
<li>如果副本分布于不同数据中心，可以容忍大规模网络中断或断电。</li>
</ul>
<p>N为总副本数，F为可容忍故障副本数，则N&#x3D;2F+1。（例如，三副本可以容忍一个副本故障，五副本则可以容忍两个副本故障，以此类推）</p>
<h4 id="去中心化"><a href="#去中心化" class="headerlink" title="去中心化"></a>去中心化</h4><p>集群中所有的节点角色都是对等的, 客户端的查询请求可以发送到集群任意节点，且每个查询可独立并发执行（无论有无冲突），意味着集群的吞吐能力可以随着节点数的增加线性提升.</p>
<p>如果一个节点接收到一个它无法直接服务的读或写请求，它会找到能够处理该请求的节点，并与它进行通信。这样你不需要知道数据位于哪里，CockroachDB会为你跟踪数据，并为每个节点启用对称行为（symmetric behavior）</p>
<h4 id="不依赖"><a href="#不依赖" class="headerlink" title="不依赖"></a>不依赖</h4><p>CRDB所有功能都在一个编译好的二进制中, 不需要依赖其它组件, 运维部署都非常方便.</p>
<h4 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h4><p>CRDB单集群支持10K节点的规模, 能够存储的数据最大为4EB, 性能可谓是强悍, 当然一般业务很难达到这个规模.</p>
<p>当然CRDB由于支持强一致性又想要高性能, 架构设计自然是花费了很多功夫, 这个就是后话了.</p>
<h3 id="CRDB术语"><a href="#CRDB术语" class="headerlink" title="CRDB术语"></a>CRDB术语</h3><p>了解CRDB之前, 需要了解下CRDB里常见的几个术语</p>
<table>
<thead>
<tr>
<th align="left">术语</th>
<th align="left">定义</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Cluster（集群）</strong></td>
<td align="left">你部署的CockroachDB集群，它包含一个或多个数据库，对外则像一个逻辑应用程序一样提供服务。</td>
</tr>
<tr>
<td align="left"><strong>Node（节点）</strong></td>
<td align="left">运行CockroachDB的单个机器。 许多节点连接在一起以创建你的集群。</td>
</tr>
<tr>
<td align="left"><strong>Store (存储)</strong></td>
<td align="left">真正KV数据的存储，一个节点可以启用多个Store, 一个Store包含多个Range.</td>
</tr>
<tr>
<td align="left"><strong>Range (数据分片)</strong></td>
<td align="left">集群中一组连续的已排序的数据,每个Range分片默认为64M。</td>
</tr>
<tr>
<td align="left"><strong>Replicas（副本）</strong></td>
<td align="left">range的副本，存储在至少3个节点上，以确保可用性。</td>
</tr>
<tr>
<td align="left"><strong>Range Lease（range租约）</strong></td>
<td align="left">对于每个range，其中的一个replicas持有“range lease“，该replicas（称为leaseholder）是接收和协调该range的所有读写请求的replicas</td>
</tr>
</tbody></table>
<p>看多了这种分布式数据库你会发现, 每种数据库都有自己的各种概念, 其实原理上很相近</p>
<h3 id="CRDB架构"><a href="#CRDB架构" class="headerlink" title="CRDB架构"></a>CRDB架构</h3><h4 id="分层架构图"><a href="#分层架构图" class="headerlink" title="分层架构图"></a>分层架构图</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200206180949.png"></p>
<p>CRDB的分为以下几层，这些层直接与在它上面和下面的层交互，提供相对不透明的服务，详细可看<a href="http://doc.cockroachchina.baidu.com/#architecture/overview/">这里</a></p>
<table>
<thead>
<tr>
<th>层</th>
<th align="left">顺序</th>
<th align="left">目的</th>
</tr>
</thead>
<tbody><tr>
<td><a href="http://doc.cockroachchina.baidu.com/#architecture/sql-layer.md">SQL</a></td>
<td align="left">1</td>
<td align="left">将客户端SQL查询转换为KV操作。</td>
</tr>
<tr>
<td><a href="http://doc.cockroachchina.baidu.com/#architecture/transaction-layer.md">Transactional</a></td>
<td align="left">2</td>
<td align="left">允许对多个KV条目进行原子性改变。</td>
</tr>
<tr>
<td><a href="http://doc.cockroachchina.baidu.com/#architecture/distribution-layer.md">Distribution</a></td>
<td align="left">3</td>
<td align="left">将复制的KV range作为单个实体。</td>
</tr>
<tr>
<td><a href="http://doc.cockroachchina.baidu.com/#architecture/replication-layer.md">Replication</a></td>
<td align="left">4</td>
<td align="left">跨越多节点的一致性和同步复制KV range。此层还允许通过租约实现一致的读取。</td>
</tr>
<tr>
<td><a href="http://doc.cockroachchina.baidu.com/#architecture/storage-layer.md">Storage</a></td>
<td align="left">5</td>
<td align="left">在磁盘上写入和读取KV数据</td>
</tr>
</tbody></table>
<p>CRDB做为一个数据库, 其实也是分层的结构，在最上层是提供给使用人员的是SQL层, 使用人员可直接使用如Mysql数据库一样的sql语句结构来操作CRDB</p>
<p>CRDB本质是个KV数据库, 那直接也存在从SQL –&gt; KV的转换, 这一系列操作由SQL层进行处理, 既然CRDB是个数据库, 那自然也少不了比如<strong>SQL语句的解析、执行计划、执行器等</strong>一系列过程, 具体过程可查看<a href="http://doc.cockroachchina.baidu.com/#architecture/sql-layer/">这里</a></p>
<h4 id="Store架构图"><a href="#Store架构图" class="headerlink" title="Store架构图"></a>Store架构图</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200206181047.png"></p>
<p>每个Store包含多个Range，Range为KV层数据管理的最小单元，每个Range的多个副本之间使用Raft协议进行同步。如下图所示，每个Range有3个副本，同一Range的副本用相同颜色标识，副本之间使用Raft协议同步</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.cockroachchina.cn/?p=1088">http://www.cockroachchina.cn/?p=1088</a></li>
<li><a href="http://doc.cockroachchina.baidu.com/">http://doc.cockroachchina.baidu.com/</a></li>
<li><a href="https://github.com/cockroachdb/cockroach/blob/master/docs/design.md">https://github.com/cockroachdb/cockroach/blob/master/docs/design.md</a></li>
<li><a href="http://doc.cockroachchina.baidu.com/#architecture/overview/">http://doc.cockroachchina.baidu.com/#architecture/overview/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 6.x上为Docker容器配置物理网段的静态IP</title>
    <url>/2016/07/30/CentOS-6-x%E4%B8%8A%E4%B8%BADocker%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE%E7%89%A9%E7%90%86%E7%BD%91%E6%AE%B5%E7%9A%84%E9%9D%99%E6%80%81IP/</url>
    <content><![CDATA[<p>虚拟机安装centos6.5(ip范围从192.168.150.130-255，虚拟机网关192.168.150.2)</p>
<h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤:"></a><strong>实验步骤:</strong></h3><p>0.停止docker:<br><code>service docker stop</code></p>
<p>1.安装brctl<br><code>yum install bridge-utils</code></p>
<span id="more"></span>

<p>2.配置桥接接口</p>
<p><code>brctl addbr br0</code></p>
<p><code>brctl addif br0 eth0</code></p>
<p>3.这是br0的配置文件ifcfg-br0:</p>
<p><code>vim /etc/sysconfig/network-script/ifcfg-br0（可复制ifcfg-eth0）</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip1.png" alt="centos-docker-ip1"></p>
<p>4.这是eth0的配置文件ifcfg-eth0:</p>
<p><code>vim /etc/sysconfig/network-script/ifcfg-eth0</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip2.png" alt="centos-docker-ip2"></p>
<p>5.重启网络:<br><code> service network restart</code><br><code>ip addr</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip3.png" alt="centos-docker-ip3"></p>
<p><code>ifcofnig</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip4.png" alt="centos-docker-ip4"></p>
<p>6.配置Docker使用br0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip link set dev docker0 down`</span><br><span class="line">brctl delbr docker0</span><br><span class="line">iptables -t nat -F POSTROUTING</span><br></pre></td></tr></table></figure>

<p>7.修改docker变量使用br0:</p>
<p><code>vim /etc/sysconfig/docker</code><br><code>other_args=&quot;-b=br0&quot;</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip5.png" alt="centos-docker-ip5"></p>
<p>8.启动docker:service docker start<br>查看 iptables是否正常：<br><code>iptables -t nat -L -n</code></p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip6.png" alt="centos-docker-ip6"></p>
<p>9.启动docker容器并设置ip:<br>启动docker容器： </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -ti --name docker1 --net=none --rm -v /usr/local/nginx/html:/usr/share/nginx/html:ro nginx:V4 /bin/bash</span><br></pre></td></tr></table></figure>

<p><code>docker attach</code></p>
<p>查看ip addr 只有一个lo</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip7.png" alt="centos-docker-ip7"></p>
<p>10.在宿主机上执行如下脚本 格式如下：脚本 容器id 容器ip 子网掩码 网关 网络通道名</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./docker-staticIP.sh e4cbb0f311a7 192.168.150.145 24 192.168.150.2 veth2</span><br></pre></td></tr></table></figure>

<p>脚本如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#/bin/bash</span><br><span class="line">if [ -z $1 ] || [ -z $2 ] || [ -z $3 ] || [ -z $4 ] || [ -z $5 ];</span><br><span class="line">then</span><br><span class="line">  echo &quot;*****Input the necessary parameters: CONTAINERID IP MASK GATEWAY ETHNAME&quot;</span><br><span class="line">  echo &quot;*****Call the script like: sh manual_con_static_ip.sh b0e18b6a4432 192.168.5.123 24 192.168.5.1 deth0&quot;</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line">CONTAINERID=$1</span><br><span class="line">SETIP=$2</span><br><span class="line">SETMASK=$3</span><br><span class="line">GATEWAY=$4</span><br><span class="line">ETHNAME=$5</span><br><span class="line">#判断宿主机网卡是否存在</span><br><span class="line">ifconfig $ETHNAME &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">  read -p &quot;$ETHNAME exist,do you want delelte it? y/n &quot; del</span><br><span class="line">  if [[ $del == &#x27;y&#x27; ]]; then</span><br><span class="line">  ip link del $ETHNAME</span><br><span class="line">  else</span><br><span class="line">  exit</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line">pid=`docker inspect -f &#x27;&#123;&#123;.State.Pid&#125;&#125;&#x27; $CONTAINERID`</span><br><span class="line">mkdir -p /var/run/netns</span><br><span class="line">find -L /var/run/netns -type l -delete</span><br><span class="line">if [ -f /var/run/netns/$pid ]; then</span><br><span class="line">  rm -f /var/run/netns/$pid</span><br><span class="line">fi</span><br><span class="line">ln -s /proc/$pid/ns/net /var/run/netns/$pid</span><br><span class="line">ip link add $ETHNAME type veth peer name B</span><br><span class="line">brctl addif br3 $ETHNAME</span><br><span class="line">ip link set $ETHNAME up</span><br><span class="line">ip link set B netns $pid</span><br><span class="line">#先删除容器内已存在的eth0</span><br><span class="line">ip netns exec $pid ip link del eth0 &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">#设置容器新的网卡eth0</span><br><span class="line">ip netns exec $pid ip link set dev B name eth0</span><br><span class="line">ip netns exec $pid ip link set eth0 up</span><br><span class="line">ip netns exec $pid ip addr add $SETIP/$SETMASK dev eth0</span><br><span class="line">ip netns exec $pid ip route add default via $GATEWAY</span><br></pre></td></tr></table></figure>

<p>11.在容器中ip addr就多一个eth0网络</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip8.png" alt="centos-docker-ip8"></p>
<p>12.在宿主机中查看ip addr 也多了一个veth2</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip9.png" alt="centos-docker-ip9"></p>
<p>13.游览器中访问容器的nginx,显示成功且容器中打印日志</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip10.png" alt="centos-docker-ip10"></p>
<p>14.测试时遇到如下问题：</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/centos-docker-ip11.png" alt="centos-docker-ip11"></p>
<p>解决办法：安装最新版的iproute:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://repos.fedorapeople.org/openstack/EOL/openstack-grizzly/epel-6/iproute-2.6.32-130.el6ost.netns.2.x86_64.rpm</span><br><span class="line">rpm -ivh --replacefiles ./iproute-2.6.32-130.el6ost.netns.2.x86_64.rpm</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://chenyang.me/2015/09/09/CentOS%206.x%E4%B8%8A%E4%B8%BADocker%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE%E7%89%A9%E7%90%86%E7%BD%91%E6%AE%B5%E7%9A%84%E9%9D%99%E6%80%81IP/">http://chenyang.me/2015/09/09/CentOS%206.x%E4%B8%8A%E4%B8%BADocker%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE%E7%89%A9%E7%90%86%E7%BD%91%E6%AE%B5%E7%9A%84%E9%9D%99%E6%80%81IP/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(elastalert自定义邮件模板)</title>
    <url>/2020/05/28/EFLK-elastalert-mail/</url>
    <content><![CDATA[<p>elastalert是专门为elastsearch开源的日志关键字监控工具, 支持非常多的告警方式且自定义程度很高, 代码写的也非常清晰, 最近调研了这个开源库进行了二次开发, 用在了业务日志关键字监控上,还是很不错的.这里站在用户使用的角度对邮件告警进行了改造, 期间踩了不少坑.</p>
<p>下次会记录一下常见的监控<code>rule</code>类型及接入微信告警的方式.</p>
<span id="more"></span>



<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install elastalert</span><br></pre></td></tr></table></figure>



<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><h4 id="物理机"><a href="#物理机" class="headerlink" title="物理机"></a>物理机</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/bin/python -m elastalert.elastalert --rule /etc/elastalert/rules/my_rule.yaml --verbose</span><br></pre></td></tr></table></figure>

<h4 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h4><p>官方给出的docker启动方式, 但比较粗糙, 不建议, 建议直接运行在k8s中.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/bitsensor/elastalert.git; <span class="built_in">cd</span> elastalert</span><br><span class="line">docker run -d -p 3030:3030 \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config/elastalert.yaml:/opt/elastalert/config.yaml \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/config/config.json:/opt/elastalert-server/config/config.json \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/rules:/opt/elastalert/rules \</span><br><span class="line">    -v `<span class="built_in">pwd</span>`/rule_templates:/opt/elastalert/rule_templates \</span><br><span class="line">    --net=<span class="string">&quot;host&quot;</span> \</span><br><span class="line">    --name elastalert bitsensor/elastalert:latest</span><br></pre></td></tr></table></figure>

<h4 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a>k8s</h4><p>这里推荐一个在elastalert的基础上集成restfulapi接口的库, <a href="https://github.com/nsano-rururu/elastalert-server">elastalert-server</a></p>
<p>直接支持最新的<code>python3.8+elastalert2.0.4</code>, 可通过http 接口进行<code>CURD</code></p>
<h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><p>elastalert在启动时会自动创建相关索引，当然也可事先使用<code>elastalert-create-index</code>提示创建索引</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@python-debug-5fdd9d9cd5-fkfkh:/usr/local/lib/python3.5<span class="comment"># elastalert-create-index</span></span><br><span class="line">Enter Elasticsearch host: 127.0.0.1</span><br><span class="line">Enter Elasticsearch port: 9200</span><br><span class="line">Use SSL? t/f: f</span><br><span class="line">Enter optional basic-auth username (or leave blank): </span><br><span class="line">Enter optional basic-auth password (or leave blank): </span><br><span class="line">Enter optional Elasticsearch URL prefix (prepends a string to the URL of every request): </span><br><span class="line">New index name? (Default elastalert_status) realtysense.elastalert_status</span><br><span class="line">New <span class="built_in">alias</span> name? (Default elastalert_alerts) realtysense.elastalert_status</span><br><span class="line">Name of existing index to copy? (Default None) </span><br><span class="line">Elastic Version: 5.4.1</span><br><span class="line">Reading Elastic 5 index mappings:</span><br><span class="line">Reading index mapping <span class="string">&#x27;es_mappings/5/silence.json&#x27;</span></span><br><span class="line">Reading index mapping <span class="string">&#x27;es_mappings/5/elastalert_status.json&#x27;</span></span><br><span class="line">Reading index mapping <span class="string">&#x27;es_mappings/5/elastalert.json&#x27;</span></span><br><span class="line">Reading index mapping <span class="string">&#x27;es_mappings/5/past_elastalert.json&#x27;</span></span><br><span class="line">Reading index mapping <span class="string">&#x27;es_mappings/5/elastalert_error.json&#x27;</span></span><br><span class="line">New index realtysense.elastalert_status created</span><br><span class="line">Done!</span><br></pre></td></tr></table></figure>

<p>上述的5个索引主要用于保存elastalert运行期间的相关数据, 会直接写回到es中, 主要用于计算比如聚合,发现日志关键字时的日志现场等,同时也会记录elastalert本身的数据.</p>
<h3 id="配置热更新"><a href="#配置热更新" class="headerlink" title="配置热更新"></a>配置热更新</h3><p>elastalert代码中使用apscheduler库定时检查rule文件的hash值，在运行周期到达时会比对rules的hash值是否有变化从而修改apscheduler的modify_job来实现热更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_rule_hashes = self.rules_loader.get_hashes(self.conf, self.args.rule)</span><br><span class="line">        <span class="comment"># Check each current rule for changes</span></span><br><span class="line">        <span class="keyword">for</span> rule_file, hash_value <span class="keyword">in</span> self.rule_hashes.items():</span><br><span class="line">            <span class="keyword">if</span> rule_file <span class="keyword">not</span> <span class="keyword">in</span> new_rule_hashes:</span><br><span class="line">                <span class="comment"># Rule file was deleted</span></span><br><span class="line">                elastalert_logger.info(<span class="string">&#x27;Rule file %s not found, stopping rule execution&#x27;</span> % (rule_file))</span><br><span class="line">                <span class="keyword">for</span> rule <span class="keyword">in</span> self.rules:</span><br><span class="line">                    <span class="keyword">if</span> rule[<span class="string">&#x27;rule_file&#x27;</span>] == rule_file:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                self.scheduler.remove_job(job_id=rule[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line">                self.rules.remove(rule)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> hash_value != new_rule_hashes[rule_file]:</span><br><span class="line">                <span class="comment"># Rule file was changed, reload rule</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    new_rule = self.rules_loader.load_configuration(rule_file, self.conf)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> new_rule:</span><br><span class="line">                        logging.error(<span class="string">&#x27;Invalid rule file skipped: %s&#x27;</span> % rule_file)</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;is_enabled&#x27;</span> <span class="keyword">in</span> new_rule <span class="keyword">and</span> <span class="keyword">not</span> new_rule[<span class="string">&#x27;is_enabled&#x27;</span>]:</span><br><span class="line">                        elastalert_logger.info(<span class="string">&#x27;Rule file %s is now disabled.&#x27;</span> % (rule_file))</span><br><span class="line">                        <span class="comment"># Remove this rule if it&#x27;s been disabled</span></span><br><span class="line">                        self.rules = [rule <span class="keyword">for</span> rule <span class="keyword">in</span> self.rules <span class="keyword">if</span> rule[<span class="string">&#x27;rule_file&#x27;</span>] != rule_file]</span><br><span class="line">                        <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>

<p>因此如果是rule文件是使用的k8s的configmap进行部署, 则可直接修改configmap，不需要重启.</p>
<h3 id="告警方式"><a href="#告警方式" class="headerlink" title="告警方式"></a>告警方式</h3><p>elastalert支持非常多的报警方式, 而且官方也支持直接对报警方式进行二次开发，只要实现对应的类即可, 详情<a href="https://elastalert.readthedocs.io/en/latest/ruletypes.html#alerts">alerts</a>, 这里以邮件为例来自定义邮件模板</p>
<h3 id="自定义邮件格式"><a href="#自定义邮件格式" class="headerlink" title="自定义邮件格式"></a>自定义邮件格式</h3><h4 id="邮件subject"><a href="#邮件subject" class="headerlink" title="邮件subject"></a>邮件subject</h4><p>默认的邮件的subject的格式是<code>ElastAlert: index realty contain is invalid</code></p>
<p><code>index realty contain is invalid</code>为rule文件中定义的名字</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">alert_subject: <span class="string">&quot;Issue &#123;0&#125; occurred at &#123;1&#125;&quot;</span></span><br><span class="line">alert_subject_args:  <span class="comment"># 这两个参数会被替换到上面两个占位参数中.</span></span><br><span class="line">- issue.name</span><br><span class="line">- <span class="string">&quot;@timestamp&quot;</span></span><br></pre></td></tr></table></figure>

<p>这里使用的python中的字符串占位替换.类似于<code>&quot;&#123;&#125;, &#123;&#125;&quot;.format(xxx, yyy)</code></p>
<h4 id="邮件内容"><a href="#邮件内容" class="headerlink" title="邮件内容"></a>邮件内容</h4><p>官方支持的邮件内容是纯文本的，非常不美观， 好在直接在rule文件中使用<code>alert_text</code>字段来指定使用的模板,可以使用html格式，同样，使用参数来进行占位替换.</p>
<p><strong>但这存在一个问题是,每一个rule文件都需要写一大坨的html模板,虽然内容相差无几, 但还是需要根据需要展示的参数的个数来修改html模板, 因此，这里使用固定的html模板样式动态地对参数个数进行支持, 使用到jinja2来做, 最终实现的效果是: 使用者不需要关注模板内容, 直接指定alert_text_args的参数即可</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">alert:</span><br><span class="line">- <span class="string">&quot;email&quot;</span></span><br><span class="line">email_format: html</span><br><span class="line">alert_text_type: alert_text_only <span class="comment"># alert_text_only: 不发送默认模板内容</span></span><br><span class="line">alert_subject: <span class="string">&quot;IDS Event From &#123;0&#125; Priority: &#123;1&#125;&quot;</span></span><br><span class="line">alert_subject_args:</span><br><span class="line">- hostname</span><br><span class="line">- priority</span><br><span class="line"><span class="comment">#alert_text: | # 不需要指定模板，由jinja根据alert_text_args字段动态生成.</span></span><br><span class="line"><span class="comment">#	&lt;html content&gt;</span></span><br><span class="line">alert_text_args:</span><br><span class="line">- msg</span><br><span class="line">- <span class="string">&quot;@timestamp&quot;</span></span><br><span class="line">- <span class="string">&quot;@version&quot;</span></span><br><span class="line">- _id</span><br><span class="line">- _index</span><br><span class="line">- _type</span><br><span class="line">- service_name</span><br><span class="line">- kubernetes.host <span class="comment"># 如果是嵌套的json. 需要用.的方式引用</span></span><br><span class="line">- num_hits</span><br><span class="line">- num_matches</span><br><span class="line">email:</span><br><span class="line">- <span class="string">&quot;zhoushuke@sensetime.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>邮件效果如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200602165705.png"></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>镜像编译时提示以下<code>invalid syntax</code></li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200526124725.png"></p>
<p>原因: python3.7的版本需要jira2.0以上的版本, 默认requirements.txt中指定的jira版本为1.x的版本, 不相容.</p>
<p>解决: 升级jira的版本.</p>
<ol start="2">
<li>启动时提示 <code>duplicate rule name</code></li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200526153919.png"></p>
<p>原因: 由于elastalert是递归的遍历rules规则，如果rules是使用的kubernetes的configmap挂载进来的，最终在rules目录下生成的文件会是下面的这样</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200526154551.png"></p>
<p>在..2020_05_26_07_37_04.030814278目录下也会存在一个同名的规则文件, 因此会提示rule 重复.</p>
<p>解决: 在elastalert的配置文件中可指定<code>scan_subdirectories: False</code>即不对rules目录下的子目录进行扫描即可解决.</p>
<ol start="3">
<li>发送邮件时出现以下错误:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ERROR elastalert-server:</span><br><span class="line">ProcessController:  ERROR:root:Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/elastalert.py&quot;</span>, line 1450, <span class="keyword">in</span> alert</span><br><span class="line">    <span class="built_in">return</span> self.send_alert(matches, rule, alert_time=alert_time, retried=retried)</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/elastalert.py&quot;</span>, line 1544, <span class="keyword">in</span> send_alert</span><br><span class="line">    alert.alert(matches)</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/alerts.py&quot;</span>, line 484, <span class="keyword">in</span> alert</span><br><span class="line">    body = self.create_alert_body(matches)</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/alerts.py&quot;</span>, line 291, <span class="keyword">in</span> create_alert_body</span><br><span class="line">    body += str(BasicMatchString(self.rule, match))</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/alerts.py&quot;</span>, line 171, <span class="keyword">in</span> __str__</span><br><span class="line">    self._add_custom_alert_text()</span><br><span class="line">  File <span class="string">&quot;/opt/elastalert/elastalert/alerts.py&quot;</span>, line 99, <span class="keyword">in</span> _add_custom_alert_text</span><br><span class="line">    alert_text = alert_text.format(*alert_text_values)</span><br><span class="line">KeyError: <span class="string">&#x27;\n            background-color&#x27;</span></span><br></pre></td></tr></table></figure>

<p>原因: 由于使用了jinja2的语法, 在hmtl模板文件中会使用大括号来占位, 但是html的style有时也会使用大括号, 不知道为何jinja2错误地把style中的大括号当成是占位参数，从而出现以下错误, 类似下面这种</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;style&gt;</span><br><span class="line">  table<span class="comment">#t01 tr:nth-child(even) &#123;</span></span><br><span class="line">     background-color: <span class="comment">#eee;</span></span><br><span class="line">  &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>

<p>我直接把以下内容删掉后就没问题了,这个有点坑，要注意.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     background-color: <span class="comment">#eee;</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>



<h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p>如果在日志中出现query hits为0的多半原因是因为转换后的es 查询语句查询出来的结果就是为0</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ProcessController:  INFO:elastalert:Queried rule index realty find invalid keyword from 2020-05-27 06:55 UTC to 2020-05-27 07:00 UTC: 0 / 0 hits</span><br><span class="line">    </span><br><span class="line">07:00:51.888Z ERROR elastalert-server:</span><br><span class="line">    ProcessController:  INFO:elastalert:Ran index realty find invalid keyword from 2020-05-27 06:55 UTC to 2020-05-27 07:00 UTC: 0 query hits (0 already seen), 0 matches, 0 alerts sent</span><br></pre></td></tr></table></figure>

<p>可打开<code>es_debug_trace</code>将es查询记录到指定的文件中, 如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> -XGET <span class="string">&#x27;http://localhost:9200/demo.demo-*/_search?pretty&amp;_source_include=%2A%2C%40timestamp&amp;ignore_unavailable=true&amp;scroll=30s&amp;size=10&#x27;</span> -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;query&quot;: &#123;</span></span><br><span class="line"><span class="string">    &quot;bool&quot;: &#123;</span></span><br><span class="line"><span class="string">      &quot;filter&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;bool&quot;: &#123;</span></span><br><span class="line"><span class="string">          &quot;must&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;range&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;@timestamp&quot;: &#123;</span></span><br><span class="line"><span class="string">                  &quot;gt&quot;: &quot;2020-05-27T08:31:43.695209Z&quot;,</span></span><br><span class="line"><span class="string">                  &quot;lte&quot;: &quot;2020-05-27T08:33:15.189058Z&quot;</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">            &#125;,</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;wildcard&quot;: &#123; # 重点关注这里</span></span><br><span class="line"><span class="string">                &quot;msg&quot;: &quot;*unaryInterceptor*&quot;</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  &#125;,</span></span><br><span class="line"><span class="string">  &quot;sort&quot;: [</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;@timestamp&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;order&quot;: &quot;asc&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  ]</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在这里要注意的是es的语法中, 使用<code>wildcard</code>时只能使用小写的字符串, 这是一个坑, 原因可参考<a href="https://izsk.me/2020/05/27/EFK-prombles/">zsk-blog</a></p>
<h4 id="使用关键字参数"><a href="#使用关键字参数" class="headerlink" title="使用关键字参数"></a>使用关键字参数</h4><p>除了使用位置参数外，也支持在模板中使用关键字参数, 如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">any</span></span><br><span class="line"><span class="attr">filter:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">term:</span></span><br><span class="line">        <span class="attr">_type:</span> <span class="string">elastalert_error</span></span><br><span class="line"><span class="attr">index:</span> <span class="string">elastalert_status</span></span><br><span class="line"><span class="attr">alert_subject:</span> <span class="string">&quot;Error on rule elastalert &quot;</span></span><br><span class="line"><span class="attr">alert_text_kw:</span></span><br><span class="line">     <span class="attr">data:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">alert_text:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    Error elastalert :</span></span><br><span class="line"><span class="string">    - &#123;data&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="elastalert-server支持es-debug-trace"><a href="#elastalert-server支持es-debug-trace" class="headerlink" title="elastalert-server支持es_debug_trace"></a>elastalert-server支持es_debug_trace</h4><p>默认的elastalert-server对elastalert参够使用的参数不是很多, 目前只支持<code>--verbose</code>跟<code>--debug</code>这两个调试参数, 但是elastalert还有一个参数<code>es_debug_trace</code>在elastalert-server的配置文件中是不支持的，这对于需要debug es的查询语句时很不方便，需要修改源码支持</p>
<p><code>elastalert-server/src/controllers/process/index.js</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (config.get(<span class="string">&#x27;es_debug_trace&#x27;</span>) !== undefined &amp;&amp; config.get(<span class="string">&#x27;es_debug_trace&#x27;</span>) !== <span class="string">&#x27;&#x27;</span>) &#123;</span><br><span class="line">  logger.info(<span class="string">&#x27;Setting ElastAlert es_debug_trace mode. Enable logging from Elasticsearch queries as curl command. Queries will be logged to file.&#x27;</span>);</span><br><span class="line">  startArguments.push(<span class="string">&#x27;--es_debug_trace&#x27;</span>, config.get(<span class="string">&#x27;es_debug_trace&#x27;</span>));</span><br></pre></td></tr></table></figure>

<p><code>elastalert-server/config/config.json</code>添加&#96;  “es_debug_trace”: “&#x2F;tmp&#x2F;“即可</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/Yelp/elastalert/issues/2690">https://github.com/Yelp/elastalert/issues/2690</a></li>
<li><a href="https://github.com/nsano-rururu/elastalert-server">https://github.com/nsano-rururu/elastalert-server</a></li>
<li><a href="https://chunlife.top/2019/03/27/es%E5%91%8A%E8%AD%A6%E5%8A%9F%E8%83%BD%E2%80%94%E2%80%94elastalert/">https://chunlife.top/2019/03/27/es%E5%91%8A%E8%AD%A6%E5%8A%9F%E8%83%BD%E2%80%94%E2%80%94elastalert/</a></li>
<li><a href="https://segmentfault.com/a/1190000017553282">https://segmentfault.com/a/1190000017553282</a></li>
<li><a href="http://www.mamicode.com/info-detail-2269787.html">http://www.mamicode.com/info-detail-2269787.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(elastalertRule常用规则)</title>
    <url>/2020/05/29/EFLK-elastalert-rules/</url>
    <content><![CDATA[<p>这次着重看一看elastalert的配置及支持的Rule规则. 对于一般的业务需求基本是可以满足的了.</p>
<span id="more"></span>



<h3 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">es_host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">es_port:</span> <span class="number">9200</span></span><br><span class="line"><span class="attr">rules_folder:</span> <span class="string">rules</span></span><br><span class="line"><span class="attr">run_every:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">buffer_time:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Option basic-auth username and password for elasticsearch</span></span><br><span class="line"><span class="comment">#es_username: someusername</span></span><br><span class="line"><span class="comment">#es_password: somepassword</span></span><br><span class="line"><span class="attr">writeback_index:</span> <span class="string">demo.elastalert_status</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_time_limit:</span></span><br><span class="line">  <span class="attr">hours:</span> <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="attr">skip_invalid:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">scan_subdirectories:</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>全局配置文件比较好理解, 比较重要的是</p>
<blockquote>
<ul>
<li><code>run_every</code>: 指定每次查询的周期</li>
<li>buffer_time: 查询es的时间窗口</li>
<li><code>writeback_index</code>: 存储所有数据的索引, <a href="https://elastalert.readthedocs.io/en/latest/elastalert_status.html#elastalert-status">参考</a></li>
<li><code>old_query_limit</code>: The maximum time between queries for ElastAlert to start at the most recently run query.<br>When ElastAlert starts, for each rule, it will search <code>elastalert_metadata</code> for the most recently run query and start from that time, unless it is older than <code>old_query_limit</code>, in which case it will start from the present time. The default is one week.</li>
</ul>
</blockquote>
<p>注意如果需要改动这个配置文件, 需要重启.</p>
<h3 id="rule类型"><a href="#rule类型" class="headerlink" title="rule类型"></a>rule类型</h3><p>常用的rule类型有以下几类:</p>
<h4 id="frequency"><a href="#frequency" class="headerlink" title="frequency"></a>frequency</h4><p>说明：当给定时间范围内至少有一定数量的事件时，此规则匹配。 这可以按照每个query_key来计数</p>
<p>规则: 5分钟之内如果出现10次匹配到关键字，则触发报警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">es_host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">es_port:</span> <span class="number">9200</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">index</span> <span class="string">demo</span> <span class="string">find</span> <span class="string">invalid</span> <span class="string">keyword</span> <span class="comment"># 规则的名字必须是唯一的</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">frequency</span></span><br><span class="line"><span class="attr">num_events:</span> <span class="number">10</span> <span class="comment"># 匹配的数量</span></span><br><span class="line"><span class="comment"># type: flatline</span></span><br><span class="line"><span class="comment"># threshold: 10  # 指定最小阈值</span></span><br><span class="line"><span class="attr">index:</span> <span class="string">demo.demo-*</span></span><br><span class="line"><span class="attr">match_enhancements:</span> <span class="comment"># 指定使用增强功能</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;elastalert.enhancements.TimeEnhancement&#x27;</span></span><br><span class="line"><span class="attr">timeframe:</span> <span class="comment"># 时间段</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">query_key:</span> <span class="comment">#alert去重的字段名（多个字段会导致检索上新建一个组合字段名以用于查询）</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">name</span></span><br><span class="line"><span class="attr">realert:</span>  </span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">5</span> <span class="comment">#设置一个时长，在该时间内，相同 query_key 的报警只发一个,期间产生的alert被简单丢弃</span></span><br><span class="line"><span class="comment">#exponential_realert:</span></span><br><span class="line"><span class="comment">#  minutes: 5				#设置一个时长，必须大于realert 设置，则在realert到exponential_realert之间，每次报警之后，realert 自动翻倍</span></span><br><span class="line"><span class="attr">filter:</span> <span class="comment"># filter是es中的语法.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">query:</span></span><br><span class="line">    <span class="attr">wildcard:</span></span><br><span class="line">      <span class="attr">backendMod:</span> <span class="string">&#x27;*senseface*&#x27;</span></span><br><span class="line"><span class="comment"># 以上规则： 5分钟之内如果出现10次匹配的日志数，则触发报警</span></span><br><span class="line"><span class="attr">alert:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;email&#x27;</span></span><br><span class="line"><span class="attr">alert_subject:</span> <span class="string">&#x27;ELASTLOG: FIND EVENT IN INDEX [&#123;&#125;] FROM [&#123;&#125;] TO [&#123;&#125;]&#x27;</span> <span class="comment"># 邮件title,占位参数由下面3个参数替换</span></span><br><span class="line"><span class="attr">alert_subject_args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_index</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">starttime</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">endtime</span></span><br><span class="line"><span class="attr">alert_text_args:</span>  <span class="comment"># 邮件正文参数, 这里会直接传递给邮件模板, 进行了二次开发</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;@timestamp&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;@version&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_id</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_index</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">_type</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">...</span> <span class="comment"># 省略</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">starttime</span>  <span class="comment"># starttime可以直接从self.rule.get(&#x27;starttime&#x27;)中获取,并不存在于match_body中.</span></span><br><span class="line">  						<span class="comment">#如果参数中没有传递starttime,endtime，则enttime是以当前的时间为准，去计算starttime</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">endtime</span>  <span class="comment"># 注意endtime对应的是执行查询时的当前时间， 因此没有保存，如果需要使用 需要修改源码</span></span><br><span class="line"></span><br><span class="line"><span class="attr">smtp_host:</span> <span class="string">demo.mail.com</span></span><br><span class="line"><span class="attr">smtp_port:</span> <span class="number">587</span></span><br><span class="line"><span class="attr">smtp_auth_file:</span> <span class="string">/opt/elastalert/auth/smtp_auth_file.yaml</span></span><br><span class="line"><span class="attr">email_reply_to:</span> <span class="string">demo@top.com</span></span><br><span class="line"><span class="attr">from_addr:</span> <span class="string">demo@top.com</span></span><br><span class="line"></span><br><span class="line"><span class="attr">email_format:</span> <span class="string">html</span></span><br><span class="line"><span class="attr">email:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;no-reply@top.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">alert_text_type:</span> <span class="string">alert_text_only</span> <span class="comment"># 不发送默认内容, 如果不加这行, 则除了发送上面的邮件内容之外，还会连同发送原始内容.</span></span><br><span class="line"><span class="attr">use_local_time:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># include: [&quot;ip_address&quot;, &quot;hostname&quot;, &quot;status&quot;] #限制输出的检索字段</span></span><br></pre></td></tr></table></figure>

<p>下面几种rule规则的配置只会贴出与上面配置不一样的地方.官方的说明很详细</p>
<h4 id="any"><a href="#any" class="headerlink" title="any"></a>any</h4><p>说明：任何规则都会匹配， <strong>查询返回的每个命中将生成一个警报</strong>。</p>
<p>规则：当匹配status字段为<code>anystatus</code>，触发告警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">any</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>一定要慎用这个规则, 因为每个命中都会生成告警</p>
<h4 id="flatline"><a href="#flatline" class="headerlink" title="flatline"></a>flatline</h4><p>说明：当一个时间段内的事件总数低于一个给定的阈值时，匹配规则</p>
<p>规则: 5分钟之内如果关键字的文档数小于给定阈值，则触发报警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">threshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">flatline</span></span><br></pre></td></tr></table></figure>

<h4 id="spike"><a href="#spike" class="headerlink" title="spike"></a>spike</h4><p>说明：当某个时间段内的事件量比上一个时间段的spike_height时间大或小时，这个规则是匹配的。它使用两个滑动窗口来比较事件的当前和参考频率。 我们将这两个窗口称为“参考”和“当前”。</p>
<p>规则：当前窗口数据量为3，当前窗口超过参考窗口数据量次数1次，触发告警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">spike</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">threshold_cur:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">spike_height:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">spike_type:</span> <span class="string">&quot;up&quot;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><code>threshold_cur</code>：当前窗口初始值</li>
<li><code>spike_height</code>：当前窗口数据量连续比参考窗口数据量高(&#x2F;低)的次数</li>
<li><code>spike_type</code>：高或低</li>
</ul>
</blockquote>
<h4 id="change"><a href="#change" class="headerlink" title="change"></a>change</h4><p>说明：此规则将监视某个字段，并在该字段更改时进行匹配，该领域必须相对于最后一个事件发生相同的变化。</p>
<p>规则：当server字段值相同，codec字段值不同时，触发告警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">change</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">compare_key:</span> <span class="string">codec</span></span><br><span class="line"><span class="attr">ignore_null:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">query_key:</span> <span class="string">server</span></span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><code>compare_key</code>：与上一条记录做对比的字段</li>
<li>query_key<code>：与上一条记录相同的字段</code></li>
<li><code>ignore_null</code>：忽略记录不存在compare_key字段的情况</li>
</ul>
</blockquote>
<h4 id="blacklist"><a href="#blacklist" class="headerlink" title="blacklist"></a>blacklist</h4><p>说明：黑名单规则将检查黑名单中的某个字段，如果它在黑名单中则匹配。</p>
<p>规则：当字段status匹配到关键字sensefacexxx，触发告警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">blacklist</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">compare_key:</span> <span class="string">backendMod</span></span><br><span class="line"><span class="attr">blacklist:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;sensefacexxx&quot;</span></span><br></pre></td></tr></table></figure>

<p>要注意的是最终转换成es的查询语句时会将blacklist的值也加入到查询条件中，如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> -XGET <span class="string">&#x27;http://localhost:9200/demo.demo-*/_search?pretty&amp;_source_include=%40timestamp%2C%2A%2Cschema.device_id%2CbackendMod&amp;ignore_unavailable=true&amp;scroll=30s&amp;size=1000</span></span><br><span class="line"><span class="string">  &quot;query&quot;: &#123;                                    </span></span><br><span class="line"><span class="string">    &quot;bool&quot;: &#123;                                                                                                                                                                         </span></span><br><span class="line"><span class="string">      &quot;filter&quot;: &#123;                                                                                                                   </span></span><br><span class="line"><span class="string">        &quot;bool&quot;: &#123;                             </span></span><br><span class="line"><span class="string">          &quot;must&quot;: [                        </span></span><br><span class="line"><span class="string">            &#123;    </span></span><br><span class="line"><span class="string">              &quot;range&quot;: &#123;                                  </span></span><br><span class="line"><span class="string">                &quot;@timestamp&quot;: &#123;                    </span></span><br><span class="line"><span class="string">                  &quot;gt&quot;: &quot;2020-06-01T13:54:33.079080Z&quot;,</span></span><br><span class="line"><span class="string">                  &quot;lte&quot;: &quot;2020-06-01T13:59:33.079080Z&quot;</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">              &#125;                                                                                                                     </span></span><br><span class="line"><span class="string">            &#125;,                                </span></span><br><span class="line"><span class="string">            &#123;                              </span></span><br><span class="line"><span class="string">              &quot;wildcard&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;backendMod&quot;: &quot;*senseface*&quot;</span></span><br><span class="line"><span class="string">              &#125;                                    </span></span><br><span class="line"><span class="string">            &#125;,                                                                                                                                                                        </span></span><br><span class="line"><span class="string">            &#123;                       </span></span><br><span class="line"><span class="string">              &quot;query_string&quot;: &#123;</span></span><br><span class="line"><span class="string">                &quot;query&quot;: &quot;backendMod:\&quot;sensefacexxx\&quot;&quot;</span></span><br><span class="line"><span class="string">              &#125;  </span></span><br><span class="line"><span class="string">            &#125;                                             </span></span><br><span class="line"><span class="string">          ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  &#125;,       </span></span><br><span class="line"><span class="string">  &quot;sort&quot;: [        </span></span><br><span class="line"><span class="string">    &#123;                </span></span><br><span class="line"><span class="string">      &quot;@timestamp&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;order&quot;: &quot;asc&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;                                           </span></span><br><span class="line"><span class="string">  ]                                             </span></span><br><span class="line"><span class="string">&#125;&#x27;</span>        </span><br></pre></td></tr></table></figure>

<h4 id="writelist"><a href="#writelist" class="headerlink" title="writelist"></a>writelist</h4><p>说明：与黑名单类似，此规则将某个字段与白名单进行比较，如果列表中不包含该字词，则匹配</p>
<p>blacklist与writelist都没有rule title.</p>
<h4 id="cardinality"><a href="#cardinality" class="headerlink" title="cardinality"></a>cardinality</h4><p>说明：当一个时间范围内的特定字段的唯一值的总数高于或低于阈值时，该规则匹配</p>
<p>规则：1分钟内，level的唯一数量超过2个(不包括2个)，触发告警。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">type:</span> <span class="string">cardinality</span></span><br><span class="line"><span class="attr">timeframe:</span></span><br><span class="line">    <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">cardinality_field:</span> <span class="string">level</span></span><br><span class="line"><span class="attr">max_cardinality:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">query_key:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">schema.device_id</span> <span class="comment"># query_key会保留下来 </span></span><br><span class="line"><span class="comment"># min_cardinality: 1 如果同时存在max,min，两者是or的关系</span></span><br></pre></td></tr></table></figure>

<p><strong>注意:cardinality类型的因为查询的是唯一值，因此不会返回match_body的内容, 所以任何引用match_body的字段都会返回<code>&lt;MISSING VALUE&gt;</code></strong>,当然可以使用enhancement.</p>
<h3 id="关于时间格式"><a href="#关于时间格式" class="headerlink" title="关于时间格式"></a>关于时间格式</h3><p>首先要说明的是: 默认情况下, elasticsearch存储的日志的<code>@timestamp</code>为<code>UTC</code>格式的, 因此所有查询es的时间窗口都会被转换成<code>UTC</code></p>
<p>而elastalert也很人性化的会根据本地时区对日期进行格式化输出.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># endtime 永远都为当着时间，因此会首先获取utc时间然后根据时区转换为当地时间</span></span><br><span class="line">datetime.datetime.utcnow().replace(tzinfo=dateutil.tz.tzutc()) <span class="comment"># 2020-05-30 13:41:33.359737+00:00</span></span><br><span class="line"><span class="comment"># starttime 则会根据endtime与buffer_time计算出来的</span></span><br><span class="line"><span class="comment"># 所有的格式化输出都会调用时间转换函数为当地时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pretty_ts</span>(<span class="params">timestamp, tz=<span class="literal">True</span></span>): <span class="comment"># tz为rule文件中指定use_local_time，默认为true,因此可以不指定该项</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pretty-format the given timestamp (to be printed or logged hereafter).</span></span><br><span class="line"><span class="string">    If tz, the timestamp will be converted to local time.</span></span><br><span class="line"><span class="string">    Format: YYYY-MM-DD HH:MM TZ&quot;&quot;&quot;</span></span><br><span class="line">    dt = timestamp</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(timestamp, datetime.datetime):</span><br><span class="line">        dt = ts_to_dt(timestamp)</span><br><span class="line">    <span class="keyword">if</span> tz:</span><br><span class="line">        dt = dt.astimezone(dateutil.tz.tzlocal())</span><br><span class="line">    <span class="keyword">return</span> dt.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M %Z&#x27;</span>) <span class="comment"># 2020-05-30 21:19 CST</span></span><br></pre></td></tr></table></figure>

<p><strong>大多数情况下，查询es默认都是以@timestamp为基准</strong>,如果使用其它的field, 需要在rule文件中指定以下三个相应的配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">timestamp_field: datetime</span><br><span class="line">timestamp_type: custom</span><br><span class="line">timestamp_format: &quot;%Y-%m-%dT%H:%M:%S.%fZ&quot;</span><br><span class="line">timestamp_format_expr: &quot;ts[:23] + ts[26:]&quot;</span><br></pre></td></tr></table></figure>



<h3 id="日志说明"><a href="#日志说明" class="headerlink" title="日志说明"></a>日志说明</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Queried rule index realty find invalid keyword from 2020-05-29 08:49 UTC to 2020-05-29 08:54 UTC: 482 / 482 hits</span><br><span class="line">INFO:elastalert:Ignoring match <span class="keyword">for</span> silenced rule index realty find invalid keyword.d0:94:66:6a:6f:2b</span><br><span class="line">INFO:elastalert:Ignoring match <span class="keyword">for</span> silenced rule index realty find invalid keyword.d0:94:66:6a:6f:2b</span><br><span class="line">...</span><br><span class="line">INFO:elastalert:Ran index realty find invalid keyword from 2020-05-29 08:49 UTC to 2020-05-29 08:54 UTC: 482 query hits (108 already seen), 37 matches, 1 alerts sent</span><br><span class="line">INFO:elastalert:Reloading configuration <span class="keyword">for</span> rule rules/demo.yaml</span><br></pre></td></tr></table></figure>

<p>elastalert索引中，<code>hits</code>表示规则命中条数；<code>matches</code>表示规则命中条数，并且匹配规则触发告警数量。<br>num_hits表示的是根据filter条件及查询时间段从es返回的记录,而num_matches表示的是预计会产生多少条报警<br>因此 num_matches &#x3D; num_hits &#x2F; num_events, 会四舍五入,所以在告警内容中会发现两者都是这样的关系</p>
<p>elastalert打印的以下日志包含以下几个信息非常有用.</p>
<blockquote>
<ul>
<li>时间段:  2020-05-29 08:49 to 2020-05-29 08:54</li>
<li>查询的文档数(<strong>匹配查询条件返回的记录</strong>): 482</li>
<li>已处理文档数(already seen): 108</li>
<li>报警数: 37</li>
<li>发送报警数: 1</li>
</ul>
</blockquote>
<p>如果每次运行的时间(run_every)跟查询窗口时间(buffer_time)有重叠的,则会出现<code>already seen</code>, 比如run_every为3分钟, buffer_time为5分钟, 则每3分钟查询5分钟的es, 待下次查询时还查5分钟内的文档, 则前2分钟在上一个执行周期内已经查询过了, 因此<code>already seen</code>就类似于2分钟内有108个文档.</p>
<p>如果在rule配置文件中配置了realert,比如2分钟, 则会在日志中可能看到Ignoring match，这些都是被丢弃的alert, realert的意思是同一类的match产生的报警在2分钟之内不会重复发送，直接被丢弃</p>
<p>如果设置了realert&#x3D;0,则每个match都会产生alert.这个要注意告警风暴.</p>
<h3 id="match-body"><a href="#match-body" class="headerlink" title="match_body"></a>match_body</h3><p><code>match_body</code>这个字典是最重要的结构体，当查询到符合报警的文档时，会复制一份到elastalert的索引中, 内容如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_index&quot;:</span> <span class="string">&quot;demo.elastalert_status&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_type&quot;:</span> <span class="string">&quot;elastalert&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_id&quot;:</span> <span class="string">&quot;AXJfrrKDPz_v1Z2rKLI3&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_score&quot;:</span> <span class="literal">null</span>,</span><br><span class="line">  <span class="attr">&quot;_source&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;match_body&quot;:</span> &#123;  <span class="comment"># 从原日志复制而来</span></span><br><span class="line">     <span class="comment"># 以下是原日志中的内容</span></span><br><span class="line">      <span class="attr">&quot;msg&quot;:</span> <span class="string">&quot;\&quot;unaryInterceptor done\&quot;&quot;</span>,</span><br><span class="line">     <span class="comment"># ...</span></span><br><span class="line">    <span class="attr">&quot;rule_name&quot;:</span> <span class="string">&quot;index realty find invalid keyword&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;alert_info&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;email&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;recipients&quot;:</span> [</span><br><span class="line">        <span class="string">&quot;demo@top.com&quot;</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;alert_sent&quot;:</span> <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;alert_time&quot;:</span> <span class="string">&quot;2020-05-29T09:06:22.194284Z&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;match_time&quot;:</span> <span class="string">&quot;2020-05-29T09:02:54.540Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;@timestamp&quot;</span><span class="string">:</span> <span class="string">&quot;2020-05-29T09:06:23.997639Z&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>elastalert索引中，<code>hits</code>表示规则命中条数；<code>matches</code>表示规则命中条数，并且匹配规则触发告警数量</strong></p>
<p>在上面的rule配置文件中<code>alert_text_args</code>与<code>alert_subject_args</code>指定的参数都可以直接使用match_body结构体中的字段. 嵌套的使用点(.)来引用</p>
<h3 id="num-hits-vs-num-matches"><a href="#num-hits-vs-num-matches" class="headerlink" title="num_hits vs num_matches"></a>num_hits vs num_matches</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">num_hits is the number of documents returned by elasticsearch <span class="keyword">for</span> a given query. num_matches is how many <span class="built_in">times</span> that data matched your rule, each one potentially generating an alert.</span><br><span class="line"></span><br><span class="line">If it makes a query over a 10 minute range and gets 10 hits, and you have</span><br><span class="line"></span><br><span class="line"><span class="built_in">type</span>: frequency</span><br><span class="line">num_events: 10</span><br><span class="line">timeframe:</span><br><span class="line">  minutes: 10</span><br><span class="line"><span class="keyword">then</span> you<span class="string">&#x27;ll get 1 match.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">总结:</span></span><br><span class="line"><span class="string">num_hits表示的是根据filter条件及查询时间段从es返回的记录</span></span><br><span class="line"><span class="string">而num_matches表示的是预计会产生多少条报警</span></span><br><span class="line"><span class="string">因此 num_matches = num_hits / num_events </span></span><br><span class="line"><span class="string">四舍五入</span></span><br></pre></td></tr></table></figure>



<h3 id="enhancements"><a href="#enhancements" class="headerlink" title="enhancements"></a><a href="https://elastalert.readthedocs.io/en/latest/recipes/adding_enhancements.html#enhancements">enhancements</a></h3><p>如果觉得最终返回的数据不符合要求或者需要添加自定义的字段, 那么可以在发送给告警器之前对match_body进行修改, 这就需要用到<code>enhancements</code>功能</p>
<p>比如需要对时间格式进行调整, 那么可以这样使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> arrow</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TimeEnhancement</span>(<span class="title class_ inherited__">BaseEnhancement</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self, <span class="keyword">match</span></span>):</span><br><span class="line">        tz = <span class="string">&#x27;Asia/Shanghai&#x27;</span></span><br><span class="line">        tf = <span class="string">&#x27;YYYY-MM-DD HH:mm:ss&#x27;</span></span><br><span class="line">        <span class="comment"># starttime: 2020-05-29 04:21:17.353831+00:00 ,8h</span></span><br><span class="line">        query_start = arrow.get(self.rule.get(<span class="string">&#x27;starttime&#x27;</span>)).to(tz).<span class="built_in">format</span>(tf)</span><br><span class="line">        query_end = arrow.now(tz).<span class="built_in">format</span>(tf)</span><br><span class="line">        tt = arrow.get(<span class="keyword">match</span>[<span class="string">&#x27;@timestamp&#x27;</span>]).to(tz).<span class="built_in">format</span>(tf)</span><br><span class="line">        <span class="keyword">match</span>[<span class="string">&#x27;query_start&#x27;</span>] = query_start</span><br><span class="line">        <span class="keyword">match</span>[<span class="string">&#x27;query_end&#x27;</span>] = query_end</span><br></pre></td></tr></table></figure>

<p>然后在rule的配置文件中指定:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">match_enhancements:</span> <span class="comment"># 指定使用增强功能</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;elastalert.enhancements.TimeEnhancement&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果出现以下错误<code>failed to parse [match_time]</code>:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200529152207.png"></p>
<p>原因: 这是由于enhancements.py中TimeEnhancement中改变了<code>@timestamp</code>的格式，从而使得<code>match_time</code>不符合索引中的格式</p>
<p>解决: 修改或者去掉enhancements.py默认的以下内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">match</span>[<span class="string">&#x27;@timestamp&#x27;</span>] = pretty_ts(<span class="keyword">match</span>[<span class="string">&#x27;@timestamp&#x27;</span>]</span><br></pre></td></tr></table></figure>



<h3 id="告警内容"><a href="#告警内容" class="headerlink" title="告警内容"></a>告警内容</h3><p>对于不同的rule, 生成的告警内容不太一样, 但是都是由<strong>汇总信息+自定义内容组成</strong>,对于frequency类型来说，内容如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Ref Log http://192.168.115.65</span><br><span class="line"></span><br><span class="line">At least 5 events occurred between 2020-05-26 09:18 UTC and 2020-05-26 09:23 UTC</span><br><span class="line"></span><br><span class="line"><span class="comment">#...这里是原始日志内容</span></span><br><span class="line"><span class="comment"># 省略</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">num_hits: 318</span><br><span class="line">num_matches: 6</span><br></pre></td></tr></table></figure>

<p>对于其它类型的告警内容可参考官网</p>
<p>下次跟大家分享下elastalert的源码.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.cnblogs.com/duanxz/p/11859307.html">https://www.cnblogs.com/duanxz/p/11859307.html</a></li>
<li><a href="https://www.freebuf.com/sectool/164591.html">https://www.freebuf.com/sectool/164591.html</a></li>
<li><a href="https://www.jianshu.com/p/f82812e0a743">https://www.jianshu.com/p/f82812e0a743</a></li>
<li><a href="https://github.com/Yelp/elastalert/issues/2754">https://github.com/Yelp/elastalert/issues/2754</a></li>
<li><a href="https://segmentfault.com/a/1190000017553282">https://segmentfault.com/a/1190000017553282</a></li>
<li><a href="https://blog.xizhibei.me/2017/11/19/alerting-with-elastalert/">https://blog.xizhibei.me/2017/11/19/alerting-with-elastalert/</a></li>
<li><a href="https://github.com/Yelp/elastalert/issues/1737">https://github.com/Yelp/elastalert/issues/1737</a></li>
<li><a href="https://elastalert.readthedocs.io/en/latest/elastalert_status.html#elastalert-status">https://elastalert.readthedocs.io/en/latest/elastalert_status.html#elastalert-status</a></li>
<li><a href="https://elastalert.readthedocs.io/en/latest/recipes/adding_enhancements.html#enhancements">https://elastalert.readthedocs.io/en/latest/recipes/adding_enhancements.html#enhancements</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>CockRoachDB学习(无证书集群搭建)</title>
    <url>/2019/05/20/CockroachDB-Cluster-Build-No-CA/</url>
    <content><![CDATA[<p>3节点的无证书CockroachDB 集群搭建, 由于CRDB只有一个单二进制文件, 所以部署也非常简单.</p>
<span id="more"></span>

<h3 id="物理机部署"><a href="#物理机部署" class="headerlink" title="物理机部署"></a>物理机部署</h3><p>将cockroach二进制包上传至服务器的目录&#x2F;opt， 解压</p>
<p><code>tar zxf cockroach-v19.1.1.linux-amd64.tar.gz -C /opt</code></p>
<p><code>cd /opt</code></p>
<p><code>mv  cockroach-v19.1.1.linux-amd64 cockroach</code></p>
<p><code>cd cockroach</code></p>
<p>启动cockroach第一个节点</p>
<p><code>nohup /opt/cockroach/cockroach start --insecure --store=node --host=172.16.104.19 --port=26257 --http-port=18080 &amp;</code></p>
<p>查看状态</p>
<p><code>./cockroach node status --insecure --host=172.16.104.19</code></p>
<p>重复以上命令依次重启第二个,第三个节点，启动时使用–join加入第一个节点</p>
<p><code>nohup /opt/cockroach/cockroach start --insecure --store=node --host=172.16.104.20 --port=26257 --http-port=18080 --join 172.16.104.19 &amp;</code></p>
<p><code>nohup /opt/cockroach/cockroach start --insecure --store=node --host=172.16.104.21 --port=26257 --http-port=18080 --join 172.16.104.19 &amp;</code></p>
<p>查看集群信息, 3个节点已经组成了一个集群</p>
<p>测试集群</p>
<p><code>./cockroach node status --insecure --host=172.16.104.19</code></p>
<h3 id="Docker部署"><a href="#Docker部署" class="headerlink" title="Docker部署"></a>Docker部署</h3><p>第一个节点</p>
<p><code>docker run -d --name=roach1 --hostname=roach1 -p 26257:26257 -p 8080:8080  -v &quot;$&#123;PWD&#125;/cockroach-data/roach1:/cockroach/cockroach-data&quot;  cockroachdb/cockroach:v2.0.5 start --insecure</code></p>
<p>上述命令创建了一个容器，并在其中启动了第一个CockroachDB集群节点。 涉及指令和参数的介绍：</p>
<ul>
<li><code>docker run</code>：用Docker命令启动一个容器</li>
<li><code>-d</code>： 指定容器运行在后台</li>
<li><code>--name</code>：指定容器的名字</li>
<li><code>--hostname</code>：指定容器的主机名，和<code>name</code>不同，这个是其他容器加入其所在集群的标签。</li>
<li><code>-p  26257:26257 -p 8080:8080</code>：指定内部节点和客户端节点的通信端口（26257）映射以及Admin界面从容器到主机的HTTP访问端口映射。</li>
<li><code>-v &quot;$&#123;PWD&#125;/cockroach-data/roach1:/cockroach/cockroach-data&quot;</code>： 挂载一个主机目录作为容器的磁盘，意味着数据和日志将会存在<code>$&#123;PWD&#125;/cockroach-data/roach1</code></li>
<li><code>cockroachdb/cockroach:v2.0.5 start --insecure</code> ：启动CockroachDB节点命令</li>
</ul>
<p>添加节点二</p>
<p><code>docker run -d --name=roach2 --hostname=roach2 -v &quot;$&#123;PWD&#125;/cockroach-data/roach2:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v2.0.5 start --insecure --join=roach1</code></p>
<p>添加节点三</p>
<p><code>docker run -d --name=roach3 --hostname=roach3 -v &quot;$&#123;PWD&#125;/cockroach-data/roach3:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v2.0.5 start --insecure --join=roach1</code></p>
<p>测试集群</p>
<p><code>docker exec -it roach1 ./cockroach sql --insecure</code></p>
<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><p>登录cockroach, 查看当前存在的数据库</p>
<p><code>./cockroach sql --insecure --host=172.16.104.19 --port=26257</code></p>
<p>新建数据库</p>
<p><code>CREATE database unicorn;</code></p>
<p>导入数据库表结构</p>
<p><code>./cockroach sql --insecure --host=172.16.104.19 --port=26257 --database=unicron &lt;unicron.sql</code></p>
<p>查看用户信息</p>
<p><code>./cockroach --insecure --host=172.16.104.19 --port=26257 user get root</code></p>
<p>停止一个节点</p>
<p><code>./cockroach quit --insecure --host=172.16.104.19</code></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://doc.cockroachchina.baidu.com/#maintain/remove-nodes/">http://doc.cockroachchina.baidu.com/#maintain/remove-nodes/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(Elasticsearch索引管理工具Curator)</title>
    <url>/2020/02/10/FELK-Curator/</url>
    <content><![CDATA[<p>当使用elasticsearch做为日志系统的后端存储，时间一长的话日志量也是个很可观的数字, 因此需要定时清除，之前一直都是自己写脚本做管理,  后面需要维护的索引越来越多后觉得力不从心, 而curator做为es官方推出的索引管理工具, 使用起来还是很方便的</p>
<span id="more"></span>



<p>curator的github在<a href="https://github.com/elastic/curator">这里</a></p>
<h3 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h3><p>curator是使用python写的工具, 可以直接使用pip安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install elasticsearch-curator</span><br></pre></td></tr></table></figure>

<p>当然 ，在也使用yum 安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install elasticsearch-curator</span><br></pre></td></tr></table></figure>



<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>curator的配置文件非常简单, 典型的配置如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">client:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.98</span><span class="number">.6</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">9200</span></span><br><span class="line">  <span class="attr">url_prefix:</span></span><br><span class="line">  <span class="attr">use_ssl:</span> <span class="literal">False</span></span><br><span class="line">  <span class="attr">certificate:</span></span><br><span class="line">  <span class="attr">client_cert:</span></span><br><span class="line">  <span class="attr">client_key:</span></span><br><span class="line">  <span class="attr">ssl_no_validate:</span> <span class="literal">False</span></span><br><span class="line">  <span class="attr">http_auth:</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">master_only:</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="attr">logging:</span></span><br><span class="line">  <span class="attr">loglevel:</span> <span class="string">INFO</span></span><br><span class="line">  <span class="attr">logfile:</span> <span class="string">/var/log/curator.log</span></span><br><span class="line">  <span class="attr">logformat:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">blacklist:</span> [<span class="string">&#x27;elasticsearch&#x27;</span>, <span class="string">&#x27;urllib3&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>注意: 配置文件中如果某个key的value值为空, 直接留空即可, 不能写None(会被当成字符串)</p>
<p>配置文件中指定es集群中某个节点的地址即可, 关键是需要定义action配置文件, 即执行的动作</p>
<h3 id="Action文件"><a href="#Action文件" class="headerlink" title="Action文件"></a>Action文件</h3><p>官网的github上列出了一些常用的action供参考, 详情请参考<a href="https://github.com/elastic/curator/tree/master/examples/actions">这里</a></p>
<p>典型的action配置文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">actions:</span></span><br><span class="line">  <span class="attr">1:</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">delete_indices</span></span><br><span class="line">    <span class="attr">description:</span> <span class="string">&gt;-</span></span><br><span class="line"><span class="string">      prefixed indices. Ignore the error if the filter does not result in an</span></span><br><span class="line"><span class="string">      actionable list of indices (ignore_empty_list) and exit cleanly.</span></span><br><span class="line"><span class="string"></span>    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">ignore_empty_list:</span> <span class="literal">True</span></span><br><span class="line">    <span class="attr">filters:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">filtertype:</span> <span class="string">pattern</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">regex</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&#x27;^(c|alert|d|rancher|s|web|container).*$&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">filtertype:</span> <span class="string">age</span></span><br><span class="line">      <span class="attr">source:</span> <span class="string">name</span></span><br><span class="line">      <span class="attr">direction:</span> <span class="string">older</span></span><br><span class="line">      <span class="attr">timestring:</span> <span class="string">&#x27;%Y.%m.%d&#x27;</span></span><br><span class="line">      <span class="attr">unit:</span> <span class="string">days</span></span><br><span class="line">      <span class="attr">unit_count:</span> <span class="number">60</span></span><br><span class="line">  <span class="attr">2:</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">close</span></span><br><span class="line">    <span class="attr">description:</span> <span class="string">&gt;-</span></span><br><span class="line"><span class="string">      prefixed indices. Ignore the error if the filter does not result in an</span></span><br><span class="line"><span class="string">      actionable list of indices (ignore_empty_list) and exit cleanly.</span></span><br><span class="line"><span class="string"></span>    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">ignore_empty_list:</span> <span class="literal">True</span></span><br><span class="line">    <span class="attr">filters:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">filtertype:</span> <span class="string">pattern</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">regex</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&#x27;^(c|alert|d|rancher|s|web|container).*$&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">filtertype:</span> <span class="string">age</span></span><br><span class="line">      <span class="attr">source:</span> <span class="string">name</span></span><br><span class="line">      <span class="attr">direction:</span> <span class="string">older</span></span><br><span class="line">      <span class="attr">timestring:</span> <span class="string">&#x27;%Y.%m.%d&#x27;</span></span><br><span class="line">      <span class="attr">unit:</span> <span class="string">days</span></span><br><span class="line">      <span class="attr">unit_count:</span> <span class="number">30</span></span><br></pre></td></tr></table></figure>

<p>说明:</p>
<p>action中是以字数编码指定action的个数的</p>
<p>第1个action的作用为:  删除60天之前的索引， 使用正则表达式指定了索引的范围</p>
<p>第2个action的作用为: 关闭30天之前的索引, 使用正则表达式指定了索引的范围</p>
<p>当然,action还有其它的选项，可参考github.</p>
<h3 id="定期执行"><a href="#定期执行" class="headerlink" title="定期执行"></a>定期执行</h3><p>定义好config及action文件后，即可加入crontab定期执行.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0 2 * * * /usr/bin/curator --config /data/es/curator/config.yml  /data/es/curator/action.yml</span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/elastic/curator">https://github.com/elastic/curator</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(elastalert源码分析)</title>
    <url>/2020/06/02/EFLK-elastalert-sourcecode/</url>
    <content><![CDATA[<p>在调研elastalert时几乎把整个源码都看了一遍，记录一下几个重要的部分.</p>
<span id="more"></span>



<h3 id="核心查询"><a href="#核心查询" class="headerlink" title="核心查询"></a>核心查询</h3><p>rule文件配置的filter，最终都会转换成es底层支持的格式, 核心代码如下:</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_query</span>(<span class="params">filters, starttime=<span class="literal">None</span>, endtime=<span class="literal">None</span>, sort=<span class="literal">True</span>, timestamp_field=<span class="string">&#x27;@timestamp&#x27;</span>, to_ts_func=dt_to_ts, desc=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">              five=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Returns a query dict that will apply a list of filters, filter by</span></span><br><span class="line"><span class="string">    start and end time, and sort results by timestamp.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param filters: A list of Elasticsearch filters to use.</span></span><br><span class="line"><span class="string">    :param starttime: A timestamp to use as the start time of the query.</span></span><br><span class="line"><span class="string">    :param endtime: A timestamp to use as the end time of the query.</span></span><br><span class="line"><span class="string">    :param sort: If true, sort results by timestamp. (Default True)</span></span><br><span class="line"><span class="string">    :return: A query dictionary to pass to Elasticsearch.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    starttime = to_ts_func(starttime)</span><br><span class="line">    endtime = to_ts_func(endtime)</span><br><span class="line">    filters = copy.copy(filters)</span><br><span class="line">    es_filters = &#123;<span class="string">&#x27;filter&#x27;</span>: &#123;<span class="string">&#x27;bool&#x27;</span>: &#123;<span class="string">&#x27;must&#x27;</span>: filters&#125;&#125;&#125;</span><br><span class="line">    <span class="keyword">if</span> starttime <span class="keyword">and</span> endtime:</span><br><span class="line">        es_filters[<span class="string">&#x27;filter&#x27;</span>][<span class="string">&#x27;bool&#x27;</span>][<span class="string">&#x27;must&#x27;</span>].insert(<span class="number">0</span>, &#123;<span class="string">&#x27;range&#x27;</span>: &#123;timestamp_field: &#123;<span class="string">&#x27;gt&#x27;</span>: starttime,</span><br><span class="line">                                                                                    <span class="string">&#x27;lte&#x27;</span>: endtime&#125;&#125;&#125;)</span><br><span class="line">    <span class="keyword">if</span> five:</span><br><span class="line">        query = &#123;<span class="string">&#x27;query&#x27;</span>: &#123;<span class="string">&#x27;bool&#x27;</span>: es_filters&#125;&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        query = &#123;<span class="string">&#x27;query&#x27;</span>: &#123;<span class="string">&#x27;filtered&#x27;</span>: es_filters&#125;&#125;</span><br><span class="line">    <span class="keyword">if</span> sort:</span><br><span class="line">        query[<span class="string">&#x27;sort&#x27;</span>] = [&#123;timestamp_field: &#123;<span class="string">&#x27;order&#x27;</span>: <span class="string">&#x27;desc&#x27;</span> <span class="keyword">if</span> desc <span class="keyword">else</span> <span class="string">&#x27;asc&#x27;</span>&#125;&#125;]</span><br><span class="line">    <span class="keyword">return</span> quer</span><br></pre></td></tr></table></figure>

<p>最终转换成的格式如下, 如果查询的结果与期望值不一样，也可以生产的此语句进行排查.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> -XGET <span class="string">&#x27;http://localhost:9200/demo.demo-*/_search?pretty&amp;_source_include=%40timestamp%2C%2A&amp;ignore_unavailable=true&amp;scroll=30s&amp;size=10000&#x27;</span> -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;query&quot;: &#123;</span></span><br><span class="line"><span class="string">    &quot;bool&quot;: &#123;</span></span><br><span class="line"><span class="string">      &quot;filter&quot;: &#123;                             </span></span><br><span class="line"><span class="string">        &quot;bool&quot;: &#123;                                                   </span></span><br><span class="line"><span class="string">          &quot;must&quot;: [            &#123;                                                                                </span></span><br><span class="line"><span class="string">              &quot;range&quot;: &#123;                                                                                                                                                              </span></span><br><span class="line"><span class="string">                &quot;@timestamp&quot;: &#123;                                                                                           </span></span><br><span class="line"><span class="string">                  &quot;gt&quot;: &quot;2020-05-29T11:04:00.198218Z&quot;,</span></span><br><span class="line"><span class="string">                  &quot;lte&quot;: &quot;2020-05-29T11:09:00.198218Z&quot;                                                                                                                   </span></span><br><span class="line"><span class="string">                &#125;                                          </span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">            &#125;,                                                 </span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;wildcard&quot;: &#123; # 这里是根据rule文件中的filter条件.                                                                                </span></span><br><span class="line"><span class="string">                &quot;backendMod&quot;: &quot;*senseface*&quot;     </span></span><br><span class="line"><span class="string">              &#125;              </span></span><br><span class="line"><span class="string">            &#125;                                    </span></span><br><span class="line"><span class="string">          ]                                                                                       </span></span><br><span class="line"><span class="string">        &#125;                                                             </span></span><br><span class="line"><span class="string">      &#125;                                 </span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  &#125;,</span></span><br><span class="line"><span class="string">  &quot;sort&quot;: [</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;@timestamp&quot;: &#123;                 </span></span><br><span class="line"><span class="string">        &quot;order&quot;: &quot;asc&quot;                                         </span></span><br><span class="line"><span class="string">      &#125;                                                                                             </span></span><br><span class="line"><span class="string">    &#125;                                                                                        </span></span><br><span class="line"><span class="string">  ]                                                                                     </span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="match-body"><a href="#match-body" class="headerlink" title="match_body"></a>match_body</h3><h4 id="num-hits-vs-num-matches"><a href="#num-hits-vs-num-matches" class="headerlink" title="num_hits vs num_matches"></a>num_hits vs num_matches</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">num_hits is the number of documents returned by elasticsearch <span class="keyword">for</span> a given query. num_matches is how many <span class="built_in">times</span> that data matched your rule, each one potentially generating an alert.</span><br><span class="line"></span><br><span class="line">If it makes a query over a 10 minute range and gets 10 hits, and you have</span><br><span class="line"></span><br><span class="line"><span class="built_in">type</span>: frequency</span><br><span class="line">num_events: 10</span><br><span class="line">timeframe:</span><br><span class="line">  minutes: 10</span><br><span class="line"><span class="keyword">then</span> you<span class="string">&#x27;ll get 1 match.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">总结:</span></span><br><span class="line"><span class="string">num_hits表示的是根据filter条件及查询时间段从es返回的记录,而num_matches表示的是预计会产生多少条报警</span></span><br><span class="line"><span class="string">因此 num_matches = num_hits / num_events, 会四舍五入,所以在告警内容中会发现两者都是这样的关系</span></span><br></pre></td></tr></table></figure>

<h3 id="写回到es索引-elastalert-status"><a href="#写回到es索引-elastalert-status" class="headerlink" title="写回到es索引:elastalert_status"></a>写回到es索引:elastalert_status</h3><p>同样，产生的告警的现场日志在发送到告警介质后都会写入到es索引中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">send_alert</span>(<span class="params">self, matches, rule, alert_time=<span class="literal">None</span>, retried=<span class="literal">False</span></span>):</span><br><span class="line">	<span class="comment"># ...</span></span><br><span class="line">      <span class="keyword">for</span> alert <span class="keyword">in</span> rule[<span class="string">&#x27;alert&#x27;</span>]:</span><br><span class="line">            alert.pipeline = alert_pipeline</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                alert.alert(matches)</span><br><span class="line">            <span class="keyword">except</span> EAException <span class="keyword">as</span> e:</span><br><span class="line">                self.handle_error(<span class="string">&#x27;Error while running alert %s: %s&#x27;</span> % (alert.get_info()[<span class="string">&#x27;type&#x27;</span>], e), &#123;<span class="string">&#x27;rule&#x27;</span>: rule[<span class="string">&#x27;name&#x27;</span>]&#125;)</span><br><span class="line">                alert_exception = <span class="built_in">str</span>(e)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.thread_data.alerts_sent += <span class="number">1</span></span><br><span class="line">                alert_sent = <span class="literal">True</span></span><br><span class="line">      <span class="comment"># ...</span></span><br><span class="line">      <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">            alert_body = self.get_alert_body(<span class="keyword">match</span>, rule, alert_sent, alert_time, alert_exception)</span><br><span class="line">            <span class="comment"># Set all matches to aggregate together</span></span><br><span class="line">            <span class="keyword">if</span> agg_id:</span><br><span class="line">                alert_body[<span class="string">&#x27;aggregate_id&#x27;</span>] = agg_id</span><br><span class="line">            res = self.writeback(<span class="string">&#x27;elastalert&#x27;</span>, alert_body, rule)</span><br><span class="line">            <span class="keyword">if</span> res <span class="keyword">and</span> <span class="keyword">not</span> agg_id:</span><br><span class="line">                agg_id = res[<span class="string">&#x27;_id&#x27;</span>]</span><br></pre></td></tr></table></figure>



<h3 id="starttime-x2F-endtime-x2F-original-time"><a href="#starttime-x2F-endtime-x2F-original-time" class="headerlink" title="starttime&#x2F;endtime&#x2F;original_time"></a>starttime&#x2F;endtime&#x2F;original_time</h3><p>endtime一般都是执行查询时取的当前时间，因此elastalert默认没有保存，starttime一般是由endtime减去timeframe计算而来，但是如果elastalert重启的话, starttime会跟期望的不太一样，通过源码可以看到还有一个original_time, original_time为存在在es中的上一次执行的endtime, 因此，每次查询的时候，都会从original_time处开始查询, endtime为当前时间</p>
<p>因此starttime最好设置为original_time，使用starttime有产生歧义</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">body = &#123;<span class="string">&#x27;rule_name&#x27;</span>: rule[<span class="string">&#x27;name&#x27;</span>],</span><br><span class="line">               <span class="string">&#x27;endtime&#x27;</span>: endtime,</span><br><span class="line">               <span class="string">&#x27;starttime&#x27;</span>: rule[<span class="string">&#x27;original_starttime&#x27;</span>],</span><br><span class="line">               <span class="string">&#x27;matches&#x27;</span>: num_matches,</span><br><span class="line">               <span class="string">&#x27;hits&#x27;</span>: max(self.thread_data.num_hits, self.thread_data.cumulative_hits),</span><br><span class="line">               <span class="string">&#x27;@timestamp&#x27;</span>: ts_now(),</span><br><span class="line">               <span class="string">&#x27;time_taken&#x27;</span>: time_taken&#125;</span><br><span class="line">       self.writeback(<span class="string">&#x27;elastalert_status&#x27;</span>, body)</span><br></pre></td></tr></table></figure>

<p>当然为了避免elastalert重启时离上次运行的时间间隔太久, 默认情况下启动后会从es中读取<code>starttime</code>,以这个时间点到当前时间为时间窗口，然后以run_every给定的时间进行查询,所以，在这种情况下可能会出现刚重启时就出现大量的查询，很多时间太久的历史日志监控是无意义，因此可以在elastalert.yaml中指定<code>old_query_limit: minutes: 1</code>来限定starttime为当前时间的前一分钟.</p>
<h3 id="enhancement"><a href="#enhancement" class="headerlink" title="enhancement"></a>enhancement</h3><p>elastalert提供两种方案可以让在找到匹配项后立即运行增强功能(run_enhancements_first)或者在发送告警前进行操作,比如修改match操作(match_enhancements)</p>
<h4 id="run-enhancements-first"><a href="#run-enhancements-first" class="headerlink" title="run_enhancements_first"></a>run_enhancements_first</h4><p>如果在rule配置中指定的了run_enhancements_first，源码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_rule</span>(<span class="params">self, rule, endtime, starttime=<span class="literal">None</span></span>):</span><br><span class="line">	<span class="comment"># ...</span></span><br><span class="line">  self.run_query()</span><br><span class="line">  <span class="keyword">if</span> rule.get(<span class="string">&#x27;run_enhancements_first&#x27;</span>):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">for</span> enhancement <span class="keyword">in</span> rule[<span class="string">&#x27;match_enhancements&#x27;</span>]:</span><br><span class="line">                        <span class="keyword">try</span>:</span><br><span class="line">                            enhancement.process(<span class="keyword">match</span>) <span class="comment"># point</span></span><br><span class="line">                        <span class="keyword">except</span> EAException <span class="keyword">as</span> e:</span><br><span class="line">                            self.handle_error(<span class="string">&quot;Error running match enhancement: %s&quot;</span> % (e), &#123;<span class="string">&#x27;rule&#x27;</span>: rule[<span class="string">&#x27;name&#x27;</span>]&#125;)</span><br><span class="line">                <span class="keyword">except</span> DropMatchException:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<h4 id="match-enhancements"><a href="#match-enhancements" class="headerlink" title="match_enhancements"></a>match_enhancements</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">send_alert</span>(<span class="params">self, matches, rule, alert_time=<span class="literal">None</span>, retried=<span class="literal">False</span></span>):</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">	 <span class="keyword">if</span> <span class="keyword">not</span> rule.get(<span class="string">&#x27;run_enhancements_first&#x27;</span>) <span class="keyword">and</span> <span class="keyword">not</span> retried:</span><br><span class="line">            <span class="keyword">for</span> enhancement <span class="keyword">in</span> rule[<span class="string">&#x27;match_enhancements&#x27;</span>]:</span><br><span class="line">                valid_matches = []</span><br><span class="line">                <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        enhancement.process(<span class="keyword">match</span>) <span class="comment"># point</span></span><br><span class="line">                        valid_matches.append(<span class="keyword">match</span>)</span><br><span class="line">                    <span class="keyword">except</span> DropMatchException:</span><br><span class="line">                        <span class="keyword">pass</span></span><br><span class="line">                    <span class="keyword">except</span> EAException <span class="keyword">as</span> e:</span><br><span class="line">                        self.handle_error(<span class="string">&quot;Error running match enhancement: %s&quot;</span> % (e), &#123;<span class="string">&#x27;rule&#x27;</span>: rule[<span class="string">&#x27;name&#x27;</span>]&#125;)</span><br><span class="line">                matches = valid_matches</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> matches:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    alert.alert(matches)</span><br></pre></td></tr></table></figure>

<p>很多人关心的elastalert没有保存的endtime字段，其实就可以match_enhancements中添加字段.</p>
<h3 id="自定义告警"><a href="#自定义告警" class="headerlink" title="自定义告警"></a>自定义告警</h3><h4 id="自定义邮件格式"><a href="#自定义邮件格式" class="headerlink" title="自定义邮件格式"></a>自定义邮件格式</h4><p>由于默认的邮件告警只是简单的把str格式的内容通过邮件发送出去,非常不美观，因此通过jinja2的形式自定义了邮件格式</p>
<p>首先在rule配置中定义<code>email_format: html</code></p>
<p>由于只是在发送邮件的时候才使用jinja2的html模板，因此需要对<code>alert</code>的类型进行判断</p>
<p>修改的源码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicMatchString</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Creates a string containing fields in match for the given rule. &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rule, <span class="keyword">match</span>, alert_type</span>): <span class="comment"># 新增一个alert_type，为告警类型</span></span><br><span class="line">        self.rule = rule</span><br><span class="line">        self.<span class="keyword">match</span> = <span class="keyword">match</span></span><br><span class="line">        self.alert_type = alert_type</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">render_html</span>(<span class="params">summary_str, data</span>):</span><br><span class="line">        env = Environment(loader=FileSystemLoader(</span><br><span class="line">            os.path.join(os.path.dirname(__file__), <span class="string">&quot;templates&quot;</span>)))</span><br><span class="line">        template = env.get_template(<span class="string">&#x27;mail.html.tpl&#x27;</span>)</span><br><span class="line">        out = template.render(summary_str=summary_str, data=data)</span><br><span class="line">        <span class="comment">#with open(&#x27;mail.html&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:</span></span><br><span class="line">        <span class="comment">#   f.write(out)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">_add_custom_alert_text</span>(<span class="params">self</span>):</span><br><span class="line">        missing = self.rule.get(<span class="string">&#x27;alert_missing_value&#x27;</span>, <span class="string">&#x27;&lt;MISSING VALUE&gt;&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;alert_text_args&#x27;</span> <span class="keyword">in</span> self.rule:</span><br><span class="line">            alert_text_args = self.rule.get(<span class="string">&#x27;alert_text_args&#x27;</span>)</span><br><span class="line">            <span class="comment"># 判断告警的类型</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;email&#x27;</span> == self.alert_type:</span><br><span class="line">                summary_str = self.rule[<span class="string">&#x27;type&#x27;</span>].get_match_str(self.<span class="keyword">match</span>)</span><br><span class="line">                alert_text = <span class="built_in">str</span>(self.render_html(summary_str, alert_text_args))</span><br><span class="line">            <span class="comment"># 如果不是email则保存跟源码一致</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                alert_text = <span class="built_in">str</span>(self.rule.get(<span class="string">&#x27;alert_text&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">            <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<p>由于所有的告警类型的基类都是<code>Alerter</code>,因此通过这个来传递告警类型.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Alerter</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">create_alert_body</span>(<span class="params">self, matches</span>):</span><br><span class="line">        alert_type = self.get_info()[<span class="string">&#x27;type&#x27;</span>] <span class="comment"># 通过get_info 获取alerter实例的type</span></span><br><span class="line">        body = self.get_aggregation_summary_text(matches)</span><br><span class="line">        <span class="keyword">if</span> self.rule.get(<span class="string">&#x27;alert_text_type&#x27;</span>) != <span class="string">&#x27;aggregation_summary_only&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">                body += <span class="built_in">str</span>(BasicMatchString(self.rule, <span class="keyword">match</span>, alert_type)) <span class="comment"># 传入BasicMatchString</span></span><br><span class="line">                <span class="comment"># Separate text of aggregated alerts with dashes</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(matches) &gt; <span class="number">1</span>:</span><br><span class="line">                    body += <span class="string">&#x27;\n----------------------------------------\n&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> body</span><br></pre></td></tr></table></figure>

<p>这样就完成自定义邮件格式了,模板如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200602165705.png"></p>
<p>字段直接在rule文件中alert_text_args中传入即可.</p>
<h4 id="微信报警"><a href="#微信报警" class="headerlink" title="微信报警"></a>微信报警</h4><p>elastalert本身支持自定义的告警接入，只需要实现<code> body = self.create_alert_body(matches)</code>方法即可.不难，难点在于企业微信api如何使用, 代码参考<a href="https://anjia0532.github.io/2017/02/16/elastalert-wechat-plugin/">elastalert-wechat-plugin</a>, 亲测可用.</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>如果修改源码后rule执行出现问题, 则会自动地生成exception写入到索引中且该rule会被disable掉,直到不再报错之前都不再执行.</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200601111948.png"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">03:18:59.552Z ERROR elastalert-server:</span><br><span class="line">    ProcessController:  INFO:elastalert:Disabled rules are: [<span class="string">&#x27;index realty find invalid keyword&#x27;</span>]</span><br></pre></td></tr></table></figure>



<ol start="2">
<li>es中的until字段记录了下次执行的时候(由apscheduler框架支持)</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_index&quot;:</span> <span class="string">&quot;demo.elastalert_status&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_type&quot;:</span> <span class="string">&quot;silence&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_id&quot;:</span> <span class="string">&quot;AXJt-tKUPz_v1Z2rFGRX&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_score&quot;:</span> <span class="literal">null</span>,</span><br><span class="line">  <span class="attr">&quot;_source&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;exponent&quot;:</span> <span class="number">0</span>,</span><br><span class="line">    <span class="attr">&quot;rule_name&quot;:</span> <span class="string">&quot;index realty find invalid keyword.senseface&quot;</span>,</span><br><span class="line">    <span class="string">&quot;@timestamp&quot;</span><span class="string">:</span> <span class="string">&quot;2020-06-01T03:44:13.967765Z&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;until&quot;:</span> <span class="string">&quot;2020-06-01T03:49:13.967756Z&quot;</span> <span class="comment"># here</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;fields&quot;:</span> &#123;</span><br><span class="line">    <span class="string">&quot;@timestamp&quot;</span><span class="string">:</span> [</span><br><span class="line">      <span class="number">1590983053967</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;until&quot;:</span> [</span><br><span class="line">      <span class="number">1590983353967</span></span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;sort&quot;:</span> [</span><br><span class="line">    <span class="number">1590983053967</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/Yelp/elastalert/issues/2754">https://github.com/Yelp/elastalert/issues/2754</a></li>
<li><a href="https://anjia0532.github.io/2017/02/16/elastalert-wechat-plugin/">https://anjia0532.github.io/2017/02/16/elastalert-wechat-plugin/</a></li>
<li><a href="https://work.weixin.qq.com/api/doc">https://work.weixin.qq.com/api/doc</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次排查ES FGC导致的集群宕机异常</title>
    <url>/2019/10/06/ES-FGC-Fix/</url>
    <content><![CDATA[<p>有一个私有云环境的运维同事反应项目中的ES集群无法查询, 由于是私有云环境, 没有直接运维权限, 所以没有接收到一线报警, 远程登录上去后发现ES集群已经宕机了, 这次的排查过程也发现了几个比较容易忽视的细节, 在此记录一下,比较有意思.</p>
<span id="more"></span>



<h3 id="集群状态"><a href="#集群状态" class="headerlink" title="集群状态"></a>集群状态</h3><p>ES集群为3节点.</p>
<p>首先肯定是查看集群状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl <span class="string">&#x27;http://localhost:9200/_cluster/health?pretty&#x27;</span> -u xxx:yyy</span><br><span class="line"><span class="comment">#发现状态为Red</span></span><br></pre></td></tr></table></figure>

<p>查看节点状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl <span class="string">&#x27;http://localhost:9200/_cat/node?v&#x27;</span> -u xxx:yyy</span><br><span class="line"><span class="comment">#节点数正常</span></span><br></pre></td></tr></table></figure>

<p>查看各节点使用的磁盘情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -XGET <span class="string">&#x27;localhost:9200/_cat/allocation?v&amp;pretty&#x27;</span> -u xxx:yyy</span><br><span class="line"><span class="comment">#未发现异常</span></span><br></pre></td></tr></table></figure>



<h3 id="集群日志"><a href="#集群日志" class="headerlink" title="集群日志"></a>集群日志</h3><p>登录一个ES节点查看日志，发现以下报错，查看其它2个节点，情况相同</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200204134510.png"></p>
<p>从日志来看, 可以发现几个问题:</p>
<blockquote>
<ul>
<li>Full GC很频繁, 而且时间也非常的长 </li>
<li>每次GC完之后所能释放的内存非常有限</li>
</ul>
</blockquote>
<p>到这，其它原因已经很明朗, <strong>内存不够导致的full gc 频繁，但是又不能释放太多的空间，又导致full gc的恶意循环</strong></p>
<p>如果大家对gc的日志不很了解的话, 这里解释下上面日志的各个字段含义, 假如有以下一条gc日志</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[2018-06-30T17:57:29,020][INFO ][o.e.m.j.JvmGcMonitorService] [qoo--ES] [gc][old][228385][160772] duration [5s], collections [1]/[5.1s], total [5s]/[4.4d], memory [945.4mb]-&gt;[958.5mb]/[1007.3mb], all_pools &#123;[young] [87.8mb]-&gt;[100.9mb]/[133.1mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[16.6mb]&#125;&#123;[old] [857.6mb]-&gt;[857.6mb]/[857.6mb]&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>gc old</strong> 本次是 old GC 这是第228385次 GC 检查][从 JVM 启动至今发生的第 160772次 G]</li>
<li><strong>duration</strong> [本次检查到的 GC 总耗时 <strong>5</strong> 秒，可能是多次的加和</li>
<li><strong>collections</strong> [从上次检查至今总共发生<strong>1</strong>次GC]&#x2F;[从上次检查至今已过去 <strong>5.1</strong> 秒],</li>
<li><strong>total</strong> [本次检查到的 GC 总耗时为 <strong>5</strong> 秒]&#x2F;[从 JVM 启动至今发生的 GC 总耗时为 <strong>4.4</strong> 天]，</li>
<li><strong>memory</strong> [ GC 前 Heap memory 空间]-&gt;[GC 后 Heap memory 空间]&#x2F;[Heap memory 总空间],</li>
<li><strong>all_pools</strong>(分代部分的详情) {young 区][GC 前 Memory ]-&gt;[GC后 Memory]&#x2F;[young区 Memory 总大小] }</li>
<li><strong>survivor</strong> [GC 前 Memory ]-&gt;[GC后 Memory]&#x2F;[Memory 总大小] }{old 区[GC 前 Memory ]-&gt;[GC后 Memory]&#x2F;[old区 Memory 总大小] }</li>
</ul>
<p>那为何会内存不足呢, 想起前几天服务拓展，添加收集了几个服务的日志, 日志量不算小, 而且这3个ES节点所在的机器内存只分配了8个G, 显然, 加完这几个日志之后内存不够用了.</p>
<h3 id="Index-Mapping"><a href="#Index-Mapping" class="headerlink" title="Index Mapping"></a>Index Mapping</h3><p>而且，从日志中， 还发现几处很诡异的日志</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200204141415.png"></p>
<p>红框中的错误: <strong>object mapping for [msg] tried to pasrse field [msg] as object, but found a concrete</strong></p>
<p>翻译过来就是说, 日志中的msg这个字段的类型匹配不上, ES尽力把它解析为object类型，但是它却是个确切的值.</p>
<p>就好比你在python中定义了一个字典, 但是你给它赋值一个整形, 这确定要报错.</p>
<p>但是这个mapping是从何而来的呢.</p>
<p>经过网上搜索一翻, 终于明白:</p>
<p>ES中为了加强性能，对于索引中的<strong>第一条数据, ES会默认(可设置)按照这条数据建立index mapping, 相当于是一个索引模板, 好比是mysql中表的定义一个概念, 后续这个索引中的所有日志都按照该模板进行解析</strong></p>
<p>后来经过查看源码, 我们发现，出现上图中的问题是因为代码中的日志打印不规范, 有一个地方如果抛出异常时，打印的是一个单一字符串, 而正常情况下打印的是一个json, 所以当第一条正常的日志传递给ES时，ES建立的索引 模板记录这个字段为object类型(可以类比为python中的字典, 本身是可以新增元素的), 当应用抛出异常时，日志中又传递了一个单一字符串, 但是索引模板已经记录为object类型，所以无法匹配, 这就是问题所在</p>
<h3 id="Fix-Bug"><a href="#Fix-Bug" class="headerlink" title="Fix Bug"></a>Fix Bug</h3><p>从3个方面解决上面的问题</p>
<p>首先是修复代码中日志不规范问题, 这个充暴露了code view不严谨.</p>
<p>第2个是加大ES的内存,毕竟日志量确实是上来了, 从8G添加到12G</p>
<p>第3个是优化ES的gc问题, ES中有个逻辑是：<strong>默认情况下，主节点每30秒会去检查其他节点的状态，如果任何节点的垃圾回收时间超过30秒（Garbage collection duration）且在重复次数内都超时，则会导致主节点任务该节点脱离集群, 节点脱离集群后会引起分片的rebalance</strong></p>
<p>这个机制让ES节点频繁的脱离集群，而又引起整个集群的rebalance，ES本身的内存就不够，rebalance更是雪上加霜, 如此往复就导致整个ES集群宕机.</p>
<p>可修改主节点的检查时间及重复次数,让其它节点有充分的时间进行full gc.</p>
<p>在ES的配置文件中添加以下配置:</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">discovery.zen.fd.ping_timeout:</span> <span class="string">300s</span></span><br><span class="line"><span class="attr">discovery.zen.fd.ping_retriES:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>这样, 达到这两个条件时才能触发节点脱离集群</p>
<p>至此, 依次重启ES节点, 问题圆满解决.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html">elasticsearch index mapping</a></li>
<li><a href="https://blog.csdn.net/sinat_35930259/article/details/80354732">elasticsearch篇之mapping</a></li>
<li><a href="https://segmentfault.com/a/1190000016494488">你看懂 Elasticsearch Log 中的 GC 日志了吗？</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(ELK介绍)</title>
    <url>/2017/05/16/FELK-ELK-Struct/</url>
    <content><![CDATA[<h3 id="ELK名词解释"><a href="#ELK名词解释" class="headerlink" title="ELK名词解释"></a><strong>ELK名词解释</strong></h3><p><strong>ELK:</strong> elasticsearch+logstash+kibana</p>
<p>**ELK Stack: **在 5.0 版本加入 Beats 套件后叫做Elastic Stack.</p>
<p><strong>Elasticsearch:</strong> 是实时全文搜索和分析引擎,提供搜集、分析、存储数据三大功能,是一套开放REST和JAVA API等结构提供高效搜索功能,可扩展的分布式系统.它构建于Apache Lucene搜索引擎库之上.</p>
<span id="more"></span>

<p>**Logstash: **是一个用来搜集、分析、过滤日志的工具.它支持几乎任何类型的日志,包括系统日志、错误日志和自定义应用程序日志.它可以从许多来源接收日志.</p>
<p>**Kibana: **是一个基于Web的图形界面,用于搜索、分析和可视化存储在 Elasticsearch的日志数据.它利用Elasticsearch的REST接口来检索数据,不仅允许用户创建他们自已的数据的定制仪表板视图,还允许他们以特殊的方式查询和过滤数据.</p>
<p><strong>Beats:</strong> 开源的数据获取工具集(已替代logstash-forworder),并提供比Logstash更强大的数据格式化功能,根据不同监控对象分为:filebeat,packetbeat,topbeat,winlogbeat,heartbeat.</p>
<p><strong>Lucene:</strong> Apache软件基金会的开源全文检索引擎工具包,使用Java语言编写.</p>
<p>**总结:**ELK stack是一个分布式实时数据分析架构,数据采集&#x3D;&#x3D;&gt;分析存储&#x3D;&#x3D;&gt;前端搜索展示等一套完整的技术栈.</p>
<h3 id="ELK数据流向"><a href="#ELK数据流向" class="headerlink" title="ELK数据流向"></a><strong>ELK数据流向</strong></h3><p>这里借用官网的原理图,不难理解:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/elk1.png" alt="elk1"></p>
<p>更深入的处理细节:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/elk2.PNG" alt="elk2"></p>
<h3 id="ELK常用架构"><a href="#ELK常用架构" class="headerlink" title="ELK常用架构"></a><strong>ELK常用架构</strong></h3><h4 id="ELK基本架构"><a href="#ELK基本架构" class="headerlink" title="ELK基本架构"></a><strong>ELK基本架构</strong></h4><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/elk4.png" alt="elk4"></p>
<h4 id="ELK-消息队列"><a href="#ELK-消息队列" class="headerlink" title="ELK+消息队列"></a><strong>ELK+消息队列</strong></h4><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/elk5.png" alt="elk5"></p>
<h3 id="ELK适用场景"><a href="#ELK适用场景" class="headerlink" title="ELK适用场景"></a><strong>ELK适用场景</strong></h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/elk6.png" alt="elk6"></p>
<p>上面部分截图来自本人在部门的知识分享,前期先把基本概念扯一下,下一篇开始安装.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kibana.logstash.es/content/index.html">ELK Stack中文指南</a></li>
<li><a href="https://www.elastic.co/cn/products">ELK Stack官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(将数据存储从SQLite3迁移到mysql)</title>
    <url>/2020/07/20/Grafana-migrate-sqlite-to-mysql/</url>
    <content><![CDATA[<p>线上的Grafana一直都是直接将所有数据写本地的SQLite,就单独的一个db文件，随时dashboard加的越来越多，真怕随时都可能出现磁盘宕机的问题，拓展性也差， 因此将grafana的历史数据从sqlite3迁移到mysql中，也能够保证HA, 新测有效</p>
<span id="more"></span>



<h3 id="mysql新建库及用户"><a href="#mysql新建库及用户" class="headerlink" title="mysql新建库及用户"></a>mysql新建库及用户</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">create database grafana;</span><br><span class="line">GRANT USAGE ON `grafana`.* to <span class="string">&#x27;grafana&#x27;</span>@<span class="string">&#x27;xxx&#x27;</span> identified by <span class="string">&#x27;xxx&#x27;</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON `grafana`.* to <span class="string">&#x27;grafana&#x27;</span>@<span class="string">&#x27;xxx&#x27;</span> with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>



<h3 id="导出SQLite3历史数据"><a href="#导出SQLite3历史数据" class="headerlink" title="导出SQLite3历史数据"></a>导出SQLite3历史数据</h3><p>导出历史数据前需要先将grafana停止，以防新数据写入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop grafana</span><br></pre></td></tr></table></figure>

<p>使用以下脚本将数据导出</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">DB=<span class="variable">$1</span></span><br><span class="line">TABLES=$(sqlite3 <span class="variable">$DB</span> .tables | sed -r <span class="string">&#x27;s/(\S+)\s+(\S)/\1\n\2/g&#x27;</span> | grep -v migration_log)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="variable">$TABLES</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;TRUNCATE TABLE <span class="variable">$t</span>;&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="variable">$TABLES</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> -e <span class="string">&quot;.mode insert <span class="variable">$t</span>\nselect * from <span class="variable">$t</span>;&quot;</span></span><br><span class="line"><span class="keyword">done</span> | sqlite3 <span class="variable">$DB</span></span><br></pre></td></tr></table></figure>

<p>执行 <code>./export_sqlite.sh grafana.db &gt; grafana.sql</code></p>
<p>**grafana.db为grafana本地存储的db文件， 执行完成之后即可在当前目录下生成mysql 文件</p>
<h3 id="修改grafana配置文件使用mysql"><a href="#修改grafana配置文件使用mysql" class="headerlink" title="修改grafana配置文件使用mysql"></a>修改grafana配置文件使用mysql</h3><p>因为我这里是将grafana部署在k8s中的，因此直接在容器中定义环境变量的形式</p>
<p>注意: 环境变量的规则是以<code>GF_&lt;SectionName&gt;_&lt;KeyName&gt;</code></p>
<p><code>sectionName为grafana配置文件中的分区部分，比如database,就为GF_DATABASE_xxx</code></p>
<p><code>smtp就为GF_SMTF_xxx</code> 请<a href="https://grafana.com/docs/grafana/latest/administration/configuration">参考</a></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_TYPE</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">mysql</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_HOST</span></span><br><span class="line">  <span class="attr">value:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">:3306</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_NAME</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">grafana</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_USER</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">grafana</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_PASSWORD</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">xxx</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GF_DATABASE_URL</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">mysql://xxx:xxx@127.0.0.1:3306/grafana</span></span><br></pre></td></tr></table></figure>

<p>如果是实体机部署的是，则在配置文件中使用</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">[<span class="string">database</span>]</span><br><span class="line"><span class="comment"># You can configure the database connection by specifying type, host, name, user and password</span></span><br><span class="line"><span class="comment"># as separate properties or as on string using the url properties.</span></span><br><span class="line"></span><br><span class="line"><span class="string">type</span> <span class="string">=</span> <span class="string">mysql</span></span><br><span class="line"><span class="string">host</span> <span class="string">=</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">:3306</span></span><br><span class="line"><span class="string">name</span> <span class="string">=</span> <span class="string">grafana</span></span><br><span class="line"><span class="string">user</span> <span class="string">=</span> <span class="string">xxx</span></span><br><span class="line"><span class="string">password</span> <span class="string">=</span> <span class="string">xxx</span></span><br><span class="line"><span class="string">url</span> <span class="string">=</span> <span class="string">mysql://xxx:xxx@127.0.0.1:3306/grafana</span></span><br></pre></td></tr></table></figure>

<p>修改完成之后重启grafana, 这样grafana会连上mysql数据库, 自动生成所有的表</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart grafana</span><br></pre></td></tr></table></figure>

<p>当看到日志显示数据库migration已经做完了之后，就可以再次停用grafana，然后把数据导进去</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop grafana</span><br></pre></td></tr></table></figure>



<h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -u xxx -p xxx -P3306 -D grafana &lt; grafana.sql</span><br></pre></td></tr></table></figure>

<p>导完之后再次启动grafana</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start grafana</span><br></pre></td></tr></table></figure>



<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>当grafana启动之后，登录grafana，所有的数据都在，迁移成功.</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li><p>导入数据时提示以下错误</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">ERROR 1136 (21S01) at line 277: Column count doesn&#x27;t match value count at row 1</span><br></pre></td></tr></table></figure>

<p>原因: 插入的数据字段与表中的字段不一致，由于这个grafana使用期间升级过一次版本, 因此，当连接到mysql后会以新版本的sql进行建表操作，而导出的数据中有个表中的一个字段在新版本中已经没有了，因此字段个数对不上</p>
<p>解决: 按照新的表结构直接将导出数据不存在的字段去除.</p>
</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://grafana.com/docs/grafana/latest/administration/configuration">https://grafana.com/docs/grafana/latest/administration/configuration</a></li>
<li><a href="https://jiajunhuang.com/articles/2019_10_15-grafana_moved_to_mysql.md.html">https://jiajunhuang.com/articles/2019_10_15-grafana_moved_to_mysql.md.html</a></li>
<li><a href="https://community.grafana.com/t/migrating-grafana-data-to-new-database/2454/20">https://community.grafana.com/t/migrating-grafana-data-to-new-database/2454/20</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>FELK学习(解决Unassigned shard)</title>
    <url>/2019/05/10/FELK-fix-unassigned-shard/</url>
    <content><![CDATA[<p>在使用es时，如果集群的参数配置跟node节点数之间的关系没有处理好就有可能出现集群的status为<code>yellow</code>,下面就处理一种常见的<code>unassigned shard</code>的问题.</p>
<span id="more"></span>



<h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl es-logstore.people.local/_cluster/health?pretty</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504204458.png"></p>
<p>可以看到集群的状态为yellow, 从es集群的三大状态(Red, Yellow, Green)来看,yellow状态说明有索引的副本缺失</p>
<p>查询索引确实发现有索引状态为<code>yellow</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl es-tsdb.people.local/_cat/indices</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504204851.png"></p>
<h3 id="unassigned-shard"><a href="#unassigned-shard" class="headerlink" title="unassigned shard"></a>unassigned shard</h3><p>查看shard的状态，是否存在unassigned的分片</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl es-tsdb.people.local/_cat/shards|grep UNASSIGNED</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504204952.png"></p>
<h3 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h3><p>使用<code>explain</code>查看具体原因:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504205021.png"></p>
<p>从<code>reason</code>中可以看出, 这个索引<code>unassigned</code>的原因为无法在同一个节点上保存两分副本.</p>
<p>也就是说，在该节点上已经存在了一个副本了.</p>
<p>同时另两个节点上无法保存副本的<code>reason</code>为:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504205106.png"></p>
<p>这个<code>reason</code>是说副本数量超时了这个节点能够存在副本的最大值,所以也没法分配。</p>
<p>从这里我们可以看出一个关键的参数<code>index.routing.allocation.total_shards_per_node=2</code></p>
<p>这个参数指的是一个节点上最好能保存某个索引的最大值</p>
<p>假如: 一个es集群有3个数据节点，某个索引模板设置的分片数为3, 副本数为1, 那么总的分片数&#x3D;主分片数+ 副分片数，也就是3+3&#x3D;6个, 那么3个数据节点 则<code>total_shards_per_node</code>的值可以设置为6&#x2F;3&#x3D;2,每个节点上保存2个副本，一般情况下这是合理的.</p>
<p>但是如果某个时刻,es集群出现脑裂问题的话，比如说有个节点宕机，那么6个分片需要分布在两个节点上, 那么每个节点上需要保存3个副本，这大于<code>total_shards_per_node=2</code>，所以有可能出现以上的错误，所以一般起见, <code>total_shards_per_node</code>一般设置的比最小值大点。</p>
<p><code>total_shards_per_node</code>这个值在es的配置文件中指定，同时也支持动态修改.</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -XPUT <span class="string">&#x27;http://es-tsdb.people.local/hawkeye-ts-10s-2019-05-08-18/_settings&#x27;</span> -d <span class="string">&#x27;&#123;&quot;index.routing.allocation.total_shards_per_node&quot;:3&#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>修改之后，再次查看集群的状态，变回了<code>green</code>状态.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200504205149.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/elastic/curator">https://github.com/elastic/curator</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(使用webhook实现微信图文告警)</title>
    <url>/2020/07/10/Grafana-wechatwork-webhook/</url>
    <content><![CDATA[<p>grafana支持的告警方式很多种, 偏偏不直接支持<code>wechat</code></p>
<span id="more"></span>

<p>就算最新版的prometheus也是不支持企业微信的,github已经有多个开发者提交了支持企业微信的branch, 但一直都没有合并到主分支中, 官方认为已经存在webhook的方式, 企业微信的alert完全可以由webook实现</p>
<p>因此要让grafana支持企业微信, 有两种方式:</p>
<ol>
<li>基于开发者支持的企业微信分支进行源码编译grafana,</li>
<li>使用webhook</li>
</ol>
<p>第一种方式github上有好几个实现，最新的一个分支实现可<a href="https://github.com/grafana/grafana/pull/26109">参考</a>, </p>
<p>我看了一下，这个开发者同时也使用webhook的方式实现了企业微信的对接，非常的简洁，见<a href="https://github.com/n0vad3v/g2ww">github</a></p>
<h3 id="g2ww"><a href="#g2ww" class="headerlink" title="g2ww"></a><a href="https://github.com/n0vad3v/g2ww">g2ww</a></h3><p>可以来简单分析一下, 最核心的发送消息的代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">url := <span class="string">&quot;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=&quot;</span> + c.Params(<span class="string">&quot;key&quot;</span>)</span><br><span class="line"></span><br><span class="line">		msgStr := fmt.Sprintf(<span class="string">`</span></span><br><span class="line"><span class="string">		&#123;</span></span><br><span class="line"><span class="string">			&quot;msgtype&quot;: &quot;news&quot;,</span></span><br><span class="line"><span class="string">			&quot;news&quot;: &#123;</span></span><br><span class="line"><span class="string">			  &quot;articles&quot;: [</span></span><br><span class="line"><span class="string">				&#123;</span></span><br><span class="line"><span class="string">				  &quot;title&quot;: &quot;%s&quot;,</span></span><br><span class="line"><span class="string">				  &quot;description&quot;: &quot;%s&quot;,</span></span><br><span class="line"><span class="string">				  &quot;url&quot;: &quot;%s&quot;,</span></span><br><span class="line"><span class="string">				  &quot;picurl&quot;: &quot;%s&quot;</span></span><br><span class="line"><span class="string">				&#125;</span></span><br><span class="line"><span class="string">			  ]</span></span><br><span class="line"><span class="string">			&#125;</span></span><br><span class="line"><span class="string">		  &#125;</span></span><br><span class="line"><span class="string">		`</span>, h.Title, h.Message, h.RuleUrl, h.ImageUrl)</span><br></pre></td></tr></table></figure>

<p><code>https://qyapi.weixin.qq.com/cgi-bin/webhook/send</code>可以看到使用了机器人来发送群消息</p>
<p>从msgtype可以看到，群消息机器人使用的是<code>news</code>类型, 对于支持的消息类型可以<a href="https://work.weixin.qq.com/api/doc/90000/90136/91770">参考</a></p>
<p>本人需求的最大一个点在于能够通过个人微信来接收企业微信的消息, 测试了下, 机器人发送的消息只会发送到企业微信里，而无法将信息发送到关联的个人微信中.</p>
<p>该库虽然简单，本人并未采用</p>
<h3 id="wechat-work-message-push-go"><a href="#wechat-work-message-push-go" class="headerlink" title="wechat-work-message-push-go"></a><a href="https://github.com/cloverzrg/wechat-work-message-push-go">wechat-work-message-push-go</a></h3><p>这个库同样可以实现使用webhook来实现grafana的告警发送到企业微信中，然后通过个人微信关联企业微信实现消息同步.</p>
<p>也来简单分析一下</p>
<p>通过gin实现路由规则</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SetRoute</span><span class="params">(r *gin.Engine)</span></span> &#123;</span><br><span class="line">	r.GET(<span class="string">&quot;/&quot;</span>, controller.Index)</span><br><span class="line">	r.POST(<span class="string">&quot;/push&quot;</span>, middleware.TokenMiddleware, controller.Push)</span><br><span class="line">	r.POST(<span class="string">&quot;/grafana&quot;</span>, middleware.BasicAuth(), controller.GrafaneHandler)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>实现了两个方法, 一个push，这个用于发送消息, 主要看一下&#x2F;grafana的实现</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">url := fmt.Sprintf(<span class="string">&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot;</span>, token)</span><br><span class="line">m := TextCardMessage&#123;</span><br><span class="line">	Msgtype: <span class="string">&quot;textcard&quot;</span>,</span><br><span class="line">	Agentid: config.Config.WechatWork.AgentId,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(toUser) == <span class="number">0</span> &#123;</span><br><span class="line">	m.Touser = config.Config.WechatWork.DefaultReceiverUserId</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">	m.Touser = toUser</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">m.TextCard.Title = title</span><br><span class="line">m.TextCard.URL = imageUrl</span><br><span class="line">m.TextCard.Description = content</span><br><span class="line"></span><br><span class="line">jsonStr, err := json.Marshal(m)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	logger.Error(<span class="string">&quot;sendMessage error:%s&quot;</span>, err)</span><br><span class="line">	SendMessage(err.Error(), <span class="string">&quot;&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">postJson(url, jsonStr)</span><br></pre></td></tr></table></figure>

<p><code>https://qyapi.weixin.qq.com/cgi-bin/message/send</code>这里跟上面的区别在于上面是<code>webhook</code>这里是<code>message</code>, 从企业微信的官方文档来看, 两者支持的消息的类型也不一样, 可<a href="%5Bhttps://work.weixin.qq.com/api/doc/90002/90151/90854#%E5%9B%BE%E6%96%87%E6%B6%88%E6%81%AF%5D(https://work.weixin.qq.com/api/doc/90002/90151/90854#%E5%9B%BE%E6%96%87%E6%B6%88%E6%81%AF)">参考</a> ，这里使用的是<code>textCard</code></p>
<p>同时，该库使用了一个用户密码认证的过程, 也增加了一层安全.</p>
<p>本人基于该库修改了一点源码编译后通过容器Run.</p>
<p>通过将相关的信息填入到容器的ENV中, 然后在grafana中填入相关信息即可</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200711192627.png"></p>
<p>可以来测试一下<code>/push</code>接口:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -X POST \</span><br><span class="line">  http://127.0.0.1:80/push \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/x-www-form-urlencoded&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;token: your_token&#x27;</span> \</span><br><span class="line">  -d <span class="string">&#x27;message=1234&#x27;</span></span><br></pre></td></tr></table></figure>

<p>即可在企业微信及个人微信中(前提是两者关联)收到信息</p>
<p>在上面的grafana的配置界面点击<code>send test</code>, 然后即可在微信中收到以下信息</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200711193159.png"></p>
<p>点击进去之后会出现grafana的告警现场</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200711193350.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><p><a href="https://github.com/grafana/grafana/pull/26109">https://github.com/grafana/grafana/pull/26109</a></p>
</li>
<li><p><a href="https://github.com/n0vad3v/g2ww">https://github.com/n0vad3v/g2ww</a></p>
</li>
<li><p><a href="https://github.com/cloverzrg/wechat-work-message-push-go">https://github.com/cloverzrg/wechat-work-message-push-go</a></p>
</li>
<li><p><a href="https://work.weixin.qq.com/api/doc/90002/90151/90854#%E5%9B%BE%E6%96%87%E6%B6%88%E6%81%AF">https://work.weixin.qq.com/api/doc/90002/90151/90854#%E5%9B%BE%E6%96%87%E6%B6%88%E6%81%AF</a></p>
<p><a href="https://work.weixin.qq.com/api/doc/90000/90136/91770">https://work.weixin.qq.com/api/doc/90000/90136/91770</a></p>
</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-Active Availability vs High Availability</title>
    <url>/2019/07/16/HA-MA/</url>
    <content><![CDATA[<p>当我们在讨论系统架构的时候经常听到HA(High Availability 高可用)如何如何保证, 在调研CockRoachDB时看到个MA(Multi-Active Availability 多活可用)的概念, 头一次听说, 比较新颖, 所以网上查了一上, 说到底还是CAP那一套.</p>
<span id="more"></span>



<h3 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h3><p>我们经常会错误的认为高可用就是一致性，其实并不然.</p>
<p>高可用性，是指应用程序在其中一个服务的宿主系统发生故障的时候，仍然能够不停地运行。这是通过水平拓展应用程序服务的方式来实现的（例如在多台机器和系统上部署相同的服务）。如果它们当中的其中一个服务发生故障，其他服务能够接替并提供相同的服务内容</p>
<p><strong>高可用强调的是在节点发生故障时, 系统仍然可以提供服务, 并不关心数据一致性的问题.</strong></p>
<p>两种使用最频繁的高可用性设计方案：<strong>Active-Passive和Active-Active系统</strong></p>
<h4 id="Active-Passive"><a href="#Active-Passive" class="headerlink" title="Active-Passive"></a>Active-Passive</h4><p>在Active-Passive系统中，所有流量会路由到一个“Active“的主副本。这个副本上状态的变化将同步到备份的“Passive”的从副本，系统尽可能地保证“Passive”副本与“Active”副本之间的一致性。</p>
<p>很典型的是Oracle数据库.</p>
<p>然而这种设计存在以下缺点：</p>
<blockquote>
<ul>
<li>如果用户使用异步复制策略，将无法保证所有数据成功地同步到“Passive”的从副本。这就意味着用户可能丢失数据，在特定的应用场景下这可能导致非常可怕的后果。</li>
<li>如果用户使用同步复制策略，在“Passive”的从副本出现故障的时候，用户不得不牺牲整个应用系统的可用性，否则将面临不一致的风险</li>
</ul>
</blockquote>
<h4 id="Active-Active"><a href="#Active-Active" class="headerlink" title="Active-Active"></a>Active-Active</h4><p>在Active-Active系统中，多个副本各自运行着独立的服务。负载被路由到所有副本上。如果一个副本出现故障，其他副本可以继续处理原本应该路由到故障副本上的负载。</p>
<p>对于数据库而言，Active-Active的系统对于大多数工作负载来说是难以实现的。例如：如果用户想让多个副本处理同一份数据的写操作，如何保证它们的一致性？</p>
<p>打个比说:</p>
<p>在Active-Active设计的高可用性集群当中有2个副本（<strong>A</strong>和<strong>B</strong>）。</p>
<ol>
<li><strong>A</strong>确认键<code>xyz</code>的写请求，值为<code>&#39;123&#39;</code>，随后发生故障。</li>
<li><strong>B</strong>确认键<code>xyz</code>的读请求，因为没找到匹配键值对，返回<code>NULL</code>。</li>
<li><strong>B</strong>确认键<code>xyz</code>的写请求，值为<code>&#39;456&#39;</code>。</li>
<li><strong>A</strong>重启并尝试再次与<strong>B</strong>连接，此时如何处理<code>xyz</code>值的不同？在系统里面没有一个清晰的方式去处理这种数据不一致的情况。</li>
</ol>
<p><strong>在这个例子当中，集群在整个生命周期内保持Active状态。根据<a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP理论</a>，这是AP系统，它能够在分区发生时保证可用性，而不能保证一致性。</strong></p>
<h3 id="Multi-Active-Availability"><a href="#Multi-Active-Availability" class="headerlink" title="Multi-Active Availability"></a>Multi-Active Availability</h3><p>像Active-Ative设计模式，具备多活可用性的系统当中的所有副本，都处理读和写的负载。CockroachDB在此基础上做了改进，使用“一致性副本”实现了数据之间一致性。在这种设计里，同步请求将发送到至少3个副本，且只有绝大多数副本响应了该请求，才能视作同步完成。这就意味着用户在系统不满足可用条件时仍然会遇到系统故障。</p>
<p>为了避免冲突，保证数据的一致性，集群如果丢失了绝大多数副本，将停止响应。这是因为在故障系统里的副本丧失了使数据达成一致的能力。待到绝大多数副本重启以后，数据库将恢复可用。</p>
<p>在一个多活可用的集群中有3个CockroachDB节点（<strong>A</strong>、<strong>B</strong>、<strong>C</strong>）。</p>
<ol>
<li><strong>A</strong>接收到键<code>xyz</code>的写请求，值为<code>&#39;123&#39;</code>。它与节点<strong>B</strong>和<strong>C</strong>同步写请求结果，确保它们确认了写请求结果。一旦<strong>A</strong>收到第一个确认，写请求完成。</li>
<li><strong>A</strong>随后发生故障。</li>
<li><strong>B</strong>接收到键<code>xyz</code>的读请求，返回值<code>&#39;123&#39;</code></li>
<li><strong>C</strong>接收到键<code>xyz</code>的更新请求，修改值为<code>&#39;456&#39;</code>。它与节点<strong>B</strong>同步写请求结果，确保<strong>B</strong>确认了写请求结果。在<strong>C</strong>接收到来自节点<strong>B</strong>的确认之后，写请求完成。</li>
<li><strong>A</strong>重启，再次加入集群。它接收到了键<code>xyz</code>的最新值，将旧值更新为<code>&#39;456&#39;</code>。</li>
</ol>
<p><strong>在这个例子当中，如果在A故障以后B或C发生故障，集群将停止响应。根据<a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP理论</a>，这是一个CP系统，它能够在分区发生时保证一致性，而不是保证可用性。</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP理论</a></li>
<li><a href="http://doc.cockroachchina.baidu.com/#faqs/cockroachdb-features/multi-active-availability/">cockroach.doc</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(Gateway)</title>
    <url>/2020/02/16/Istio-Gateway/</url>
    <content><![CDATA[<p>Gateway在istio中是一个比较重要的对象, 一般用于管理南北流量，即集群外向集群内的访问及集群内需要访问集群外的这部分流量. </p>
<span id="more"></span>



<p>同样, 还是以官方的示例: book-info做为例子</p>
<h3 id="Gateway"><a href="#Gateway" class="headerlink" title="Gateway"></a>Gateway</h3><p>从ingressgateway容器的启动命令中可以看到，启动跟envoy的启动命令很像</p>
<p>同样 pilot-agent为1号启动进程, 负责envoy的启动及托管, 还负责envoy的配置更新</p>
<p>envoy启动的proxy 后面是sidecar，而ingressgateway启动的proxy后面是router</p>
<h4 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h4><p>Pilot-agent启动命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --drainDuration 45s --parentShutdownDuration 1m0s --connectTimeout 10s --serviceCluster istio-egressgateway --zipkinAddress zipkin.istio-system:9411 --proxyAdminPort 15000 --statusPort 15020 --controlPlaneAuthPolicy NONE --discoveryAddress istio-pilot.istio-system:15010 --trust-domain=cluster.local</span><br></pre></td></tr></table></figure>

<p>envoy的启动命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev1.json --restart-epoch 1 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster istio-ingressgateway --service-node router~10.244.0.4~istio-ingressgateway-6d759478d8-pxbn9.istio-system~istio-system.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 1)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error</span><br></pre></td></tr></table></figure>

<h4 id="gateway"><a href="#gateway" class="headerlink" title="gateway"></a>gateway</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Gateway</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bookinfo-gateway</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">istio:</span> <span class="string">ingressgateway</span> <span class="comment"># use istio default controller</span></span><br><span class="line">  <span class="attr">servers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span></span><br><span class="line">      <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">HTTP</span></span><br><span class="line">    <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;*&quot;</span></span><br></pre></td></tr></table></figure>

<p>上面指定了istio: ingressgateway，即所有从80端口的任一域名的http协议都由ingressgateway进入, 这样就保证了所有外部流量的统一治理。</p>
<p>gateway一般与virtualService一起共用.</p>
<p>和 Kubernetes Ingress 不同，<strong>Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置）。绑定一个 VirtualService到 Gateway</strong> 上，就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量</p>
<p>为上面的gateway定义virtualservice：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bookinfo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">  <span class="attr">gateways:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">bookinfo-gateway</span>  <span class="comment">#绑定gateway</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/productpage</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/static</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/login</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/logout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/api/v1/products</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">productpage</span></span><br><span class="line">        <span class="attr">port:</span></span><br><span class="line">          <span class="attr">number:</span> <span class="number">9080</span></span><br></pre></td></tr></table></figure>

<p>这样，就可以在集群外部访问productpage服务了</p>
<p>engressgateway跟Ingressgateway其实一样，只不过一个管理出口流量，一个管理进口流量，engressgateway的作用完全可以由ingressgateway替代, 不过为了区分，让两者独立管理罢了.</p>
<p><strong>engressgateway还有一个好外是将所有的对外网访问控制engressgateway这一个范围， 而不必对每个需要访问外网的应用开启外网权限，起到一个安全的作用</strong></p>
<h3 id="ServiceEntry"><a href="#ServiceEntry" class="headerlink" title="ServiceEntry"></a>ServiceEntry</h3><p>上面gateway用于集群外需要访问集群内的服务, 而serviceEntry则是用于要访问的服务不在集群内部, 使用serviceEntry同样可以把这部分的请求流量治理起来.</p>
<p>serivceEntry通常搭配engress 一起使用, 当然 , serviceEntry没有engress也可以，只不过为了统一管理&#x2F;治理外出流量，因此将所有定义的serviceEntry的请求都统一转到engress向外转发</p>
<p><strong>serviceEntry跟Kubernetes在Servicesubset的subset中指定endpoint作用相同</strong></p>
<p>典型的serviceEntry的例子如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">googleapis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">www.googleapis.com</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">HTTP</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">HTTPS</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">DNS</span></span><br></pre></td></tr></table></figure>

<p>通过host字段来指定外部目标, 通过80&#x2F;443就能够在meseh内部访问<a href="http://www.googleapis.com了/">www.googleapis.com了</a></p>
<p>当然，还可以通过virtualservice来对这次访问进行控制, 比如给上面的访问加上个10s超时</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">timeout-for-googleapis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">www.googleapis.com</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">www.googleapis.com</span></span><br><span class="line">    <span class="attr">timeout:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p><strong>流量的重定向和转发、定义重试和超时以及错误注入策略都支持外部目标。然而由于外部服务没有多版本的概念，因此权重（基于版本）路由就无法实现了</strong></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>假如在kubernetes中已经存在了一个service访问外部ip, yaml文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-db</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-db</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">190.64</span><span class="number">.31</span><span class="number">.232</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">3306</span></span><br></pre></td></tr></table></figure>

<p>190.64.31.232这个是外部mysql的地址，在kubernetes中, 即可直接通过external-db直接访问数据库</p>
<p>如果这时，需要通过istio统一管理外向流量，则可通过以下方式:</p>
<p>保存上面的Headless service不变</p>
<p>创建一个serviceEntry:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-db</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">external-db</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tcp</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">location:</span> <span class="string">MESH_EXTERNAL</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">STATIC</span></span><br></pre></td></tr></table></figure>

<p>注意， 这里的resolution为<strong>STATIC</strong>, 因为kubernetes中已经存在了有external-db的域名(上面headless的定义)，不需要再进行解析就可以直接转到endpoint(190.64.31.232)</p>
<p>当然 如果mysql本身存在域名且在网格之外可以解析， 那就无需在kubernetes定义svc, 直接创建一个serviceEntry即可</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-db</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">external-db.xxx.yyy</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tcp</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">location:</span> <span class="string">MESH_EXTERNAL</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">DNS</span>  <span class="comment">#DNS， 需要保证hosts对应的域名可以解析</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://blog.frognew.com/2018/10/learning-istio-1.0-6-istio-ingress-gateway.html">https://blog.frognew.com/2018/10/learning-istio-1.0-6-istio-ingress-gateway.html</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNDIzODExOA==&amp;mid=2650166142&amp;idx=1&amp;sn=f675b8b31e69f441ab195cba66facc1a&amp;chksm=8ec1cf37b9b6462182a6e707cf683bbfbcd33de6c6e3da1a9d990d0edea179225084915ec661&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwNDIzODExOA==&amp;mid=2650166142&amp;idx=1&amp;sn=f675b8b31e69f441ab195cba66facc1a&amp;chksm=8ec1cf37b9b6462182a6e707cf683bbfbcd33de6c6e3da1a9d990d0edea179225084915ec661&amp;scene=21#wechat_redirect</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph学习(架构)</title>
    <url>/2018/04/28/Ceph%E5%AD%A6%E4%B9%A0(%E6%9E%B6%E6%9E%84)/</url>
    <content><![CDATA[<h2 id="Ceph简介"><a href="#Ceph简介" class="headerlink" title="Ceph简介"></a>Ceph简介</h2><blockquote>
<p>Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.<br>Ceph是统一分布式存储系统,具有优异的性能、可靠性、可扩展性.</p>
</blockquote>
<ul>
<li>存储系统: 提供对象存储,块存储,文件系统接口等服务</li>
<li>分布式:   无中心节点(Monitor-Paxos,类zookeeper)</li>
<li>统一:     所有服务基于统一的Rados</li>
</ul>
<span id="more"></span>

<p>Ceph的底层是RADOS(可靠、自动、分布式对象存储),可以通过LIBRADOS直接访问到RADOS的对象存储系统。<br>同时基于Rados提供RBD(块设备接口)、RADOS Gateway(对象存储接口)、Ceph File System(POSIX接口)等服务。</p>
<h3 id="架构介绍"><a href="#架构介绍" class="headerlink" title="架构介绍"></a>架构介绍</h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c2-1.jpg" alt="ceph-c2-1"></p>
<p>Ceph底层提供了Rados存储,用于支撑上层的librados和Rgw,Rbd,cephfs等服务.  </p>
<h3 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h3><ul>
<li>CRUSH算法: 类一致性哈希,Ceph的基础算法,用于动态计算数据块的存储位置(一个set)<strong>因为通过算法动态计算,不需要保存所有数据块和存储位置的对应关系</strong></li>
<li>Rados: Reliable, Autonomic Distributed Object Store, Ceph的核心存储,由Monitor及OSD组成.</li>
<li>Librados: 访问Rados的底层库,C&#x2F;C++ 实现,提供Java&#x2F;Python等的包装</li>
<li>OSD: Object Storage Device, Rados数据读写进程.一般每个Disk或分区对应一个OSD进程.</li>
<li>Rados Object: 任意保存至OSD的文件</li>
<li>Pool: 逻辑概念,Rados Object读写操作都是针对Pool的.</li>
<li>PG: Placement Group 放置组,Rados操作的最小粒度,由多个PG组成Pool,Object写入Pool时被指定到PG,具体由CRUSH算法计算.PG对应一到多个OSD.</li>
<li>Monitor: Ceph中的zookeeper,维护Cluster集群映射关系和配置&#x2F;Auth等.当各模块有up&#x2F;down等变化时,通知mon修改对应映射(osdmap,pgmap,poolmap等).<strong>这些映射和配置保存于Mon本地(leveldb),供客户端CRUSH计算</strong></li>
</ul>
<hr>
<ul>
<li>Rgw: Rados gateway,使Ceph对象存储兼容S3&#x2F;Swift接口的进程</li>
<li>Rbd: Rados block device,Ceph对外提供的块设备服务(无实际进程),基于librados的服务协议</li>
<li>Librbd: 基于Librados的库,用于访问Ceph 块服务</li>
<li>MDS: Ceph Metadata Server,CephFS依赖的元数据处理进程</li>
<li>CephFS:  Ceph File System,Ceph对外提供的文件系统服务</li>
</ul>
<hr>
<ul>
<li>Mgr: Manager 将底层C&#x2F;C++库封装为Python调用,旨在提供Plugin机制,鼓励用户开发插件满足特定需求.Dashboard&#x2F;Restful-API</li>
</ul>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c2-2.jpg" alt="ceph-c2-2"></p>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(VirtualService and DestinationRule流量管理)</title>
    <url>/2020/02/29/Istio-VirtualService-DestinationRule/</url>
    <content><![CDATA[<p>经过之前的对Istio的学习，大致明白了istio的机制,而要体会istio对流量的神奇魔法,自然离不开 virtualservice 与 destinationrule这两个对象了.</p>
<span id="more"></span>



<p>首先，再次说明下两个istio与kubernetes不同的地方：</p>
<ol>
<li>在kuberntes中, 服务与服务之间是通过clusterIP(kube-proxy)进行通信的， 而在istio中，对于<strong>网格间的两个服务</strong>，得益于xDS机制，通过CDS可直接得到EDS,  因此服务与服务之间不再经过clusterIP，而是直接到达对方的一个端点上(PortIP + Port)</li>
<li>在istio中，<strong>对于网格间的服务需要访问网格外的服务</strong>，仍然通过 ClusterIP 进行通信，但有一点需要注意：<strong>这里并没有 <code>kube-proxy</code> 的参与！</strong>Envoy 自己实现了一套流量转发机制，当你访问 ClusterIP 时，Envoy 就把流量转发到具体的 Pod 上去，<strong>不需要借助 kube-proxy 的 <code>iptables</code> 或 <code>ipvs</code> 规则</strong></li>
<li>对于入口流量管理，您可能会问： 为什么不直接使用 Kubernetes Ingress API ？ 原因是 Ingress API 无法表达 Istio 的路由需求。 Ingress 试图在不同的 HTTP 代理之间取一个公共的交集，因此只能支持最基本的 HTTP 路由，最终导致需要将代理的其他高级功能放入到注解（annotation）中，而注解的方式在多个代理之间是不兼容的，无法移植。Istio Gateway 通过将L4-L6配置与L7配置分离的方式克服了 Ingress 的这些缺点。 Gateway 只用于配置L4-L6功能（例如，对外公开的端口，TLS 配置），所有主流的L7代理均以统一的方式实现了这些功能。 然后，通过在 Gateway 上绑定 VirtualService 的方式，可以使用标准的 Istio 规则来控制进入 Gateway 的 HTTP 和 TCP 流量</li>
</ol>
<p>要理解virtualservice与DestinationRule，先划个重点: <strong>在Istio所提供的基本连接和发现基础上，通过virtualservice，能够将请求路由到Istio网格中的特定服务。每个virtualservice由一组DestinationRule组成，这些DestinationRule使Istio能够将virtualservice的每个给定请求匹配到网格内特定的目标服务(或目标服务子集)</strong></p>
<p>通俗来讲就是，virtualservice定义了服务请求满足什么条件应该转发到哪里，而DestinationRule则定义了这些请求具体的路由规则,比如请求目标服务的什么版本, 负载均衡策略，连接限制等. </p>
<p>以下所有的操作都是基于这个<a href="https://github.com/zhoushuke/distributed-opentracing-on-kubernetes-and-istio">demo</a></p>
<h3 id="VirtualService"><a href="#VirtualService" class="headerlink" title="VirtualService"></a>VirtualService</h3><p>一个简单的virtualservice的yaml文件格式如下.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service-b</span>  <span class="comment">#客户端访问服务的地址，扩展为 service-b.default.svc.cluster.local</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span>         <span class="comment"># 目标服务，这里不一定是service-b服务，也就是说, 虽然客户端访问service-b，但不一定就需要转到service-b，转发到其它服务也是支持的.</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span>      <span class="comment">#扩展为 service-b.default.svc.cluster.local</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span>           <span class="comment">#subset 会在destinationRule中使用</span></span><br></pre></td></tr></table></figure>

<p><strong>VirtualService 映射的就是 Envoy 中的 Http Route Table</strong></p>
<p>需要注意上面hosts字段对应的为kubernetes的service name, 不过这里进行了省略, 在实际使用时会被扩展为FQDN形式的域名.</p>
<h3 id="DestinationRule"><a href="#DestinationRule" class="headerlink" title="DestinationRule"></a>DestinationRule</h3><p>对于上面新建的virtualservice,如果没有对应的destinationrule, 则请求service-b会被返回404(BlackHoleCluster), 因为现在还没有v1版本的service-b,路由不可达</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span>  <span class="comment">#这里的名字需要跟virtualservice中定义的一致</span></span><br><span class="line">  <span class="attr">trafficPolicy:</span></span><br><span class="line">    <span class="attr">loadBalancer:</span></span><br><span class="line">      <span class="attr">simple:</span> <span class="string">RANDOM</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v1</span> <span class="comment">#这里的名字需要跟virtualservice中定义的一致</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v2</span></span><br></pre></td></tr></table></figure>

<p><strong>DestinationRule映射到 Envoy 的配置文件中就是 Cluster</strong></p>
<p>这样，virtualservice创建的路由即可达了, subset机制类似kubernetes中给对应的pod打label</p>
<p>上面虽然定义了subset存在v2的版本, 但是在virtualservice中并没有指定v2，因此100%的流量转向v1.</p>
<p>当然virtualservice与destinatiorule结合支持非常多的属性, 比如流量分配、熔断机制、 错误注入等机制.</p>
<p>总结: DestinationRule 经常和 VirtualService 结合使用，VirtualService 用到的服务子集 subset 在 DestinationRule 上也有定义。同时，在 VirtualService 上定义了一些规则，在 DestinationRule 上也定义了一些规则。那么，DestinationRule 和 VirtualService 都是用于流量治理的，为什么有些定义在 VirtualService 上，有些定义在 DestinationRule 上呢？<br>VirtualService 是一个虚拟 Service，描述的是满足什么条件的流量被哪个后端处理。可以对于这样一个 Restful 服务，每个路由规则都对应其中 Resource 中的资源匹配表达式。只是在 VirtualService 中，这个匹配条件不仅仅是路径方法的匹配，还是更开放的 Match 条件。而 DestinationRule 描述的是这个请求到达某个后端后怎么去处理，即所谓目标的规则，类似以上 Restful 服务到达的目的后端。<br>理解了这两个对象的定义，就不难理解其规则上的设计原理，从而理解负<strong>载均衡和熔断等策略为什么被定义在 DestinationRule 上</strong>。DestinationRule 定义了满足路由规则的流量到达后端后的访问策略。在 istio 中可以配置目标服务的负载均衡策略、连接池大小、异常实例驱逐规则等功能</p>
<h3 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h3><p>服务模型如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/architecture_v2.png"></p>
<p>其中, service-b为3个实例， 分为blue，与green两个版本, github地址在<a href="https://github.com/zhoushuke/distributed-opentracing-on-kubernetes-and-istio">这里</a></p>
<h4 id="流量分配"><a href="#流量分配" class="headerlink" title="流量分配"></a>流量分配</h4><p><code>service-a访问service-b时, 20%的流量转到版本v-blue， 80%的流量转到版本v-green</code></p>
<p>注意: <strong>subset指定的字段需要在pod上存在对应的label, istio不会自动给pod打上标签. 如果所有的pod上都没有该标签, 则路由不可达</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-20</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-blue</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">20</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-green</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-20</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">trafficPolicy:</span></span><br><span class="line">    <span class="attr">loadBalancer:</span></span><br><span class="line">      <span class="attr">simple:</span> <span class="string">ROUND_ROBIN</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v-blue</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v-blue</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v-green</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v-green</span></span><br></pre></td></tr></table></figure>

<p>kubernetes apply  -f servicea-to-serviceb-20-80.yaml</p>
<p>生成的pod如下, 可以看到，总共6个pod, 其中有3个pod有label为version&#x3D;1.5.2，这3个pod是无法响应请求的，</p>
<p>只有verion&#x3D;v-blue或者v-green的可接受请求.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200224175906.png"></p>
<p>从kiali上看效果可以看到, 流量的分配大致维持在2&#x2F;8的比例:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200224183640.png"></p>
<h4 id="超时重试"><a href="#超时重试" class="headerlink" title="超时重试"></a>超时重试</h4><p><code>请求service-b的v-green版本时，出现5xx、connect-failure时进行重试,最大重试5次, 重试超时时间3s,整个请求超时7s</code></p>
<p><strong>并不是所有的http code值都能够进行重试, 不能说指定200进行重试, 这个是做不到的</strong>, 重试的更多参数可参考<a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on">这里</a></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-timeout</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-blue</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">20</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-green</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">retries:</span></span><br><span class="line">      <span class="attr">attempts:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">perTryTimeout:</span> <span class="string">3s</span></span><br><span class="line">      <span class="attr">retryOn:</span> <span class="string">5xx,connect-failure</span></span><br><span class="line">    <span class="attr">timeout:</span> <span class="string">7s</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-timeout</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v-green</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v-green</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v-blue</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v-blue</span></span><br></pre></td></tr></table></figure>

<p>由于使用的这个例子已经打好的镜像, 不想再重新折腾了, 就不演示模拟5xx的场景了.</p>
<h4 id="错误注入"><a href="#错误注入" class="headerlink" title="错误注入"></a>错误注入</h4><p><code>从 service-a访问 service-b的version: v-green的所有流量请求延迟5秒钟，并把流量的100%返回400错误</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-fault</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">fault:</span></span><br><span class="line">      <span class="attr">delay:</span></span><br><span class="line">        <span class="attr">fixedDelay:</span> <span class="string">5s</span></span><br><span class="line">        <span class="attr">percentage:</span>    <span class="comment">#如果未指定，则所有的请求都将 延时5s</span></span><br><span class="line">          <span class="attr">value:</span> <span class="number">100</span></span><br><span class="line">      <span class="attr">abort:</span></span><br><span class="line">        <span class="attr">httpStatus:</span> <span class="number">400</span></span><br><span class="line">        <span class="attr">percentage:</span>    <span class="comment">#如果未指定，则所有的请求都将被返回400</span></span><br><span class="line">          <span class="attr">value:</span> <span class="number">100</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-green</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-fault</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v-green</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v-green</span></span><br></pre></td></tr></table></figure>

<p>效果:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226171926125.png"></p>
<p>从请求时间可以看到, 请求被延迟5s之后直接返回了错误<strong>fault filter abort</strong>,故障注入起到了效果.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226172121466.png"></p>
<p>需要注意的是: fault字段的故障注入是有先后顺序的, 如果把http错误跟时延顺序对调一下, 则效果又将不同.</p>
<h4 id="请求熔断"><a href="#请求熔断" class="headerlink" title="请求熔断"></a>请求熔断</h4><p>熔断机制是作用在DestinationRule上的，这个很好理解, DistinationRule控制着流量转发的具体对象. 一个典型的例子如下:</p>
<p><code>对于service-b的v-green版本的请求, 只允许最大一个连接</code></p>
<p>部署以下规则<code>kubectl apply -f servicea-to-serviceb-circuit-breaker.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-20</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-blue</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">20</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v-green</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">service-b-cb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">service-b-blue</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v-blue</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">service-b-green</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v-green</span></span><br><span class="line">      <span class="attr">trafficPolicy:</span></span><br><span class="line">        <span class="attr">connectionPool:</span></span><br><span class="line">          <span class="attr">http:</span></span><br><span class="line">            <span class="attr">http1MaxPendingRequests:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">maxRequestsPerConnection:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">tcp:</span></span><br><span class="line">            <span class="attr">maxConnections:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">outlierDetection:</span></span><br><span class="line">            <span class="attr">baseEjectionTime:</span> <span class="string">10s</span></span><br><span class="line">            <span class="attr">consecutiveErrors:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">interval:</span> <span class="string">10s</span></span><br><span class="line">            <span class="attr">maxEjectionPercent:</span> <span class="number">100</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>关于trafficPolicy的更多参数说明, 可参考<a href="https://www.servicemesher.com/istio-handbook/traffic-management/circuit-breaking-and-outlier-detection-in-istio.html">这里</a></p>
<p>为了测试效果，需要安装一个测试工具, <a href="https://github.com/istio/fortio">fortio</a>,这个工具在istio官方的httpbin目录下也存在, 可直接部署</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">samples/httpbin/sample-client/fortio-deploy.yaml</span></span><br></pre></td></tr></table></figure>

<p>效果: </p>
<p><strong>先测试一个连接的情况</strong>:</p>
<p><code>kubectl exec -it fortio-deploy-7cb865f87f-vkkhz -c fortio /usr/bin/fortio -- load -c 1 -qps 0 -n 2 -loglevel Warning http://10.244.0.97:8088/api/v1/greeting</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200227120016154.png"></p>
<p>一个连接的情况,发送的两个请求都正常返回</p>
<p><strong>测试三个连接的情况:</strong></p>
<p><code>kubectl exec -it fortio-deploy-7cb865f87f-vkkhz -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 30 -loglevel Warning http://10.244.0.97:8088/api/v1/greeting</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200227121217432.png"></p>
<p>上图中显示code -1为100%, service-a有如下红框标出的报错, 说明限制的连接数起到了熔断的作用.</p>
<p>service-a前三条有正常日志,是因为service-a调用的链路里除了调用service-b,还调用了service-c, service-c这条跟没有限制.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200227120836722.png"></p>
<p>当然还有一些如HTTP Redirect，HTTP Rewrite等操作都能在Istio实现, 但一般来讲都会选择在入口处实现, 具体的可参考<a href="https://istio.io/docs/reference/config/networking/virtual-service/">这里</a></p>
<h3 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h3><p>在使用kiali的过程中, 或多或少会遇到istio的warning提示, 官方的kiali的错误说明<a href="https://kiali.io/documentation/validations/">列表</a></p>
<h4 id="More-than-one-Virtual-Service-for-same-host"><a href="#More-than-one-Virtual-Service-for-same-host" class="headerlink" title="More than one Virtual Service for same host"></a><a href="https://kiali.io/documentation/validations/#_more_than_one_virtual_service_for_same_host">More than one Virtual Service for same host</a></h4><p> <img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226161851384.png"></p>
<p>出这个warning提示不影响istio的功能，原因在于存在多个virtualservice作用在service-b这个host上, 虽然Istio将合并配置，但istio建议不要在多个虚拟服务定义中定义相同的部分</p>
<p>这里是因为存在另一个virtualservice，host一列中定义了”*“， 自然包含了service-b这个host，因此重合了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226162402855.png"></p>
<h4 id="More-than-one-Gateway-for-the-same-host-port-combination"><a href="#More-than-one-Gateway-for-the-same-host-port-combination" class="headerlink" title="More than one Gateway for the same host port combination"></a><a href="https://kiali.io/documentation/validations/#_more_than_one_gateway_for_the_same_host_port_combination">More than one Gateway for the same host port combination</a></h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226162612642.png"></p>
<p>这个warning提示原因同上, 也因存在多个gateway作用于同一个hosts.</p>
<p><strong>通过上面两个提示可以看出, 只要存在一个gateway或者virtualservice，使用了 hosts为”*“的定义，就能出现这个报错.</strong></p>
<h4 id="mTLS-settings-of-a-non-local-Destination-Rule-are-overridden"><a href="#mTLS-settings-of-a-non-local-Destination-Rule-are-overridden" class="headerlink" title="mTLS settings of a non-local Destination Rule are overridden"></a><a href="https://kiali.io/documentation/validations/#_mtls_settings_of_a_non_local_destination_rule_are_overridden">mTLS settings of a non-local Destination Rule are overridden</a></h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200226163310584.png"></p>
<p>原因在于不存在mtls配置</p>
<p>可在spec指定即可.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">service-b</span></span><br><span class="line">  <span class="attr">trafficPolicy:</span></span><br><span class="line">    <span class="attr">tls:</span></span><br><span class="line">      <span class="attr">mode:</span> <span class="string">SIMPLE</span></span><br></pre></td></tr></table></figure>



<h4 id="ingressgateway-使用-NodePort暴露服务"><a href="#ingressgateway-使用-NodePort暴露服务" class="headerlink" title="ingressgateway 使用 NodePort暴露服务."></a>ingressgateway 使用 NodePort暴露服务.</h4><p>如果在测试的时候使用的nodeport的方式暴露服务(不是loadbalance), 在gateway&#x2F;virtualservice需要自定义域名的时候, 比如官方的bookinfo的例子，如果想通过bookinfo.local:30732&#x2F;productpage访问 productpage服务, hosts字段必须为”*“, 写bookinfo.local是行不通的,参考官方<a href="https://istio.io/docs/tasks/traffic-management/ingress/ingress-control/#accessing-ingress-services-using-a-browser">说明</a></p>
<p>这是因为当使用浏览器访问bookinfo.local:30732&#x2F;productpage时(&#x2F;etc&#x2F;hosts中指定了域名解析)，通过浏览器的debug可以看到请求中的host为Host: bookinfo.local:30732， 到达gateway时发现host中的域名(没有端口)跟请求中的域名比不对，导致404，同时, gateway中不支持域名:端口的形式，会报错, 因此, 这种情况下只能使用”*“</p>
<p>当然，在生产环境中不太可能使用Nodeport的形式暴露服务.同时也会有DNS解析服务, 因此也不太可能有上述问题.我当时测试时没想通耽误了一些时间，因此记录一下.</p>
<h4 id="只定义gateway-没有virtualservice"><a href="#只定义gateway-没有virtualservice" class="headerlink" title="只定义gateway, 没有virtualservice"></a>只定义gateway, 没有virtualservice</h4><p>最后, virtualservice还可以与gateway结合使用, 通过gateway向外暴露istio服务网格内的服务, 但当只定义了gateway而没有virtualservice时，请求会被转发到blackhole, 返回404, <a href="https://izsk.me/2020/02/16/Istio-Gateway/">Istio-Gsteway</a></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span>  <span class="string">istioctl</span> <span class="string">proxy-config</span> <span class="string">route</span> <span class="string">$INGRESS_POD</span> <span class="string">-o</span> <span class="string">json</span>  <span class="string">-n</span> <span class="string">istio-system</span></span><br><span class="line"> [</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;http.80&quot;</span>,</span><br><span class="line">         <span class="attr">&quot;virtualHosts&quot;:</span> [</span><br><span class="line">             &#123;</span><br><span class="line">                 <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;blackhole:80&quot;</span>,</span><br><span class="line">                 <span class="attr">&quot;domains&quot;:</span> [</span><br><span class="line">                     <span class="string">&quot;*&quot;</span></span><br><span class="line">                 ],</span><br><span class="line">                 <span class="attr">&quot;routes&quot;:</span> [</span><br><span class="line">                     &#123;</span><br><span class="line">                         <span class="attr">&quot;match&quot;:</span> &#123;</span><br><span class="line">                             <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;/&quot;</span></span><br><span class="line">                         &#125;,</span><br><span class="line">                         <span class="attr">&quot;directResponse&quot;:</span> &#123;</span><br><span class="line">                             <span class="attr">&quot;status&quot;:</span> <span class="number">404</span></span><br><span class="line">                         &#125;,</span><br><span class="line">                         <span class="attr">&quot;perFilterConfig&quot;:</span> &#123;</span><br><span class="line">                             <span class="attr">&quot;mixer&quot;:</span> &#123;&#125;</span><br><span class="line">                         &#125;</span><br><span class="line">                     &#125;</span><br><span class="line">                 ]</span><br><span class="line">             &#125;</span><br><span class="line">         ],</span><br><span class="line">         <span class="attr">&quot;validateClusters&quot;:</span> <span class="literal">false</span></span><br><span class="line">     &#125;</span><br><span class="line"> ]</span><br></pre></td></tr></table></figure>





<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&amp;mid=2247483799&amp;idx=1&amp;sn=482cf48661eafcdb072450286cd8bc97&amp;chksm=fbee415acc99c84c7a7ef3f2da726815203097f62bb8409102c619a1a1c6e7e3ea85c0154e76&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&amp;mid=2247483799&amp;idx=1&amp;sn=482cf48661eafcdb072450286cd8bc97&amp;chksm=fbee415acc99c84c7a7ef3f2da726815203097f62bb8409102c619a1a1c6e7e3ea85c0154e76&amp;scene=21#wechat_redirect</a></li>
<li><a href="https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/">https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/</a></li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></li>
<li><a href="https://jimmysong.io/posts/istio-traffic-management-basic-concepts/">https://jimmysong.io/posts/istio-traffic-management-basic-concepts/</a></li>
<li><a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/</a></li>
<li><a href="https://kiali.io/documentation/validations/#_more_than_one_gateway_for_the_same_host_port_combination">https://kiali.io/documentation/validations/#_more_than_one_gateway_for_the_same_host_port_combination</a></li>
<li><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on">https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on</a></li>
<li><a href="https://octopus.com/blog/istio/istio-virtualservice">https://octopus.com/blog/istio/istio-virtualservice</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(在Kubernetes集群中安装istio)</title>
    <url>/2020/01/03/Istio-Install-Upon-Kubernetes/</url>
    <content><![CDATA[<p>Istio做为ServiceMesh的事实标准, 越来越被广泛地使用, 特别是work on Kubernetes之后, 再加持Istio来做更加细度地诸如流量管理、访问控制、灰度发布等方面的事情，不要更爽. </p>
<span id="more"></span>



<h3 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h3><p>Istio的部署还是非常简单的, 先下载istioctl二进制文件进行安装, 参考<a href="https://istio.io/docs/setup/getting-started/">这里</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -L https://istio.io/downloadIstio | sh -</span><br><span class="line">cd istio-1.4.2</span><br><span class="line">export PATH=$PWD/bin:$PATH</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果没有loadBalance需要修改发下文件:</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">install/kubernetes/istio-demo.yaml中istio-ingressgateway的service从loadBalance修改成NodePort</span></span><br></pre></td></tr></table></figure>



<h3 id="默认Profile安装"><a href="#默认Profile安装" class="headerlink" title="默认Profile安装"></a>默认Profile安装</h3><p>istio内置几种默认的Profile, 某种Profile下有默认的manifest文件, 定义了需要安装的组件, 在安装的时候可以指定Profile进行安装, 各Profile如下, 详细的信息可查看<a href="https://istio.io/docs/setup/additional-setup/config-profiles/">这里</a></p>
<table>
<thead>
<tr>
<th></th>
<th>default</th>
<th>demo</th>
<th>minimal</th>
<th>sds</th>
<th>remote</th>
</tr>
</thead>
<tbody><tr>
<td>Core components</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>istio-citadel</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><code>istio-egressgateway</code></td>
<td></td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>istio-galley</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr>
<td><code>istio-ingressgateway</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr>
<td><code>istio-nodeagent</code></td>
<td></td>
<td></td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr>
<td><code>istio-pilot</code></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td></td>
</tr>
<tr>
<td><code>istio-policy</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr>
<td><code>istio-sidecar-injector</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><code>istio-telemetry</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
<tr>
<td>Addons</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>grafana</code></td>
<td></td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>istio-tracing</code></td>
<td></td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>kiali</code></td>
<td></td>
<td>X</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>prometheus</code></td>
<td>X</td>
<td>X</td>
<td></td>
<td>X</td>
<td></td>
</tr>
</tbody></table>
<p>注: 显示X的为在该Profile模式下需要安装的组件.</p>
<p>每种Profile说明如下:</p>
<blockquote>
<ul>
<li><strong>default</strong>：根据<a href="https://istio.io/docs/reference/config/istio.operator.v1alpha12.pb/"><code>IstioControlPlane</code>API</a>的默认设置启用组件 （建议用于生产部署）。您可以通过运行命令显示默认设置<code>istioctl profile dump</code></li>
<li><strong>demo</strong>：旨在展示Istio功能且资源需求适中的配置。适合运行<a href="https://istio.io/docs/examples/bookinfo/">Bookinfo</a>应用程序和相关任务。这是随<a href="https://istio.io/docs/setup/getting-started/">快速入门</a>说明一起安装的配置，但是 如果您想探索更高级的任务，则可以稍后<a href="https://istio.io/docs/setup/install/istioctl/#customizing-the-configuration">自定义配置</a>以启用其他功能此配置文件可实现高级别的跟踪和访问日志记录，因此不适合进行性能测试。</li>
<li><strong>minimal</strong>：使用Istio的<a href="https://istio.io/docs/tasks/traffic-management/">流量管理</a>功能所需的最少组件集。</li>
<li><strong>sds</strong>：类似于<strong>默认</strong>配置文件，但也启用了Istio的<a href="https://istio.io/docs/tasks/security/citadel-config/auth-sds">SDS（秘密发现服务）</a>。此配置文件具有默认情况下启用的其他身份验证功能（严格双向TLS）。</li>
<li><strong>远程</strong>：用于配置的远程簇 <a href="https://istio.io/docs/ops/deployment/deployment-models/#multiple-clusters">的多组啮合</a>与 <a href="https://istio.io/docs/setup/install/multicluster/shared-vpn/">共享控制平面</a>配置。</li>
</ul>
</blockquote>
<p>因为demo涉及到的组件最符合目前的需求, 使用以下命令生成manifest文件并按该manifest部署.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl manifest apply --<span class="built_in">set</span> profile=demo</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209151532.png"></p>
<p>在Kubernetes中查看生成的CRD的对象</p>
<p><code>kubectl get CustomResourceDefinition</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209151612.png"></p>
<p>生成的pod信息如下</p>
<p><code>kubectl get pod -n istio-system</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209151644.png"></p>
<p>以上pod的功能:</p>
<blockquote>
<ul>
<li>grafana-*              &#x2F;&#x2F;监控数据可视化工具</li>
<li>istio-citadel－*    &#x2F;&#x2F;证书管理</li>
<li>istio-egressgateway-*  &#x2F;&#x2F;出口流量网关</li>
<li>istio-galley－*     &#x2F;&#x2F;配置检查</li>
<li>istio-ingressgateway-*  &#x2F;&#x2F;入口流量网关</li>
<li>istio-pilot-*         &#x2F;&#x2F;Envoy 服务发现，外部化配置</li>
<li>istio-policy-*      &#x2F;&#x2F;Mixer 混合器策略检查</li>
<li>istio-sidecar-injector-*         &#x2F;&#x2F;边车注入</li>
<li>istio-telemetry-*       &#x2F;&#x2F;Mixer混合器指标收集</li>
<li>Istio-tracing-*          &#x2F;&#x2F; 全链路跟踪工具</li>
<li>kiali-*                         &#x2F;&#x2F;Service Mesh可视化工具</li>
<li>prometheus-*          &#x2F;&#x2F;监控报警</li>
</ul>
</blockquote>
<h3 id="自定义安装"><a href="#自定义安装" class="headerlink" title="自定义安装"></a>自定义安装</h3><p>当默认的Profile无法满足业务需求的时候, 比如，目前系统中存在了<code>Prometheus</code>, 不想在istio再次安装，同时又需要<code>Tracing</code>跟<code>kiali</code>那上面的所有profile都无法满足, istio也提供了可自定义安装，使用<code>--set</code>开关</p>
<p>比如可使用以下命令实现以下的需求</p>
<p><code> istioctl manifest apply --set profile=demo --set telemetry.prometheus.enabled=false --set telemetry.prometheusOperator.enabled=false</code></p>
<p>这样就从demo中把原本会安装的Prometheus修改为不安装</p>
<p>组件的对应关系如下:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Components</th>
</tr>
</thead>
<tbody><tr>
<td><code>base</code></td>
<td><code>CRDs</code></td>
</tr>
<tr>
<td><code>trafficManagement</code></td>
<td><code>pilot</code></td>
</tr>
<tr>
<td><code>policy</code></td>
<td><code>policy</code></td>
</tr>
<tr>
<td><code>telemetry</code></td>
<td><code>telemetry</code></td>
</tr>
<tr>
<td><code>security</code></td>
<td><code>citadel</code>, <code>nodeAgent</code>, <code>certManager</code></td>
</tr>
<tr>
<td><code>configManagement</code></td>
<td><code>galley</code></td>
</tr>
<tr>
<td><code>gateways</code></td>
<td><code>ingressGateway</code>, <code>egressGateway</code></td>
</tr>
<tr>
<td><code>autoInjection</code></td>
<td><code>injector</code></td>
</tr>
<tr>
<td><code>coreDNS</code></td>
<td><code>coreDNS</code></td>
</tr>
<tr>
<td><code>thirdParty</code></td>
<td><code>cni</code></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Components</th>
</tr>
</thead>
<tbody><tr>
<td><code>telemetry</code></td>
<td><code>prometheus</code>, <code>prometheusOperator</code>, <code>grafana</code>, <code>kiali</code>, <code>tracing</code></td>
</tr>
</tbody></table>
<p>至此, Istio已经在Kubernetes中安装完成.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://istio.io/docs/setup/additional-setup/config-profiles/">https://istio.io/docs/setup/additional-setup/config-profiles/</a></li>
<li><a href="https://juejin.im/post/5e0062ae6fb9a0163a483ea5">https://juejin.im/post/5e0062ae6fb9a0163a483ea5</a></li>
<li><a href="https://istio.io/docs/setup/getting-started/">https://istio.io/docs/setup/getting-started/</a></li>
<li><a href="https://istio.io/docs/setup/install/istioctl/#customizing-the-configuration">https://istio.io/docs/setup/install/istioctl/#customizing-the-configuration</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(架构)</title>
    <url>/2020/02/09/Istio-Struct/</url>
    <content><![CDATA[<p>上次在Kubernetes中安装好Istio之后, 忙了很长的时间在处理另一个事, 今天学习一下istio的架构及一些概念，后续分析整个数据的流向.</p>
<span id="more"></span>



<h3 id="WhatFor"><a href="#WhatFor" class="headerlink" title="WhatFor"></a>WhatFor</h3><p>先来看看istio能做什么，从<a href="istio.io">官网</a>的首页很明确地指出了istio的作用</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209153912.png"></p>
<p>主要提供四个方面的能力, 简单总结一下:</p>
<blockquote>
<ul>
<li>Connect: 主要用于工作负载之内的流量管理</li>
<li>Secure: 主要用于工作负载之间的认证</li>
<li>Control: 主要用于配置策略及遥测功能</li>
<li>Observe: 主要用于监控、日志、调用链等可观察性功能</li>
</ul>
</blockquote>
<p><code>从实际来看, 虽然Istio支持了很多的平台, 但是与之最为紧密结合使用的还是Kubernetes.</code></p>
<h3 id="Struct"><a href="#Struct" class="headerlink" title="Struct"></a>Struct</h3><p>istio的整体架构图</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209154612.png"></p>
<p>再细分, istio分为控制面及数据面, 如下: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209212447.png"></p>
<p>从架构图可以看出几个很重要的信息</p>
<blockquote>
<ul>
<li>istio在数据面的工作负载中使用了<code>sidecar</code>(envoy)功能,这个是kubernets原生支持的</li>
<li>istio的原理借鉴了kubernetes的service设计理念, 在kubernetes中，kube-proxy劫持的是进出kubernetes节点的流量，而istio的sidecar则劫持的是进出pod的流量, Kubernetes 管理的对象是 Pod，那么 Service Mesh 中管理的对象就是一个个 Service</li>
<li><code>istio使用了Adapter来对接抽象各种平台能力, 这使得istio的拓展性更强.</code></li>
</ul>
</blockquote>
<p>具体sidecar是如何劫持流量并转发出去的，不在这篇说明. </p>
<p>再来简单说明一下各个组件的作用.</p>
<h3 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h3><h4 id="数据面"><a href="#数据面" class="headerlink" title="数据面"></a>数据面</h4><h5 id="envoy"><a href="#envoy" class="headerlink" title="envoy"></a>envoy</h5><p>数据面很简单, 就一个proxy(envoy)</p>
<p>envoy有如下的特点:</p>
<blockquote>
<ul>
<li>HTTP 7层路由</li>
<li>支持gRPC、HTTP&#x2F;2</li>
<li>服务发现和动态配置</li>
<li>健康检查</li>
<li>高级负载均衡</li>
</ul>
</blockquote>
<p><code>简单来说, istio通过注入(手工注入入自动注入)等方式将envoy加入到业务Pod中, 然后通过initContainer生成的iptables规则将所在进出业务容器的流量都进行劫持</code></p>
<h4 id="控制面"><a href="#控制面" class="headerlink" title="控制面"></a>控制面</h4><h5 id="pilot"><a href="#pilot" class="headerlink" title="pilot"></a>pilot</h5><p><code>Pilot 是Istio实现流量管理的核心组件，它主要的作用是配置和管理Envoy代理</code></p>
<blockquote>
<ul>
<li>从平台（如Kubernetes）获取服务信息，完成服务发现</li>
<li>获取Istio的各项配置，转换成Envoy代理可读的格式并分发, 使用XDS协议.</li>
</ul>
</blockquote>
<p>piolt架构图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209155817.png"></p>
<h5 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h5><p><code>Mixer的主要功能是提供策略控制，并从Envoy代理收集遥测数据</code>。每次网络通信时Envoy代理都会向Mixer发出预检要求，用来检测调用者的合法性。调用之后Envoy代理会发送遥测数据供Mixer收集。一般情况下Sidecar代理可以缓存这些数据，不需要频繁地调用Mixer.</p>
<p>​	同时,<code>Mixer彩用配器模式</code>, 可以支持不同的后端，如日志后端，监控后端, 调用链后端.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209160002.png"></p>
<h5 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h5><p>Citadel是与安全相关的组件，主要负责密钥和证书的管理。它可以提供服务间和终端用户的身份认证，还可以加密服务网格中的流量。</p>
<h5 id="galley"><a href="#galley" class="headerlink" title="galley"></a>galley</h5><p>在2019年3月份发布的1.1版本中，Gally作为一个独立的组件被添加到了架构当中（在此之前的版本中Gally并未独立出现），它现在是Istio主要的配置管理组件，负责配置的获取、处理和分发。Gally使用了一种叫作MCP（Mesh Configuration Protocol，网格配置协议）的协议与其他组件进行通信。</p>
<p>重点学习Pilot及Mix即可, citadel及galley一般都不会变动频繁, 知道怎么一回事就可以了.</p>
<p>上面主要学习了下istio整体的框架,没有说明地很详细, 至于istio如何注入sidecar, sidecar又是如何劫持流向转发, 又是如何找到对端的容器地址的，挺有意思, 后续再更.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://jimmysong.io/istio-handbook/concepts/sidecar-injection-deep-dive.html">https://jimmysong.io/istio-handbook/concepts/sidecar-injection-deep-dive.html</a></li>
<li><a href="https://jimmysong.io/istio-handbook/concepts/sidecar-traffic-routing-deep-dive.html">https://jimmysong.io/istio-handbook/concepts/sidecar-traffic-routing-deep-dive.html</a></li>
<li><a href="https://mp.weixin.qq.com/s/C-NoiknOj9cga7FYUjdKaA">https://mp.weixin.qq.com/s/C-NoiknOj9cga7FYUjdKaA</a></li>
<li><a href="https://jimmysong.io/">https://jimmysong.io</a></li>
<li><a href="https://istio.io/">Https://istio.io</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(istioctl常用命令)</title>
    <url>/2020/02/14/Istio-istioctl/</url>
    <content><![CDATA[<p>istioctl是istio的客户端命令行工具, 常用于对istio中的出现的各种问题进行诊断.istioctl支持非常多的命令, 这篇主要以proxy status、 proxy-config两个命令为主, 更多其它的命令, 可参考<a href="https://istio.io/docs/reference/commands/istioctl">这里</a></p>
<p>所有操作都是基于官方提供的book-info示例进行，也是对上一篇对envoy的配置文件进行流量分析的补充.</p>
<span id="more"></span>



<h3 id="istioctl-proxy-status"><a href="#istioctl-proxy-status" class="headerlink" title="istioctl proxy-status"></a>istioctl proxy-status</h3><p>istioctl proxy-status, 可以简写这istioctl ps， 一般用于检测envoy是否与pilot的连接成功, 如果不成功, 则无法获得pilot下发的配置文件</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210174020.png"></p>
<h3 id="istioctl-proxy-config"><a href="#istioctl-proxy-config" class="headerlink" title="istioctl proxy-config"></a>istioctl proxy-config</h3><p>Istioctl proxy-config, 可以简写为istioctl pc, 主要用于查看各种资源对象，这里主要列举几个比较常用的命令</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210174442.png"></p>
<p>istioctl是个多级命令行工具, 都可直接通过-h 来查看用法</p>
<p>比如这里可以使用istioctl pc bootstrap -h 来查看二级命令bootstrap能够接受的参数</p>
<p>以下都会使用details-v1-78d78fbddf-z9cpj.default这个应用来做为示例</p>
<p>比如说， <code>从details中访问reviews:9080, envoy的处理逻辑是这样的</code></p>
<p>listener –&gt; route –&gt; cluster –&gt; endpoint</p>
<p>下面通过istioctl命令行进行分析</p>
<h4 id="pc-bootstrap"><a href="#pc-bootstrap" class="headerlink" title="pc bootstrap"></a>pc bootstrap</h4><p> 检索pod 中 Envoy 实例的 bootstrap 配置的信息</p>
<p>从这个命令可以得到envoy启动时的一些静态参数，比如pilot地址, prometheus地址等. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc bootstrap details-v1-78d78fbddf-z9cpj.default</span><br></pre></td></tr></table></figure>



<h4 id="pc-listener"><a href="#pc-listener" class="headerlink" title="pc listener"></a>pc listener</h4><p>检索pod 中 Envoy 实例的监听器配置的信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc listener details-v1-78d78fbddf-z9cpj.default --port=9080 --address=0.0.0.0</span><br></pre></td></tr></table></figure>

<p>从listener中获取routeconfigname:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210185242.png"></p>
<h4 id="pc-route"><a href="#pc-route" class="headerlink" title="pc route"></a>pc route</h4><p>检索pod中Envoy 实例的路由配置的信息, 从listener获取到的9080的routeconfigname来获取route</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc route details-v1-78d78fbddf-z9cpj.default</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210184528.png"></p>
<p>可以看到跟9080相关的route有5条, 找到与reviews相关的信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc route details-v1-78d78fbddf-z9cpj.default --name=9080 -ojson</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210190501.png"></p>
<p>virutalhosts中的domains一项列出的所有的跟reviews相关的域名及域名:端口,<code>envoy会把domain中的字符串与请求中的host做对比</code></p>
<p>这里匹配&#x2F; 然后转到cluster为<code>outbound|9080||reviews.default.svc.cluster.local</code></p>
<h4 id="pc-cluster"><a href="#pc-cluster" class="headerlink" title="pc cluster"></a>pc cluster</h4><p>检索pod 中 Envoy 实例的集群配置的信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc cluster details-v1-78d78fbddf-z9cpj.default</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210181002.png"></p>
<p>说明</p>
<ul>
<li>PORT: 指定端口, 一般与kubernetes中的svc端口一致</li>
<li>SUBSET: 使用该字段对集群进行过滤</li>
<li>DIRECTION: 表明是出向流量还是入向流量</li>
<li>TYPE: 这个svc是静态(STATIC)配置还是通过EDS动态获取的</li>
</ul>
<p>使用–fqdn 指定svc, 先看一个动态类型(EDS)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc cluster details-v1-78d78fbddf-z9cpj.default --fqdn details.default.svc.cluster.local --direction outbound -ojson</span><br></pre></td></tr></table></figure>

<p>![image-20200210180657173](&#x2F;Users&#x2F;zhoushuke&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200210180657173.png)</p>
<p>再看一个静态类型(STATIC)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc cluster details-v1-78d78fbddf-z9cpj.default --direction inbound  --fqdn details.default.svc.cluster.local -ojson</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210181845.png"></p>
<p>如果是入向流量, 在cluster中的endpoint是127.0.0.1，因为这时请求已经被转发到了该容器上,直接响应即可</p>
<p>这里找到reviews与有关的cluster</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc cluster details-v1-78d78fbddf-z9cpj.default --fqdn=reviews.default.svc.cluster.local -ojson</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210190943.png"></p>
<h4 id="pc-endpoint"><a href="#pc-endpoint" class="headerlink" title="pc endpoint"></a>pc endpoint</h4><p>检索特定 pod 中 Envoy 实例的 endpoint 配置的信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc endpoints details-v1-78d78fbddf-z9cpj.default</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210182450.png"></p>
<p>第一列的endpoint即为真正的业务容器ip，如果实例有多个话就会有多行</p>
<p>比如这里通过上面动态类型的cluster来获取ep</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istioctl pc endpoints details-v1-78d78fbddf-z9cpj.default --cluster=<span class="string">&quot;outbound|9080||reviews.default.svc.cluster.local&quot;</span></span><br></pre></td></tr></table></figure>

<p>![image-20200210191055227](&#x2F;Users&#x2F;zhoushuke&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200210191055227.png)</p>
<p>由于reviews有3个实例, 因此有3条记录</p>
<p>最终根据发起方配置的负载均衡策略选出一个实例进行响应</p>
<p>至此整个请求路由就完成, 当然实际使用中还会进行一些其它的操作如重试、熔断等策略, 其原理是一样的.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.bookstack.cn/read/istio-1.4-zh/9e3da989f08c46bf.md">https://www.bookstack.cn/read/istio-1.4-zh/9e3da989f08c46bf.md</a></li>
<li><a href="https://istio.io/docs/reference/commands/istioctl/">https://istio.io/docs/reference/commands/istioctl/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(jmeter基础使用)</title>
    <url>/2020/03/09/Jmeter-ops/</url>
    <content><![CDATA[<p>最近kubernetes新上线了个服务,需要基于Kubernetes架构下压测一下该服务的性能, 之前用过ab, 但是ab还是相对简单了点, 网上调研了一番发现了jmeter这个由apache出品的压测工具,特别是报告非常清晰, 使用起来还挺简单.</p>
<span id="more"></span>



<h3 id="java环境配置"><a href="#java环境配置" class="headerlink" title="java环境配置"></a>java环境配置</h3><p>jmeter是基于java开发的,因此需要有java环境且java的版本需要1.8版本及以上</p>
<p>java环境的配置不再详述</p>
<p>以下测试基于Mac系统</p>
<h3 id="jmeter安装"><a href="#jmeter安装" class="headerlink" title="jmeter安装"></a>jmeter安装</h3><p>我在使用tgz包安装时提示以下错误, 开始以为是java环境变量设置的问题, 后来一顿操作始终无法解决,后来直接使用了<code>brew install jmeter</code>安装后就没问题了.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200401181115.png"></p>
<p>补充: 当使用brew安装完jmeter后我使用非root用户使用<code>sh jmeter</code>启动发现居然没问题了, 不清楚上面的报错是因为使用了root还是其它问题被brew install之后给修复了, 现在能用就不管了.</p>
<h3 id="jmeter启动"><a href="#jmeter启动" class="headerlink" title="jmeter启动"></a>jmeter启动</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zxf apache-jmeter-5.2.1.tgz</span><br><span class="line">cd apache-jmeter-5.2.1/bin</span><br><span class="line">sh jmeter</span><br></pre></td></tr></table></figure>

<p>即可打开gui界面, 当然在bin下还有很多启动脚本, 对应jmeter不同的用途, 可查看官<a href="https://jmeter.apache.org/usermanual/get-started.html">文档</a></p>
<h3 id="TestPlan"><a href="#TestPlan" class="headerlink" title="TestPlan"></a>TestPlan</h3><p>因为我这里已经配置了几个test case， 第一次启动看到的界面在左侧的节点只有一个TestPlan</p>
<p>依次在TestPlan右键Add –&gt; Threads(Users) –&gt; ThreadGroup 即会出现配置界面, 如下图</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402133047.png"></p>
<p>ThreadGroup相当于比如说要模拟多少个用户进行操作就会组成一个threadgroup, 每个用户使用一个thread</p>
<p>这里有几个重要的参数说明一下:</p>
<blockquote>
<ul>
<li><p>Number of Threads(users): 从users来看也说明了指定需要模拟多少个用户, 比如500</p>
</li>
<li><p>Ramp-up period(seconds): 这个参数比较重要, 指定Number of Threads指定的用户数需要在多少秒内启动, 比如这里设置为25, 说明500个用户需要在25秒内启动完,也就是每秒都会启动20(500&#x2F;25)个用户.</p>
<p><strong>如果这个值不填或者为0, 说明立即启动所有的用户,相当于完全的并发</strong></p>
</li>
<li><p>Loop Count: 循环的次数, 可以指定整个流程循环的次数, 也可以勾选<code>infinite</code> 永远运行</p>
</li>
<li><p>Specify Thread lifetime: (使用该操作需要勾选<code>infinite</code>)也可以指定持续运行的时间, 比如需要500用户持续操作2小时(7200s)</p>
<ul>
<li>Duration(seconds): 持续时间</li>
<li>Startup delay(seconds): 延迟启动时间, 可为空, 表示不延迟</li>
</ul>
</li>
</ul>
</blockquote>
<p>以上参数跟业务场景息息相关.</p>
<p>举个例子: 比如 Number of Thread(Users): 500, Ramp-up period(seconds): 50, Duration(seconds): 300</p>
<p>整个测试意思是: jmeter将在50s内将启动和运行500个用户, 每隔10s增加10个, 最终在50s(不是精确值)后会同时存在500个用户, 整个测试流程持续300s</p>
<p>thread数据的变化可以从测试结果反应出来</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402160241.png"></p>
<p>测试是从15:18开始的, 从大约在15:20时 达到了thread&#x3D;500, 从这里也，也能反应Ramp-up period不是个精确值</p>
<h3 id="HTTP-HEADER-MANAGER"><a href="#HTTP-HEADER-MANAGER" class="headerlink" title="HTTP HEADER MANAGER"></a>HTTP HEADER MANAGER</h3><p>http请求时大多都需要定义http header, jmeter支持定义header，那么在每次请求时都会使用该header</p>
<p>ThreadGroup右键 Add –&gt; Config Element –&gt; HTTP HEADER MANAGER</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402140140.png"></p>
<h3 id="HTTP-REQUEST"><a href="#HTTP-REQUEST" class="headerlink" title="HTTP REQUEST"></a>HTTP REQUEST</h3><h4 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402140302.png"></p>
<p>可以在Parameters指定url参数或者在BodyData一栏指定请求体</p>
<p>所有参数都指定完之后即可保存该测试计划, 注意<strong>一定不要在gui页面执行</strong>, gui只是用于调试使用, 使用命令行执行整个测试计划, 这个在jmeter启动时也有warning提示, 同时也建议调大HEAP的值, 这个可以直接在jmeter脚本中改</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402140933.png"></p>
<h4 id="post请求"><a href="#post请求" class="headerlink" title="post请求"></a>post请求</h4><p>如果是post请求, Method选择post, 同时也支持在Parameters指定url参数或者在BodyData一栏指定请求体,</p>
<p>还可以通过直接上传文件的方式</p>
<h3 id="Assert"><a href="#Assert" class="headerlink" title="Assert"></a>Assert</h3><p>断言是个十分有用的元件, 通过assert来判断压测的请求是否符合预期, <strong>不符合预期的请求最终将反应在报告中</strong>.</p>
<p>assert的工作原理就是在response返回的数据中通过对比assert定义的动作来看是否存在相符的情况</p>
<p>官方支持的assert有13种之多, 几乎涵盖了大部分的场景, 用的最多的是ResponseAssertion,通过返回信息来判断</p>
<p>ThreadGroup右键 Add –&gt; Assertions –&gt; responseAssert</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402151546.png"></p>
<p>同时也可以自定义产生错误时的msg.</p>
<p>增加断言动作后还可以增加查看断言结果, 如果出现不符合预期的结果将显示在这里</p>
<p>ThreadGroup右键 Add –&gt; Listener –&gt; AssertResult</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402151616.png"></p>
<h3 id="命令行执行"><a href="#命令行执行" class="headerlink" title="命令行执行"></a>命令行执行</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jmeter -n -t xxx.jmx -l k8s.csv -e -o k8s</span><br><span class="line"><span class="comment"># 可以使用jmeter -?打印参数说明</span></span><br><span class="line"><span class="comment"># -n: 表示non-gui, 也就是不启动gui</span></span><br><span class="line"><span class="comment"># -t: 保存的测试计划文件</span></span><br><span class="line"><span class="comment"># -l: 测试结果文件，jtl或者csv格式</span></span><br><span class="line"><span class="comment"># -e: 设置生成测试报告</span></span><br><span class="line"><span class="comment"># -o: web测试结果保存目录, 这个目录必须为空或者不存在</span></span><br></pre></td></tr></table></figure>

<p>执行结果如下图: 其中的错误就是assert的原由</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402153623.png"></p>
<p>执行完成之后将会生成一个k8s.csv及k8s文件夹, 该文件夹内有web类型的报告，web报告的来源是通过 -l参数指定的jtl或csv文件,有过解析该文件生成html.</p>
<p>报告文件类似下图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200402141535.png"></p>
<p>比如重要的是Dashboard一栏, 还有ResponseTime及LatencyTime等相关的黄金指标.</p>
<p>jmeter能够覆盖的场景非常广泛, 官网的文档也非常清晰, 后续有需求再深入研究下.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://jmeter.apache.org/usermanual/test_plan.html">https://jmeter.apache.org/usermanual/test_plan.html</a></li>
<li><a href="https://www.cnblogs.com/imyalost/p/6024306.html">https://www.cnblogs.com/imyalost/p/6024306.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(kong basic-auth及basic-auth-api认证)</title>
    <url>/2020/10/28/Kong-basic-auth/</url>
    <content><![CDATA[<p>kong中提供了两种最基本认证方式,即用户密码认证， basic-auth及basic-auth-api，这两种方式非常好理解，使用的场景类似</p>
<p>basic-auth的认证方式是直接验证用户名密码</p>
<p>而basic-auth-api会将提交的用户名密码到指定的api接口认证，比如统一认证服务</p>
<p>basic auth可以为Route或者Service添加简单的用户名密码认证</p>
<span id="more"></span>

<p>要使用basic-auth,直接在ingress的annotations中添加<code>plugins.konghq.com: xxx</code>即可，在2.x的版本中, <code>plugins.konghq.com</code>已经被抛弃, 变成<code>konghq.com/plugins</code></p>
<p>使用非常简单:</p>
<p>假设现在要有一个应用需要通过用户名才能访问, 则</p>
<p>创建kongConsumer及KongCredential</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongConsumer</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">user1</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">demo-echo</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">&quot;nginx&quot;</span> <span class="comment">#最好指定ingress.class，且跟ingress的保持一致</span></span><br><span class="line"><span class="attr">username:</span> <span class="string">echo-user1</span> <span class="comment"># 这里的用户名跟</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongCredential</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">user1-basic-auth</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">demo-echo</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">&quot;nginx&quot;</span> <span class="comment">#最好指定ingress.class，且跟ingress的保持一致</span></span><br><span class="line"><span class="attr">consumerRef:</span> <span class="string">user1</span>  <span class="comment"># 这里绑定KongConsumer</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">basic-auth</span>  <span class="comment"># 指定认证类型</span></span><br><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">user1</span> <span class="comment"># 这里指定用户名密码</span></span><br><span class="line">  <span class="attr">password:</span> <span class="number">123456</span></span><br></pre></td></tr></table></figure>

<p>2.x的版本<code>KongCredential</code>正在被淘汰，建议使用<code>credential Secrets</code></p>
<p>首先是申明了kongConsumer及KongCredential这两个对象，在kong中，</p>
<p>同时，需要申明kongplugin, </p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongPlugin</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">user1-basic-auth</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kong</span></span><br><span class="line"><span class="attr">disabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">plugin:</span> <span class="string">basic-auth</span> <span class="comment"># 指定plugin绑定的类型</span></span><br></pre></td></tr></table></figure>

<p>之后则添加ingress</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">items:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">plugins.konghq.com:</span> <span class="string">user1-basic-auth</span> <span class="comment"># 这里指定插件名称</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">infra-model-manager-ingress</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">            <span class="attr">serviceName:</span> <span class="string">nginx-svc</span></span><br><span class="line">            <span class="attr">servicePort:</span> <span class="number">30222</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/</span></span><br></pre></td></tr></table></figure>

<p>如果是basic-auth-api的话，则kongplugin为</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">auth_url:</span> <span class="string">http://infra-console.default:8087/v1/kong/authorization</span> <span class="comment"># 这里则是认证api地址</span></span><br><span class="line">  <span class="attr">keepalive:</span> <span class="number">60000</span></span><br><span class="line">  <span class="attr">method:</span> <span class="string">POST</span></span><br><span class="line">  <span class="attr">retry_count:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">timeout:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongPlugin</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">basicauth</span></span><br><span class="line"><span class="attr">plugin:</span> <span class="string">basic-auth-api</span> <span class="comment"># 指定插件类型</span></span><br></pre></td></tr></table></figure>

<p>效果为:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201109225550.png"></p>
<p>这个界面是不是很熟悉，没错，这个其实就是nginx的认证， kong本身就是基于nginxLUA的封装</p>
<p>如果密码输入错误的话，则会直接提示<code>&#123;&quot;message&quot;:&quot;Invalid authentication credentials&quot;&#125;</code></p>
<p>basic-auth-api的方式可以更好地把统一认证服务集成到应用中，这样避免每个服务都需要加入认证相关的代码.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/Kong/">https://github.com/Kong/</a></li>
<li><a href="https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2018/09/30/kong-features-01-auth.html#basic-auth">https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2018/09/30/kong-features-01-auth.html#basic-auth</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(Kubernetes中部署Kong)</title>
    <url>/2020/08/16/Kong-deploy-on-kubernetes/</url>
    <content><![CDATA[<p>在kubernetes中部署kong做为api网关，分为两种方式, 一种是无数据库, 这种方式数据都是保存于内存中,不推荐在生产中使用，另一种则支持数据库(pg或cassandra)，下面分别介绍.</p>
<span id="more"></span>



<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>具体的部署过程不打算贴在这里，官方都有详细的文档, 可参考<a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/README.md">kubernetes-ingress-controller</a></p>
<p>由于kong提供了控制面的接口，因此会使用以下几个端口来接收相关的请求，如下图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200809172538.png"></p>
<h4 id="DBless"><a href="#DBless" class="headerlink" title="DBless"></a>DBless</h4><p>对于DBless的部署方式下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200812211815.png"></p>
<p>只需要一个deployment, 包含2个容器，一个kong,充当proxy，这个proxy监听8000(http),8443(https), 8100(metrics)</p>
<p> 另一个为ingress controller, 用于生成kong的配置</p>
<p><strong>由于每个deployment都是这样的，不存在数据库, 因此直接扩大实例可以处理更多的请求</strong></p>
<h4 id="DBWith"><a href="#DBWith" class="headerlink" title="DBWith"></a>DBWith</h4><p>对于使用数据库的部署:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200812212325.png"></p>
<p>对于数据平面来说，可以使用daemonset来部署, 里面只有一个容器,即kong，充当proxy, 它会负责所有流量的转发</p>
<p>所有从外部80&#x2F;443的流量都会转到8000&#x2F;8444上, 正好是kong-proxy的监听端口</p>
<p>而对于控制面，使用deployment部署，包含有两个容器, 一个kong,另一个则是kong-ingress-controller</p>
<p>控制面不转发任何的流量, 只负责将api-server的变动同步到kong的数据库中, 如果存在多个deployment，则会进行<strong>选主,确保任一时刻只有一个实例对kong的数据库进行配置</strong>.</p>
<p><strong>Kong集群中的节点通过gossip协议自动发现其他节点，当通过一个Kong节点的管理API进行一些变更时也会通知其他节点。每个Kong节点的配置信息是会缓存的，如插件，那么当在某一个Kong节点修改了插件配置时，需要通知其他节点配置的变更</strong></p>
<h3 id="nginx-ingress-VS-kong-ingress"><a href="#nginx-ingress-VS-kong-ingress" class="headerlink" title="nginx ingress VS kong ingress"></a>nginx ingress VS kong ingress</h3><p>如果是用惯了<a href="https://izsk.me/2019/07/27/Kubernetes-ingress-nginx/">nginx-ingress-controller</a>，再来使用kong ingress controller，开始会觉得很奇怪，为何kong ingress controller使用的是deployment部署呢，先来看看官方的<a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/deployment.md">描述</a>:</p>
<p><code>Ingress Controller deployment</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Kong Ingress deployment consists of the Ingress Controller deployed alongside Kong. The deployment will be different depending on <span class="keyword">if</span> a database is being used or not.</span><br><span class="line">The deployment(s) is the core <span class="built_in">which</span> actually runs the Kong Ingress Controller.</span><br><span class="line">See the database section below <span class="keyword">for</span> details.</span><br></pre></td></tr></table></figure>

<p>翻译: Kong Ingress部署包括与Kong一起部署的Ingress Controller。部署将根据是否正在使用数据库而有所不同。 deployment是实际运行Kong Ingress Controller的核心</p>
<p><code>Kong Proxy service</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Once Kong Ingress Controller is deployed, one service is needed to expose Kong outside the Kubernetes cluster so that it can receive all traffic that is destined <span class="keyword">for</span> the cluster and route it appropriately. kong-proxy is a Kubernetes service <span class="built_in">which</span> points to the Kong pods <span class="built_in">which</span> are capable of proxying request traffic. This service will be usually of <span class="built_in">type</span> LoadBalancer, however it is not required to be such. The IP address of this service should be used to configure DNS records of all the domains that Kong should be proxying, to route the traffic to Kong.</span><br></pre></td></tr></table></figure>

<p>翻译: 部署Kong Ingress Controller之后，需要一项服务来在Kubernetes群集之外公开Kong，以便它可以接收发往群集的所有流量并进行适当的路由。 kong-proxy是Kubernetes服务，它指向能够代理请求流量的Kong pod。该服务通常将是LoadBalancer类型，但不是必需的。此服务的IP地址应用于配置Kong应该代理的所有域的DNS记录，以将流量路由到Kong.</p>
<p>其实nginx ingress controller之所以使用daemonset部署, 是因为ingress controller与proxy都被nginx+LUA实现了，就直接部署在一个pod中了, 而kong ingress controller从上面看proxy也是kong在起作用，其实也可以合为一个pod而使用daemonset部署, 只不过需要分为数据面+控制面,而且kong后续打算向serviceMesh演进,  因此proxy使用daemonset部署, controller使用deployment部署</p>
<p>理解这个就比较容易从nginx ingress controller区分开来了</p>
<p>这里要指出的是，<strong>ingress是kubernetes里面的属性</strong>,而nginx与kong只是其中的两种实现.</p>
<p>当kong部署起来之后，即可当做是ingress controller使用了, 下面举个例子.</p>
<h3 id="Kong-ingress-controller-Example"><a href="#Kong-ingress-controller-Example" class="headerlink" title="Kong ingress controller Example"></a>Kong ingress controller Example</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">&quot;kong&quot;</span></span><br><span class="line">    <span class="attr">configuration.konghq.com:</span> <span class="string">&quot;servicea-kongingress&quot;</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">&quot;servicea-kongingress&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">test.local.xyz</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">serviceA</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/api</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">serviceB</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/</span></span><br></pre></td></tr></table></figure>

<p>测试:</p>
<p><code>curl test.local.xyz:8000/api/auth/currentUser</code></p>
<p>在出现404,从打印出来的路由可以看到，path变成了&#x2F;atuh&#x2F;currentUser,源path为&#x2F;api&#x2F;auth&#x2F;current， 这是为什么呢?</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200812191413.png"></p>
<p>添加kongIngress</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongIngress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">servicea-kongingress</span></span><br><span class="line"><span class="attr">route:</span></span><br><span class="line">  <span class="attr">protocols:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">strip_path:</span> <span class="literal">false</span> </span><br></pre></td></tr></table></figure>

<p>然后将该kongIngress添加到ingress的annotations中</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">&quot;kong&quot;</span></span><br><span class="line">    <span class="attr">configuration.konghq.com:</span> <span class="string">&quot;servicea-kongingress&quot;</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">&quot;servicea-kongingress&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>再次访问就没有404的问题了,使用了kong ingress controller的版本为0.5， 我不知道是不是哪里出了问题，我感觉不应该会出现这种情况, 默认情况下，proxy应该是原封不动地转发path, 在controller 0.5的版本下一直没有成功, 不知道是不是因为版本有点旧，giithub上也没有找到相应的说明</p>
<p>当然，在最新版的kong ingress controller默认是直接转发path，不会做处理,如果需要分隔path，也可以直接在ingress中直接添加以下的annotation</p>
<p><code>konghq.com/strip-path: &quot;true&quot;</code></p>
<p>当然也可以使用kongIngress，指定<code>strip_path: true </code>即可,参考<a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/references/annotations.md#konghqcomstrip-path">konghqcomstrip-path</a></p>
<h3 id="konga"><a href="#konga" class="headerlink" title="konga"></a>konga</h3><p>konga是kong的admin的封装, 可以可视化的对service,route,upstream等对象进行管理, 非常方便.</p>
<h4 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">konga</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">konga</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">konga</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">konga</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">pantsel/konga</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">1337</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">konga-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kong-proxy</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">1337</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">1337</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">30337</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">konga</span></span><br></pre></td></tr></table></figure>

<p><code>kubectl apply -f konga.yaml</code>发布到集群中即可.</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200813110344.png"></p>
<p>Name: 这里填写一个独一无二的名字</p>
<p>Kong Admin URL: 为kong的8001监听地址</p>
<p>创建连接之后konga就与kong建立了连接，可以通过konga进行对象的操作了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200813112131.png"></p>
<p>页面化的操作往往屏蔽了很多细节，因此建议作为辅助工具使用.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2019/07/27/Kubernetes-ingress-nginx/">https://izsk.me/2019/07/27/Kubernetes-ingress-nginx/</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/README.md">https://github.com/Kong/kubernetes-ingress-controller/blob/main/README.md</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/references/annotations.md#konghqcomstrip-path">https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/references/annotations.md#konghqcomstrip-path</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/deployment.md">https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/deployment.md</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(kong ingress代理https请求到upstream)</title>
    <url>/2020/09/25/Kong-ingress-proxy-https-to-upstream/</url>
    <content><![CDATA[<p>在有些对安全要求更高的场景下，需要保证端到端的请求都是https, 正常情况下, 请求到达kong ingress后都会将https转换成http后再proxy到upstream, 所以如果需要将https直接proxy到upstream，需要做些额外的操作，好在kong ingress原生就支持.</p>
<h3 id=""><a href="#" class="headerlink" title=""></a><span id="more"></span></h3><p>如果直接使用kong ingress 访问后端协议为https的service会提示以下错误:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">This combination of host and port requires TLS.</span><br></pre></td></tr></table></figure>

<p>原因: 除了kongingress中需要proxy.protocol指定为https外，在k8s的service中也需要绑定kongingress.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">configuration.konghq.com:</span> <span class="string">https-upstream</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">https-upstream</span></span><br></pre></td></tr></table></figure>

<p>比如身份验证服务，本身只支持https协议，如果使用kong进行转发的话，需要以下3个操作</p>
<p>定认一个kongingress, 重点是proxy.protocol:https</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongIngress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;https-upstream&quot;</span></span><br><span class="line"><span class="attr">proxy:</span></span><br><span class="line">  <span class="attr">protocol:</span> <span class="string">https</span> <span class="comment"># 这行</span></span><br><span class="line">  <span class="attr">connect_timeout:</span> <span class="number">10000</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">read_timeout:</span> <span class="number">10000</span></span><br><span class="line">  <span class="attr">write_timeout:</span> <span class="number">10000</span></span><br><span class="line"><span class="attr">route:</span></span><br><span class="line">  <span class="attr">protocols:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https</span></span><br><span class="line"><span class="attr">strip_path:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>再定义一个ingress</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">configuration.konghq.com:</span> <span class="string">https-upstream</span> <span class="comment"># 这个配置在0.8以上已被弃用，使用konghq.com/override</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">https-upstream</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">kong-common</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cas-sso-kong-ingress</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">cas-sso-kong-service</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8443</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/</span></span><br></pre></td></tr></table></figure>

<p>在k8s的service添加annotations</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span> <span class="comment"># 这两行</span></span><br><span class="line">    <span class="attr">configuration.konghq.com:</span> <span class="string">https-upstream</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">https-upstream</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cas-sso-kong-service</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cas-sso-kong-service</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">8443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8443</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cas-sso-kong-service</span></span><br><span class="line">  <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>再次访问后即可正常</p>
<p>kong ingress也支持在proxy https到upstream时使用指定的证书信息, 如果是自签证书的话，浏览器还是会提示站点不安全, 要么手动导入到浏览器中或者使用第三方平台(如阿里云)上提供的权威证书签发功能</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://discuss.konghq.com/t/ingress-controller-registers-https-services-as-http-and-port-443-as-port-80/2784/6">https://discuss.konghq.com/t/ingress-controller-registers-https-services-as-http-and-port-443-as-port-80/2784/6</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/issues/406">https://github.com/Kong/kubernetes-ingress-controller/issues/406</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/issues/69">https://github.com/Kong/kubernetes-ingress-controller/issues/69</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(请求路由分析)</title>
    <url>/2020/02/15/Istio-Traffic-Manager/</url>
    <content><![CDATA[<p>前面大致学习了下Istio的整体架构, 没有涉及它里面的细节, 这次主要说明一下istio是如何将请求进行转发的. 信息量确实比较大.</p>
<span id="more"></span>



<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>还是放一下Istio整体架构图</p>
<p>各个组件的说明, 请参考<a href="https://izsk.me/2020/02/09/Istio-Struct/">这里</a></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209212447.png"></p>
<h3 id="请求路由"><a href="#请求路由" class="headerlink" title="请求路由"></a>请求路由</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209214241.png"></p>
<p>同其它的云原生工具一样, <code>Istio中做服务发现也是基于Kubernetes的List/Watch机制来做</code></p>
<p>Pilot中会保存两大类信息</p>
<ol>
<li>静态信息: 诸如Pilot实例的地址, mixer对接的后端实例地址等信息</li>
<li>动态信息: 所有跟应用相关的动态变化信息, 如cluster, endpoint等</li>
</ol>
<p>这两类信息会被Pilot转化成Pod中的envoy所能识别的Manifest, 通过XDS协议进行下发到envoy中</p>
<p>为了更好地说明路由之间的转发, 在Istio中常用到的CRD概念说明如下</p>
<h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul>
<li><p><a href="https://istio.io/docs/reference/config/networking/sidecar/"><strong>Sidecar</strong></a>：缺省情况下，Pilot将会把和Envoy Sidecar所在namespace的所有services的相关配置，包括inbound和outbound listenter, cluster, route等，都下发给Enovy。使用Sidecar可以对Pilot向Envoy Sidcar下发的配置进行更细粒度的调整，例如只向其下发该Sidecar 所在服务需要访问的那些外部服务的相关outbound配置。</p>
</li>
<li><p><a href="https://istio.io/docs/reference/config/networking/virtual-service/"><strong>Virtualservice</strong></a>：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。</p>
</li>
<li><p><a href="https://istio.io/docs/reference/config/networking/destination-rule/"><strong>DestinationRule</strong></a>：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。</p>
</li>
<li><p><a href="https://jimmysong.io/istio-handbook/data-plane/envoy-xds-protocol.html"><strong>xDS</strong></a>: 这里的x是个泛指, xDS是一类发现服务的总称，包含LDS，RDS，CDS，EDS，ADS以及 SDS。Envoy通过xDS API可以动态获取Listener(监听器)， Route(路由)，Cluster(集群)，Endpoint(集群成员)以 及Secret(证书)配置</p>
</li>
<li><p><a href="https://istio.io/docs/reference/config/networking/service-entry/"><strong>ServiceEntry</strong></a>：可以使用ServiceEntry向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。</p>
</li>
<li><p><a href="https://istio.io/docs/reference/config/networking/gateway/"><strong>Gateway</strong></a>：为网格配置网关，以允许一个服务可以被网格外部访问。</p>
<p>这里要说明一下Istio gateway vs kubernetes ingress:</p>
<p>ingress只支持http，无法配置4层协议， gateway支持4-6层的协议，且支持7层的设置与virtualservice绑定</p>
</li>
<li><p><a href="https://istio.io/docs/reference/config/networking/envoy-filter/"><strong>EnvoyFilter</strong></a>：可以为Envoy配置过滤器。由于Envoy已经支持Lua过滤器，因此可以通过EnvoyFilter启用Lua过滤器，动态改变Envoy的过滤链行为。我之前一直在考虑如何才能动态扩展Envoy的能力，EnvoyFilter提供了很灵活的扩展性。</p>
</li>
</ul>
<p>今天讲路由转发主要会聚焦在前四个概念上, 后面几个后续再更, 这里不过多介绍.</p>
<h3 id="sidecar注入"><a href="#sidecar注入" class="headerlink" title="sidecar注入"></a>sidecar注入</h3><h4 id="自动注入"><a href="#自动注入" class="headerlink" title="自动注入"></a>自动注入</h4><p>默认情况下，istio开启了自动注入功能，但namespace是disabled的，可以通过给ns打label，使istio的sidecar自动注入到pod中.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200209223637.png"></p>
<p>同时, 需要apiserver启动参数–enable-admission -plugins需要开启<a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">MutatingAdmissionWebhook</a></p>
<p>这里简单说明下MutatingAdmissionWebhook的流程</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210101851.png"></p>
<p>Istio 和 sidecar 配置保存在 <code>istio</code> 和 <code>istio-sidecar-injector</code> 这两个 ConfigMap 中，其中包含了 Go template,自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移MutatingAdmissionWebhook 中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#apiserver </span></span><br><span class="line">--enable-admission-plugins=NodeRestriction,MutatingAdmissionWebhook</span><br><span class="line"></span><br><span class="line">kubectl label namespace default istio-injection=enabled</span><br><span class="line"> </span><br><span class="line"><span class="comment">#如要禁止defaule namespace自动注入sidecar功能，则使用</span></span><br><span class="line">kubectl label namespace default istio-injection</span><br></pre></td></tr></table></figure>

<p>自动注入功能开启之后，在ns中生成的deploy，statefulset, rs等对象都将会自动添加envoy-proxy容器(用于流量代理)及initcontainer(用于生成iptables规则)</p>
<h4 id="手动注入"><a href="#手动注入" class="headerlink" title="手动注入"></a>手动注入</h4><p>如果不开启自动注入的话，在需要的时候也可以手动注入，方法如下</p>
<p><code>istioctl kube-inject -f &lt;your-app-spec&gt;.yaml | kubectl apply -f -</code></p>
<p><code>sidecar 默认不能被注入到 kube-system 和 kube-public，istio-system 这三个 namespace</code></p>
<p><strong>下面所提到的一些静态配置都是由注入模块生成的,大家可使用kubectl get cm -nistio-system查看具体内容</strong></p>
<h3 id="Example-Bookinfo"><a href="#Example-Bookinfo" class="headerlink" title="Example Bookinfo"></a>Example Bookinfo</h3><p>这里使用istio官网使用的例子进行分析,这里已经事先将gateway中的loadbalance改成了NodePort类型代理出来.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br><span class="line"></span><br><span class="line">curl -o /dev/null -s -w <span class="string">&quot;%&#123;http_code&#125;\n&quot;</span> http://10.4.97.112:30732/productpage</span><br><span class="line"><span class="comment"># 返回200</span></span><br><span class="line"><span class="comment"># 这里为了简单，没有使用域名.直接通过宿主机ip访问.</span></span><br><span class="line"><span class="comment"># 其中 10.4.97.112为宿主机ip</span></span><br><span class="line"><span class="comment"># 30732为宿主机的随机端口</span></span><br></pre></td></tr></table></figure>



<h3 id="Istio-agent容器启动"><a href="#Istio-agent容器启动" class="headerlink" title="Istio-agent容器启动"></a>Istio-agent容器启动</h3><p>最后生成的pod yaml文件如下:</p>
<details> <summary>pod yaml</summary>
<pre><code>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">sidecar.istio.io/status:</span> <span class="string">&#x27;&#123;&quot;version&quot;:&quot;8d80e9685defcc00b0d8c9274b60071ba8810537e0ed310ea96c1de0785272c7&quot;,&quot;initContainers&quot;:[&quot;istio-init&quot;],&quot;containers&quot;:[&quot;istio-proxy&quot;],&quot;volumes&quot;:[&quot;istio-envoy&quot;,&quot;istio-certs&quot;],&quot;imagePullSecrets&quot;:null&#125;&#x27;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2020-01-02T11:18:50Z&quot;</span></span><br><span class="line">  <span class="attr">generateName:</span> <span class="string">sleep-f8cbf5b76-</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">sleep</span></span><br><span class="line">    <span class="attr">pod-template-hash:</span> <span class="string">f8cbf5b76</span></span><br><span class="line">    <span class="attr">security.istio.io/tlsMode:</span> <span class="string">istio</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sleep-f8cbf5b76-z4g5w</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">ownerReferences:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">    <span class="attr">blockOwnerDeletion:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">controller:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">sleep-f8cbf5b76</span></span><br><span class="line">    <span class="attr">uid:</span> <span class="string">8fbae215-c254-4795-b316-cbe15c72da5f</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;409671&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/pods/sleep-f8cbf5b76-z4g5w</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">352f3bbb-d77d-4133-9bcd-55e18374f667</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">3650d</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">governmentpaas/curl-ssl</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">sleep</span></span><br><span class="line">    <span class="attr">resources:</span> &#123;&#125;</span><br><span class="line">    <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">    <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/sleep/tls</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">sleep-token-t2xgb</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">proxy</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sidecar</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--domain</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">$(POD_NAMESPACE).svc.cluster.local</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--configPath</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/etc/istio/proxy</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--binaryPath</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/usr/local/bin/envoy</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--serviceCluster</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sleep.$(POD_NAMESPACE)</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--drainDuration</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">45s</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--parentShutdownDuration</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">1m0s</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--discoveryAddress</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">istio-pilot.istio-system:15010</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--zipkinAddress</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">zipkin.istio-system:9411</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--proxyLogLevel=warning</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--proxyComponentLogLevel=misc:error</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--connectTimeout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">10s</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--proxyAdminPort</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;15000&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--concurrency</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--controlPlaneAuthPolicy</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">NONE</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--dnsRefreshRate</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">300s</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--statusPort</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;15020&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--applicationPorts</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--trust-domain=cluster.local</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">INSTANCE_IP</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">SERVICE_ACCOUNT</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">spec.serviceAccountName</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST_IP</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">status.hostIP</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_POD_PORTS</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        [</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string"></span>    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_CLUSTER_ID</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">Kubernetes</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_POD_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_CONFIG_NAMESPACE</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">SDS_ENABLED</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_INTERCEPTION_MODE</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">REDIRECT</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_INCLUDE_INBOUND_PORTS</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_METAJSON_LABELS</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        &#123;&quot;app&quot;:&quot;sleep&quot;,&quot;pod-template-hash&quot;:&quot;f8cbf5b76&quot;&#125;</span></span><br><span class="line"><span class="string"></span>    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_WORKLOAD_NAME</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_OWNER</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">kubernetes://api/apps/v1/namespaces/default/deployments/sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ISTIO_META_MESH_ID</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">cluster.local</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">docker.io/istio/proxyv2:1.4.2</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">istio-proxy</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">15090</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">http-envoy-prom</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">failureThreshold:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/healthz/ready</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">15020</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">40Mi</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1337</span></span><br><span class="line">    <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">    <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/istio/proxy</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">istio-envoy</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/certs/</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">istio-certs</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">sleep-token-t2xgb</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">  <span class="attr">enableServiceLinks:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">istio-iptables</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-p</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;15001&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-z</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;15006&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-u</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;1337&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-m</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">REDIRECT</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-i</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-x</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-b</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-d</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;15020&quot;</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">docker.io/istio/proxyv2:1.4.2</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">istio-init</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">50Mi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">      <span class="attr">runAsNonRoot:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">    <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">sleep-token-t2xgb</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">g105e1900156</span></span><br><span class="line">  <span class="attr">priority:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">  <span class="attr">securityContext:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">serviceAccount:</span> <span class="string">sleep</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">sleep</span></span><br><span class="line">  <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">node.kubernetes.io/not-ready</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">    <span class="attr">tolerationSeconds:</span> <span class="number">300</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">node.kubernetes.io/unreachable</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">    <span class="attr">tolerationSeconds:</span> <span class="number">300</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">sleep-secret</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sleep-token-t2xgb</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">sleep-token-t2xgb</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">emptyDir:</span></span><br><span class="line">      <span class="attr">medium:</span> <span class="string">Memory</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">istio-envoy</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">istio-certs</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">istio.sleep</span></span><br></pre></td></tr></table></figure>
</code></pre>
</details>

<h4 id="istio-proxy-envoy"><a href="#istio-proxy-envoy" class="headerlink" title="istio-proxy(envoy)"></a>istio-proxy(envoy)</h4><p>从上面的pod中可以看到, 其中 envoy的启动命令，涉及的端口如下:</p>
<ul>
<li>9080: productpage进程对外提供的服务端口</li>
<li>15001: Envoy的Virtual Outbound监听器，iptable会将productpage服务发出的出向流量导入该端口中由Envoy进行处理</li>
<li>15006: Envoy的Virtual Inbound监听器，iptable会将发到productpage的入向流量导入该端口中由Envoy进行处理</li>
<li>15000: Envoy管理端口，该端口绑定在本地环回地址上，只能在Pod内访问。</li>
<li>15090: 指向127.0.0.1:15000&#x2F;stats&#x2F;prometheus, 用于对外提供Envoy的性能统计指标</li>
<li>15020: 监测envoy的健康状态的端口. 在iptables中已经将该端口排除，不对该端口的流量进行操作.</li>
</ul>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210125921.png"></p>
<h4 id="istio-init"><a href="#istio-init" class="headerlink" title="istio-init"></a>istio-init</h4><p>其中, <code>init容器 istio-init中生成一系列iptables规则后退出, 这些规则用于将流量劫持到envoy容器</code>，详细过程可查看<a href="https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/">这里</a></p>
<p>而istio-proxy容器的Dockerfile启动entrypoint为&#x2F;usr&#x2F;local&#x2F;bin&#x2F;pilot-agent, 加上上述参数形成了istio-proxy容器的启动命令, pid&#x3D;1的进程为pilot-agent，该进程又启动了&#x2F;usr&#x2F;local&#x2F;bin&#x2F;envoy，envoy的配置文件为&#x2F;etc&#x2F;istio&#x2F;proxy&#x2F;envoy-rev0.json</p>
<p>&#x2F;etc&#x2F;istio&#x2F;proxy&#x2F;envoy-rev0.json的内容里主要定义了各种服务的配置信息.在envoy进行流量转发时会按照这些配置对流量进行处理.</p>
<h4 id="envoy-rev0-json"><a href="#envoy-rev0-json" class="headerlink" title="envoy-rev0.json"></a>envoy-rev0.json</h4><details> <summary>envoy-rev0.json</summary> <pre><code>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;node&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;:</span> <span class="string">&quot;sidecar~10.244.0.21~sleep-f8cbf5b76-z4g5w.default~default.svc.cluster.local&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;cluster&quot;:</span> <span class="string">&quot;sleep.default&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;locality&quot;:</span> &#123;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;metadata&quot;:</span> &#123;<span class="string">&quot;CLUSTER_ID&quot;</span><span class="string">:&quot;Kubernetes&quot;</span>,<span class="string">&quot;CONFIG_NAMESPACE&quot;</span><span class="string">:&quot;default&quot;</span>,<span class="string">&quot;EXCHANGE_KEYS&quot;</span><span class="string">:&quot;NAME</span>,<span class="string">NAMESPACE</span>,<span class="string">INSTANCE_IPS</span>,<span class="string">LABELS</span>,<span class="string">OWNER</span>,<span class="string">PLATFORM_METADATA</span>,<span class="string">WORKLOAD_NAME</span>,<span class="string">CANONICAL_TELEMETRY_SERVICE</span>,<span class="string">MESH_ID</span>,<span class="string">SERVICE_ACCOUNT&quot;</span>,<span class="string">&quot;INCLUDE_INBOUND_PORTS&quot;</span><span class="string">:&quot;&quot;</span>,<span class="string">&quot;INSTANCE_IPS&quot;</span><span class="string">:&quot;10.244.0.21&quot;</span>,<span class="string">&quot;INTERCEPTION_MODE&quot;</span><span class="string">:&quot;REDIRECT&quot;</span>,<span class="string">&quot;ISTIO_PROXY_SHA&quot;</span><span class="string">:&quot;istio-proxy:a7b2578494d79fe098f0e99282e0236946562fa0&quot;</span>,<span class="string">&quot;ISTIO_VERSION&quot;</span><span class="string">:&quot;1.4.2&quot;</span>,<span class="string">&quot;LABELS&quot;</span><span class="string">:</span>&#123;<span class="string">&quot;app&quot;</span><span class="string">:&quot;sleep&quot;</span>,<span class="string">&quot;pod-template-hash&quot;</span><span class="string">:&quot;f8cbf5b76&quot;</span>&#125;,<span class="string">&quot;MESH_ID&quot;</span><span class="string">:&quot;cluster.local&quot;</span>,<span class="string">&quot;NAME&quot;</span><span class="string">:&quot;sleep-f8cbf5b76-z4g5w&quot;</span>,<span class="string">&quot;NAMESPACE&quot;</span><span class="string">:&quot;default&quot;</span>,<span class="string">&quot;OWNER&quot;</span><span class="string">:&quot;kubernetes://api/apps/v1/namespaces/default/deployments/sleep&quot;</span>,<span class="string">&quot;POD_NAME&quot;</span><span class="string">:&quot;sleep-f8cbf5b76-z4g5w&quot;</span>,<span class="string">&quot;POD_PORTS&quot;</span><span class="string">:&quot;</span>[<span class="string">\n</span>]<span class="string">&quot;,&quot;</span><span class="string">SERVICE_ACCOUNT&quot;:&quot;sleep&quot;</span>,<span class="string">&quot;WORKLOAD_NAME&quot;</span><span class="string">:&quot;sleep&quot;</span>,<span class="string">&quot;app&quot;</span><span class="string">:&quot;sleep&quot;</span>,<span class="string">&quot;pod-template-hash&quot;</span><span class="string">:&quot;f8cbf5b76&quot;</span>&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;stats_config&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;use_all_default_tags&quot;:</span> <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;stats_tags&quot;:</span> [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;cluster_name&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^cluster\\.((.+?(\\..+?\\.svc\\.cluster\\.local)?)\\.)&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;tcp_prefix&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^tcp\\.((.*?)\\.)\\w+?$&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(response_code=\\.=(.+?);\\.;)|_rq(_(\\.d&#123;3&#125;))$&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;response_code&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;response_code_class&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;_rq(_(\\dxx))$&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;http_conn_manager_listener_prefix&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^listener(?=\\.).*?\\.http\\.(((?:[_.[:digit:]]*|[_\\[\\]aAbBcCdDeEfF[:digit:]]*))\\.)&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;http_conn_manager_prefix&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^http\\.(((?:[_.[:digit:]]*|[_\\[\\]aAbBcCdDeEfF[:digit:]]*))\\.)&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;listener_address&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^listener\\.(((?:[_.[:digit:]]*|[_\\[\\]aAbBcCdDeEfF[:digit:]]*))\\.)&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;mongo_prefix&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;^mongo\\.(.+?)\\.(collection|cmd|cx_|op_|delays_|decoding_)(.*?)$&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(reporter=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;reporter&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_namespace=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_namespace&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_workload=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_workload&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_workload_namespace=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_workload_namespace&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_principal=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_principal&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_app=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_app&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(source_version=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;source_version&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_namespace=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_namespace&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_workload=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_workload&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_workload_namespace=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_workload_namespace&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_principal=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_principal&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_app=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_app&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_version=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_version&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_service=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_service&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_service_name=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_service_name&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(destination_service_namespace=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;destination_service_namespace&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(request_protocol=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;request_protocol&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(response_flags=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;response_flags&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(connection_security_policy=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;connection_security_policy&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(permissive_response_code=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;permissive_response_code&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(permissive_response_policyid=\\.=(.+?);\\.;)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;permissive_response_policyid&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(cache\\.(.+?)\\.)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;cache&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(component\\.(.+?)\\.)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;component&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="attr">&quot;regex&quot;:</span> <span class="string">&quot;(tag\\.(.+?)\\.)&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;tag_name&quot;:</span> <span class="string">&quot;tag&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;stats_matcher&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;inclusion_list&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;patterns&quot;:</span> [</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;reporter=&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;component&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;cluster_manager&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;listener_manager&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;http_mixer_filter&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;tcp_mixer_filter&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;server&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;cluster.xds-grpc&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">          <span class="attr">&quot;suffix&quot;:</span> <span class="string">&quot;ssl_context_update_by_sds&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;admin&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;access_log_path&quot;:</span> <span class="string">&quot;/dev/null&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;address&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;socket_address&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;address&quot;:</span> <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;port_value&quot;:</span> <span class="number">15000</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;dynamic_resources&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;lds_config&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;ads&quot;:</span> &#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;cds_config&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;ads&quot;:</span> &#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;ads_config&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;api_type&quot;:</span> <span class="string">&quot;GRPC&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;grpc_services&quot;:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;envoy_grpc&quot;:</span> &#123;</span><br><span class="line">            <span class="attr">&quot;cluster_name&quot;:</span> <span class="string">&quot;xds-grpc&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;static_resources&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;clusters&quot;:</span> [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;prometheus_stats&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;STATIC&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;connect_timeout&quot;:</span> <span class="string">&quot;0.250s&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;lb_policy&quot;:</span> <span class="string">&quot;ROUND_ROBIN&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;socket_address&quot;:</span> &#123;</span><br><span class="line">              <span class="attr">&quot;protocol&quot;:</span> <span class="string">&quot;TCP&quot;</span>,</span><br><span class="line">              <span class="attr">&quot;address&quot;:</span> <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">              <span class="attr">&quot;port_value&quot;:</span> <span class="number">15000</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;xds-grpc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;STRICT_DNS&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;dns_refresh_rate&quot;:</span> <span class="string">&quot;300s&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;dns_lookup_family&quot;:</span> <span class="string">&quot;V4_ONLY&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;connect_timeout&quot;:</span> <span class="string">&quot;10s&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;lb_policy&quot;:</span> <span class="string">&quot;ROUND_ROBIN&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;socket_address&quot;:</span> &#123;<span class="attr">&quot;address&quot;:</span> <span class="string">&quot;istio-pilot.istio-system&quot;</span>, <span class="attr">&quot;port_value&quot;:</span> <span class="number">15010</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;circuit_breakers&quot;:</span> &#123;</span><br><span class="line">          <span class="attr">&quot;thresholds&quot;:</span> [</span><br><span class="line">            &#123;</span><br><span class="line">              <span class="attr">&quot;priority&quot;:</span> <span class="string">&quot;DEFAULT&quot;</span>,</span><br><span class="line">              <span class="attr">&quot;max_connections&quot;:</span> <span class="number">100000</span>,</span><br><span class="line">              <span class="attr">&quot;max_pending_requests&quot;:</span> <span class="number">100000</span>,</span><br><span class="line">              <span class="attr">&quot;max_requests&quot;:</span> <span class="number">100000</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              <span class="attr">&quot;priority&quot;:</span> <span class="string">&quot;HIGH&quot;</span>,</span><br><span class="line">              <span class="attr">&quot;max_connections&quot;:</span> <span class="number">100000</span>,</span><br><span class="line">              <span class="attr">&quot;max_pending_requests&quot;:</span> <span class="number">100000</span>,</span><br><span class="line">              <span class="attr">&quot;max_requests&quot;:</span> <span class="number">100000</span></span><br><span class="line">            &#125;</span><br><span class="line">          ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;upstream_connection_options&quot;:</span> &#123;</span><br><span class="line">          <span class="attr">&quot;tcp_keepalive&quot;:</span> &#123;</span><br><span class="line">            <span class="attr">&quot;keepalive_time&quot;:</span> <span class="number">300</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;http2_protocol_options&quot;:</span> &#123; &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      ,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;zipkin&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;STRICT_DNS&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;dns_refresh_rate&quot;:</span> <span class="string">&quot;300s&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;dns_lookup_family&quot;:</span> <span class="string">&quot;V4_ONLY&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;connect_timeout&quot;:</span> <span class="string">&quot;1s&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;lb_policy&quot;:</span> <span class="string">&quot;ROUND_ROBIN&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;socket_address&quot;:</span> &#123;<span class="attr">&quot;address&quot;:</span> <span class="string">&quot;zipkin.istio-system&quot;</span>, <span class="attr">&quot;port_value&quot;:</span> <span class="number">9411</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;listeners&quot;</span><span class="string">:</span>[</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;address&quot;:</span> &#123;</span><br><span class="line">          <span class="attr">&quot;socket_address&quot;:</span> &#123;</span><br><span class="line">            <span class="attr">&quot;protocol&quot;:</span> <span class="string">&quot;TCP&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;address&quot;:</span> <span class="string">&quot;0.0.0.0&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;port_value&quot;:</span> <span class="number">15090</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;filter_chains&quot;:</span> [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;filters&quot;:</span> [</span><br><span class="line">              &#123;</span><br><span class="line">                <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;envoy.http_connection_manager&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;config&quot;:</span> &#123;</span><br><span class="line">                  <span class="attr">&quot;codec_type&quot;:</span> <span class="string">&quot;AUTO&quot;</span>,</span><br><span class="line">                  <span class="attr">&quot;stat_prefix&quot;:</span> <span class="string">&quot;stats&quot;</span>,</span><br><span class="line">                  <span class="attr">&quot;route_config&quot;:</span> &#123;</span><br><span class="line">                    <span class="attr">&quot;virtual_hosts&quot;:</span> [</span><br><span class="line">                      &#123;</span><br><span class="line">                        <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;backend&quot;</span>,</span><br><span class="line">                        <span class="attr">&quot;domains&quot;:</span> [</span><br><span class="line">                          <span class="string">&quot;*&quot;</span></span><br><span class="line">                        ],</span><br><span class="line">                        <span class="attr">&quot;routes&quot;:</span> [</span><br><span class="line">                          &#123;</span><br><span class="line">                            <span class="attr">&quot;match&quot;:</span> &#123;</span><br><span class="line">                              <span class="attr">&quot;prefix&quot;:</span> <span class="string">&quot;/stats/prometheus&quot;</span></span><br><span class="line">                            &#125;,</span><br><span class="line">                            <span class="attr">&quot;route&quot;:</span> &#123;</span><br><span class="line">                              <span class="attr">&quot;cluster&quot;:</span> <span class="string">&quot;prometheus_stats&quot;</span></span><br><span class="line">                            &#125;</span><br><span class="line">                          &#125;</span><br><span class="line">                        ]</span><br><span class="line">                      &#125;</span><br><span class="line">                    ]</span><br><span class="line">                  &#125;,</span><br><span class="line">                  <span class="attr">&quot;http_filters&quot;:</span> &#123;</span><br><span class="line">                    <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;envoy.router&quot;</span></span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            ]</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">  ,</span><br><span class="line">  <span class="attr">&quot;tracing&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;http&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;envoy.zipkin&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;config&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;collector_cluster&quot;:</span> <span class="string">&quot;zipkin&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;collector_endpoint&quot;:</span> <span class="string">&quot;/api/v1/spans&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;trace_id_128bit&quot;:</span> <span class="string">&quot;true&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;shared_span_context&quot;:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
</details>

<p>从Envoy初始化配置文件中，我们可以大致看到Istio通过Envoy来实现服务发现和流量管理的基本原理。<strong>即控制面将xDS server信息通过static resource的方式配置到Envoy的初始化配置文件中，Envoy启动后，控制面通过xDS将dynamic resource下发给envoy，包括网格中的service信息及路由规则</strong></p>
<p>可以看一下容器中打开的端口:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210130019.png"></p>
<p>从图中可以看到，连着几个控制面应用的端口.</p>
<h4 id="Envoy配置初始化流程"><a href="#Envoy配置初始化流程" class="headerlink" title="Envoy配置初始化流程"></a>Envoy配置初始化流程</h4><p><img src="https://zhaohuabing.com/img/2019-12-05-istio-traffic-management-impl-intro/envoy-config-init.png" alt="img"></p>
<blockquote>
<ul>
<li>Pilot-agent根据启动参数和K8S API Server中的配置信息生成Envoy的初始配置文件envoy-rev0.json，该文件告诉Envoy从xDS server中获取动态配置信息，并配置了xDS server的地址信息，即控制面的Pilot。</li>
<li>Pilot-agent使用envoy-rev0.json启动Envoy进程。</li>
<li>Envoy根据初始配置获得Pilot地址，采用xDS接口从Pilot获取到Listener，Cluster，Route等动态配置信息。</li>
<li>Envoy根据获取到的动态配置启动Listener，并根据Listener的配置，结合Route和Cluster对拦截到的流量进行处理</li>
</ul>
</blockquote>
<h3 id="VirtualService、DestinationRule"><a href="#VirtualService、DestinationRule" class="headerlink" title="VirtualService、DestinationRule"></a>VirtualService、DestinationRule</h3><p>这两个是istio中最核心的两个东西, 鉴于本篇的篇幅已经很好, 就不在这里展开说明了,后续再更</p>
<p>这里贴个两者的配置格式</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-namespace</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">istio</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">my-svc</span>  <span class="comment">#virtualservice 中的规则只影响与 hosts 匹配的请求。hosts可以是通配符前缀或 CIDR 前缀的 DNS 名称,在kubernetes中，全称为my-svc.istio.svc.cluster.local</span></span><br><span class="line">  <span class="comment">#注意: 这里的namespace是以virtualservice所在的ns为准，而不是pod所在的ns.</span></span><br><span class="line">  <span class="comment">#所以建议写服务完整的fqns以免产生歧义.</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/svc-1</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">svc-1.my-namespace.svc.cluster.local</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/svc-2</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">svc-2.my-namespace.svc.cluster.local</span></span><br><span class="line">        </span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-destination-rule</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">svc-1.my-namespace.svc.cluster.local</span></span><br><span class="line">  <span class="attr">trafficPolicy:</span></span><br><span class="line">    <span class="attr">loadBalancer:</span></span><br><span class="line">      <span class="attr">simple:</span> <span class="string">RANDOM</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">trafficPolicy:</span></span><br><span class="line">      <span class="attr">loadBalancer:</span></span><br><span class="line">        <span class="attr">simple:</span> <span class="string">ROUND_ROBIN</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v3</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v3</span></span><br></pre></td></tr></table></figure>



<h3 id="xDS"><a href="#xDS" class="headerlink" title="xDS"></a>xDS</h3><p>Xds是istio控制面实现的协议，主要为数据面(envoy, pilot-agent)提供配置中心和服务中心两个功能，并把配置发现和服务发现以一组统一的 <code>xDS</code> 接口提供出来，数据面的 Envoy 通过 xDS 获取需要的信息来做服务间通信和服务治理, 分为以下几种</p>
<blockquote>
<ul>
<li>cds(cluster discovery service):  提供相同服务的一组pod(pod中不同的版本为不同的集群)在istio中为一个集群, 类似k8s中的service</li>
<li>lds(listener discovery service)：</li>
<li>rds(route discovery service): 当我们做灰度发布、金丝雀发布时，同一个服务会同时运行多个版本，每个版本对应一个 cluster。这时需要通过 <code>route</code> 规则规定请求如何路由到其中的某个版本的 cluster 上</li>
<li>eds(endpoint discovery service)：一个具体的「应用实例」，对应 ip 和端口号，类似 Kubernetes 中的一个 Pod</li>
<li>ads(aggregated discovery service): 可以通过ads实现在同一个 gRPC 连接上实现多种 xds 来下发配置，从而节省网络连接，ads 还有一个非常重要的作用是解决 <code>cds</code>、<code>rds</code> 信息更新顺序依赖的问题，从而保证以一定的顺序同步各类配置信息</li>
</ul>
</blockquote>
<p>通过istio-pilot接口查看xds数据:</p>
<p>获取istio-pilot的svc的ip，8080是pilot http端口，15010是pilot向envoy提供配置下发端口(grpc协议端口, 会与envoy保持长连接)，已不支持curl操作.</p>
<p>不过我们也可以从envoy提供的管理端口15000查询，15000是绑定在envoy容器里回环地址上的端口，因为只能从容器内访问:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PILOT_SVC_IP=$(kubectl -n istio-system get svc istio-pilot -o go-template=<span class="string">&#x27;&#123;&#123;.spec.clusterI&#x27;</span>)&#125;<span class="string">&#x27;</span></span><br><span class="line"><span class="string"># 通过envoy管理端口访问</span></span><br><span class="line"><span class="string">kubectl exec -it productpage-v1-6d8bc58dd7-ts8kw -c istio-proxy curl http://127.0.0.1:15000/config_dump &gt; config_dump</span></span><br></pre></td></tr></table></figure>

<p>查看eds</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -s http://<span class="variable">$PILOT_SVC_IP</span>:8080/debug/edsz|grep -A100  <span class="string">&quot;outbound|9080||reviews.default.svc.clusterlocal&quot;</span></span><br></pre></td></tr></table></figure>

<p>查看cds:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://<span class="variable">$PILOT_SVC_IP</span>:8080/debug/cdsz</span><br></pre></td></tr></table></figure>

<p>查看ads:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl http://<span class="variable">$PILOT_SVC_IP</span>:8080/debug/adsz</span><br></pre></td></tr></table></figure>



<h3 id="envoy配置文件详解"><a href="#envoy配置文件详解" class="headerlink" title="envoy配置文件详解"></a>envoy配置文件详解</h3><p>envoy的配置文件非常长, 因为包含了整个集群的静态及动态信息, 主要有用的信息分为以下几个红框标识出来的:</p>
<blockquote>
<ul>
<li>Bootstrap</li>
<li>clusters</li>
<li>listeners</li>
<li>Routes</li>
</ul>
</blockquote>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210160835.png"></p>
<h4 id="bootstrap"><a href="#bootstrap" class="headerlink" title="bootstrap"></a>bootstrap</h4><p>文件中的内容和之前介绍的envoy-rev0.json是一致的, 是envoy初始化的配置, 主要包含了如Piolt, prometheus等后端实例的静态地址信息, 分为以下几部分:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210161308.png"></p>
<ul>
<li><p>node: 部分描述了pod相关信息</p>
</li>
<li><p>static_resources: 描述了一些静态地址，主要是cluster部分涉及zipkin, prometheus,等路由地址，特别重要的是指定了pilot的地址，如上图红框中所示.</p>
</li>
<li><p>Dynamic_resources: 则是envoy启动后,pilot下发的动态配置, 这里主要注意ads_config, 这里指定了ads指向的cluster_name为xds-grpc, 就是上图中的pilot地址.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210161421.png"></p>
</li>
</ul>
<h4 id="clusters"><a href="#clusters" class="headerlink" title="clusters"></a>clusters</h4><p>Clusters是一个服务集群（类比kubernetes对应serivce的概念），包含一个到多个endpoint，每个endpoint都可以提供服务，Envoy根据负载均衡算法将请求发送到这些endpoint中。</p>
<ul>
<li><p>static_clusters: 同bootstrap中的static_resources。</p>
</li>
<li><p>dynamic_active_clusters : 是通过xDS接口从Istio控制面获取的动态服务信息。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200107164542498.png"></p>
</li>
</ul>
<p>​     对于一个风格服务来说，比如这里的productpage服务，其它的服务对它来说就是网格外的服务，如果productpage请求review, 这条请求对于productpage来就是出口流量(outbound),所以的出口流量都会先被envoy拦截(iptables定义所有的出口流量都被envoy的15001监听器拦截转发到envoy)，对于productpage来说入口(iptables定义所有的入口流量都被envoy的15006监听器拦截转发到envoy)流量只有一个，就是别的服务请求它自己，所以在上面我们只会看到一个(inbound), 它的地址是127.0.0.1，而127.0.0.1上的请求被iptables指定不会经过envoy处理，直接被转到productpage本身了.</p>
<p>​    同时，对于outbound来说，比如上图中的cluster: outbound|14250||jaeger-collector.istio-system.svc.cluster.local, 则会dymanic_routing_config中找出最终服务的svc ip.</p>
<p>​    因为outbound|14250||jaeger-collector.istio-system.svc.cluster.local为动态资源，需要通过eds查询得到该cluster中有几个endpoint , 类似使用pilot的debug端口查看cluster的endpoint，当然实际情况会使用缓存</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl 10.96.255.184:8080/debug/edsz|grep <span class="string">&#x27;outbound|9080||reviews.default.svc.cluster.local&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210162331.png"></p>
<h4 id="listeners"><a href="#listeners" class="headerlink" title="listeners"></a>listeners</h4><p>listeners: Envoy采用listener来接收并处理downstream发过来的请求，listener的处理逻辑是插件式的，可以通过配置不同的filter来插入不同的处理逻辑。</p>
<ul>
<li>static_listeners</li>
<li>dynamic_active_listeners</li>
</ul>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210163918.png"></p>
<h4 id="routes"><a href="#routes" class="headerlink" title="routes"></a>routes</h4><p><code>routes: 配置Envoy的路由规则。Istio下发的缺省路由规则中对每个端口设置了一个路由规则，根据host来对请求进行路由分发。</code></p>
<ul>
<li><p>static_route_configs</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210163031.png"></p>
</li>
<li><p>dynamic_route_configs</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210163343.png"></p>
</li>
</ul>
<h4 id="endpoint"><a href="#endpoint" class="headerlink" title="endpoint"></a>endpoint</h4><p>endpoint对应的就是cluster后端的所有实例列表中的一个了, 这个概念跟kubernetes中的endpoint打平</p>
<p>endpoin是动态获取的,当请求转到cluster时，通过xDS动态查找cluster后端的ep list, 然后根据配置的负载均衡策略选择一个将请求直接转发过去, 而不是像kubernetes中一样需要经过kube-proxy.</p>
<p>具体流程参考下面端到端分析.</p>
<h3 id="envoy端到端调用分析"><a href="#envoy端到端调用分析" class="headerlink" title="envoy端到端调用分析"></a>envoy端到端调用分析</h3><p>下图描述了istio对于服务间端到端调用链流量分析.</p>
<p>Productpage服务调用Reviews服务的请求流程</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200210162221.png"></p>
<p>详细分析过程:</p>
<p>当 productpage请求reviews时，(get <a href="http://reviews:9080/reviews/0">http://reviews:9080/reviews/0</a>)</p>
<ol>
<li><p>对于productpage业说， 这个请求为向外请求(outbound)，所有的outbound请求都会被iptables设置的规则捕获(envoy init容器设置)，重新定位到15001端口，在15001端口上监听的Envoy Virtual Outbound Listener收到了该请求</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version_info&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:06Z/13&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;listener&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;virtualOutbound&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;socket_address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0.0.0&quot;</span><span class="punctuation">,</span> <span class="comment">//接受所有15001端口来的流量</span></span><br><span class="line">                <span class="attr">&quot;port_value&quot;</span><span class="punctuation">:</span> <span class="number">15001</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">         <span class="attr">&quot;use_original_dst&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span> <span class="comment">//请求转发给和原始目的IP:Port匹配的listener,而不是直接处理</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_updated&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:22.919Z&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>“use_original_dst”: true 参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再转发给由其他最符合请求原始目标的监听器(如果找不到任何匹配的虚拟监听器，它会将请求发送给返回 404 的 <code>BlackHoleCluster</code>) 处理而不是直接转发出去.默认为false, 后续该参数会被废弃</p>
</li>
<li><p>请求被Virtual Outbound Listener根据”use_original_dst”: true配置进行转发，根据原目标IP（通配所有的ipv4地址）和端口9080(该端口是请求review应用带的端口),转发到name为0.0.0.0_9080这个outbound listener（这条规则是envoy启动之后动态获取的）</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version_info&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:06Z/13&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;listener&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0.0.0_9080&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;socket_address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0.0.0&quot;</span><span class="punctuation">,</span>  <span class="comment">//通配所有的ipv4地址</span></span><br><span class="line">                <span class="attr">&quot;port_value&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;filter_chains&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;filters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.http_connection_manager&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;typed_config&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;@type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;stat_prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;outbound_0.0.0.0_9080&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;http_filters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mixer&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                    ......</span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.cors&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.fault&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.router&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span></span><br><span class="line">                            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                            ......</span><br><span class="line">                            </span><br><span class="line">                            <span class="attr">&quot;rds&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;config_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;ads&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="punctuation">&#125;</span>    <span class="comment">//表明是动态获取的</span></span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;route_config_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;9080&quot;</span> <span class="comment">//采用“9080” route进行分发</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;deprecated_v1&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;bind_to_port&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;listener_filters_timeout&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.100s&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;traffic_direction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;OUTBOUND&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;continue_on_listener_filters_timeout&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_updated&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:22.822Z&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span>   </span><br></pre></td></tr></table></figure>

<p>从上面可以看到，这个outbound listener的路由转到了”route_config_name”: “9080”，名为9080的route config name.</p>
</li>
<li><p>找到”route_config_name”&#x3D; “9080”</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200109143222173.png"></p>
</li>
</ol>
<p>​		从route_config_name&#x3D;9080来看，下面有好多个virtual_hosts，这说明，存在多个应用是通过9080提供服务，然后根据请求的域名为reviews，可发现route为 “outbound|9080||reviews.default.svc.cluster.local”</p>
<p>​        这里简单说一下<code>outbound|9080||reviews.default.svc.cluster.local</code>格式</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">outbound说明是出向流量</span></span><br><span class="line"><span class="number">9080</span><span class="string">为端口</span></span><br><span class="line"><span class="string">||之间其实是subset的key，这个是在配置的destinatiorule中定义，如果没有，则为空</span></span><br><span class="line"><span class="string">reviews.default.svc.cluster.local即是kubernetes中完整的svc的域名.</span> </span><br></pre></td></tr></table></figure>



<ol start="4">
<li><p>route&#x3D; “outbound|9080||reviews.default.svc.cluster.local”</p>
<p>cluster为动态资源，通过eds查询得到该cluster中有3个endpoint</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;clusterName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;outbound|9080||reviews.default.svc.cluster.local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;endpoints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lbEndpoints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;endpoint&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;socketAddress&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.40.0.15&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;portValue&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;filterMetadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;envoy.transport_socket_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;tlsMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;istio&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;istio&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kubernetes://reviews-v1-75b979578c-pw8zs.default&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;loadBalancingWeight&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;endpoint&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;socketAddress&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.40.0.16&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;portValue&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;filterMetadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;envoy.transport_socket_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;tlsMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;istio&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;istio&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kubernetes://reviews-v3-54c6c64795-wbls7.default&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;loadBalancingWeight&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;endpoint&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;socketAddress&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.40.0.17&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;portValue&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;filterMetadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;envoy.transport_socket_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;tlsMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;istio&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;istio&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;uid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kubernetes://reviews-v2-597bf96c8f-l2fp8.default&quot;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;loadBalancingWeight&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;loadBalancingWeight&quot;</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>请求被转发到其中一个endpoint 10.40.0.15，即Reviews-v1所在的Pod。</p>
<p>在reviews端，该请求被iptable规则拦截，所有的入向请求(inbound)都被重定向到本地的15006端口。</p>
<p>在15006端口上监听的Envoy Virtual Inbound Listener收到了该请求。</p>
<p>根据匹配条件，请求被Virtual Inbound Listener内部配置的Http connection manager filter处理，该filter设置的路由配置为将其发送给inbound|9080|http|reviews.default.svc.cluster.local这个inbound cluster</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version_info&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:07:44Z/12&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;listener&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;virtualInbound&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;socket_address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.0.0.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;port_value&quot;</span><span class="punctuation">:</span> <span class="number">15006</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;filter_chains&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            ......</span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;filter_chain_match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;prefix_ranges&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;address_prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.40.0.15&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;prefix_len&quot;</span><span class="punctuation">:</span> <span class="number">32</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;destination_port&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;filters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.http_connection_manager&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;typed_config&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;@type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;stat_prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound_10.40.0.15_9080&quot;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;http_filters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;istio_authn&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                    ......</span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mixer&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                    ......</span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.cors&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.fault&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;envoy.router&quot;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span></span><br><span class="line">                            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                            ......</span><br><span class="line">                            <span class="attr">&quot;route_config&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound|9080|http|reviews.default.svc.cluster.local&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;virtual_hosts&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                    <span class="punctuation">&#123;</span></span><br><span class="line">                                        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound|http|9080&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                        <span class="attr">&quot;domains&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                            <span class="string">&quot;*&quot;</span></span><br><span class="line">                                        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                        <span class="attr">&quot;routes&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                                            <span class="punctuation">&#123;</span></span><br><span class="line">                                                <span class="attr">&quot;match&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                                    <span class="attr">&quot;prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/&quot;</span></span><br><span class="line">                                                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                                                ......</span><br><span class="line">                                                <span class="attr">&quot;route&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                                    <span class="attr">&quot;timeout&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0s&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                                    <span class="attr">&quot;max_grpc_timeout&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0s&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                                    <span class="attr">&quot;cluster&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound|9080|http|reviews.default.svc.cluster.local&quot;</span> <span class="comment">//对应的inbound cluster</span></span><br><span class="line">                                                <span class="punctuation">&#125;</span></span><br><span class="line">                                            <span class="punctuation">&#125;</span></span><br><span class="line">                                        <span class="punctuation">]</span></span><br><span class="line">                                    <span class="punctuation">&#125;</span></span><br><span class="line">                                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                                <span class="attr">&quot;validate_clusters&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                ......</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>“route_config_name”&#x3D;”inbound|9080|http|reviews.default.svc.cluster.local”</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;version_info&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:06Z/13&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cluster&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound|9080|http|productpage.default.svc.cluster.local&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;STATIC&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;connect_timeout&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1s&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;circuit_breakers&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;thresholds&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;max_connections&quot;</span><span class="punctuation">:</span> <span class="number">4294967295</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_pending_requests&quot;</span><span class="punctuation">:</span> <span class="number">4294967295</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_requests&quot;</span><span class="punctuation">:</span> <span class="number">4294967295</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;max_retries&quot;</span><span class="punctuation">:</span> <span class="number">4294967295</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;load_assignment&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;cluster_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;inbound|9080|http|productpage.default.svc.cluster.local&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;endpoints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;lb_endpoints&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                        <span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;endpoint&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                    <span class="attr">&quot;socket_address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                                        <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1&quot;</span><span class="punctuation">,</span> <span class="comment">//cluster配置的endpoint地址</span></span><br><span class="line">                                        <span class="attr">&quot;port_value&quot;</span><span class="punctuation">:</span> <span class="number">9080</span></span><br><span class="line">                                    <span class="punctuation">&#125;</span></span><br><span class="line">                                <span class="punctuation">&#125;</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">]</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;last_updated&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-12-04T03:08:22.658Z&quot;</span></span><br></pre></td></tr></table></figure>

<p>到此，一条完整的调用链分析完成.</p>
<p>当然，以上过程除了可以从配置文件分析而来， 也可以通过Istioctl命令行的方式获取, 这里不展开说明, 后续有时间单独更新.</p>
<p><strong>需要注意的是，服务网格内的应用仍然通过 ClusterIP 与网格外的应用通信，但有一点需要注意：这里并没有 <code>kube-proxy</code> 的参与！Envoy 自己实现了一套流量转发机制，当你访问 ClusterIP 时，Envoy 就把流量转发到具体的 Pod 上去，不需要借助 kube-proxy 的 <code>iptables</code> 或 <code>ipvs</code> 规则</strong></p>
<p><strong>可以看出请求首先到达 <code>Listener</code>，然后通过 <code>Http Route Table(HTTP 的路由规则，例如请求的域名，Path 符合什么规则【envoy会将domain中的字符串与请求中的header进行匹配】，从而决定转发给哪个 Cluster)</code> 转到具体的 <code>Cluster</code>，再通过cluster动态查询ep list,最后由具体的某个ep pod对请求做出响应</strong></p>
</li>
</ol>
</li>
</ol>
<h3 id="Istio缺点"><a href="#Istio缺点" class="headerlink" title="Istio缺点"></a>Istio缺点</h3><p>从上面的流量分析来看，最影响性能的就是一但集群中的实例数太多, 当有某个实例有更新都要<code>全量的</code>下发整个集群的配置给envoy,envoy需要重启等, 一旦更新的比较频繁, 这必然会影响性能, 所以这块社区也一直在想办法解决, 比如如何按需下发配置、限定只在同一个namespace下的工作负载下发更新配置等,相信不久的将来会有更完美的解决方案.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://jimmysong.io/istio-handbook/data-plane/envoy-xds-protocol.html">https://jimmysong.io/istio-handbook/data-plane/envoy-xds-protocol.html</a></li>
<li><a href="https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/">https://jimmysong.io/posts/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/</a></li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></li>
<li><a href="https://jimmysong.io/posts/istio-traffic-management-basic-concepts/">https://jimmysong.io/posts/istio-traffic-management-basic-concepts/</a></li>
<li><a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(strip_path使用)</title>
    <url>/2020/09/23/Kong-strip-path/</url>
    <content><![CDATA[<p>kong中的strip_path用于是否将请求中的url中path前缀进行剥离，在kong controller的不同版本中, 使用方法不同</p>
<span id="more"></span>

<p>在1.x的版本中, strip_path需要使用kongingress进行指定</p>
<p>而在2.x中, 直接在ingress中指定annotataion即可</p>
<p>这里主要以2.x的新版本来实践一番</p>
<h3 id="strip-path"><a href="#strip-path" class="headerlink" title="strip_path"></a>strip_path</h3><p>部署echoserver服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create namespace <span class="built_in">echo</span></span><br><span class="line">namespace/echo created</span><br><span class="line">$ kubectl apply -n <span class="built_in">echo</span> -f https://bit.ly/echo-service</span><br><span class="line">service/echo created</span><br><span class="line">deployment.apps/echo created</span><br></pre></td></tr></table></figure>

<p>这里为了体现strip_path的功能, 分别部署echo1,echo2两个deploy、svc</p>
<p>部署ingress</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">echo</span> <span class="string">&#x27;</span></span><br><span class="line"><span class="string">apiVersion: extensions/v1beta1</span></span><br><span class="line"><span class="string">kind: Ingress</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  annotations:</span></span><br><span class="line"><span class="string">    konghq.com/strip-path: &quot;true&quot; # 打开路径前缀strip.</span></span><br><span class="line"><span class="string">  name: my-app</span></span><br><span class="line"><span class="string">  namespace: echo</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  rules:</span></span><br><span class="line"><span class="string">  - host: myapp.example.com</span></span><br><span class="line"><span class="string">    http:</span></span><br><span class="line"><span class="string">      paths:</span></span><br><span class="line"><span class="string">      - path: /echo1/</span></span><br><span class="line"><span class="string">        backend:</span></span><br><span class="line"><span class="string">          serviceName: echo</span></span><br><span class="line"><span class="string">          servicePort: 8080</span></span><br><span class="line"><span class="string">       - path: /echo2/</span></span><br><span class="line"><span class="string">        backend:</span></span><br><span class="line"><span class="string">          serviceName: echo</span></span><br><span class="line"><span class="string">          servicePort: 8080</span></span><br><span class="line"><span class="string">&#x27;</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">create</span> <span class="string">-f</span> <span class="bullet">-</span></span><br></pre></td></tr></table></figure>

<p>请求</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># PROXY_IP 为kong proxy svc 地址</span></span><br><span class="line"><span class="built_in">export</span> PROXY_IP=10.109.157.29</span><br><span class="line"><span class="comment"># 分别请求echo1/echo2</span></span><br><span class="line"><span class="comment"># echo1</span></span><br><span class="line">curl -svX GET http://myapp.example.com/echo1/myapp/foo --resolve myapp.example.com:80:<span class="variable">$PROXY_IP</span></span><br><span class="line"><span class="comment"># echo2</span></span><br><span class="line">curl -svX GET http://myapp.example.com/echo2/myapp/foo --resolve myapp.example.com:80:<span class="variable">$PROXY_IP</span></span><br></pre></td></tr></table></figure>

<p>请求echo2返回:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">Hostname:</span> <span class="string">echo2-786fd5f6f5-tc596</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Pod Information:</span></span><br><span class="line">    <span class="attr">node name:</span>    <span class="string">g105e1900156</span></span><br><span class="line">    <span class="attr">pod name:</span>    <span class="string">echo2-786fd5f6f5-tc596</span></span><br><span class="line">    <span class="attr">pod namespace:</span>    <span class="string">echo</span></span><br><span class="line">    <span class="attr">pod IP:</span>    <span class="number">10.244</span><span class="number">.0</span><span class="number">.48</span> <span class="comment"># echo2的pod的IP</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Server values:</span></span><br><span class="line">    <span class="string">server_version=nginx:</span> <span class="attr">1.13.3 - lua:</span> <span class="number">10008</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Request Information:</span></span><br><span class="line">    <span class="string">client_address=::ffff:10.244.0.37</span></span><br><span class="line">    <span class="string">method=GET</span></span><br><span class="line">    <span class="string">real</span> <span class="string">path=/myapp/foo</span> <span class="comment"># 真正请求到达pod时可以看到请求已经将echo2前缀给strip掉了.</span></span><br><span class="line">    <span class="string">query=</span></span><br><span class="line">    <span class="string">request_version=1.1</span></span><br><span class="line">    <span class="string">request_scheme=http</span></span><br><span class="line">    <span class="string">request_uri=http://myapp.example.com:8080/myapp/foo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Request Headers:</span></span><br><span class="line">    <span class="string">accept=*/*</span></span><br><span class="line">    <span class="string">connection=keep-alive</span></span><br><span class="line">    <span class="string">host=myapp.example.com</span></span><br><span class="line">    <span class="string">user-agent=curl/7.47.0</span></span><br><span class="line">    <span class="string">x-forwarded-for=10.244.0.1</span></span><br><span class="line">    <span class="string">x-forwarded-host=myapp.example.com</span></span><br><span class="line">    <span class="string">x-forwarded-port=8000</span></span><br><span class="line">    <span class="string">x-forwarded-prefix=/echo2/myapp/foo</span> <span class="comment"># 这里是真正的请求路径.</span></span><br><span class="line">    <span class="string">x-forwarded-proto=http</span></span><br><span class="line">    <span class="string">x-real-ip=10.244.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Request Body:</span></span><br><span class="line">    <span class="string">-no</span> <span class="string">body</span> <span class="string">in</span> <span class="string">request-</span></span><br></pre></td></tr></table></figure>

<p>因此可以通过<code>strip_path</code>来实现每个应用使用特定的路径前缀.</p>
<p>在旧版本中，需要这样使用</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">&quot;kong-demo&quot;</span></span><br><span class="line">    configuration.konghq.com: <span class="string">&quot;kongingress-demo&quot;</span></span><br><span class="line">  name: demo-ingress</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: test-service</span><br><span class="line">          servicePort: 80</span><br><span class="line">        path: /</span><br><span class="line">---</span><br><span class="line">apiVersion: configuration.konghq.com/v1</span><br><span class="line">kind: KongIngress</span><br><span class="line">metadata:</span><br><span class="line">  name: kongingress-demo</span><br><span class="line">route:</span><br><span class="line">  protocols:</span><br><span class="line">  - http</span><br><span class="line">  - https</span><br><span class="line">  strip_path: <span class="literal">false</span>  <span class="comment"># 在这里开始strip_path,然后在ingress上绑定kongingress</span></span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/Kong/">https://github.com/Kong/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(kong介绍)</title>
    <url>/2020/08/15/Kong-introdution/</url>
    <content><![CDATA[<p>Kong是一个Mashape开源的高性能高可用的API网关和API服务管理层，基于OpenResty,进行API管理，并提供了插件实现了API的AOP功能, 官方更是有这样的描述: <code>Next-Generation API Platform for Multi-Cloud and Hybrid Organizations</code>,可见kong的野心还是非常大的，也确实非常受欢迎,目前的Kong的版本为2.1.</p>
<p>这里简单提一下, 如果看过视频的都会发现，kong其实应该读康，而不是读空.</p>
<span id="more"></span>

<p>kong使用也有一段时间了，这里把相关的使用心得记录一下.</p>
<h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><p>kong做为API网关,总结起来有有以下特性:</p>
<ul>
<li>云原生: 与平台无关，Kong 可以从裸机运行到 Kubernetes</li>
<li>高性能 : 背靠非阻塞通信的 nginx，性能自不用说</li>
<li>插件机制 : 提供众多开箱即用的插件，且有易于扩展的自定义插件接口，用户可以使用 Lua 自行开发插件</li>
<li>熔断：可以通过插件实现熔断，避免系统雪崩</li>
<li>日志: 可以记录通过 Kong 的 HTTP，TCP，UDP 请求和响应。</li>
<li>鉴权: 权限控制，IP 黑白名单，同样是 OpenResty 的特性</li>
<li>SSL: Setup a Specific SSL Certificate for an underlying service or API.</li>
<li>监控: Kong 提供了实时监控插件</li>
<li>认证: 如数支持 HMAC, JWT, Basic, OAuth 2.0 等常用协议</li>
<li>限流：可以通过插件实现单个服务某些接口的限流，避免服务过载导致不可用</li>
<li>REST API: 通过 Rest API 进行配置管理，从繁琐的配置文件中解放</li>
<li>健康检查：自动检查，被动检查；节点不可用同步到所有的kong节点需要 1 - 2 秒</li>
<li>动态路由：Kong 的背后是 OpenResty+Lua，所以从 OpenResty 继承了动态路由的特性</li>
</ul>
<p>当然，从上面这些概括可以看到:一方面，插件化的形式让kong可以很方便地提供了各种功能，对API层的管理有天然的粘合性</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200611144926.png"></p>
<h3 id="资源对象"><a href="#资源对象" class="headerlink" title="资源对象"></a>资源对象</h3><p>当然对于一款API网关软件来说, 有一些重要的资源对象，先来熟悉一下名词:</p>
<ul>
<li><strong>Upstream</strong></li>
</ul>
<p>Upstream 对象表示虚拟主机名，可用于通过多个服务（目标）对传入请求进行负载均衡。例如：service.v1.xyz 为Service对象命名的上游host是service.v1.xyz对此服务的请求将代理到上游定义的目标。<strong>可以不存在upstream, 相当于没有负载均衡</strong>.</p>
<ul>
<li><strong>Target</strong></li>
</ul>
<p>目标IP地址&#x2F;主机名，其端口表示后端服务的实例。每个上游都可以有多个target,并且可以动态添加Target。</p>
<p>由于上游维护Target的更改历史记录，因此无法删除或者修改Target。要禁用目标，请发布一个新的Targer weight&#x3D;0,或者使用DELETE来完成相同的操作。</p>
<ul>
<li><strong>Service</strong></li>
</ul>
<p>顾名思义，服务实体是每个上游服务的抽象。服务的示例是数据转换微服务，计费API等。</p>
<p>服务的主要属性是它的URL（其中，Kong应该代理流量），其可以被设置为单个串或通过指定其protocol， host，port和path。</p>
<p>服务与路由相关联（服务可以有许多与之关联的路由）。路由是Kong的入口点，并定义匹配客户端请求的规则。一旦匹配路由，Kong就会将请求代理到其关联的服务。</p>
<ul>
<li><strong>Route</strong></li>
</ul>
<p>路由实体定义规则以匹配客户端的请求。<strong>每个Route与一个Service相关联</strong>，一个服务可能有多个与之关联的路由。与给定路由匹配的每个请求都将代理到其关联的Service上。</p>
<p>Service 和 Route 的组合（以及它们之间的关注点分离）提供了一种强大的路由机制，通过它可以在Kong中定义细粒度的入口点，从而使基础架构路由到不同上游服务。</p>
<ul>
<li><strong>Consumer</strong></li>
</ul>
<p>Consumer 对象表示服务的使用者或者用户。您可以依靠Kong作为主数据库存储，也可以将使用者列表与数据库映射，以保持Kong与现有的主数据存储之间的一致性。</p>
<ul>
<li><strong>Plugin</strong></li>
</ul>
<p>插件实体表示将在HTTP请求&#x2F;响应生命周期期间执行的插件配置。它是如何为在Kong后面运行的服务添加功能的，例如身份验证或速率限制。</p>
<p>将插件配置添加到服务时，客户端向该服务发出的每个请求都将运行所述插件。如果某个特定消费者需要将插件调整为不同的值，您可以通过创建一个单独的插件实例，通过service和consumer字段指定服务和消费者 。</p>
<p>上述总结引用<a href="https://www.cnkirito.moe/kong-introduction/">kong-introduction</a></p>
<p>说实话，在CloudNavi中的很多软件都或多或少的会引入一些名词，特别尴尬的是有些名词名字相同但是所表达的意思却完全不一样，比如上面的service，在Kubernetes也有service这个对象，但是表达的是两种不相同的事项，到后来这类东西学的越多，就越容器混淆.</p>
<p>详解可以参考官网<a href="https://docs.konghq.com/2.1.x/admin-api/">admin-api</a></p>
<p>总结起来流量的走向,<strong>client –&gt; router –&gt; service –&gt; [upstream] –&gt; target</strong>, 可以使用下面的图来表示,非常清晰:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200107141102186.png"></p>
<p>后面会重点围绕这些资源对象来测试kong的使用</p>
<h3 id="API使用"><a href="#API使用" class="headerlink" title="API使用"></a>API使用</h3><p>kong做为一个基于openresty实现的api网关, 可以通过API直接管理对象，比如要生成以下的一段nginx配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">upstream</span> <span class="string">hello</span> &#123;</span><br><span class="line">    <span class="string">server</span> <span class="string">localhost:3000</span> <span class="string">weight=100;</span></span><br><span class="line">    <span class="string">server</span> <span class="string">localhost:3001</span> <span class="string">weight=50;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">server</span> &#123;</span><br><span class="line">    <span class="string">listen</span>  <span class="number">80</span><span class="string">;</span></span><br><span class="line">    <span class="string">location</span> <span class="string">/hello</span> &#123;</span><br><span class="line">        <span class="string">proxy_pass</span> <span class="string">http://hello;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的nginx规则如果使用kong的api实现的话,对应以下接口</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 8001为kong admin的http监听端口</span></span><br><span class="line"><span class="comment"># 创建upstream,名字为hello</span></span><br><span class="line"><span class="string">curl</span> <span class="string">-X</span> <span class="string">POST</span> <span class="string">http://localhost:8001/upstreams</span> <span class="string">--data</span> <span class="string">&quot;name=hello&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为upstream, 创建两个target,分别设置权重</span></span><br><span class="line"><span class="string">curl</span> <span class="string">-X</span> <span class="string">POST</span> <span class="string">http://localhost:8001/upstreams/hello/targets</span> <span class="string">--data</span> <span class="string">&quot;target=localhost:3000&quot;</span> <span class="string">--data</span> <span class="string">&quot;weight=100&quot;</span></span><br><span class="line"><span class="string">curl</span> <span class="string">-X</span> <span class="string">POST</span> <span class="string">http://localhost:8001/upstreams/hello/targets</span> <span class="string">--data</span> <span class="string">&quot;target=localhost:3001&quot;</span> <span class="string">--data</span> <span class="string">&quot;weight=50&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建service</span></span><br><span class="line"><span class="string">curl</span> <span class="string">-X</span> <span class="string">POST</span> <span class="string">http://localhost:8001/services</span> <span class="string">--data</span> <span class="string">&quot;name=hello&quot;</span> <span class="string">--data</span> <span class="string">&quot;host=hello&quot;</span></span><br><span class="line"><span class="comment"># 成功后会返回service.id</span></span><br><span class="line"><span class="string">8695cc65-16c1-43b1-95a1-5d30d0a50409</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为service创建路由</span></span><br><span class="line"><span class="string">curl</span> <span class="string">-X</span> <span class="string">POST</span> <span class="string">http://localhost:8001/routes</span> <span class="string">--data</span> <span class="string">&quot;paths[]=/hello&quot;</span> <span class="string">--data</span> <span class="string">&quot;service.id=8695cc65-16c1-43b1-95a1-5d30d0a50409&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试路由</span></span><br><span class="line"><span class="string">curl</span> <span class="string">http://localhost:8000/hello/hi</span></span><br></pre></td></tr></table></figure>

<p>这里只是展示一下kong admin的使用，在实际使用中， 并不会直接通过api来生成对象，更多的是<code>动态场景</code>，要不然效率太差.</p>
<p>kong admin API还可以通过UI工具来直接管理, 比如最常用的<code>konga</code></p>
<h3 id="关于service"><a href="#关于service" class="headerlink" title="关于service"></a>关于service</h3><p>上面简单介绍了下kong的常用对象，做为api网关，一般都处于流量的最前端，那自然少不了路由的匹配, 因此Route这个对象还是比较好理解的。</p>
<p>Route匹配之后那要转到哪个服务进行响应呢? 这就是service, 但是这个service并不是真正相应请求的实体，以官方的说法，service是每个上游服务(upstream)的抽象，是个抽象的概念.</p>
<p>target才是真正对请求进行响应的, 在kubernetes就是部署在pods中的程序</p>
<p>而upstream则是个loadbalance，对多个target进行lb, 可以指定多种lb方式，健康检查机制等.</p>
<p>对于kong service与kubernetes service的区别, 以官方的一张图<a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/design.md">解释</a>: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200811213605.png"></p>
<p>Let’s go through how Kubernetes resources are being mapped to Kong’s configuration:</p>
<ul>
<li><p>An <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> resource in Kubernetes defines a set of rules for proxying traffic. These rules corresponds to the concept of Route in Kong.</p>
<p>kubernetes中的ingress定义了流量转发规则，这些规则对kong中的路由进行响应</p>
</li>
<li><p>A <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> inside Kubernetes is a way to abstract an application that is running on a set of pods. This maps to two objects in Kong: Service and Upstream. The service object in Kong holds the information on the protocol to use to talk to the upstream service and various other protocol specific settings. The Upstream object defines load balancing and healthchecking behavior.</p>
<p>Kubernetes内的<a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>是在一组Pod上运行的应用程序的方法的抽象。这映射到Kong中的两个对象：服务和上游。 Kong中的服务对象保存有关用于与上游服务进行通信的协议信息以及各种其他特定于协议的设置。上游对象定义负载平衡和运行状况检查行为</p>
</li>
<li><p>Pods associated with a Service in Kubernetes map as a Target belonging to the Upstream (the upstream corresponding to the Kubernetes Service) in Kong. Kong load balances across the Pods of your service. <strong>This means that all requests flowing through Kong are not directed via kube-proxy but directly to the pod</strong>.</p>
<p>Kubernetes中与服务相关联的Pod映射为Kong中上游（与Kubernetes Service对应的上游）的目标。 Kong在您的服务Pod中平衡负载。这意味着所有流经Kong的请求都不会通过kube-proxy定向，而是直接定向到pod</p>
</li>
</ul>
<p>这里有个重点是: kong中的请求到达最终的端点target并没有通过Kube-proxy，因此不存在需要遍历iptables规则，因此会对kubernetes原生的service效率会高</p>
<p>另一个疑问是: kong是否可以不需要service？从route匹配后直接到upstream?</p>
<p>kong中的service还是包含了一些信息的, 比如<code>protocol, host, port and path</code>等信息，如果没有这层信息, 请求从route直接到达upstream后是不知道到底是http还是https的，就相当需要把这段代码:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen  80; <span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>加到upstream中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">upstream hello &#123;</span><br><span class="line">    server localhost:3000 weight=100;</span><br><span class="line">    server localhost:3001 weight=50;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而kong又是基于nginx的，这样调整的话就与nginx的语法不相符了,因此service是有必要的.</p>
<p>kong还是比较强大的，比如在Kubernetes中做为ingress使用、如何集成插件化对流量进行管理等等，这里只是简单介绍了一下kong，有个整体的理解.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/design.md">https://github.com/Kong/kubernetes-ingress-controller/blob/main/docs/concepts/design.md</a></li>
<li><a href="https://www.cnkirito.moe/kong-introduction/">https://www.cnkirito.moe/kong-introduction/</a></li>
<li><a href="https://docs.konghq.com/2.1.x/admin-api/">https://docs.konghq.com/2.1.x/admin-api/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(解决诡异的Kong Error超时及重试问题)</title>
    <url>/2020/11/17/Kong-upstream-timeout-60s-and-retry-5/</url>
    <content><![CDATA[<p>现在业务使用的kong做为api gateway, 最近碰到一个的kong error超时及重试的问题，记录下排查过程</p>
<span id="more"></span>

<p>在浏览器中下载后端一个好几G的文件，会出现Kong Error错误，如下图所示</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117193533.png"></p>
<p>这里说一下，下载文件这个动作整个端到端的请求路径如下:</p>
<p><code>浏览器 &lt;--&gt; kong &lt;--&gt; frontend(VUE + nginx) &lt;--&gt; k8s service &lt;--&gt; backend &lt;--&gt; resource-factory</code></p>
<p>backend是则java，提供数据下载，然后返回给前端</p>
<p>frontend与backend之间是通过k8s service 进行访问的</p>
<p>在frontend请求下载的过程中，backend需要先从resource-factory中获取文件保存在bakend本地，这个时间会比较长，backend下载完成之后才会返回到frontend</p>
<h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>从kong的错误提示来看，很明显，<code>upstream server</code>超时（这个<code>upstream</code>很重要），最开始没注意，只关注到<code>timeout</code>这个关键字了</p>
<h4 id="backend"><a href="#backend" class="headerlink" title="backend"></a>backend</h4><p>先从backend的日志开始排查，意料之中，backend日志出现一堆的error，但是意料之外的是，在这过程中出现了每隔60s就会对文件重新下载， 总共进行了5次重新下载的操作，这个比较奇怪，正常只是在页面上点击了一次操作，为何会出现连续下载5次的流程，而且每次下载都没有下载完就进行重试，而且都是每隔60s， 像是有重试的操作，比较诡异</p>
<p>从日志可以肯定的是，<strong>下载文件的代码逻辑是没有问题的</strong>，因此在这里出现了两个问题:</p>
<ol>
<li>下载代码没问题，但下载为何会失败</li>
<li><strong>60s, 5次下载是怎么产生的</strong></li>
</ol>
<p>下面主要是在排查问题2是如何产生的，还是比较有意思</p>
<h4 id="frontend"><a href="#frontend" class="headerlink" title="frontend"></a>frontend</h4><p>从backend没有找到最有价值的信息，那么看看frontend打印的日志中有没有什么思路，由于frontend中包含了一个nginx，大部分的请求都会被记录下来，首先将nginx的日志调整为<code>debug</code></p>
<ol>
<li><code>client xxx.xxx.xxx.xxx closed keepalive connection</code></li>
<li><code>epoll_wait() reported that client prematurely closed connection</code></li>
<li>出现nginx 499状态码</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117103815.png"></p>
<p>关于第1个<code>错误</code>(后来被证实这不是错误)看到keepalive closed，那么是不是很自然地想到nginx中有关于keepalive的相关参数，keepalive默认情况下是75s,跟60s好像也不太符合，但是不是确实是这个值太小了呢? 因些作者还是调整了frontend中nginx的keepalive为600s, 但是问题依旧</p>
<p>因此转到第2个错误，网上查找了一翻发现这个跟上面两行出现代码499的原因一致，<code>nginx 499 </code>这个状态码可能接触地比较少，当时也不是很明白这个状态码应对的含义，如果有不清楚的可以参考<a href="https://imajinyun.xyz/2019/11/15/nginx-499-faq/">nginx-499-faq</a></p>
<p>499一句话概括就是: <strong>Nginx 把请求转发上游服务器，上游服务器慢吞吞的处理，客户端等不及了主动断开链接，Nginx 就负责记录了 <code>499</code></strong>, 是不是刚好跟epoll_wait那句话的意思相近。</p>
<p>因此出现错误499及错误2的原因就是客户端主动关闭了连接，</p>
<p>由于下载的文件非常大，大小在4G+， 作者还以为是nginx某些配置的timeout设置的不准确</p>
<p>frontend中的nginx主要调整过的配置项如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">keepalive_timeout 1200s;</span><br><span class="line">send_timeout 1200s;</span><br><span class="line">proxy_read_timeout 1200s;</span><br><span class="line">proxy_send_timeout 1200s;</span><br></pre></td></tr></table></figure>

<p>重启之后发现问题依旧, 这时作者怀疑是浏览器是不是也存在timeout不够长导致连接被断开了</p>
<p>因此直接使用<strong>curl的方式请求</strong>，发现问题依旧，<strong>因此可以排除浏览器的问题</strong></p>
<p>这时作者想起另外一个也同样使用VUE写的前端，VUE的axios(<a href="https://blog.csdn.net/qq_36727756/article/details/93738441">参考</a>)是可以设置http请求的超时时间的，那是不是会由于前端对每个http设置的timeout太短了呢，经过询问前端的同事，果然，前端配置了这个时间为60s，这明显不够，作者欣喜若狂，以为见到了曙光, 然而</p>
<p>** too young, too simple**</p>
<p>前端将60s调整为600s，<strong>问题依旧</strong> 同样提示上面的错误，同样重复下载5次</p>
<p>** WHAT FXXK**</p>
<p>同时，作者跟前端同事反复确认frontend的逻辑中会不会有重试的代码，前端同事很明确地说：NO</p>
<p>冷静下来</p>
<p>VUE的axios超时时间一定有影响，消除这部分的影响问题依旧，那一定还有别的地方存在timeout从而引起重试机制，既然对于frotend的nginx来说，是客户端主动断开了连接，除了frontend代码本身，那客户端还有谁呢?</p>
<h4 id="kong"><a href="#kong" class="headerlink" title="kong"></a>kong</h4><p>在frontend前面的就是kong了，那么它就是客户端，难道是kong这边把连接主动断开了？</p>
<p>为了验证是不是kong这边的问题，作者使用curl直接请求frontend，绕过了kong这一层，发现居然下载没问题了，同时，也不存在每隔60s重复下载一次的现象</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117110709.png"></p>
<p>从下载的流程来看，前2分44s应该是backend向resource-factory中获取文件下载到本地的时间，果然时间比较长</p>
<p>后面的10分37s则是backend向frontend的nginx加传文件的时间，时间也比较长</p>
<p>好歹直接通过frontend可以下载文件成功.</p>
<p><strong>柳暗花明又一村</strong></p>
<p>截止到现在归纳一下做过的调整</p>
<ol>
<li>调整了VUE的axios的超时时间，这个是对每个经过frontend的请求都生效</li>
<li>调整了frontend中nginx的timeout配置</li>
</ol>
<p>老规矩，看kong proxy的日志如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117114330.png"></p>
<p>也是timeout,不过这次作者终于看到了<strong>upstream</strong>这个词了</p>
<p><strong>这里说的upstream不就是frontend么, 难道是当backend向resource-factory下载文件的时候，也就是2分44s这个时间内，frontend没有收到backend返回的response，frontend也就没有response返回给kong， 在一定时间内(也就是日志中出现的60s)kong无法从frontend中获取到response，出现timeout,然后开始重试，重试5次之后frontend还是没有response，最后kong将这条请求断开了.</strong></p>
<p>这个猜想其实是合理的，符合看到的日志报错，同时也符合5次重试的这个现象</p>
<p><strong>但是5次重启到底是怎么回事，upstream真的就是frontend</strong></p>
<p>作者这里也想到kong其实openresty, 不也是nginx呢?那是不是kong也需要设置一下nginx的timeout配置呢?</p>
<p>嗯，加上再说</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- name: KONG_NGINX_PROXY_CLIENT_BODY_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_PROXY_CLIENT_HEADER_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_PROXY_KEEPALIVE_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_PROXY_PROXY_READ_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_PROXY_PROXY_SEND_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_PROXY_SEND_TIMEOUT</span><br><span class="line">  value: 600s</span><br><span class="line">- name: KONG_NGINX_UPSTREAM_KEEPALIVE_TIMEOUT</span><br><span class="line">  value: 600s</span><br></pre></td></tr></table></figure>

<p>按照<a href="https://docs.konghq.com/1.1.x/configuration/">官方文档</a>将这些参数到容器的env中，然后重启kong</p>
<p><strong>擦，问题依旧</strong></p>
<p>这里看到问题没有解决，第一时间难道是加的参数都没有生效，不能使用environment，作者还特意到容器中验证了一下验证</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117120250.png"></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117120312.png"></p>
<p>发现都是没有问题的，那为什么还是不行呢?</p>
<p>再次网上搜索一翻，发现kong中对于service(<strong>是kong中的service,而不是k8s中的service</strong>),还有3个timeout参数</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117225634.png"></p>
<p>然后到kong中查看对应的service, 如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117164812.png"></p>
<p><strong>发现这3个timeout都是默认值，为60s， 而且，retries为5</strong>，刚好跟重试的现象对应上了</p>
<p>先不管，使用如下命令更新一下这个记录，将timeout改长一点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -X PATCH --url https://127.0.0.1:8444/services/791f94ae-7971-54e5-8996-e66083af6617 \</span><br><span class="line">-d <span class="string">&#x27;connect_timeout=800000&#x27;</span> \</span><br><span class="line">-d <span class="string">&#x27;read_timeout=800000&#x27;</span> \</span><br><span class="line">-d <span class="string">&#x27;write_timeout=800000&#x27;</span></span><br></pre></td></tr></table></figure>

<p>发现提示下面的错误:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117170805.png"></p>
<p>真是一步一个坎， 更新操作不支持，作者也试过删除操作，发现也是同样的错误</p>
<p>这个错误的原因在于作者使用的kong是DB-less的模式，对于db-less下是不能修改绑定在kong上的<code>entities serivces</code>的</p>
<p>那不能更新也不能删除，那么就只能重建了service了,kong中的service对应的是<code>kongingress中的proxy</code>,更新如下</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongIngress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kongingress-resource</span></span><br><span class="line"><span class="attr">proxy:</span>  <span class="comment"># proxy是新增的</span></span><br><span class="line">  <span class="attr">connect_timeout:</span> <span class="number">800000</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">protocols:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">read_timeout:</span> <span class="number">800000</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">write_timeout:</span> <span class="number">800000</span></span><br></pre></td></tr></table></figure>

<p>这样更新之后会发现kong中serivce的记录还是不变，经过多次的实验，正确的操作如下:</p>
<ol>
<li>先将ingress删除后，会发现kong中的service记录也删除掉了</li>
<li>然后将k8s中的service加上annotations指定<code>konghq.com/override: kongingress-resource</code></li>
<li>再发布ingress, 同样指定<code>konghq.com/override: kongingress-resource</code></li>
</ol>
<p>经过上面的3个操作之后，再来看kong中service的记录，更新成功</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201117174434.png"></p>
<p>这个时候再测试页面的下载功能，就没有再出现过超时及重试的现象了，</p>
<p><strong>完美</strong></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于这次<code>诡异</code>的timeout问题可以看出， 中间的转发层一多，因为每一层都可能超时，原因排查起来就比较费力</p>
<p>在排查过程中，每个错误都可能是关键信息，不要选择性忽略，对于这次作者其实就忽略了<code>upstream</code>这个信息</p>
<p>当然如果从一开始就关注到upstream，也还是要将上面的排查过程走一遍，因为确实有好几个地方都是有问题的</p>
<p>比如UVE的axios的超时配置，nginx的超时配置，作者在这过程中甚至还想到过k8s中service的超时机制</p>
<p>有用的知识又增加了一些.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.konghq.com/1.1.x/configuration/">https://docs.konghq.com/1.1.x/configuration/</a></li>
<li><a href="https://docs.konghq.com/1.1.x/configuration/">https://docs.konghq.com/1.1.x/configuration/</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/issues/472">https://github.com/Kong/kubernetes-ingress-controller/issues/472</a></li>
<li><a href="https://github.com/Kong/kubernetes-ingress-controller/issues/905">https://github.com/Kong/kubernetes-ingress-controller/issues/905</a></li>
<li><a href="https://linuxops.org/blog/kong/admin.html">https://linuxops.org/blog/kong/admin.html</a></li>
<li><a href="https://docs.konghq.com/1.1.x/admin-api/#update-service">https://docs.konghq.com/1.1.x/admin-api/#update-service</a></li>
<li><a href="https://imajinyun.xyz/2019/11/15/nginx-499-faq/">https://imajinyun.xyz/2019/11/15/nginx-499-faq/</a></li>
<li><a href="https://lanjingling.github.io/2016/06/11/nginx-https-keepalived-youhua/">https://lanjingling.github.io/2016/06/11/nginx-https-keepalived-youhua/</a></li>
<li><a href="https://luanlengli.github.io/2019/07/02/Kong-Ingress-Controller%E9%83%A8%E7%BD%B2.html">https://luanlengli.github.io/2019/07/02/Kong-Ingress-Controller%E9%83%A8%E7%BD%B2.html</a></li>
<li><a href="https://blog.csdn.net/qq_36727756/article/details/93738441">https://blog.csdn.net/qq_36727756/article/details/93738441</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(当使用kubectl apply -f 多个资源时,资源创建的顺序是怎么样的)</title>
    <url>/2020/12/27/Kubernetes-Apply-Directory-Order/</url>
    <content><![CDATA[<p>前几天一个开发同学跑过来跟我说，在自动发布一个应用到k8s集群中时，会出现发布成功，但是容器中的配置文件还是旧配置文件的情况，让我看看原因.第一时间我觉得不可能，但是一细想，好像是有可能发生, 立马在我脑海中出现一个疑问:</p>
<p><strong>使用了这么多年的k8s, <code>kubectl apply -f &lt;directory&gt;</code>时，目录下多个资源文件的apply顺序到底是什么?</strong></p>
<span id="more"></span>

<h3 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h3><p>等到下一秒的我，又可以很清晰且自信地回答上面的问题, 在之前的使用<code>apply -f &lt;directory&gt;</code>时，出现过几次提示没有<code>namespace</code>的情况，类似如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls test</span></span><br><span class="line">config.yml</span><br><span class="line">deploy.yml</span><br><span class="line">ns.yml</span><br><span class="line">service.yml</span><br></pre></td></tr></table></figure>

<p>如上所示，如果直接使用<code>kubectl apply -f .</code>则会出现在创建<code>deploy.yml</code>时提示找不到<code>namespace</code></p>
<p>从输出的结果可以得到结论: <strong>在创建多个资源文件时，创建的顺序是以文件名排序的顺序,这样的话就可能存在依赖失败</strong></p>
<p>按道理这个依赖关系对kubernetes来说不是很复杂，为何不支持呢? 个人感觉如果是原生的资源对象，那确实不复杂，但后面引入的<code>CRD</code>机制，这个依赖关系就不好处理了，所以就是现在这个局面，说不定新版本已经支持.</p>
<p>从官方文档上也有这么<a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/">一段话</a>:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201227151502.png"></p>
<p>翻译: apply 也支持指定目录，会在目录下查找<code>yaml、yml、json</code>为后缀的文件名进行apply</p>
<p>同时，也建议将同一个服务的所有的资源放在一个文件中，这也就是为什么现在很多的服务安装时提示的url都是在一个文件中放置所有的资源.</p>
<p>本人也查找了<code>kubernetes/kubectl</code> 的github，也未发现具体的原因，有如下讨论的issue:</p>
<blockquote>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/issues/24649">https://github.com/kubernetes/kubernetes/issues/24649</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/64203">https://github.com/kubernetes/kubernetes/issues/64203</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/44511">https://github.com/kubernetes/kubernetes/issues/44511</a></li>
</ul>
</blockquote>
<p>回到开发同学的问题，如果是按照上面的逻辑确实可能存在那个问题, 即</p>
<p>如果apply的资源在集群中已经存在，apply 又是使用的是文件名排序进行patch的，如果deploy中引用的secret，则就有可能在创建deploy时使用了集群中已存在的secret，因为此时新的secret还没有apply</p>
<p>解决的办法也很简单:</p>
<ol>
<li>要么，按照依赖的顺序将所有资源放到同一个文件中</li>
<li>要么，修改资源文件的名，依赖项靠前即可</li>
<li>使用<code>kubectl replace --force -f </code>在创建前先删除</li>
</ol>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><p>从kubectl的源码来看，确实没有看到处理资源依赖相关的代码</p>
<p>这里使用的是v1.15.5版本的kubectl，新版本不太清楚是否已支持依赖资源的创建</p>
<h3 id="kustomize"><a href="#kustomize" class="headerlink" title="kustomize"></a>kustomize</h3><p>但是，当前环境使用的是<code>argocd + kustomize</code>做的CI&#x2F;CD pipeline，没有直接使用kubectl,发布的时候使用的是kustomize，<code>难道kustomize也不能保证顺序</code>?</p>
<p>kustomize的版本是v1.15.9</p>
<p>目前使用的kustomizition.yaml文件是统一的，格式如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">service.yml.j2</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">configmap.yml.j2</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">deployment.yml.j2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">prod</span></span><br></pre></td></tr></table></figure>

<p>为了验证kustomize生成yaml文件的顺序，可以使用<code>kustomize build .</code>来查看<code>resources</code>下的资源的顺序是怎样的，build有额外的参数可以控制输出结果，这里没有使用，因为生成的结果太长， 就不贴上来了，直接说结论:</p>
<p>通过测试可以知道, <code>resources下的资源文件不管怎么放，都是一样的顺序且不存在资源依赖问题</code></p>
<p>也就是说，如果deployment依赖于configmap，则一定是configmap先apply.</p>
<p>那为何kubectl没有解决而kustomize解决了呢? 也是从官方文档看到这么一句话</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Resource Creation Ordering</span><br><span class="line">Certain Resource Types may be dependent on other Resource Types being created first. e.g. Namespaced</span><br><span class="line">Resources on the Namespaces, RoleBindings on Roles, CustomResources on the CRDs, etc.</span><br><span class="line"></span><br><span class="line">When used with a kustomization.yaml, Apply sorts the Resources by Resource <span class="built_in">type</span> to ensure Resources</span><br><span class="line">with these dependencies are created <span class="keyword">in</span> the correct order.</span><br></pre></td></tr></table></figure>

<p>翻译: 简单来说，就是kustomize会解决资源之间的依赖关系.</p>
<p>只能从kustomize build的源码来看实现了</p>
<h4 id="源码分析-1"><a href="#源码分析-1" class="headerlink" title="源码分析"></a>源码分析</h4><p>这里不展开所有的代码了，从build子命令一步一步按调用顺序，</p>
<p>最终在<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/resid/gvk.go">gvk.go</a>中发现有如下的order</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">var orderFirst = []string&#123;</span><br><span class="line">	<span class="string">&quot;Namespace&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ResourceQuota&quot;</span>,</span><br><span class="line">	<span class="string">&quot;StorageClass&quot;</span>,</span><br><span class="line">	<span class="string">&quot;CustomResourceDefinition&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ServiceAccount&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PodSecurityPolicy&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Role&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRole&quot;</span>,</span><br><span class="line">	<span class="string">&quot;RoleBinding&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRoleBinding&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ConfigMap&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Secret&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Endpoints&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Service&quot;</span>,</span><br><span class="line">	<span class="string">&quot;LimitRange&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PriorityClass&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PersistentVolume&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PersistentVolumeClaim&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line">	<span class="string">&quot;StatefulSet&quot;</span>,</span><br><span class="line">	<span class="string">&quot;CronJob&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PodDisruptionBudget&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">var orderLast = []string&#123;</span><br><span class="line">	<span class="string">&quot;MutatingWebhookConfiguration&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ValidatingWebhookConfiguration&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">var typeOrders = func() map[string]int &#123;</span><br><span class="line">	m := map[string]int&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> i, n := range orderFirst &#123;</span><br><span class="line">		m[n] = -len(orderFirst) + i</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> i, n := range orderLast &#123;</span><br><span class="line">		m[n] = 1 + i</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">return</span> m</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure>

<p>这里引用了一个名词，<code>gvk</code>， 可参考这篇<a href="https://liqiang.io/post/kubernetes-all-about-crd-part02-api-server-and-gvkr-dc53e1f1">文章</a></p>
<p>简单来说<code>gvk</code>其实就是<code>group、kind、version</code>,是定位一种类型的方式,k8s中使用的资源都会使用到</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1  <span class="comment"># group version</span></span><br><span class="line">kind: DaemonSet  <span class="comment"># kind</span></span><br><span class="line">metadata:</span><br><span class="line">  name: node-exporter</span><br></pre></td></tr></table></figure>

<p>回到上面的代码底下那部分代码，是给资源对象计算资源id的，这个id就是在执行<code>kustomize build . </code>命令时用于控制输出的，相关代码在<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/builtins/LegacyOrderTransformer.go">这里</a></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *LegacyOrderTransformerPlugin)</span></span> Transform(m resmap.ResMap) (err <span class="type">error</span>) &#123;</span><br><span class="line">	resources := <span class="built_in">make</span>([]*resource.Resource, m.Size())</span><br><span class="line">	ids := m.AllIds()</span><br><span class="line">	sort.Sort(resmap.IdSlice(ids))</span><br><span class="line">	<span class="keyword">for</span> i, id := <span class="keyword">range</span> ids &#123;</span><br><span class="line">		resources[i], err = m.GetByCurrentId(id)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> errors.Wrap(err, <span class="string">&quot;expected match for sorting&quot;</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	m.Clear()</span><br><span class="line">	<span class="keyword">for</span> _, r := <span class="keyword">range</span> resources &#123;</span><br><span class="line">		m.Append(r)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>m.AllIds()</code>先解析<code>kustomization.yaml</code>中resource字段中的资源文件, 获取到所有的资源ids</p>
<p><code>sort.Sort(resmap.IdSlice(ids))</code>对所有ids进行排序</p>
<p>最后将排序后的资源文件内容写回到文件中，这个文件就是最终build的结果</p>
<p>分析到了这里，基本可以排除开发同学反馈的问题.</p>
<h3 id="helm"><a href="#helm" class="headerlink" title="helm"></a>helm</h3><p>另一个使用最多的资源生成工具, helm又是什么情况呢?</p>
<p>从helm的源码来看，很直接，直接维护了个资源的优先级，v2版本是集成在tiller中，参考<a href="https://github.com/helm/helm/blob/release-2.10/pkg/tiller/kind_sorter.go#L29">这里</a></p>
<p>v3版本因为废弃了tiller，代码移到了<a href="https://github.com/helm/helm/blob/master/pkg/releaseutil/kind_sorter.go">这里</a></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> InstallOrder KindSortOrder = []<span class="type">string</span>&#123;</span><br><span class="line">	<span class="string">&quot;Namespace&quot;</span>,</span><br><span class="line">	<span class="string">&quot;NetworkPolicy&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ResourceQuota&quot;</span>,</span><br><span class="line">	<span class="string">&quot;LimitRange&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PodSecurityPolicy&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PodDisruptionBudget&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ServiceAccount&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Secret&quot;</span>,</span><br><span class="line">	<span class="string">&quot;SecretList&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ConfigMap&quot;</span>,</span><br><span class="line">	<span class="string">&quot;StorageClass&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PersistentVolume&quot;</span>,</span><br><span class="line">	<span class="string">&quot;PersistentVolumeClaim&quot;</span>,</span><br><span class="line">	<span class="string">&quot;CustomResourceDefinition&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRole&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRoleList&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRoleBinding&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ClusterRoleBindingList&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Role&quot;</span>,</span><br><span class="line">	<span class="string">&quot;RoleList&quot;</span>,</span><br><span class="line">	<span class="string">&quot;RoleBinding&quot;</span>,</span><br><span class="line">	<span class="string">&quot;RoleBindingList&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Service&quot;</span>,</span><br><span class="line">	<span class="string">&quot;DaemonSet&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Pod&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ReplicationController&quot;</span>,</span><br><span class="line">	<span class="string">&quot;ReplicaSet&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line">	<span class="string">&quot;HorizontalPodAutoscaler&quot;</span>,</span><br><span class="line">	<span class="string">&quot;StatefulSet&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Job&quot;</span>,</span><br><span class="line">	<span class="string">&quot;CronJob&quot;</span>,</span><br><span class="line">	<span class="string">&quot;Ingress&quot;</span>,</span><br><span class="line">	<span class="string">&quot;APIService&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在使用Helm部署时，是以这个列表的顺序进行发布的，这们就不存在资源依赖的问题, 卸载时则使用相反的顺序.</p>
<p>有点意思</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/">https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/24649">https://github.com/kubernetes/kubernetes/issues/24649</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/64203">https://github.com/kubernetes/kubernetes/issues/64203</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/44511">https://github.com/kubernetes/kubernetes/issues/44511</a></li>
<li><a href="https://github.com/helm/helm/blob/release-2.10/pkg/tiller/kind_sorter.go#L29">https://github.com/helm/helm/blob/release-2.10/pkg/tiller/kind_sorter.go#L29</a></li>
<li><a href="https://github.com/helm/helm/blob/master/pkg/releaseutil/kind_sorter.go">https://github.com/helm/helm/blob/master/pkg/releaseutil/kind_sorter.go</a></li>
<li><a href="https://liqiang.io/post/kubernetes-all-about-crd-part02-api-server-and-gvkr-dc53e1f1">https://liqiang.io/post/kubernetes-all-about-crd-part02-api-server-and-gvkr-dc53e1f1</a></li>
<li><a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/builtins/LegacyOrderTransformer.go">https://github.com/kubernetes-sigs/kustomize/blob/master/api/builtins/LegacyOrderTransformer.go</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(Bootstrap Token机制)</title>
    <url>/2020/01/05/Kubernetes-Bootstrap-Node/</url>
    <content><![CDATA[<p>CKA考试时可能会遇到一个大题: 大致题意是: 在已经存在的一个Kubernetes Cluster中新增一个Node, 使其加入集群及相关证书可自动续签.</p>
<span id="more"></span>

<p>我们一般搭建k8s集群，都使用了一些开源工具来做，这类工具都自动帮我们做了很多事情，因此我们只需要简单执行一条命令后，一个集群就完成搭建了, 非常方便，同时也隐藏了一些k8s的知识点, 比如Bootstarp Token就是上面这道题的考察内容.</p>
<p>要了解Bootstrap Token前，我们要知道Kubernetes集群的<strong>认证(Kubernetes)及鉴权(Kubernetes)机制</strong>.</p>
<p>Kubernetes中的存在非常多的证书, 原于Kubernetes的有些组件同时充当服务端及客户端, 比如apiserver, 其它组件访问它时，它是服务端, 同时，它也会访问etcd或者kubelet，这时它是客户端. 这种访问是需要通过认证的,kubernetes的认证也支持多种方式, 证书为其中最常用的, Kubernetes的证书可以存在多套,每套的证书可以由不同的**根证书(CA)**签发, apiserver访问kubelet跟访问etcd可以通过使用由不同根证书签发的证书来访问, 但是一般情况下，一个Kuberntes集群中我们习惯使用一个CA来签署所有的证书, 这也是几乎所有的部署工具采用的方式.</p>
<p>**Bootstrap Token主要涉及认证阶段(TLS)**，至于鉴权机制(RBAC)，不是本文重点.</p>
<p><strong>集群启用RBAC后各组件之间的通信是基于TLS加密的，client和server需要通过证书中的CN，O来确定用户的user和group，因此client和server都需要持有有效证书, Bootstrap Token只用于kubelet tls 到apiserver的认证, 如果k8s集群本身都未开启tls,那自然就不需要了</strong></p>
<p><strong>Node在Kubelet的启动参数中指定事先在集群中配置好的Bootstrap Token，这个Bootstrap Token只具有引导Node加入集群的权限限定，除此之外，没有其它权限, 这样通过限定权限可以保障集群安全</strong></p>
<h3 id="Kubelet申请证书大致流程"><a href="#Kubelet申请证书大致流程" class="headerlink" title="Kubelet申请证书大致流程"></a>Kubelet申请证书大致流程</h3><blockquote>
<ul>
<li>集群产生一个低权账号用户组，通过TOKEN进行认证创建ClusterRole使其具有创建证书申请CSR的权限</li>
<li>给这个组添加ClusterRoleBinding，使得具有这个组的账号的kubelet具有上述权限</li>
<li>给添加ClusterRoleBinding，使得controller-manager自动同意上述两个证书的下发</li>
<li>调整 Controller Manager确保启动tokencleaner和bootstrapsigner（4中自动证书下发的功能）</li>
<li>基于上述TOKEN生成bootstrap.kubeconfig文件，并下发给node节点</li>
<li>node节点的kubelet拿着这个bootstrap.kubeconfig向master的apiserver发起CSR(certificate signing request)</li>
<li>master自动同意并下发第一个证书</li>
<li>node节点的kubelet自动拿着第一个证书与master的apiserver通信申请第二个证书</li>
<li>master自动同意并下发第二个证书</li>
<li>node节点加入集群</li>
</ul>
</blockquote>
<p><strong>上述过程要求，kubelet跟apiserver具有相同的CA证书</strong></p>
<p>假设现阶段已经存在一个k8s集群, 然后存在一台需要加入改集群的node.</p>
<p>先对Node做一些准备工作:</p>
<h3 id="Node系统初始化"><a href="#Node系统初始化" class="headerlink" title="Node系统初始化"></a><strong>Node系统初始化</strong></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line">modprobe ip_vs_rr</span><br><span class="line"></span><br><span class="line">modprobe br_netfilter</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/sysctl.conf &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables=1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-ip6tables=1</span></span><br><span class="line"><span class="string">  EOF</span></span><br><span class="line">sysctl -p /etc/sysctl.conf</span><br></pre></td></tr></table></figure>



<h3 id="Node安装docker-x2F-kubelet"><a href="#Node安装docker-x2F-kubelet" class="headerlink" title="Node安装docker&#x2F;kubelet"></a>Node安装docker&#x2F;kubelet</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install -y docker kubelet</span><br></pre></td></tr></table></figure>

<p>此时我们查看kubelet会发现它启动失败,因为目前还未配置bootstrap相关配置，它无法连接上apiserver</p>
<h3 id="调整kube-apiserver"><a href="#调整kube-apiserver" class="headerlink" title="调整kube-apiserver"></a>调整kube-apiserver</h3><p>要使用bootstrap token引导Node加入集群, kube-apiserver 需要开启参数	<code> --enable-bootstrap-token-auth=true</code></p>
<h3 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h3><h4 id="生成一个token"><a href="#生成一个token" class="headerlink" title="生成一个token"></a>生成一个token</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;$(head -c 6 /dev/urandom | md5sum | head -c 6)&quot;.&quot;$(head -c 16 /dev/urandom | md5sum | head -c 16)&quot;</span><br><span class="line">4b0928.8d7687fdb10e3707</span><br></pre></td></tr></table></figure>

<p>Kubernetes的官方有详细说明, 点击查看<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#bootstrap-tokens">Bootstrap Token</a></p>
<p>既然token只需要符合格式就行，CKA考试的时候可以直接使用页面上的Token来做，毕竟上面这一大串的命令不是谁都能记得住.</p>
<h4 id="创建-Bootstrap-Token-Secret"><a href="#创建-Bootstrap-Token-Secret" class="headerlink" title="创建 Bootstrap Token Secret"></a>创建 Bootstrap Token Secret</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bootstrap-token-4b0928</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Type MUST be &#x27;bootstrap.kubernetes.io/token&#x27;</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">bootstrap.kubernetes.io/token</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="comment"># Human readable description. Optional.</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">&quot;The default bootstrap token generated by &#x27;kubeadm init&#x27;.&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Token ID and secret. Required.</span></span><br><span class="line">  <span class="attr">token-id:</span> <span class="string">4b0928</span></span><br><span class="line">  <span class="attr">token-secret:</span> <span class="string">8d7687fdb10e3707</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Expiration. Optional. 要注意这个过期时间, 如果相对于现在的时间已经过期, 则会被自动删除</span></span><br><span class="line">  <span class="attr">expiration:</span> <span class="number">2021-03-10T03:22:11Z</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Allowed usages.</span></span><br><span class="line">  <span class="attr">usage-bootstrap-authentication:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">usage-bootstrap-signing:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot;</span></span><br><span class="line">  <span class="attr">auth-extra-groups:</span> <span class="string">system:bootstrappers:worker,system:bootstrappers:ingress</span></span><br></pre></td></tr></table></figure>

<p>secret的格式有要求,如下:</p>
<blockquote>
<ul>
<li>namespace必须是kube-system</li>
<li>name的后半部分token-id必须是token的前部分</li>
<li>Expiration必须是未来的某个时刻, 不然的话一新建就会被删除</li>
<li>token必须属于system:bootstrappers开头的组.</li>
</ul>
</blockquote>
<p>更多信息可看这里<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet-tls-bootstrapping</a></p>
<p>上述操作只是创建出了这个token, 下面需要给这个token绑定合适的权限, 用于引导Node.</p>
<h4 id="创建-ClusterRole-和-ClusterRoleBinding"><a href="#创建-ClusterRole-和-ClusterRoleBinding" class="headerlink" title="创建 ClusterRole 和 ClusterRoleBinding"></a>创建 ClusterRole 和 ClusterRoleBinding</h4><p>查看集群必须存在3个clusterrole及3个clusterrolebinding,如下:</p>
<p>3个clusterrole:</p>
<blockquote>
<ul>
<li>system:certificates.k8s.io:certificatesigningrequests:nodeclient 用于第一次证书认证</li>
<li>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 用于证书续签</li>
<li>system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</li>
</ul>
</blockquote>
<p><strong>nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的</strong></p>
<p>重点关注前两个，后面那个k8s新版本下会自动新建</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span></span><br><span class="line"><span class="comment"># node client credentials.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:nodeclient</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;certificates.k8s.io&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;certificatesigningrequests/nodeclient&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span></span><br><span class="line"><span class="comment"># own client credentials.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;certificates.k8s.io&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;certificatesigningrequests/selfnodeclient&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span></span><br><span class="line"><span class="comment"># serving cert matching its client cert.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;certificates.k8s.io&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>同时，需要有以下3个clusterrolebinding 到 clusterrole</p>
<blockquote>
<ul>
<li>system:node-bootstrapper</li>
<li>system:certificates.k8s.io:certificatesigningrequests:nodeclient</li>
<li>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</li>
</ul>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># enable bootstrapping nodes to create CSR</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">create-csrs-for-bootstrapping</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:bootstrappers</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:node-bootstrapper</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Approve all CSRs for the group &quot;system:bootstrappers&quot;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">auto-approve-csrs-for-group</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:bootstrappers</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:nodeclient</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Approve renewal CSRs for the group &quot;system:nodes&quot;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">auto-approve-renewals-for-nodes</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:nodes</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<p>自动批准组<code>system:bootstrappers</code>所有CSR 请求(nodeclient)</p>
<p>自动批准组<code>system:nodes</code>的证书重签CSR请求.</p>
<p>所以这就是为何token需要属于<code>system:bootstrappers</code>组</p>
<p>更多信息可看这里<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#bootstrap-tokens">bootstrap-tokens</a></p>
<h3 id="调整kube-controllermanager启动参数"><a href="#调整kube-controllermanager启动参数" class="headerlink" title="调整kube-controllermanager启动参数"></a>调整kube-controllermanager启动参数</h3><p>kube-controllermanager需要开启支持自动签署证书的参数<code> --controllers=*,bootstrapsigner,tokencleaner</code></p>
<p>这样，每当有Node通过Bootstrap token来请求CSR(certificate signing request)</p>
<p>对于 controller manager 来说，TLS bootstrapping 下 kubelet 发起的 CSR 请求大致分为以下三种</p>
<blockquote>
<ul>
<li>nodeclient: kubelet 以 <code>O=system:nodes</code> 和 <code>CN=system:node:(node name)</code> 形式发起的 CSR 请求</li>
<li>selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN)</li>
<li>selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求</li>
</ul>
</blockquote>
<p>对应上面的3个clusterrole</p>
<h3 id="生成kubeconfig文件"><a href="#生成kubeconfig文件" class="headerlink" title="生成kubeconfig文件"></a>生成kubeconfig文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置集群参数</span></span><br><span class="line">kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-cluster kubernetes --server=&#x27;https://10.170.0.2:6443&#x27; --certificate-authority=/etc/kubernetes/pki/ca.crt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置客户端认证参数</span></span><br><span class="line">kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-credentials system:bootstrap:4b0928 --token=4b0928.8d7687fdb10e3707</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置上下文参数</span></span><br><span class="line">kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-context kubernetes --user=system:bootstrap:4b0928 --cluster=kubernetes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置默认上下文</span></span><br><span class="line">kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig use-context kubernetes</span><br></pre></td></tr></table></figure>

<p>通过上面的命令最终会在&#x2F;var&#x2F;lib&#x2F;kubelet目录下生bootstrap-kubeconfig文件, 将该文件放到node节点上, <strong>kubelet指定改文件路径,启动时从改文件中获取apiserver及Bootstrap Token</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">clusters:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">cluster:</span></span><br><span class="line">    <span class="attr">certificate-authority:</span> <span class="string">/etc/kubernetes/pki/ca.crt</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">https://10.170.0.2:6443</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">contexts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">context:</span></span><br><span class="line">    <span class="attr">cluster:</span> <span class="string">kubernetes</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">system:bootstrap:4b0928</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">current-context:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Config</span></span><br><span class="line"><span class="attr">preferences:</span> &#123;&#125;</span><br><span class="line"><span class="attr">users:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">system:bootstrap:4b0928</span></span><br><span class="line">  <span class="attr">user:</span></span><br><span class="line">    <span class="attr">token:</span> <span class="string">4b0928.8d7687fdb10e3707</span> <span class="comment">#正是上面生成的token.</span></span><br></pre></td></tr></table></figure>



<h3 id="调整Node上kubelet启动参数"><a href="#调整Node上kubelet启动参数" class="headerlink" title="调整Node上kubelet启动参数"></a>调整Node上kubelet启动参数</h3><p>node上需要有ca.crt(或者ca.pem)</p>
<p><code>vim /lib/systemd/system/kubelet.service</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">[<span class="string">Unit</span>]</span><br><span class="line"><span class="string">Description=kubelet:</span> <span class="string">The</span> <span class="string">Kubernetes</span> <span class="string">Node</span> <span class="string">Agent</span></span><br><span class="line"><span class="string">Documentation=https://kubernetes.io/docs/home/</span></span><br><span class="line"></span><br><span class="line">[<span class="string">Service</span>]</span><br><span class="line"><span class="string">ExecStart=/usr/bin/kubelet</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--bootstrap-kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--kubeconfig=/var/lib/kubelet/kubeconfig</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--config=/var/lib/kubelet/config.yaml</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--network-plugin=cni</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--rotate-certificates=true</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--log-dir=/var/lib/kubelet/kubelet.log</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--register-node=true</span> <span class="string">\</span></span><br><span class="line">  <span class="string">--v=2</span></span><br><span class="line"><span class="string">Restart=always</span></span><br><span class="line"><span class="string">StartLimitInterval=0</span></span><br><span class="line"><span class="string">RestartSec=10</span></span><br><span class="line"></span><br><span class="line">[<span class="string">Install</span>]</span><br><span class="line"><span class="string">WantedBy=multi-user.target</span></span><br></pre></td></tr></table></figure>

<p><strong>其中, 在启动时，如果在–kubeconfig指定的路径下存在kubeconfig文件,则使用该文件做认证,如果不存在，则认证为第一次认证，则使用参数bootstrap-kubeconfig指定的文件, 正是包含bootstrap token的引导文件</strong></p>
<p>**通过kube-controller自动签署之后会把最终的kubeconfig写到–kubeconfig参数指定的路径，后续–bootstrap-kubeconfig则没有作用了. **</p>
<p>config.yaml是kubelet的其它参数配置文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat /var/lib/kubelet/config.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">address:</span> <span class="number">10.170</span><span class="number">.0</span><span class="number">.3</span></span><br><span class="line"><span class="attr">port:</span> <span class="number">10250</span></span><br><span class="line"><span class="attr">readOnlyPort:</span> <span class="number">10255</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">systemd</span></span><br><span class="line"><span class="attr">clusterDNS:</span> [<span class="string">&quot;10.96.0.10&quot;</span>]</span><br><span class="line"><span class="attr">clusterDomain:</span> <span class="string">cluster.local</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>



<h3 id="流程总结"><a href="#流程总结" class="headerlink" title="流程总结"></a>流程总结</h3><blockquote>
<ul>
<li>kubelet 读取 bootstrap.kubeconfig，使用其 CA 与 Token 向 apiserver 发起第一次 CSR 请求(nodeclient),使用组<code>system:bootstrappers</code></li>
<li>apiserver 根据 RBAC 规则自动批准首次 CSR 请求(approve-node-client-csr)，并下发证书(kubelet-client.crt)</li>
<li>kubelet **使用刚刚签发的证书(O&#x3D;system:nodes, CN&#x3D;system:node:NODE_NAME)**与 apiserver 通讯，并发起申请 10250 server 所使用证书的 CSR 请求</li>
<li>apiserver 根据 RBAC 规则自动批准 kubelet 为其 10250 端口申请的证书(kubelet-server-current.crt)</li>
<li>证书即将到期时，kubelet 自动向 apiserver 发起用于与 apiserver 通讯所用证书的 renew CSR 请求和 renew 本身 10250 端口所用证书的 CSR 请求</li>
<li>apiserver 根据 RBAC 规则自动批准两个证书</li>
<li>kubelet 拿到新证书后关闭所有连接，reload 新证书，以后便一直如此</li>
</ul>
</blockquote>
<p>建议大家都全手动搭建一次kubernetes,这样对Bootstrap Token机制会了解更深入.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note/">Kubernetes TLS bootstrapping 那点事</a></li>
<li><a href="https://mritd.me/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/">使用 Bootstrap Token 完成 TLS Bootstrapping</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet-tls-bootstrapping</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#bootstrap-tokens">Bootstrap Token</a></li>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">kubernetes-the-hard-way</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong学习(代理多grpc后端)</title>
    <url>/2021/05/15/Kong-grpc-proxy-multi/</url>
    <content><![CDATA[<p>kong在v1.3的版本原生支持了grpc协议, 这里说说使用kong来代理多个grpc请求.</p>
<span id="more"></span>

<p>需求是这样的，有个应用需要调用多个grpc服务, 内部测试环境都是直接通过kubernetes nodeport进行调用的，显然不是很优雅,</p>
<p>而kong做为我们的统一api网关，当然可以完美解决这个需求</p>
<p>整个链路如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210513145435.png"></p>
<p>从这个图可以看出，kong接入到grpc请求首先也是通过请求的path来转到配置的kong的service，这个service的host就可以是kubernetes对应中的service了</p>
<p>有两个方式实现,如下:</p>
<h3 id="添加service-x2F-route"><a href="#添加service-x2F-route" class="headerlink" title="添加service&#x2F;route"></a>添加service&#x2F;route</h3><p>这里要注意的是，因为需要新建service&#x2F;route等对象，因此需要kong-db模式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service-a</span></span><br><span class="line">curl -XPOST 127.0.0.1:8001/services --data name=service-a-grpc --data protocol=grpc --data host=service-A.default.svc.cluster.local --data port=8091</span><br><span class="line">curl -XPOST 127.0.0.1:8001/services/service-a-grpc/routes --data protocols=grpc --data name=service-A-grpc-root --data paths=/service-a-grpc.WorkflowEngineService</span><br><span class="line"><span class="comment"># 其中，127.0.0.1:8001为kong的管理端口，paths对应的具体的grpc后端</span></span><br><span class="line"><span class="comment"># 其它的grpc后端按上新建即可</span></span><br></pre></td></tr></table></figure>

<p>这样，在客户端就可以通过kong暴露出来的统一端口进行访问了，而如果像本人使用的db-less的模式，有没有办法呢? 当然是有的</p>
<h3 id="使用ingress"><a href="#使用ingress" class="headerlink" title="使用ingress"></a>使用ingress</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">grpc-service-ingress</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">kong-grpcproxy</span></span><br><span class="line">    <span class="attr">konghq.com/protocols:</span> <span class="string">grpc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/service-a-grpc.WorkflowEngineService</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">service-a</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8091</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/service-b-grpc.ResManServer</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">service-b-grpc</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">9011</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/service-c-grpc.SprCenterService</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">service-b-grpc</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">9112</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同时，需要kuberntes中的service如service-a的annotation添加<code>konghq.com/protocols: grpc</code>即可，如下</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">konghq.com/protocols:</span> <span class="string">grpc</span></span><br></pre></td></tr></table></figure>

<p>这样达到的效果跟上面是一样的，优点是kong使用的是db-less模式</p>
<p>kong在api-gateway方面还是非常方便的</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.jianshu.com/p/0a7e3a8fa8a0">https://www.jianshu.com/p/0a7e3a8fa8a0</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/140351496">https://zhuanlan.zhihu.com/p/140351496</a></li>
<li><a href="https://blog.csdn.net/qq_42728912/article/details/106396421">https://blog.csdn.net/qq_42728912/article/details/106396421</a></li>
<li><a href="https://konghq.com/blog/manage-grpc-services-kong/">https://konghq.com/blog/manage-grpc-services-kong/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Flannel Vxlan封包原理剖析</title>
    <url>/2022/03/25/Kubernetes-Flannel-Vxlan/</url>
    <content><![CDATA[<p>flannel在kubernetes中算是比较常用的cni, 其中vxlan又是flannel中常用的模式,最近又详细地看了一遍vxlan的实现方式, 每看一次都会发现新大陆，听的太多人说flannel的vxlan需要封包解包，但并不是所有的人都知道到底是把什么封到包里，<strong>包</strong>指的又是什么？ </p>
<span id="more"></span>

<p>本文主要来解析vxlan的封包解包过程，flannel的一些基础概念,可参考<a href="https://izsk.me/2020/01/05/Kubernetes-flannel-details/">Kubernetes学习(flannel深入学习)</a></p>
<p>先简单说一下flannel支持的三种模式:</p>
<blockquote>
<ul>
<li>Udp: 模式性能最差,现在几乎已不再使用，不再过多解释</li>
<li>host-gw: host-gw则是个纯三层的解决方案，直接把host当成是容器通信路径里的<strong>网关</strong>进行路由，为了达到这个目的，需要集群宿主机之间二层是互通的, 虽host-gw在三种模式中性能最好，但不是所有的集群都可以做到二层互通，还是有一定的局限</li>
<li>vxlan:</li>
</ul>
</blockquote>
<p>关于这三者之间的比较也不是本文的重点，感兴趣的同学可以参考<a href="https://www.cnblogs.com/zlw-xyz/p/14968730.html">Flannel的三种工作模式</a></p>
<p>接下来请出本次的主角: vxlan, 但在说这之前，还是需要先提一下vlan</p>
<h3 id="vlan"><a href="#vlan" class="headerlink" title="vlan"></a>vlan</h3><p>由于篇幅有限，不打算过多地去解释vlan技术,很大程度上这属于网络领域范畴，这里作者用一张图来说明vlan技术要解决的问题</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220322214144.png"></p>
<p>从这张图可以看到，vlan用于将一个物理的LAN在逻辑上划分成多个虚拟广播域的通信技术, 通过划分不同的vlan起到网络隔离，vlan内的主机间可以直接通信，而vlan间不能直接互通，从而将广播报文限制在一个vlan内</p>
<p>但是vlan有个很大的局限就是它能够划分的虚拟局域网个数非常有限，数量只有4000个左右，无法满足大二层网络的租户间隔离需求，因此诞生了vxlan</p>
<h3 id="vxlan"><a href="#vxlan" class="headerlink" title="vxlan"></a>vxlan</h3><p>Vxlan: (Virtual Extensible LAN 虚拟可扩展局域网），是在vlan的基础之上进行的扩展, 可以划分的vlan个数扩大到16M个</p>
<p><strong>vxlan通过构建虚拟隧道达到大二层网络互通的目的</strong></p>
<p>为了更好地理解flannel中使用vxlan实现跨主机通信的原理，跟vxlan相关的几个关键名词还是需要好好地介绍一下</p>
<h4 id="VTEP"><a href="#VTEP" class="headerlink" title="VTEP"></a>VTEP</h4><p>VTEP（VXLAN Tunnel Endpoints，VXLAN隧道端点），它可以是个物理设备，也可以是虚拟设备，flannel创建的flannel.1就是vtep设备,flannel中vxlan所说的封包解包就是由这个设备完成</p>
<p>vtep设置即有ip地址，也有mac地址.</p>
<h4 id="VNI"><a href="#VNI" class="headerlink" title="VNI"></a>VNI</h4><p>VNI（VXLAN Network Identifier，VXLAN 网络标识符），VNI是一种类似于VLAN ID的用户标识，一个VNI代表了一个租户</p>
<p>在flannel中，vni默认都是1, 所以这就是为什么flannel创建的vtep设备的名称叫做flannel.1的原因</p>
<p>vxlan还有很多其它的技术细节，作者着实觉得不是网络专业出身的真的很难看懂，如果感兴趣的可以看看华为论坛的一篇文章，<a href="https://info.support.huawei.com/info-finder/encyclopedia/zh/VXLAN.htm">什么是VXLAN</a>，看不懂也不影响接下来的理解，让我们回到flannel的vxlan模式</p>
<h3 id="节点通信"><a href="#节点通信" class="headerlink" title="节点通信"></a>节点通信</h3><p>重点说明的是，<strong>以下只讨论pod间通信，不涉及service的ClusterIP，如果是ClusterIP的话，那还涉及到怎么通过ClusterIP找到目的pod的IP,这属于coreDNS、Kube-proxy的范畴，不在此处讨论</strong></p>
<h4 id="同pod"><a href="#同pod" class="headerlink" title="同pod"></a>同pod</h4><p>同pod通信最为简单, 由于pod内部的多个container通过<strong>pause容器</strong>共享一个网络，自然通过localhost通信即可</p>
<h4 id="同节点"><a href="#同节点" class="headerlink" title="同节点"></a>同节点</h4><p>对于在一个节点上的两个pod进行通信, 是不需要经过CNI的, 这其实跟是不是flannel没有关系,由于部署完flannel后，flanneld进程会为每个节点都分配一段IP,IP信息存储在etcd中并保证节点间不会重复</p>
<p>由于同节点上的pod分配到的ip都在一个网段, 且cni0充当了node上所有pod的网桥，所有pod的veth一头都插在cni0上，那自然是可以直接通信的，因此，当请求从源pod的eth0网口出来后，到达cni0， cni0发现其目的地址与自己同属一个网络，就直接转到目的pod中去了.</p>
<h4 id="跨节点"><a href="#跨节点" class="headerlink" title="跨节点"></a>跨节点</h4><p>来看<a href="https://www.kancloud.cn/pshizhsysu/network/2202538">一张图</a>，如果你能看明白，那基本不需要再往下看了，请直接滑走</p>
<p>假如node1上的pod1 <strong>10.224.1.2</strong> ，要访问node2上的pod3 <strong>10.224.2.2</strong>,那么这条访问是怎样的呢?</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413173855.png"></p>
<p>首先数据包从pod1内的eth0出来到达cni0网桥，cni0网桥接收到数据包后发现目的IP跟自己不在一个网段,那么自然需要转发出去，而Linux Bridge有个特殊规则： <strong>网桥不会将这个数据包转发给任何设备，而是直接转交给主机的三层协议栈进行处理</strong>, 因此通过本机的route得知，目的地址为10.224.1.0段的数据包都将转到flannel.1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[node1]<span class="comment"># route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">10.224.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br><span class="line">10.224.1.0      10.224.1.0      255.255.255.0   UG    0      0        0 flannel.1</span><br></pre></td></tr></table></figure>

<p>我们抛开所有因素不谈，先说一个前提，<strong>两台节点通信，不管虚拟网络上怎么实现，最终还是需要通过物理网卡进行</strong></p>
<p>flannel为<code>overlay</code>的容器网络，是个<strong>大二层</strong>互通的解决方案， 但最终网络包还是要借助物理网卡出去</p>
<p><strong>任何一个VXLAN设备创建时都会指定一个三层物理网络设备作为VTEP</strong>,flannel.1也不例外, 可通过以下命令进行查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ip -d <span class="built_in">link</span> show flannel.1</span><br><span class="line"><span class="comment"># 在输出中可看到flannel.1是基于eth0上</span></span><br></pre></td></tr></table></figure>

<p>当数据包到达flannel.1后, 它如果要将这个包转到目的节点上去，它需要对<code>vxlan报文进行填充</code></p>
<p><strong>在源节点上flannel填充这些信息的过程就叫封包，在目标节点上解开这些信息就叫解包</strong></p>
<p>先来看一下vxlan的报文形式</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220324225909.png"></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413193513.png"></p>
<p>右边的为原始报文，左边的即为vxlan封装报文,我们一一来介绍一下.</p>
<h5 id="Original-Ethernet-Frame"><a href="#Original-Ethernet-Frame" class="headerlink" title="Original Ethernet Frame"></a>Original Ethernet Frame</h5><p>首先， Original Ethernet Frame是原始的报文，也就是pod1访问pod2的报文，因为是个正常网络报文，包含IP header、Ethernet header、及playload。</p>
<p>playload不多说，就是数据</p>
<p>IP header 很自然也就是pod1及pod2的ip地址信息</p>
<p>而Ethernet header需要注意的是，不是pod1及pod2的MAC地址，而应该是<strong>两端flannel.1的MAC地址</strong>,因为报文到flannel.1后通过<code>route -n</code>得到下一跳的地址为<code>10.224.2.0/32</code>，需要得到10.224.2.0的mac地址，这部分信息在<strong>flanneld中进行维护，叫ARP表</strong>，即通过IP可得到对应的MAC地址，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[node1]<span class="comment"># ip neigh show dev flannel.1</span></span><br><span class="line">10.224.2.0 lladdr 92:8d:c4:85:16:ad PERMANENT</span><br></pre></td></tr></table></figure>

<h5 id="Vxlan-Header"><a href="#Vxlan-Header" class="headerlink" title="Vxlan Header"></a>Vxlan Header</h5><p>Vxlan header这里只需要关注一个字段，那就是VNI,前文简单提到过, 在目标node上的flannel.1上会对这个VNI字段进行check，看是否与自己的VNI一致，一致的话才会进行处理.</p>
<h5 id="UDP-Header"><a href="#UDP-Header" class="headerlink" title="UDP Header"></a>UDP Header</h5><p>从上图中的颜色可以看出, UDP是把整个Original Ethernet Frame及Vxlan Header都囊括在一起了，我们知道，udp header中包含有源端口，目的端口，如图所示</p>
<p>所以很自然Src.port为node1上的flannel.1的端口，该端口是根据封装的内部数据帧计算出的一个哈希值</p>
<p>Dst.port(上面也显示为VxlanPort)为node2上flannel.1的端口，Linux内核中默认为VXLAN分配的UDP监听端口为8472</p>
<h5 id="Outer-IP-header"><a href="#Outer-IP-header" class="headerlink" title="Outer IP header"></a>Outer IP header</h5><p>上面的信息其实还都是在flannel中, 上面说过任何虚拟出来的网络最终都是要经过实体网卡设备出去,目前封装出来的udp header是不能在宿主机网络中传递的，只能进一步把udp再封装成正常的网络包通过节点物理网络发送出去，根据tcp&#x2F;ip协议，有了UDP header自然是需要封装在ip报文中，所以有ip header</p>
<p>ip header中包含有源ip及目的ip，源ip即为flannel.1所绑定的物理ip,即node1节点的eth0 ip</p>
<p>而目标ip，那肯定是node2的eth0 ip了, 这个ip是需要根据目标flannel.1的mac地址获得，这部分信息同样维护在flanneld中的，可通过以下命令查询</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[node1 ~]<span class="comment"># bridge fdb show dev flannel.1 | grep 92:8d:c4:85:16:ad</span></span><br><span class="line">92:8d:c4:85:16:ad dev flannel.1 dst 192.168.2.91 self permanent</span><br></pre></td></tr></table></figure>

<p>192.168.2.19即为目标ip</p>
<p>可以总结一下，flanneld中维护了这两部分信息:</p>
<blockquote>
<ul>
<li>flannel.1的ip与mac地址对应关系，通过flannel.1的ip可以查询到flannel.1 的mac地址</li>
<li>flannel.1的mac地址及其所在node ip对应关系，通过flannel.1的mac地址可以查询到node ip</li>
</ul>
</blockquote>
<h5 id="Outer-MAC-Header"><a href="#Outer-MAC-Header" class="headerlink" title="Outer MAC Header"></a>Outer MAC Header</h5><p>而ip header则又需要由封装在mac header中，通过上面的查询，mac header 自然就是两台node的mac信息了</p>
<p>这样的话，整个封装包组成了标准的tcp&#x2F;ip协议包，可以在物理网络上传递了</p>
<h5 id="解包"><a href="#解包" class="headerlink" title="解包"></a>解包</h5><p>当数据包到达node2的8472端口后（实际上就是VXLAN模块），VXLAN模块就会比较这个VXLAN Header中的VNI和本机的VTEP（VXLAN Tunnel End Point，就是flannel.1）的VNI是否一致，然后比较Inner Ethernet Header中的目的MAC地址与本机的flannel.1是否一致，都一致后，则去掉数据包的VXLAN Header和Inner Ethernet Header，然后把数据包从flannel.1网卡进行发送。</p>
<p>然后，在node2上会有如下的路由（由flanneld维护），根据路由判断要把数据包发送到cni0网卡上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[node2 ~]# route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">...</span><br><span class="line">10.224.2.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br></pre></td></tr></table></figure>

<p>最后通过 inner ip header中的ip知道需要由10.224.2.2的pod进行响应.</p>
<p>vxlan在内核中进行封装、解封装的过程，致使flannel vxlan的性能有所下降.</p>
<p>&lt;完&gt;</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2020/01/05/Kubernetes-flannel-details/">https://izsk.me/2020/01/05/Kubernetes-flannel-details/</a></li>
<li><a href="https://www.kancloud.cn/pshizhsysu/network/2202538">https://www.kancloud.cn/pshizhsysu/network/2202538</a></li>
<li><a href="https://www.cnblogs.com/zlw-xyz/p/14968730.html">https://www.cnblogs.com/zlw-xyz/p/14968730.html</a></li>
<li><a href="https://support.huawei.com/enterprise/zh/doc/EDOC1100177391/2845c625">https://support.huawei.com/enterprise/zh/doc/EDOC1100177391/2845c625</a></li>
<li><a href="https://info.support.huawei.com/info-finder/encyclopedia/zh/VXLAN.html">https://info.support.huawei.com/info-finder/encyclopedia/zh/VXLAN.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP之长连接及短连接</title>
    <url>/2017/08/12/HTTP%E4%B9%8B%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%8F%8A%E7%9F%AD%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<p>我们都说HTTP协议是无状态的,我们又都说HTTP分长连接及短连接,’无状态’到底是个什么意思？</p>
<span id="more"></span>

<h3 id="无状态"><a href="#无状态" class="headerlink" title="无状态"></a><strong>无状态</strong></h3><p>HTTP无状态其实指的是协议对于事务处理没有记忆能力,服务器不知道客户端是什么状态.也就是说,打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系.HTTP是一个无状态的面向连接的协议,无状态不代表HTTP不能保持TCP连接,更不能代表HTTP使用的是UDP协议(无连接).</p>
<p>那http又是如何保持长连接的呢?</p>
<p>在<code>HTTP/1.0</code>中,默认使用的是短连接.也就是说,浏览器和服务器每进行一次HTTP操作,就建立一次连接,但任务结束就中断连接.如果客户端浏览器访问的某个HTML或其他类型的 Web页中包含有其他的Web资源,如JavaScript文件、图像文件、CSS文件等；当浏览器每遇到这样一个Web资源,就会建立一个HTTP会话.<br>但从<code> HTTP/1.1</code>起,默认使用长连接,用以保持连接特性.使用长连接的HTTP协议,会在响应头有加入这行代码:</p>
<p><code>Connection:keep-alive</code></p>
<p>　　在使用长连接的情况下,当一个网页打开完成后,客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭,如果客户端再次访问这个服务器上的网页,会继续使用这一条已经建立的连接.<code>Keep-Alive</code>不会永久保持连接,它有一个保持时间,可以在不同的服务器软件（如Apache）中设定这个时间.实现长连接要客户端和服务端都支持长连接.</p>
<p>当然,在很多web服务器中可以设置这个keep-Alive的保持时间,比如nginx的配置文件nginx.conf中可直接使用:</p>
<p><code>keepalive_timeout  60;</code></p>
<p><strong>HTTP协议的长连接和短连接,实质上是TCP协议的长连接和短连接.</strong></p>
<p>那长短连接各自对应的场景有哪些呢(各自的优缺点)?</p>
<h3 id="短连接"><a href="#短连接" class="headerlink" title="短连接"></a><strong>短连接</strong></h3><p>场景: client向server发起连接请求,server接到请求,然后双方建立连接.client向server 发送消息,server回应client,然后一次读写就完成了,这时候双方任何一个都可以发起close操作,不过一般都是client先发起 close操作.为什么呢,一般的server不会回复完client后立即关闭连接的,当然不排除有特殊的情况.从上面的描述看,短连接一般只会在 client&#x2F;server间传递一次读写操作</p>
<p><strong>短连接管理起来比较简单,存在的连接都是有用的连接,不需要额外的控制手段,但是频繁的创建大量的连接也很耗资源</strong></p>
<h3 id="长连接"><a href="#长连接" class="headerlink" title="长连接"></a><strong>长连接</strong></h3><p>流程是:client向server发起连接,server接受client连接,双方建立连接.Client与server完成一次读写之后,它们之间的连接并不会主动关闭,后续的读写操作会继续使用这个连接.</p>
<p>首先说一下TCP&#x2F;IP详解上讲到的TCP保活功能,保活功能主要为服务器应用提供,服务器应用希望知道客户主机是否崩溃,从而可以代表客户使用资源.如果客户已经消失,使得服务器上保留一个半开放的连接,而服务器又在等待来自客户端的数据,则服务器将应远等待客户端的数据,保活功能就是试图在服务 器端检测到这种半开放的连接.</p>
<p>如果一个给定的连接在两小时内没有任何的动作,服务器就向客户发一个探测报文段,客户主机必须处于以下4个状态之一:</p>
<blockquote>
<ul>
<li>客户主机依然正常运行,并从服务器可达.客户的TCP响应正常,而服务器也知道对方是正常的,服务器在两小时后将保活定时器复位.</li>
<li>客户主机已经崩溃,并且关闭或者正在重新启动.在任何一种情况下,客户的TCP都没有响应.服务端将不能收到对探测的响应,并在75秒后超时.服务器总共发送10个这样的探测 ,每个间隔75秒.如果服务器没有收到一个响应,它就认为客户主机已经关闭并终止连接.</li>
<li>客户主机崩溃并已经重新启动.服务器将收到一个对其保活探测的响应,这个响应是一个复位,使得服务器终止这个连接.</li>
</ul>
</blockquote>
<p>客户机正常运行,但是服务器不可达,这种情况与2类似,TCP能发现的就是没有收到探查的响应.</p>
<p><strong>从上可以看出http要保持长连接需要做很多次的向客户端试探是否存活,这对性能是个很大的损耗,但是从另一方面来说长连接减少了很多的创关闭连接带来的开销,节约时间.</strong></p>
<h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a><strong>场景</strong></h3><p><code>长连接</code>多用于操作频繁,点对点的通讯,而且连接数不能太多情况,.每个TCP连接都需要三步握手,这需要时间,如果每个操作都是先连接,再操作的话那么处理速度会降低很多,所以每个操作完后都不断开,次处理时直接发送数据包就OK了,不用建立TCP连接.例如：数据库的连接用长连接, 如果用短连接频繁的通信会造成socket错误,而且频繁的socket 创建也是对资源的浪费. </p>
<p>而像WEB网站的http服务一般都用<code>短链接</code>,因为长连接对于服务端来说会耗费一定的资源,而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源,如果用长连接,而且同时有成千上万的用户,如果每个用户都占用一个连接的话,那可想而知吧.所以并发量大,但每个用户无需频繁操作情况下需用短连好.</p>
<p><strong>没有哪一个更好,只有哪一个最适合,一切跟着场景走,总有一款适合你!</strong></p>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践一)</title>
    <url>/2021/07/02/Kubernetes-Infinitband-SRIOV-network-1-backgroud/</url>
    <content><![CDATA[<p>在HPC场景下，底层网络的性能直接影响最终的结果，HPC往往会伴随海量的数据交换，特别是在跨Node之间，10Gb的以太网网络传输肯定要比100Gb的IB网络传输慢，本人所负责的训练平台也是如此，在一次训练过程中，GPU存在海量数据交换，虽然是在GPU之间，但本质还是通过网络进行传输，但是以太网的传输在HPC场景下往往显得不那么高效，因此可以通过RDMA或者如果预算够的话，通过硬件IB实现数据交换，能大大提高训练质量</p>
<span id="more"></span>



<p>本人所负责的训练平台是完全基于k8s的，现在要迁移至IB网络同时不影响现有业务进行，大大增加了实施难度，在经过一番调研之后，最终的方案为在原有的容器网络基础之上，再加一层IB网络层，即在k8s集群中同时存在2个网络，这在k8s是完全支持的，即在容器中除了eth0，由flannel cni分配， 还会有一个net1网络，由IB cni分配, eth0的网络用于集群之间的通信，而net1则用于训练时pod与pod节点进行通信，在实践之前先介绍几个比较重要的技术</p>
<h3 id="Infinitband"><a href="#Infinitband" class="headerlink" title="Infinitband"></a>Infinitband</h3><p>infinitband做专门为高带宽，低时延，高可靠，不占用cpu资源的网络互联技术，大量被用于超算、AI等HPC场景下</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704215748.png"></p>
<p>这里不对infintband展开细说，感兴趣的可以参考下列连接</p>
<p><a href="https://zhuanlan.zhihu.com/p/74238082">https://zhuanlan.zhihu.com/p/74238082</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/163104439">https://zhuanlan.zhihu.com/p/163104439</a></p>
<p><a href="https://blog.csdn.net/swingwang/article/details/72935461">https://blog.csdn.net/swingwang/article/details/72935461</a></p>
<h3 id="sr-iov"><a href="#sr-iov" class="headerlink" title="sr-iov"></a>sr-iov</h3><p>SR-IOV (Single Root Input&#x2F;Output Virtualization) 的主要作用是将一个物理设备模拟成多个虚拟设备,在虚拟化技术中使用非常频繁，</p>
<p><strong>sr-iov需要硬件上的支持，好在目前大部分服务器都支持了该技术</strong>，这里也不过多介绍其中的细节，后续用到的两个比较重要的概念:</p>
<p>SR-IOV引入了两种新的PCIe的Function：</p>
<ol>
<li>PFs：完整功能的PCIe Function包含了SR-IOV Extended Capability。这个Capability被用来配置和管理SR-IOV的功能。</li>
<li>VFs：一部分轻量级的PCIe function，只包含必要的用于数据移动的最小可配置的资源。</li>
</ol>
<p>一个 具备SR-IOV能力的设备可以被配置成被枚举出多个Virtual Functions（数量可控制），而且每个Funciton 都有自己的完整有BAR的配置空间。每个VF 分配给虚拟或者容器</p>
<p><a href="https://zhuanlan.zhihu.com/p/347204232">https://zhuanlan.zhihu.com/p/347204232</a></p>
<p><a href="https://blog.csdn.net/wangdd_199326/article/details/90476728">https://blog.csdn.net/wangdd_199326/article/details/90476728</a></p>
<h3 id="资源清单"><a href="#资源清单" class="headerlink" title="资源清单"></a>资源清单</h3><p>下列的资源清单为部分集群资源且机器ip不为真实ip</p>
<table>
<thead>
<tr>
<th>机器</th>
<th>角色</th>
<th>以太网&#x2F;IB网卡</th>
<th>内核&#x2F;发行版</th>
</tr>
</thead>
<tbody><tr>
<td>10.4.52.2</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.3</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.4</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.5</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.6</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.7</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
</tbody></table>
<p>master做为控制面，用于集群间的管理角色</p>
<p>node上做为数据面，以太网用于与控制面进行交互，同时IB网卡用于HPC业务</p>
<h3 id="组网图"><a href="#组网图" class="headerlink" title="组网图"></a>组网图</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704191648.png"></p>
<p>在集群物理机器上，实际存在3个网络:</p>
<ol>
<li>管理网(eth1), 用于机器的管控网，一般叫做ipmi或者带外、外带等,在服务器上有专门的网口标识</li>
<li>业务网(eth0), 用于flannel的启动参数指定的网卡名，这个网络在pod中用于与k8s控制面进行交互</li>
<li>IB网(eth100), 用于ib网卡,在这个网卡上会通过sriov虚拟出若干网卡用于pod中</li>
</ol>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704191622.png"></p>
<h3 id="实现目的"><a href="#实现目的" class="headerlink" title="实现目的"></a>实现目的</h3><p>最终要实现的目标为: 通过sriov在IB网卡上实现高性能容器网络以加速分布式训练的GPU数据交换</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/347204232">https://zhuanlan.zhihu.com/p/347204232</a></li>
<li><a href="https://blog.csdn.net/wangdd_199326/article/details/90476728">https://blog.csdn.net/wangdd_199326/article/details/90476728</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/74238082">https://zhuanlan.zhihu.com/p/74238082</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/163104439">https://zhuanlan.zhihu.com/p/163104439</a></li>
<li><a href="https://blog.csdn.net/swingwang/article/details/72935461">https://blog.csdn.net/swingwang/article/details/72935461</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(手动创建Kubernetes集群所需证书)</title>
    <url>/2019/11/16/Kubernetes-CA-Byhand/</url>
    <content><![CDATA[<p>多数情况下，在搭建kubernetes集群时都会选择使用工具，常用的比如官方的kubeadm， kubespray, minikube等，简单到一条命令就能完成一个kubernetes集群的搭建, 但同时，我们也忽略了k8s里面的一些原理性的东西, 刚好最近在准备CKA考试，准备手工尝试一下证书的使用. </p>
<span id="more"></span>

<p>大部分命令都是在学习这个 <strong>kubernetes-the-hard-way</strong>项目用到的，大家可以完整地走一遍，收益颇丰.</p>
<ol>
<li><p>安装cfssl证书签发工具</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl</span><br><span class="line">curl https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson</span><br><span class="line"><span class="built_in">chmod</span> +x /usr/local/bin/cfssl /usr/local/bin/cfssljson</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建根证书 ca, 后续所有使用到的证书都由根证书签发:</p>
<p><code>apt-get install cfssl</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">ca-config.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;signing&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;default&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;8760h&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;profiles&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;kubernetes&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;usages&quot;:</span> [<span class="string">&quot;signing&quot;</span>, <span class="string">&quot;key encipherment&quot;</span>, <span class="string">&quot;server auth&quot;</span>, <span class="string">&quot;client auth&quot;</span>],</span><br><span class="line">        <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;8760h&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">ca-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;Kubernetes&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;Kubernetes&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;CA&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#“CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</span></span><br><span class="line"><span class="comment">#“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">-initca</span> <span class="string">ca-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">ca</span></span><br><span class="line"><span class="comment">#生成以下文件</span></span><br><span class="line"><span class="string">ca.csr</span></span><br><span class="line"><span class="string">ca-key.pem</span>  </span><br><span class="line"><span class="string">ca.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kubernetes集群admin用户 的证书, 该证书使用上面的根证书签发</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">admin-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;admin&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;system:masters&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">-ca=ca.pem</span> <span class="string">-ca-key=ca-key.pem</span> <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line"><span class="string">-profile=kubernetes</span> <span class="string">admin-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">admin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">admin.csr</span></span><br><span class="line"><span class="string">admin-key.pem</span></span><br><span class="line"><span class="string">admin.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kubelet证书(<code>只有在手工引导node节点时才需要, 在使用bootstrap token引导时不需要这一步骤</code>)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">kubelet-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;system:node:$&#123;instance&#125;&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;system:nodes&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#instance为节点的主机名</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-hostname=$&#123;instance&#125;,$&#123;EXTERNAL_IP&#125;,$&#123;INTERNAL_IP&#125;</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">$&#123;instance&#125;-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">$&#123;instance&#125;</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#instance为节点的主机名</span></span><br><span class="line"><span class="comment">#EXTERNAL_IP为公网ip, 可以不设置</span></span><br><span class="line"><span class="comment">#INTERNAL_IP为私网ip</span></span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line">&#123;<span class="string">instance</span>&#125;<span class="string">.csr</span></span><br><span class="line">&#123;<span class="string">instance</span>&#125;<span class="string">-key.pem</span></span><br><span class="line"><span class="string">instance.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kube-controllermanager</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">kube-controller-manager-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;system:kube-controller-manager&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;system:kube-controller-manager&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">kube-controller-manager-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">kube-controller-manager</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">kube-controller-manager.csr</span></span><br><span class="line"><span class="string">kube-controller-manager-key.pem</span></span><br><span class="line"><span class="string">kube-controller-manager.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kube-proxy证书</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">kube-proxy-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;system:kube-proxy&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;system:node-proxier&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">kube-proxy-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">kube-proxy</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">kube-proxy.csr</span></span><br><span class="line"><span class="string">kube-proxy-key.pem</span></span><br><span class="line"><span class="string">kube-proxy.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kube-scheduler证书</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">kube-scheduler-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;system:kube-scheduler&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;system:kube-scheduler&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">kube-scheduler-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">kube-scheduler</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">kube-scheduler-key.csr</span></span><br><span class="line"><span class="string">kube-scheduler-key.pem</span></span><br><span class="line"><span class="string">kube-scheduler.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建kube-api证书</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">kubernetes-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;kubernetes&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">      <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">      <span class="string">&quot;10.170.0.2&quot;</span>,</span><br><span class="line">      <span class="string">&quot;10.170.0.3&quot;</span>,</span><br><span class="line">      <span class="string">&quot;10.170.0.4&quot;</span>,</span><br><span class="line">      <span class="string">&quot;10.96.0.1&quot;</span>,</span><br><span class="line">      <span class="string">&quot;senserealty.sensetime.com&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes.default&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes.default.svc&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes.default.svc.cluster&quot;</span>,</span><br><span class="line">      <span class="string">&quot;kubernetes.default.svc.cluster.local&quot;</span></span><br><span class="line">    ],</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;Kubernetes&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#host中需要指定3个master节点ip,3个etcd节点的ip(一般情况下master 跟etcd部署在一起) service-cluster-ip-range的第一个ip(在apiserver启动时指定), 最好再加一个业务域名</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">kubernetes-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">kubernetes</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">kubernetes.csr</span></span><br><span class="line"><span class="string">kubernetes-key.pem</span></span><br><span class="line"><span class="string">kubernetes.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>创建serviceaccount证书</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">service-account-csr.json</span> <span class="string">&lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;service-accounts&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;Portland&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;Kubernetes&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;Kubernetes The Hard Way&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;Oregon&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">cfssl</span> <span class="string">gencert</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca=ca.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-ca-key=ca-key.pem</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-config=ca-config.json</span> <span class="string">\</span></span><br><span class="line">  <span class="string">-profile=kubernetes</span> <span class="string">\</span></span><br><span class="line">  <span class="string">service-account-csr.json</span> <span class="string">|</span> <span class="string">cfssljson</span> <span class="string">-bare</span> <span class="string">service-account</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#生成</span></span><br><span class="line"><span class="string">service-account-key.csr</span></span><br><span class="line"><span class="string">service-account-key.pem</span></span><br><span class="line"><span class="string">service-account.pem</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>证书分发</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将 ca.pem 分发到worker instance</span></span><br><span class="line"><span class="comment">#将 ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem  分发到controller instance</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>到此，一个集群所需要的证书就全部创建好了.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github pages快速搭建轻博客系统</title>
    <url>/2015/01/01/Hexo+Github%20Pages%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E8%BD%BB%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h3 id="准备"><a href="#准备" class="headerlink" title="准备:"></a><strong>准备:</strong></h3><blockquote>
<p>Node.js环境<br>Git<br>Git-2.6.3-64-bit.exe</p>
</blockquote>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装:"></a><strong>安装:</strong></h3><span id="more"></span>

<p>Windows配置Node.js环境下载Node.js安装文件：<br>根据自己的Windows版本选择相应的安装文件，要是不知道，就安装32-bit的吧：</p>
<p>保持默认设置即可，一路Next，安装很快就结束了。 然后我们检查一下是不是要求的组件都安装好了，同时按下Win和R，打开运行窗口：</p>
<p>在新打开的窗口中输入cmd，敲击回车，打开命令行界面。在打开的命令行界面中，输入:</p>
<p><code>node -v</code><br><code>npm -v</code></p>
<p>如果结果如下图所示，则说明安装正确，可以进行下一步了，如果不正确，则需要回头检查自己的安装过程。<br>配置Git环境下载Git安装文件：</p>
<p>然后就进入了Git的安装界面<br>和Node.js一样，大部分设置都只需要保持默认，但是出于我们操作方便考虑，建议PATH选项按照下图选择：<br>一样的，我们来检查一下Git是不是安装正确了，打开命令行，输入：</p>
<p><code>git --version</code></p>
<p>如果结果如下图所示，则说明安装正确，可以进行下一步了，如果不正确，则需要回头检查自己的安装过程。</p>
<h3 id="配置Github"><a href="#配置Github" class="headerlink" title="配置Github:"></a><strong>配置Github:</strong></h3><p>注册账号如果已经拥有账号，请跳过此步~<br>打开<a href="https://github.com/%EF%BC%8C%E5%9C%A8%E4%B8%8B%E5%9B%BE%E7%9A%84%E6%A1%86%E4%B8%AD%EF%BC%8C%E5%88%86%E5%88%AB%E8%BE%93%E5%85%A5%E8%87%AA%E5%B7%B1%E7%9A%84%E7%94%A8%E6%88%B7%E5%90%8D%EF%BC%8C%E9%82%AE%E7%AE%B1%EF%BC%8C%E5%AF%86%E7%A0%81%E3%80%82">https://github.com/，在下图的框中，分别输入自己的用户名，邮箱，密码。</a><br>然后前往自己刚才填写的邮箱，点开Github发送给你的注册确认信，确认注册，结束注册流程。<br>一定要确认注册，否则无法使用gh-pages！<br>创建代码库登陆之后点击页面右上角的加号选择New repository：</p>
<p>进入代码库创建页面：<br>在Repository name下填写yourname.github.io，Description (optional)下填写一些简单的描述（不写也没有关系），如图所示：<br>正确创建之后，你将会看到如下界面：<br>开启gh-pages功能点击界面右侧的Settings，你将会打开这个库的setting页面，向下拖动，直到看见GitHub Pages，如图：<br>最开始的时候必须提交东西到这个仓库才能在source中选择master branch 然后save，yourname.github.io这个网址就可以正常访问了~ 如果yourname.github.io已经可以正常访问了，那么Github一侧的配置已经全部结束了。</p>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo:"></a><strong>安装Hexo:</strong></h3><p>在自己认为合适的地方创建一个文件夹，然后在文件夹空白处按住Shift+鼠标右键，然后点击在此处打开命令行窗口。<br>在命令行中输入：</p>
<p><code>npm install hexo-cli -g</code></p>
<p>然后你将会看到:<br>可能你会看到WARN，但是不用担心，这不会影响你的正常使用。 然后输入:</p>
<p><code>npm install hexo --save</code></p>
<p>然后你会看到命令行窗口刷了一大堆白字，下面我们来看一看Hexo是不是已经安装好了。 在命令行中输入：</p>
<p><code>hexo -v</code></p>
<p>如果你看到了如图文字，则说明已经安装成功了。<br>初始化Hexo接着上面的操作，输入：</p>
<p><code>hexo init</code></p>
<p>然后输入：</p>
<p><code>npm install</code></p>
<p>之后npm将会自动安装你需要的组件，只需要等待npm操作即可。</p>
<p>继续操作，同样是在命令行中，输入：</p>
<p><code>hexo g</code></p>
<p>然后输入：</p>
<p><code>hexo s</code></p>
<p>然后会提示：</p>
<p><code>INFO  Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop.</code></p>
<p>在浏览器中打开<a href="http://localhost:4000，你将会看到：">http://localhost:4000，你将会看到：</a><br>到目前为止，Hexo在本地的配置已经全都结束了。<br>使用Hexo在配置过程中请使用yamllint来保证自己的yaml语法正确</p>
<h3 id="配置Hexo"><a href="#配置Hexo" class="headerlink" title="配置Hexo:"></a><strong>配置Hexo:</strong></h3><p>修改全局配置文件此段落引用自<a href="https://hexo.io/docs/"><strong>Hexo官方文档</strong></a></p>
<p>您可以在 _config.yml 中修改大部份的配置。</p>
<p>配置项详解请稳步<a href="https://hexo.io/docs/"><strong>Hexo官方文档</strong></a></p>
<h3 id="配置Deployment"><a href="#配置Deployment" class="headerlink" title="配置Deployment:"></a><strong>配置Deployment:</strong></h3><p>假设你已经知道了这个配置文件的内容，你需要为自己配置身份信息，打开命令行，然后输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail&quot;</span><br></pre></td></tr></table></figure>

<p>同样在_config.yml文件中，找到Deployment，然后按照如下修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https://github.com/yourname/yourname.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>如果使用git方式进行部署，执行<br><code>npm install hexo-deployer-git --save</code><br>来安装所需的插件<br>然后在当前目录打开命令行，输入：</p>
<p><code>hexo d</code></p>
<p>随后按照提示，分别输入自己的Github账号用户名和密码，开始上传。 然后通过http: &#x2F;&#x2F;yourname.github.io&#x2F;来访问自己刚刚上传的网站。<br>添加新文章打开Hexo目录下的source文件夹，所有的文章都会以md形式保存在_post文件夹中，只要在_post文件夹中新建md类型的文档，就能在执行hexo g的时候被渲染。 </p>
<h3 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题:"></a><strong>更换主题:</strong></h3><p>可以在<a href="https://hexo.io/themes/">这里</a>下载自己喜欢的主题文件，保存到Hexo目录下的themes文件夹下。然后在_config.yml文件中修改：</p>
<p><code>theme: landscape //themes文件夹中对应文件夹的名称</code></p>
<p>然后先执行hexo clean，然后重新hexo g，并且hexo d，很快就能看到新主题的效果了~</p>
<h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名:"></a><strong>绑定域名:</strong></h3><p>由于本人没有使用域名,如有此方面需求的人,可以移步该博客:<a href="https://xuanwo.org/2015/03/26/hexo-intor/"><strong>更换域名</strong></a></p>
<h3 id="Hexo常见命令"><a href="#Hexo常见命令" class="headerlink" title="Hexo常见命令:"></a><strong>Hexo常见命令:</strong></h3><blockquote>
<p>hexo g &#x2F;&#x2F; 增量生成新的静态博客html文件<br>hexo d &#x2F;&#x2F; 部署静态html文件到public文件<br>hexo s &#x2F;&#x2F; 预览博客<br>hexo clean &#x2F;&#x2F; 清除db.json,再次执行hexo g全量编译<br>hexo n “文章名” &#x2F;&#x2F; 新生成文章<br>hexo n page “文章名” &#x2F;&#x2F; 新生成网页<br>hexo g –debug &#x2F;&#x2F; 编译调试在 根目录下会生成debug.log</p>
</blockquote>
<h3 id="为博客添加转载许可"><a href="#为博客添加转载许可" class="headerlink" title="为博客添加转载许可:"></a><strong>为博客添加转载许可:</strong></h3><p>转载自该<a href="http://colin1994.github.io/2016/06/02/hexo-copyright-and-donate/"><strong>博客</strong></a></p>
<p>具体实现步骤如下：</p>
<ol>
<li>在博客根目录下（和 source 同级），新建一个名为 scripts 的文件夹。</li>
<li>在 scripts 文件夹内, 新建一个 <code>AddTail.js</code> 脚本文件，脚本具体内容详见下文。</li>
<li>在博客根目录下，新建一个 <code>tail.md</code> 文件，里面写想要展示的版本说明内容。示例如下文所示。</li>
</ol>
<p><code>AddTail.js</code> 脚本文件：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Filename: AddTail.js</span></span><br><span class="line"><span class="comment">// Author: Colin</span></span><br><span class="line"><span class="comment">// Date: 2016/06/02</span></span><br><span class="line"><span class="comment">// Based on the script by KUANG Qi: http://kuangqi.me/tricks/append-a-copyright-info-after-every-post/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Add a tail to every post from tail.md</span></span><br><span class="line"><span class="comment">// Great for adding copyright info</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>);</span><br><span class="line"></span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">filter</span>.<span class="title function_">register</span>(<span class="string">&#x27;before_post_render&#x27;</span>, <span class="keyword">function</span>(<span class="params">data</span>)&#123;</span><br><span class="line">	<span class="keyword">if</span>(data.<span class="property">copyright</span> == <span class="literal">false</span>) <span class="keyword">return</span> data;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// Add seperate line</span></span><br><span class="line">	data.<span class="property">content</span> += <span class="string">&#x27;\n___\n&#x27;</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// Try to read tail.md</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">var</span> file_content = fs.<span class="title function_">readFileSync</span>(<span class="string">&#x27;tail.md&#x27;</span>);</span><br><span class="line">		<span class="keyword">if</span>(file_content &amp;&amp; data.<span class="property">content</span>.<span class="property">length</span> &gt; <span class="number">50</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			data.<span class="property">content</span> += file_content;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">		<span class="keyword">if</span> (err.<span class="property">code</span> !== <span class="string">&#x27;ENOENT&#x27;</span>) <span class="keyword">throw</span> err;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// No process for ENOENT error</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// 添加具体文章链接, 不需要去掉即可</span></span><br><span class="line">	<span class="keyword">var</span> permalink = <span class="string">&#x27;\n本文链接：&#x27;</span> + data.<span class="property">permalink</span>;</span><br><span class="line">	data.<span class="property">content</span> += permalink;</span><br><span class="line">  </span><br><span class="line">	<span class="keyword">return</span> data;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p><code>tail.md</code>内容如下:</p>
<ul>
<li>本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自xxx，并保持转载后文章内容的完整。本人保留所有版权相关权利。</li>
</ul>
<p>如此，<code>hero clean</code> 后重新 <code>hexo generate</code> 即可</p>
<h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题:"></a><strong>遇到的问题:</strong></h3><p>1.配置文件_config.yml中url及root的设置如下:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/hexo-github6.JPG" alt="hexo-github6.JPG"></p>
<p>URL不能是<a href="https://github.com/zhoushuke/zhoushuke.github.io">https://github.com/zhoushuke/zhoushuke.github.io</a> 这能导致文章中引用图片时提示找不到本地资源</p>
<p>2.配置文件_config.yml中Deployment设置如下:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/hexo-github7.png" alt="hexo-github8.png"><br>hexo3的配置如上 而不是hexo2的配置:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/hexo-github7.png" alt="hexo-github7.png"></p>
<p>3.hexo3 文章中引用图片的两种方式:<br>a.在source中新建image文件夹 在文章中使用相对路径<br>b._config.yml 中有 post_asset_folder:true<br>每次使用hexo n “test”新建文章时都会在_post中生成与文章名一致的文件夹 可将静态次放入该文件夹中 然后使用如下方式引用图片:</p>
<p><code>![png](six-pages/1.png)</code></p>
<p>更推荐的方法是使用这种方式: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% asset_img 图片 &quot;图片描述&quot; %&#125;</span><br></pre></td></tr></table></figure>

<p>而外链的使用方法如下：</p>
<p><code>![图片](图片地址)</code></p>
<p>4.hexo g出现如下错误:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/hexo-github8.png" alt="hexo-github8.png"></p>
<p>这是由于md文件中出现了{ {(这里是为了显示方便中间隔了一个空格,以下同),这两个一起出现需要转义,github 上给出的方法是在需要显示{ {符号的地方加上特定标签来标记这部分不需要解析。例如文章中可能会出现{ {的片段，以如下的形式写就可以: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% raw %&#125;&#123;&#123; something &#125;&#125;&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure>

<p>5.md文章标签不是以,或空格分隔的，有如下两方式:<br>    a. tags:<br>     - a<br>     - b<br>    b.tags: [a,b]</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://hexo.io/docs/">https://hexo.io/docs/</a></li>
<li><a href="https://xuanwo.org/2015/03/26/hexo-intor/">https://xuanwo.org/2015/03/26/hexo-intor/</a></li>
<li><a href="http://theme-next.iissnan.com/">http://theme-next.iissnan.com/</a></li>
<li><a href="http://colin1994.github.io/2016/06/02/hexo-copyright-and-donate/">http://colin1994.github.io/2016/06/02/hexo-copyright-and-donate/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>捣鼓手册</category>
      </categories>
      <tags>
        <tag>捣鼓手册</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践一)</title>
    <url>/2021/07/02/Kubernetes-Infinitband-SRIOV-network-1-background/</url>
    <content><![CDATA[<p>在HPC场景下，底层网络的性能直接影响最终的结果，HPC往往会伴随海量的数据交换，特别是在跨Node之间，10Gb的以太网网络传输肯定要比100Gb的IB网络传输慢，本人所负责的训练平台也是如此，在一次训练过程中，GPU存在海量数据交换，虽然是在GPU之间，但本质还是通过网络进行传输，但是以太网的传输在HPC场景下往往显得不那么高效，因此可以通过RDMA或者如果预算够的话，通过硬件IB实现数据交换，能大大提高训练质量</p>
<span id="more"></span>



<p>本人所负责的训练平台是完全基于k8s的，现在要迁移至IB网络同时不影响现有业务进行，大大增加了实施难度，在经过一番调研之后，最终的方案为在原有的容器网络基础之上，再加一层IB网络层，即在k8s集群中同时存在2个网络，这在k8s是完全支持的，即在容器中除了eth0，由flannel cni分配， 还会有一个net1网络，由IB cni分配, eth0的网络用于集群之间的通信，而net1则用于训练时pod与pod节点进行通信，在实践之前先介绍几个比较重要的技术</p>
<h3 id="Infinitband"><a href="#Infinitband" class="headerlink" title="Infinitband"></a>Infinitband</h3><p>infinitband做专门为高带宽，低时延，高可靠，不占用cpu资源的网络互联技术，大量被用于超算、AI等HPC场景下</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704215748.png"></p>
<p>这里不对infintband展开细说，感兴趣的可以参考下列连接</p>
<p><a href="https://zhuanlan.zhihu.com/p/74238082">https://zhuanlan.zhihu.com/p/74238082</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/163104439">https://zhuanlan.zhihu.com/p/163104439</a></p>
<p><a href="https://blog.csdn.net/swingwang/article/details/72935461">https://blog.csdn.net/swingwang/article/details/72935461</a></p>
<h3 id="sr-iov"><a href="#sr-iov" class="headerlink" title="sr-iov"></a>sr-iov</h3><p>SR-IOV (Single Root Input&#x2F;Output Virtualization) 的主要作用是将一个物理设备模拟成多个虚拟设备,在虚拟化技术中使用非常频繁，</p>
<p><strong>sr-iov需要硬件上的支持，好在目前大部分服务器都支持了该技术</strong>，这里也不过多介绍其中的细节，后续用到的两个比较重要的概念:</p>
<p>SR-IOV引入了两种新的PCIe的Function：</p>
<ol>
<li>PFs：完整功能的PCIe Function包含了SR-IOV Extended Capability。这个Capability被用来配置和管理SR-IOV的功能。</li>
<li>VFs：一部分轻量级的PCIe function，只包含必要的用于数据移动的最小可配置的资源。</li>
</ol>
<p>一个 具备SR-IOV能力的设备可以被配置成被枚举出多个Virtual Functions（数量可控制），而且每个Funciton 都有自己的完整有BAR的配置空间。每个VF 分配给虚拟或者容器</p>
<p><a href="https://zhuanlan.zhihu.com/p/347204232">https://zhuanlan.zhihu.com/p/347204232</a></p>
<p><a href="https://blog.csdn.net/wangdd_199326/article/details/90476728">https://blog.csdn.net/wangdd_199326/article/details/90476728</a></p>
<h3 id="资源清单"><a href="#资源清单" class="headerlink" title="资源清单"></a>资源清单</h3><p>下列的资源清单为部分集群资源且机器ip不为真实ip</p>
<table>
<thead>
<tr>
<th>机器</th>
<th>角色</th>
<th>以太网&#x2F;IB网卡</th>
<th>内核&#x2F;发行版</th>
</tr>
</thead>
<tbody><tr>
<td>10.4.52.2</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.3</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.4</td>
<td>k8s master&#x2F;v1.15.5</td>
<td>Y&#x2F;N</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.5</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.6</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
<tr>
<td>10.4.52.7</td>
<td>k8s node&#x2F;v1.15.5</td>
<td>Y&#x2F;Y，一个ib网口</td>
<td>Linux-4.19 centos7.4</td>
</tr>
</tbody></table>
<p>master做为控制面，用于集群间的管理角色</p>
<p>node上做为数据面，以太网用于与控制面进行交互，同时IB网卡用于HPC业务</p>
<h3 id="组网图"><a href="#组网图" class="headerlink" title="组网图"></a>组网图</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704191648.png"></p>
<p>在集群物理机器上，实际存在3个网络:</p>
<ol>
<li>管理网(eth1), 用于机器的管控网，一般叫做ipmi或者带外、外带等,在服务器上有专门的网口标识</li>
<li>业务网(eth0), 用于flannel的启动参数指定的网卡名，这个网络在pod中用于与k8s控制面进行交互</li>
<li>IB网(eth100), 用于ib网卡,在这个网卡上会通过sriov虚拟出若干网卡用于pod中</li>
</ol>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704191622.png"></p>
<h3 id="实现目的"><a href="#实现目的" class="headerlink" title="实现目的"></a>实现目的</h3><p>最终要实现的目标为: 通过sriov在IB网卡上实现高性能容器网络以加速分布式训练的GPU数据交换</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/347204232">https://zhuanlan.zhihu.com/p/347204232</a></li>
<li><a href="https://blog.csdn.net/wangdd_199326/article/details/90476728">https://blog.csdn.net/wangdd_199326/article/details/90476728</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/74238082">https://zhuanlan.zhihu.com/p/74238082</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/163104439">https://zhuanlan.zhihu.com/p/163104439</a></li>
<li><a href="https://blog.csdn.net/swingwang/article/details/72935461">https://blog.csdn.net/swingwang/article/details/72935461</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kind实践)</title>
    <url>/2020/08/07/Kubernetes-Kind/</url>
    <content><![CDATA[<p>Kind(kubernetes in docker的缩写)， 用于快速地在本地部署kubernetes集群,顾名思义，就是将 <code>Kubernetes</code> 所需要的所有组件，全部部署在一个 <code>Docker</code> 容器中，可以很方便的搭建 <code>Kubernetes</code> 集群。</p>
<p><code>Kind</code> 已经广泛的应用于 <code>Kubernetes</code> 上游及相关项目的 <code>CI</code> 环境中</p>
<span id="more"></span>

<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ol>
<li>最小的安装依赖，仅需要安装 <code>Docker</code> 即可</li>
<li>使用方法简单，只需 <code>Kind Cli</code> 工具即可快速创建集群</li>
<li><strong>使用容器来模似 <code>Kubernetes</code> 节点</strong></li>
<li>内部使用 <code>Kubeadm</code> 的官方主流部署工具</li>
</ol>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><code>Kind</code> 使用容器来模拟每一个 <code>Kubernetes</code> 节点，并在容器里面运行 <code>Systemd</code>。 容器里的 <code>Systemd</code> 托管了 <code>Kubelet</code> 和 <code>Containerd</code>，然后容器内部的 <code>Kubelet</code> 把其它 <code>Kubernetes</code> 组件：<code>Kube-Apiserver</code>、<code>Etcd</code>、<code>CNI</code> 等等组件运行起来。</p>
<p>Kind<code>内部使用了</code>Kubeadm<code>这个工具来做集群的部署，包括高可用集群也是借助</code>Kubeadm<code>提供的特性来完成的。在高用集群下还会额外部署了一个</code>Nginx来提供VIP</p>
<p>这是官方一张图, 可以很形象的说明上面的观点.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200808214319.png"></p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="Kind安装"><a href="#Kind安装" class="headerlink" title="Kind安装"></a>Kind安装</h4><p>kind是以golang开发的，因此直接下载对应平台的二进制包即可，我是mac，因此直接brew安装即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ brew install kind</span><br></pre></td></tr></table></figure>

<p>可以来看一下kind支持的参数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kind -h</span><br><span class="line">kind creates and manages <span class="built_in">local</span> Kubernetes clusters using Docker container <span class="string">&#x27;nodes&#x27;</span></span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kind [<span class="built_in">command</span>]</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  build       Build one of [node-image]</span><br><span class="line">  completion  Output shell completion code <span class="keyword">for</span> the specified shell (bash, zsh or fish)</span><br><span class="line">  create      Creates one of [cluster]</span><br><span class="line">  delete      Deletes one of [cluster]</span><br><span class="line">  <span class="built_in">export</span>      Exports one of [kubeconfig, logs]</span><br><span class="line">  get         Gets one of [clusters, nodes, kubeconfig]</span><br><span class="line">  <span class="built_in">help</span>        Help about any <span class="built_in">command</span></span><br><span class="line">  load        Loads images into nodes</span><br><span class="line">  version     Prints the kind CLI version</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">  -h, --<span class="built_in">help</span>              <span class="built_in">help</span> <span class="keyword">for</span> kind</span><br><span class="line">      --loglevel string   DEPRECATED: see -v instead</span><br><span class="line">  -q, --quiet             silence all stderr output</span><br><span class="line">  -v, --verbosity int32   info <span class="built_in">log</span> verbosity</span><br><span class="line">      --version           version <span class="keyword">for</span> kind</span><br><span class="line"></span><br><span class="line">Use <span class="string">&quot;kind [command] --help&quot;</span> <span class="keyword">for</span> more information about a <span class="built_in">command</span></span><br></pre></td></tr></table></figure>

<h4 id="单节点"><a href="#单节点" class="headerlink" title="单节点"></a>单节点</h4><p>搭建单节点集群是 <code>Kind</code> 最基础的功能，当然使用起来也很简单，仅需一条指令即可完成。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kind create cluster --name my-cluster --image kindest/node:v1.15.3</span><br><span class="line">Creating cluster <span class="string">&quot;my-cluster&quot;</span> ...</span><br><span class="line"> ✓ Ensuring node image (kindest/node:v1.15.3) ?</span><br><span class="line"> ✓ Preparing nodes ?</span><br><span class="line"> ✓ Creating kubeadm config ?</span><br><span class="line"> ✓ Starting control-plane ?️</span><br><span class="line">Cluster creation complete. You can now use the cluster with:</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=<span class="string">&quot;<span class="subst">$(kind get kubeconfig-path --name=<span class="string">&quot;my-cluster&quot;</span>)</span>&quot;</span></span><br><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure>

<p>以上命令中 <code>--name</code> 是可选参数。如果不指定，默认创建出来的集群名字为 <code>kind</code>。</p>
<p>使用默认安装的方式时，我们没有指定任何配置文件。从安装过程的输出来看，一共分为 4 步：</p>
<ul>
<li>检查本地环境是否存在一个基础的安装镜像，默认是 <code>kindest/node:v1.15.3</code>，该镜像里面包含了所有需要安装的东西，包括：<code>kubectl</code>、<code>kubeadm</code>、<code>kubelet</code> 的二进制文件，以及安装对应版本 <code>Kubernetes</code> 所需要的镜像。</li>
<li>准备 <code>Kubernetes</code> 节点，主要就是启动容器、解压镜像这类的操作。</li>
<li>建立对应的 <code>kubeadm</code> 的配置，完成之后就通过 <code>kubeadm</code> 进行安装。安装完成后还会做一些清理操作，比如：删掉主节点上的污点，否则对于没有容忍的 <code>Pod</code> 无法完成部署。</li>
<li>上面所有操作都完成后，就成功启动了一个 <code>Kubernetes</code> 集群并输出一些操作集群的提示信息。</li>
</ul>
<p>默认情况下，<code>Kind</code> 会先下载 <code>kindest/node:v1.15.3</code> 镜像。如果你想指定不同版本，可以使用 <code>--image</code> 参数，类似这样：<code>kind create cluster --image kindest/node:v1.15.3</code> </p>
<p>kindest&#x2F;node<code>这个镜像目前托管于 Docker Hub 上，下载时可能会较慢。同样的问题</code>Kind<code> 进行集群的创建也是存在的，</code>Kind<code>实际使用</code>Kubeadm<code>进行集群的创建。对</code>Kubeadm&#96; 有所了解的同学都知道它默认使用的镜像在国内是不能访问的，所以一样需要自行解决网络问题。</p>
<p>可以来验证一下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> KUBECONFIG=<span class="string">&quot;<span class="subst">$(kind get kubeconfig-path --name=<span class="string">&quot;my-cluster&quot;</span>)</span>&quot;</span></span><br><span class="line"></span><br><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                       STATUS    ROLES     AGE       VERSION</span><br><span class="line">my-cluster-control-plane   Ready     master    2m        v1.15.3</span><br><span class="line"></span><br><span class="line">$ kubectl get po -n kube-system</span><br><span class="line">NAME                                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-86c58d9df4-6g66f                              1/1     Running   0          21m</span><br><span class="line">coredns-86c58d9df4-pqcc4                              1/1     Running   0          21m</span><br><span class="line">etcd-my-cluster-control-plane                         1/1     Running   0          20m</span><br><span class="line">kube-apiserver-my-cluster-control-plane               1/1     Running   0          20m</span><br><span class="line">kube-controller-manager-my-cluster-control-plane      1/1     Running   0          20m</span><br><span class="line">kube-proxy-cjgnt                                      1/1     Running   0          21m</span><br><span class="line">kube-scheduler-my-cluster-control-plane               1/1     Running   0          21m</span><br><span class="line">weave-net-ls2v8                                       2/2     Running   0          21m</span><br></pre></td></tr></table></figure>

<p>从上面可以看出，对于使用kind部署一个单节点来说，单节点集群默认方式启动的节点类型是 <code>control-plane</code>，该节点包含了所有的组件。这些组件分别是：<code>2*Coredns</code>、<code>Etcd</code>、<code>Api-Server</code>、<code>Controller-Manager</code>、<code>Kube-Proxy</code>、<code>Sheduler</code> 和网络插件 <code>Weave</code></p>
<p>单节点的只有一个Node容器，如果想部署多个节点呢?</p>
<h4 id="多节点"><a href="#多节点" class="headerlink" title="多节点"></a>多节点</h4><p>多节点的集群可以使用配置文件来指定，<code>Kind</code> 在创建集群的时候，支持通过 <code>--config</code> 参数传递配置文件给 <code>Kind</code>，配置文件可修改的内容主要有 role 和 节点使用的镜像</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim my-cluster-multi-node.yaml</span><br><span class="line">kind: Cluster</span><br><span class="line">apiVersion: kind.sigs.k8s.io/v1alpha3</span><br><span class="line">nodes:</span><br><span class="line">- role: control-plane <span class="comment"># 一个控制节点</span></span><br><span class="line">- role: worker <span class="comment"># 一个工作节点</span></span><br><span class="line"></span><br><span class="line">$ kind create cluster --config my-cluster-multi-node.yaml --name my-cluster-multi-node --image kindest/node:v1.15.3</span><br></pre></td></tr></table></figure>

<p>可以发现这个配置文件的形式跟kubeadm的配置文件非常的类似, 更多的参数可以参考<a href="https://kind.sigs.k8s.io/docs/user/configuration/">这里</a></p>
<p>验证</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME                                  STATUS   ROLES    AGE     VERSION</span><br><span class="line">my-cluster-multi-node-control-plane   Ready    master   3m20s   v1.15.3</span><br><span class="line">my-cluster-multi-node-worker          Ready    &lt;none&gt;   3m8s    v1.15.3</span><br><span class="line"></span><br><span class="line">$ kubectl get po -n kube-system</span><br><span class="line">NAME                                                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-86c58d9df4-cnqhc                                      1/1     Running   0          5m29s</span><br><span class="line">coredns-86c58d9df4-hn9mv                                      1/1     Running   0          5m29s</span><br><span class="line">etcd-my-cluster-multi-node-control-plane                      1/1     Running   0          4m24s</span><br><span class="line">kube-apiserver-my-cluster-multi-node-control-plane            1/1     Running   0          4m17s</span><br><span class="line">kube-controller-manager-my-cluster-multi-node-control-plane   1/1     Running   0          4m21s</span><br><span class="line">kube-proxy-8t4xt                                              1/1     Running   0          5m27s</span><br><span class="line">kube-proxy-skd5v                                              1/1     Running   0          5m29s</span><br><span class="line">kube-scheduler-my-cluster-multi-node-control-plane            1/1     Running   0          4m18s</span><br><span class="line">weave-net-nmfq2                                               2/2     Running   0          5m27s</span><br><span class="line">weave-net-srdfw                                               2/2     Running   0          5m29s</span><br></pre></td></tr></table></figure>

<h4 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h4><p>高可用的集群跟多节点类似, 只不过<code>control-plane</code>的角色需要至少需要3个Node</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim my-cluster-ha.yaml</span><br><span class="line"></span><br><span class="line">kind: Cluster</span><br><span class="line">apiVersion: kind.sigs.k8s.io/v1alpha3</span><br><span class="line">kubeadmConfigPatches:</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">  kind: ClusterConfiguration</span><br><span class="line">  metadata:</span><br><span class="line">    name: config</span><br><span class="line">  networking:</span><br><span class="line">    serviceSubnet: 10.0.0.0/16</span><br><span class="line">  imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">  nodeRegistration:</span><br><span class="line">    kubeletExtraArgs:</span><br><span class="line">      pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">- |</span><br><span class="line">  apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">  kind: InitConfiguration</span><br><span class="line">  metadata:</span><br><span class="line">    name: config</span><br><span class="line">  networking:</span><br><span class="line">    serviceSubnet: 10.0.0.0/16</span><br><span class="line">  imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">nodes:</span><br><span class="line">- role: control-plane    <span class="comment"># 三个 control-plane 节点</span></span><br><span class="line">- role: control-plane</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: worker    <span class="comment"># 三个work节点</span></span><br><span class="line">- role: worker</span><br><span class="line">- role: worker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ kind create cluster --name my-cluster-ha --config my-cluster-ha.yaml --image kindest/node:v1.15.3</span><br></pre></td></tr></table></figure>

<p>验证</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                           STATUS   ROLES    AGE     VERSION</span><br><span class="line">my-cluster-ha-control-plane    Ready    master   3m42s   v1.15.3</span><br><span class="line">my-cluster-ha-control-plane2   Ready    master   3m24s   v1.15.3</span><br><span class="line">my-cluster-ha-control-plane3   Ready    master   2m13s   v1.15.3</span><br><span class="line">my-cluster-ha-worker           Ready    &lt;none&gt;   96s     v1.15.3</span><br><span class="line">my-cluster-ha-worker2          Ready    &lt;none&gt;   98s     v1.15.3</span><br><span class="line">my-cluster-ha-worker3          Ready    &lt;none&gt;   95s     v1.15.3</span><br></pre></td></tr></table></figure>

<h4 id="删除集群"><a href="#删除集群" class="headerlink" title="删除集群"></a>删除集群</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kind delete  cluster --name my-cluster</span><br></pre></td></tr></table></figure>

<p><strong>从集群的部署来看, 使用kind部署出来的集群除了把容器当Node使用之外, 其它的差别并不大</strong></p>
<h3 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h3><p>Kind中使用到了两种image， base image以及Node image</p>
<h4 id="baseImage"><a href="#baseImage" class="headerlink" title="baseImage"></a><a href="https://kind.sigs.k8s.io/docs/design/base-image/">baseImage</a></h4><p>baseimage主要是安装一些systemd,kubernetes需要的依赖，基于ubuntu.</p>
<h4 id="NodeImage"><a href="#NodeImage" class="headerlink" title="NodeImage"></a>NodeImage</h4><p><code>Node</code> 镜像的构建比较复杂，目前是通过运行 <code>Base</code> 镜像并在 <code>Base</code> 镜像内执行操作，再保存此容器内容为镜像的方式来构建的，包含的操作有：</p>
<ul>
<li>构建 <code>Kubernetes</code> 相关资源，比如：二进制文件和镜像。</li>
<li>运行一个用于构建的容器</li>
<li>把构建的 <code>Kubernetes</code> 相关资源复制到容器里</li>
<li>调整部分组件配置参数，以支持在容器内运行</li>
<li>预先拉去运行环境需要的镜像</li>
<li>通过 <code>docker commit</code> 方式保存当前的构建容器为 <code>Node</code> 镜像</li>
</ul>
<p><strong>最后再强调一下: kind部署出来的集群是不适用生产环境的,官方推荐只是用于需要一个k8s集群用于ci或者testing.</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kind/">https://github.com/kubernetes-sigs/kind/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/design/initial">https://kind.sigs.k8s.io/docs/design/initial</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/user/configuration/">https://kind.sigs.k8s.io/docs/user/configuration/</a></li>
<li><a href="https://kind.sigs.k8s.io/docs/design/base-image/">https://kind.sigs.k8s.io/docs/design/base-image/</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1512777">https://cloud.tencent.com/developer/article/1512777</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践三)</title>
    <url>/2021/07/05/Kubernetes-Infinitband-SRIOV-network-3-erros/</url>
    <content><![CDATA[<p>这次把sriov+infinitband部署期间遇到的问题做一个总结，给有需要的同学做个参考.</p>
<span id="more"></span>

<h3 id="容器内IB网络之间不通"><a href="#容器内IB网络之间不通" class="headerlink" title="容器内IB网络之间不通"></a>容器内IB网络之间不通</h3><p>现象: 在容器内使用<code>ibdev2netdev -v</code>查看<code>ib</code>网络显示状态是<code>Down</code>,同时容器之间<code>ping ib</code>对应的网卡不通</p>
<p>原因: 经过排查后发现，是由于<code>opensmd</code>服务不正常所致, 正常,多个<code>opensmd</code>服务一定会有一个实例是处于<code>master</code>状态，但查看后发现两个实例都处于<code>standby</code>状态,导致<code>IB</code>子网有问题</p>
<p>关于<code>opensmd</code>，有个很重要的配置是<code>root guid</code>文件(通过opensm.conf配置路径),可以通过<code>ibstat|grep &#39;System image GUID&#39;</code>命令将所有IB节点的GUID号记录到<code>root guid</code>文件中，一行一个即可，然后重新启动后恢复正常，一定要让一个实例处于<code>master</code>状态</p>
<h3 id="CNI-invalid-verion"><a href="#CNI-invalid-verion" class="headerlink" title="CNI: invalid verion"></a>CNI: invalid verion</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210616185012.png"></p>
<p>原因: 作者线上环境的k8s版本相对较旧，新版本的kubernetes需要添加cniVersion字段</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;cniVersion&quot;</span>: <span class="string">&quot;0.3.1&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="Priority-node-critical-not-permitted"><a href="#Priority-node-critical-not-permitted" class="headerlink" title="Priority-node-critical not permitted"></a>Priority-node-critical not permitted</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210622115355.png"></p>
<p>原因: 同上也是因为k8s集群为1.17版本之前，对于设置了priorityclasses: system-node-critical， v1.17版本之前只能部署在kube-system，其它ns是不允许的，在v1.17之后可选，如果必须部署在其它ns，则需要重新打sriov-network-operator及config-daemon镜像.</p>
<h3 id="no-match-for-kind-“MutatingwebhookConfigration”"><a href="#no-match-for-kind-“MutatingwebhookConfigration”" class="headerlink" title="no match for kind “MutatingwebhookConfigration”"></a>no match for kind “MutatingwebhookConfigration”</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210623144028.png"></p>
<p>原因: 查看operator的Dockerfile发现包含了bindata目录，而bindata目录下的webhook中使用的版本为v1,v1.16之前webhook已不再支持v1，需要切换到v1beta1, 如果不想重新打镜像的话，可以使用configmap挂载替换镜像里的对应文件.</p>
<h3 id="runtime-error-index-out-of-range"><a href="#runtime-error-index-out-of-range" class="headerlink" title="runtime error: index out of range"></a>runtime error: index out of range</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210624114102.png"></p>
<p>原因: operator维护了一个ib硬件类型列表，在deploy&#x2F;configmap.yaml文件中，如果使用的硬件不在该supported-nic-ids设备类型中, 可按readme中的格式进行手工添加即可，但官方有提示，不在列表中的硬件也许也work，但不保证.</p>
<h3 id="invalid-CIDR-address"><a href="#invalid-CIDR-address" class="headerlink" title="invalid CIDR address"></a>invalid CIDR address</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210629154253.png"></p>
<p>原因: 自定义网络中的subnet ip网段格式写错，多写了一位</p>
<h3 id="Got-invalid-base-LID-65535-from-network"><a href="#Got-invalid-base-LID-65535-from-network" class="headerlink" title="Got invalid base LID 65535 from network"></a>Got invalid base LID 65535 from network</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210705231941.png"></p>
<p>网卡一直处于初始化状态，同时查看opensm的错误，默认位于&#x2F;var&#x2F;log&#x2F;opensm.log,会发现以下错误</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210705134012.png"></p>
<p>原因: 网卡的Base LID不能是65535, 可通过重新绑定解决</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0000:83:00.6 &gt; /sys/bus/pci/drivers/mlx5_core/unbind</span><br><span class="line"><span class="built_in">echo</span> 11:22:33:44:77:66:77:60 &gt; /sys/class/infiniband/mlx5_0/device/sriov/5/node</span><br><span class="line"><span class="built_in">echo</span> 11:22:33:44:77:66:77:60 &gt; /sys/class/infiniband/mlx5_0/device/sriov/5/port</span><br><span class="line"><span class="built_in">echo</span> 0000:83:00.6 &gt; /sys/bus/pci/drivers/mlx5_core/bind</span><br><span class="line"><span class="built_in">echo</span> Follow &gt; /sys/class/infiniband/mlx5_0/device/sriov/5/policy</span><br><span class="line"><span class="comment"># 再次查看后Base LID已经重新分配,网卡正常</span></span><br></pre></td></tr></table></figure>

<h3 id="部署了sriov的Node会无缘无故被cordor从而导致无法调度"><a href="#部署了sriov的Node会无缘无故被cordor从而导致无法调度" class="headerlink" title="部署了sriov的Node会无缘无故被cordor从而导致无法调度"></a>部署了sriov的Node会无缘无故被cordor从而导致无法调度</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210705221717.png"></p>
<p>原因: 由于本人开始时指定的<code>/sys/class/net/ib0/device/sriov_numvfs</code>为32, 后来又改成了8, 期间没有重启，这样会导致<code>sriov-network-config-daemon</code>给node打上<code>sriovnetwork.openshift.io/state: Draining</code>的注释，从而cordor节点，这个从<code>sriov-network-config-daemon</code>的日志也可以看到，会提示需要重启节点(这里忘截图了)，节点重启后就ok了.</p>
<p>感兴趣的可以查看源码，非常清晰</p>
<p>下次总结一个整个方案使用到的细节</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践二)</title>
    <url>/2021/07/03/Kubernetes-Infinitband-SRIOV-network-2-deploy/</url>
    <content><![CDATA[<p>上回简单介绍了下技术背景，这次真正进入实践阶段,之前提到过，sriov是需要硬件支持, 一般开启sriov需要在IBOS中进行设置，这个根据服务器型号不同而操作不同, 具体可参考官网.</p>
<span id="more"></span>



<h3 id="k8s集群"><a href="#k8s集群" class="headerlink" title="k8s集群"></a>k8s集群</h3><p>现有的k8s集群使用的容器网络为flannel，k8s版本为v1.15.9， 这里要特别说明一下，1.15.9的版本相对来说已经较旧了，后面部署sriov-network-operator遇到好几个坑都是因为版本问题，后面会细说，其它没什么特别的.</p>
<h3 id="硬件开启SRIOV功能"><a href="#硬件开启SRIOV功能" class="headerlink" title="硬件开启SRIOV功能"></a>硬件开启SRIOV功能</h3><p>由于服务器的型号不一样，操作方法不一样，但是万变不离其综，在IBOS中多找找，一般都可以找到.</p>
<p>参考: <a href="https://docs.mellanox.com/pages/viewpage.action?pageId=19798214">https://docs.mellanox.com/pages/viewpage.action?pageId=19798214</a></p>
<h3 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget  <span class="string">&#x27;http://www.mellanox.com/page/mlnx_ofed_eula?mtag=linux_sw_drivers&amp;mrequest=downloads&amp;mtype=ofed&amp;mver=MLNX_OFED-4.5-1.0.1.0&amp;mname=MLNX_OFED_LINUX-4.5-1.0.1.0-rhel7.4-x86_64.tgz&#x27;</span></span><br><span class="line"></span><br><span class="line">yum install -y tcsh pciutils lsof python-devel gtk2 atk cairo tcl gcc-gfortran tk python-devel redhat-rpm-config rpm-build</span><br><span class="line"></span><br><span class="line">tar xf /home/xx/MLNX_OFED_LINUX-4.5-1.0.1.0-rhel7.4-x86_64.tgz -C /home/xx/ &amp;&amp; <span class="built_in">cd</span> /home/xx/MLNX_OFED_LINUX-4.5-1.0.1.0-rhel7.4-x86_64 &amp;&amp; ./mlnxofedinstall --add-kernel-support --skip-repo</span><br></pre></td></tr></table></figure>

<p>在安装完驱动之后<strong>机器需要重启</strong>，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重启完之后ib的驱动会自动启动，且一般默认开机自启</span></span><br><span class="line">systemctl status openibd</span><br></pre></td></tr></table></figure>

<p>这时需要为 ib 端口配置一个 IP 地址，以方便网络的识别和诊 断，配置方法和以太网端口的 IP 配置方法一样，具体步骤如下： </p>
<p>步骤 1:  在节点的<code>/etc/sysconfig/network-scripts/</code>创建 ifcfg-ib0 网络配置文件，IP 地址和子网掩码 按照 LLD 规划设计，内容如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/sysconfig/network-scripts/ifcfg-ib0 </span></span><br><span class="line">IPADDR=10.4.52.5</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">STARTMODE=auto</span><br><span class="line">CONNECTED_MODE=no</span><br><span class="line">TYPE=InfiniBand</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=<span class="built_in">yes</span></span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=<span class="built_in">yes</span></span><br><span class="line">IPV6_AUTOCONF=<span class="built_in">yes</span></span><br><span class="line">IPV6_DEFROUTE=<span class="built_in">yes</span></span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ib0</span><br><span class="line">DEVICE=ib0</span><br><span class="line">ONBOOT=<span class="built_in">yes</span></span><br></pre></td></tr></table></figure>

<p>步骤 2: 启动 ib0 端口，命令为<code> ifup ib0</code></p>
<p>步骤 3: 按照以上两步完成所有节点 IB 网络的 IP 配置</p>
<h3 id="开启vf"><a href="#开启vf" class="headerlink" title="开启vf"></a>开启vf</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mst status</span></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">MST modules:</span><br><span class="line">------------</span><br><span class="line">    MST PCI module is not loaded</span><br><span class="line">    MST PCI configuration module is not loaded</span><br><span class="line"></span><br><span class="line">PCI Devices:</span><br><span class="line">------------</span><br><span class="line"></span><br><span class="line">83:00.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 MST (Mellanox Software Tools)</span></span><br><span class="line"><span class="comment"># mst start</span></span><br><span class="line">Starting MST (Mellanox Software Tools) driver <span class="built_in">set</span></span><br><span class="line">Loading MST PCI module - Success</span><br><span class="line">Loading MST PCI configuration module - Success</span><br><span class="line">Create devices</span><br><span class="line">-W- Missing <span class="string">&quot;lsusb&quot;</span> <span class="built_in">command</span>, skipping MTUSB devices detection</span><br><span class="line">Unloading MST PCI module (unused) - Success</span><br><span class="line"></span><br><span class="line"><span class="comment"># mst status</span></span><br><span class="line">MST modules:</span><br><span class="line">------------</span><br><span class="line">    MST PCI module is not loaded</span><br><span class="line">    MST PCI configuration module loaded</span><br><span class="line"></span><br><span class="line">MST devices:</span><br><span class="line">------------</span><br><span class="line">/dev/mst/mt4119_pciconf0         - PCI configuration cycles access.</span><br><span class="line">                                   domain:bus:dev.fn=0000:83:00.0 addr.reg=88 data.reg=92</span><br><span class="line">                                   Chip revision is: 00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置SRIOV是否开启、虚拟化数量，需要重启机器</span></span><br><span class="line"><span class="comment"># mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=8</span></span><br><span class="line"></span><br><span class="line">Device <span class="comment">#1:</span></span><br><span class="line">----------</span><br><span class="line"></span><br><span class="line">Device <span class="built_in">type</span>:    ConnectX5</span><br><span class="line">Name:           N/A</span><br><span class="line">Description:    N/A</span><br><span class="line">Device:         /dev/mst/mt4119_pciconf0  <span class="comment"># 这里是设备</span></span><br><span class="line"></span><br><span class="line">Configurations:                              Next Boot       New</span><br><span class="line">         SRIOV_EN                            True(1)         True(1)</span><br><span class="line">         NUM_OF_VFS                          8               8</span><br><span class="line"></span><br><span class="line"> Apply new Configuration? (y/n) [n] : y</span><br><span class="line">Applying... Done!</span><br><span class="line">-I- Please reboot machine to load new configurations.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启机器</span></span><br><span class="line"><span class="comment"># systemctl reboot</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意：机器启动后还无法通过lspci看到VFS，只有在MLNX OFED驱动程序上启用SR-IOV后，你才能看到它们</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置固件中VFS数量，上面是配置设备的</span></span><br><span class="line"><span class="comment"># echo 8 &gt; /sys/class/net/ib0/device/sriov_numvfs</span></span><br><span class="line"><span class="comment"># 注意，vfs最好不要超过64个</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">ibstat</span><br><span class="line">ibdev2netdev -v</span><br><span class="line"><span class="comment"># 会发现除了ib0外其它的ib网卡都是DOWN状态</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看PCI设备</span></span><br><span class="line">lspci | grep Mellanox</span><br><span class="line"><span class="comment"># 能看到有8个Mellanox， 其中7个为Virtual Function</span></span><br></pre></td></tr></table></figure>

<p>开启ib0 vf网卡的state从disable为enable, 每次机器重启后，都会变回到disable,因此这条语句需要加入到开机启动中,</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ((i = 0 ; i &lt; 8 ; i++ )); <span class="keyword">do</span> ip <span class="built_in">link</span> <span class="built_in">set</span> dev ib0 vf <span class="variable">$i</span> state <span class="built_in">enable</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://codimd.mcl.math.ncu.edu.tw/s/SyG8zjJpE">https://codimd.mcl.math.ncu.edu.tw/s/SyG8zjJpE</a></p>
<p>使用ip看到所有VF的mac地址都是全0，但实际这个是ip命令的bug，需要通过以下的方式查看具体的mac地址</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@sh-office-10-5-40-94 bin]<span class="comment"># cat cd/virtfn0/net/ib1/address</span></span><br><span class="line">20:00:08:a5:fe:80:00:00:00:00:00:00:dc:56:31:60:6a:ac:03:bd</span><br></pre></td></tr></table></figure>

<p>Ibstat 显示除了mlx5_0的 state为active外，其它的7个(mlx5_1-mlx5_8)的state都为Down,目前这个是正常的</p>
<h3 id="配置子网管理器"><a href="#配置子网管理器" class="headerlink" title="配置子网管理器"></a>配置子网管理器</h3><p>IB 网络使用子网管理器（SM, Subnet Manager）管理网络路由，子网管理器可以运行在 服务器节点或具有管理功能的 IB 交换机上，由于IB交换机作者没有权限，因此这里直接部署在两台服务器节点上，由于子网管理器也要考虑单点问题，而SM本身就支持主备切换，同时只 有一台处于 Active 状态，因此部署两台，一主一备，</p>
<p>MLNX_OFED 驱动当中集成了子网管理器 OpenSM，安装 MLNX_OFED 驱动后， OpenSM 已默认安装。启动子网管理器的方法如下： 步骤 1: 运行下述命令来创建 opensm.conf 配置文件:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">opensm --create-config /etc/opensm/opensm.conf</span><br></pre></td></tr></table></figure>

<p>步骤 2 基于上述初始化创建，OpenSM 使用默认的路由算法 minihop。当网络使用 Fat-Tree 或 Up-Down 等其它拓扑组网时，需要在 opensm.conf 配置文件中将 routing_engine 修改为 对应的路由算法</p>
<p>修改默认配置:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/opensm/opensm.conf</span></span><br><span class="line"><span class="comment"># 修改以下内容，两台sm都需要修改</span></span><br><span class="line">routing_engine updn</span><br><span class="line">sm_priority 13  <span class="comment"># 范围为0-15， 这里两台sm要不一样，数值大的为master，另一台即为back</span></span><br><span class="line"><span class="comment"># 保存后启动</span></span><br><span class="line">systemctl start opensm</span><br><span class="line"><span class="comment"># 开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> opensm</span><br><span class="line"><span class="comment"># 能够启动说明没有问题，如果无法启动可查看相关日志</span></span><br></pre></td></tr></table></figure>



<h3 id="检查网络"><a href="#检查网络" class="headerlink" title="检查网络"></a>检查网络</h3><p>检查 IB 端口状态 检查 IB 端口状态是否 up，端口速率是否达到标称值，通过 ibstat 命令来进行检查，下 面打印结果为 mlx5_0 端口 up，状态为 active，速率为 100Gbps</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210704205640.png"></p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="准备工作-可选"><a href="#准备工作-可选" class="headerlink" title="准备工作[可选]"></a>准备工作[可选]</h4><p>由于本人集群中有一些psp的限制 ，需要以下操作</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># psp.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sriov-network-psp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">allowedHostPaths:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">pathPrefix:</span> <span class="string">/</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">      <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPorts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">9796</span></span><br><span class="line">    <span class="attr">min:</span> <span class="number">9796</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">requiredDropCapabilities:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ALL</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">      <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">configMap</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">emptyDir</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">projected</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">secret</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">downwardAPI</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">persistentVolumeClaim</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">hostPath</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl apply -f psp.yaml -n kube-system </span></span><br><span class="line"><span class="comment"># 修改deploy/role.yaml, 在Role/Rules域下新增以下</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&#x27;policy&#x27;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&#x27;podsecuritypolicies&#x27;</span>]</span><br><span class="line">  <span class="attr">verbs:</span>     [<span class="string">&#x27;use&#x27;</span>]</span><br><span class="line">  <span class="attr">resourceNames:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">sriov-network-psp</span></span><br></pre></td></tr></table></figure>



<h4 id="部署multus"><a href="#部署multus" class="headerlink" title="部署multus"></a>部署multus</h4><p>由于在一个k8s中要支持多个容器网络，CNCF有几款项目可以实现这个功能，而multus即为比较好的解决方案</p>
<p><strong>multus的主要作用是: 在给pod分配ip时，会先经过multus,由multus根据pod是否存在指定的annotations来分配相关的网络，同时，需要指定一个默认的容器网络(这里是flannel)， multus其实就是个网络代理</strong></p>
<p>这里要注意的是，multus里的部署yaml文件中crd的版本因k8s的版本不同而不同，在v1.16之前的版本为v1beta1,之后为v1,请特别注意</p>
<p>v1.16版本之前的使用以下的yaml文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://github.com/k8snetworkplumbingwg/multus-cni/tree/master/images/deprecated</span><br></pre></td></tr></table></figure>

<p>请根据实际情况配置cm，如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">cni-conf.json:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;name&quot;: &quot;multus-cni-network&quot;,</span></span><br><span class="line"><span class="string">      &quot;type&quot;: &quot;multus&quot;,</span></span><br><span class="line"><span class="string">      &quot;capabilities&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;portMappings&quot;: true</span></span><br><span class="line"><span class="string">      &#125;,</span></span><br><span class="line"><span class="string">      &quot;delegates&quot;: [</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          &quot;cniVersion&quot;: &quot;0.2.0&quot;,</span></span><br><span class="line"><span class="string">          &quot;name&quot;: &quot;flannel&quot;,</span></span><br><span class="line"><span class="string">          &quot;plugins&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">              &quot;type&quot;: &quot;flannel&quot;,</span></span><br><span class="line"><span class="string">              &quot;name&quot;: &quot;cbr0&quot;,</span></span><br><span class="line"><span class="string">                &quot;delegate&quot;: &#123;</span></span><br><span class="line"><span class="string">                  &quot;isDefaultGateway&quot;: true,</span></span><br><span class="line"><span class="string">                  &quot;hairpinMode&quot;: true</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">              &#125;,</span></span><br><span class="line"><span class="string">              &#123;</span></span><br><span class="line"><span class="string">                &quot;type&quot;: &quot;portmap&quot;,</span></span><br><span class="line"><span class="string">                &quot;capabilities&quot;: &#123;</span></span><br><span class="line"><span class="string">                  &quot;portMappings&quot;: true</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">          ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      ],</span></span><br><span class="line"><span class="string">      &quot;kubeconfig&quot;: &quot;/etc/cni/net.d/multus.d/multus.kubeconfig&quot;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">multus-cni-config</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure>

<p>最终在&#96;&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;00-multus.conf的文件如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123; <span class="string">&quot;name&quot;</span>: <span class="string">&quot;multus-cni-network&quot;</span>, <span class="string">&quot;type&quot;</span>: <span class="string">&quot;multus&quot;</span>, <span class="string">&quot;capabilities&quot;</span>: &#123;<span class="string">&quot;portMappings&quot;</span>: <span class="literal">true</span>&#125;, <span class="string">&quot;kubeconfig&quot;</span>: <span class="string">&quot;/etc/cni/net.d/multus.d/multus.kubeconfig&quot;</span>, <span class="string">&quot;delegates&quot;</span>: [ &#123; <span class="string">&quot;cniVersion&quot;</span>: <span class="string">&quot;0.2.0&quot;</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;cbr0&quot;</span>, <span class="string">&quot;plugins&quot;</span>: [ &#123; <span class="string">&quot;type&quot;</span>: <span class="string">&quot;flannel&quot;</span>, <span class="string">&quot;delegate&quot;</span>: &#123; <span class="string">&quot;hairpinMode&quot;</span>: <span class="literal">true</span>, <span class="string">&quot;isDefaultGateway&quot;</span>: <span class="literal">true</span> &#125; &#125;, &#123; <span class="string">&quot;type&quot;</span>: <span class="string">&quot;portmap&quot;</span>, <span class="string">&quot;capabilities&quot;</span>: &#123; <span class="string">&quot;portMappings&quot;</span>: <span class="literal">true</span> &#125; &#125; ] &#125; ] &#125;</span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://github.com/k8snetworkplumbingwg/multus-cni">https://github.com/k8snetworkplumbingwg/multus-cni</a></p>
<h4 id="部署whereabouts"><a href="#部署whereabouts" class="headerlink" title="部署whereabouts"></a>部署whereabouts</h4><p>whereabounts是用于管理ip分配，这样多台node节点上的ip能够保证不出现重复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f https://raw.githubusercontent.com/openshift/whereabouts-cni/master/doc/daemonset-install.yaml</span></span><br><span class="line"><span class="comment"># kubectl apply -f https://raw.githubusercontent.com/openshift/whereabouts-cni/master/doc/whereabouts.cni.cncf.io_ippools.yaml</span></span><br><span class="line"><span class="comment"># kubectl apply -f https://raw.githubusercontent.com/openshift/whereabouts-cni/master/doc/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml</span></span><br></pre></td></tr></table></figure>

<p>参考:<a href="https://github.com/openshift/whereabouts-cni">https://github.com/openshift/whereabouts-cni</a></p>
<h4 id="部署sriov-network-operator"><a href="#部署sriov-network-operator" class="headerlink" title="部署sriov-network-operator"></a>部署sriov-network-operator</h4><p>sriov-network-operator是openshift开源的一个包含了cni及device-plugin的部署operator，方便直接.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. centos先安装依赖</span></span><br><span class="line">yum install skopeo golang</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 下载git源码，使用release-4.7版本</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/k8snetworkplumbingwg/sriov-network-operator</span><br><span class="line">git checkout release-4.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 修改deploy/operator.yaml中的RELEASE_VERSION从4.3.0为4.7.0</span></span><br><span class="line"><span class="comment"># 4. 这里由于k8s的版本为1.15.9,不支持crd v1的版本，需要修改Makefile中的CRD_OPTIONS ?= &quot;crd:crdVersions=&#123;v1&#125;,trivialVersions=true&quot;为CRD_OPTIONS ?= &quot;crd:crdVersions=&#123;v1beta1&#125;,trivialVersions=true&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 由于集群限制，需要添加psp, 在deploy目录下添加psp.yaml, 修改role, 见上文</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 给ib机器添加label,值为空即可</span></span><br><span class="line">kubectl label nodes spring-30 node-role.kubernetes.io/worker=</span><br><span class="line">kubectl label nodes spring-30 feature.node.kubernetes.io/custom-rdma.available=<span class="string">&#x27;true&#x27;</span></span><br><span class="line"><span class="comment"># 7. 默认部署在kube-system ns下最为方便，但不推荐，原因看注意事项</span></span><br><span class="line"><span class="built_in">export</span> GOPROXY=https://goproxy.cn</span><br><span class="line"><span class="comment"># 8. 修改Makefile ns</span></span><br><span class="line">make deploy-setup-k8s</span><br></pre></td></tr></table></figure>

<p>安装完成</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210622114208.png"></p>
<p>需要注意以下事项: </p>
<ol>
<li><p>8ks v1.17之前的版本不支持在除kube-system之外的ns在pod中指定<strong>system-node-critical priorityClass</strong>，而官方镜像sriov-network-operator及sriov-network-config-daemon镜像中的yaml写死了<strong>system-node-critical priorityClass</strong>，要部署在其它ns中对于v1.17之前的版本需要特殊处理，需要重新打sriov-network-operator跟sriov-network-config-daemon的镜像，因为bindata&#x2F;manifests目录被打进了镜像中，要么通过挂载的方式进行覆盖或者重新打镜像，操作如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先通过上述操作修改了crdversion,由v1--&gt; v1beta1, bindata下的文件会自动生成</span></span><br><span class="line"><span class="comment"># bindata下涉及到 priorityClassName: &quot;system-node-critical&quot;的文件有,可以去掉</span></span><br><span class="line">	<span class="comment"># bindata/manifests/daemon/daemonset.yaml</span></span><br><span class="line">	<span class="comment"># bindata/manifests/plugins/sriov-cni.yaml</span></span><br><span class="line">	<span class="comment"># bindata/manifests/plugins/sriov-device-plugin.yaml</span></span><br><span class="line">	<span class="comment"># deploy/operator.yaml</span></span><br><span class="line">	</span><br><span class="line"><span class="comment"># 重新Docker build	</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p><code>make deploy-setup-k8s</code>部署完之后查看<code>sriovnetworknodestates.sriovnetwork.openshift.io</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system get sriovnetworknodestates.sriovnetwork.openshift.io spring-30 -oyaml</span><br></pre></td></tr></table></figure>

<p>输出如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210623195800.png"></p>
<h4 id="发布nodepolicy-yaml"><a href="#发布nodepolicy-yaml" class="headerlink" title="发布nodepolicy.yaml"></a>发布nodepolicy.yaml</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">sriovnetwork.openshift.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">SriovNetworkNodePolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">policy-ib0</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceName:</span> <span class="string">&quot;mlnx_ib0&quot;</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">feature.node.kubernetes.io/custom-rdma.available:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">priority:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">numVfs:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">nicSelector:</span></span><br><span class="line">    <span class="attr">vendor:</span> <span class="string">&quot;15b3&quot;</span></span><br><span class="line">    <span class="attr">deviceID:</span> <span class="string">&quot;1017&quot;</span></span><br><span class="line">    <span class="attr">rootDevices:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">0000</span><span class="string">:83:00.0</span></span><br><span class="line">    <span class="attr">pfNames:</span> [ <span class="string">&quot;ib0&quot;</span> ]</span><br><span class="line">  <span class="attr">isRdma:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">linkType:</span> <span class="string">ib</span></span><br></pre></td></tr></table></figure>

<p>说明:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 自定义resourceName名，这个字段后续会绑定在 NetworkAttachmentDefinition 上</span></span><br><span class="line"><span class="comment"># 2. nicSelector字段信息都由 get sriovnetworknodestates.sriovnetwork.openshift.io 信息获得</span></span><br><span class="line">kubectl apply -f nodeploicy.yaml -n kube-system</span><br></pre></td></tr></table></figure>



<h4 id="部署NetworkAttachmentDefinition"><a href="#部署NetworkAttachmentDefinition" class="headerlink" title="部署NetworkAttachmentDefinition"></a>部署NetworkAttachmentDefinition</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">sriovnetwork.openshift.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">SriovIBNetwork</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sriovnetwork</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ipam:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;type&quot;: &quot;whereabouts&quot;,</span></span><br><span class="line"><span class="string">      &quot;range&quot;: &quot;192.168.10.0/24&quot;,</span></span><br><span class="line"><span class="string">      &quot;gateway&quot;: &quot;192.168.10.1&quot;,</span></span><br><span class="line"><span class="string">      &quot;routes&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;dst&quot;: &quot;0.0.0.0/0&quot;</span></span><br><span class="line"><span class="string">            &#125;,</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;dst&quot;: &quot;192.168.0.0/24&quot;,</span></span><br><span class="line"><span class="string">                &quot;gw&quot;: &quot;192.168.10.1&quot;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">vlan:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">resourceName:</span> <span class="string">mlnx_ib0</span></span><br></pre></td></tr></table></figure>

<p>192.168.10.1默认为网关，上述的配置ip地址192.168.10.2开始分配, 从whereabouts默认是将ip信息保存在kubernetes中,部署完whereabouts会在node上的&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;whereabounts.d下生成存储信息及认证，因此可以省略使用etcd,方便.</p>
<p>新建的<code>SriovIBNetwork</code>对象默认会创建一个同名的<code>networkattachmentdefinition</code>对象</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get networkattachmentdefinition.k8s.cni.cncf.io -n kube-system</span><br></pre></td></tr></table></figure>



<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>使用以下的yaml进行验证</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-pod-4</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">sriov</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">sample-pod-4</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">sample-pod-4</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">k8s.v1.cni.cncf.io/networks:</span> <span class="string">kube-system/sriovnetwork</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nvidia/cuda-11.2.2-devel-centos7:v0.3</span>  <span class="comment">#自定义镜像,里面包含cuda及ib_write_bw工具,未尾有Dockerfile</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">mlnx-inbox-ctr</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">capabilities:</span></span><br><span class="line">            <span class="attr">add:</span> [ <span class="string">&quot;IPC_LOCK&quot;</span> ]</span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">openshift.io/mlnx_ib0:</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">            <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">openshift.io/mlnx_ib0:</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">            <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">sleep</span> <span class="string">inf</span></span><br></pre></td></tr></table></figure>

<p>发布到镜像后可以看到容器具有两个ip</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210630195111.png"></p>
<p>同时查看pod的yaml,可以看到annotation中存在两个ip地址</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210630191658.png"></p>
<p>可以使用ib的测试工具来验证网络的传输速度</p>
<p>在其中一个pod充当服务端，执行以下命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec -it sample-pod-65b94586b4-8k784 -- bash</span></span><br><span class="line"> </span><br><span class="line">root@sample-pod-65b94586b4-8k784:/tmp<span class="comment"># ibdev2netdev</span></span><br><span class="line">mlx5_9 port 1 ==&gt; net1 (Up)</span><br><span class="line">root@sample-pod-65b94586b4-8k784:/tmp<span class="comment"># ib_write_bw -a -d mlx5_9 &amp;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210701152802.png"></p>
<p>在另一个pod中执行充当客户端，执行以下命令；</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec -it sample-pod-65b94586b4-8xn6m -- bash</span></span><br><span class="line"> </span><br><span class="line">root@sample-pod-65b94586b4-8xn6m:/tmp<span class="comment"># ibdev2netdev</span></span><br><span class="line">mlx5_7 port 1 ==&gt; net1 (Up)</span><br><span class="line">root@sample-pod-65b94586b4-8xn6m:/tmp<span class="comment"># ib_write_bw -a -F 192.168.101.1 -d mlx5_7 --report_gbits</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210630115645.png"></p>
<p>会发现，速率接近100G&#x2F;s, 符合IB网卡的理想速度.</p>
<p>测试使用的Dockerfile</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM nvidia/cuda:11.2.2-devel-centos7</span><br><span class="line"></span><br><span class="line">COPY ib_write_bw /bin/ib_write_bw</span><br><span class="line">COPY ibdev2netdev /bin/ibdev2netdev</span><br></pre></td></tr></table></figure>



<h3 id="部署开启webhook，非必须"><a href="#部署开启webhook，非必须" class="headerlink" title="部署开启webhook，非必须"></a>部署开启webhook，非必须</h3><p>Operator也可以选择开启webhook, 功能作用不大，默认也是关闭的，如果想开启的话，参考下面的方式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system create secret tls operator-webhook-service --cert=server.pem --key=serverkey.pem</span><br><span class="line">kubectl -n kube-system create secret tls network-resources-injector-secret --cert=server.pem --key=serverkey.pem</span><br><span class="line"><span class="built_in">export</span> ENABLE_ADMISSION_CONTROLLER=<span class="literal">true</span></span><br><span class="line"><span class="built_in">export</span> WEBHOOK_CA_BUNDLE=$(<span class="built_in">base64</span> -w 0 &lt; cacert.pem)</span><br></pre></td></tr></table></figure>

<p>上述的server.pem、serverkey.pem、cacert.pem可通过openssl生成</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一条命令生成 ca根证书及密钥</span></span><br><span class="line">openssl req -x509 -newkey rsa:4096 -sha256 -nodes -keyout privkey.pem -out cacert.pem -days 3650</span><br><span class="line"><span class="comment"># 按提示输入需要的信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用ca根证书签发证书</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成私钥</span></span><br><span class="line">openssl genrsa -out serverkey.pem 1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成证书csr文件</span></span><br><span class="line">openssl req -new -key serverkey.pem -out servercsr.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用证书</span></span><br><span class="line">openssl ca -<span class="keyword">in</span> servercsr.pem -out server.pem -cert cacert.pem -keyfile privkey.pem -days 3650</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后在部署operator时export以下变量即可</span></span><br><span class="line"><span class="built_in">export</span> ENABLE_ADMISSION_CONTROLLER=<span class="literal">true</span></span><br><span class="line"><span class="built_in">export</span> WEBHOOK_CA_BUNDLE=$(<span class="built_in">base64</span> -w 0 &lt; cacert.pem)</span><br><span class="line">make deploy-setup-k8s</span><br></pre></td></tr></table></figure>



<p>整体都跑起来了，但是其中还是遇到这么多的坑，下次做过完整总结</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/openshift/whereabouts-cni">https://github.com/openshift/whereabouts-cni</a></li>
<li><a href="https://docs.mellanox.com/pages/viewpage.action?pageId=19798214">https://docs.mellanox.com/pages/viewpage.action?pageId=19798214</a></li>
<li><a href="https://docs.mellanox.com/pages/releaseview.action?pageId=39266293#RDG:KubernetesclusterdeploymentwithNVIDIAGPUoverInfiniBandfabric.-NodesConfiguration">https://docs.mellanox.com/pages/releaseview.action?pageId=39266293#RDG:KubernetesclusterdeploymentwithNVIDIAGPUoverInfiniBandfabric.-NodesConfiguration</a></li>
<li><a href="https://www.jianshu.com/p/2796f8ddf2c2">https://www.jianshu.com/p/2796f8ddf2c2</a></li>
<li><a href="https://docs.mellanox.com/pages/viewpage.action?pageId=19798214">https://docs.mellanox.com/pages/viewpage.action?pageId=19798214</a></li>
<li><a href="https://support.huawei.com/view/PdfRead/EDOC1000172992/SUPE_DOC/k001/document.pdf">https://support.huawei.com/view/PdfRead/EDOC1000172992/SUPE_DOC/k001/document.pdf</a></li>
<li><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/pdf/configuring_infiniband_and_rdma_networks/Red_Hat_Enterprise_Linux-8-Configuring_InfiniBand_and_RDMA_networks-zh-CN.pdf">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/pdf/configuring_infiniband_and_rdma_networks/Red_Hat_Enterprise_Linux-8-Configuring_InfiniBand_and_RDMA_networks-zh-CN.pdf</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(pod驱逐机制及OOM流程)</title>
    <url>/2023/02/15/Kubernetes-Out-Of-Memory-2/</url>
    <content><![CDATA[<p>最过在深入排查oom问题时有幸看到一个在kubernetes中探讨oom-killer问题的文章，本人觉得写得非常详尽且解答了本人的诸多疑惑，遂决定翻译成中文，方便日后求解。</p>
<p>在翻译的过程中，我会尽可能地使用原文的意思，同时也会补充一些知识，会添加一些的本人的理解</p>
<span id="more"></span>

<h2 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h2><p>以下是原文信息，感兴趣且英文好的可直接阅读</p>
<p>原文: <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/">Out-of-memory (OOM) in Kubernetes; Part 4: Pod evictions, OOM scenarios and flows leading to them</a></p>
<p>作者: <a href="https://mihai-albert.com/author/mihaialbert/">Mihai Albert</a></p>
<p>系列总共由四部分组成的文章，本人将对第2、4章节进行翻译，每章的大概内容如下:</p>
<blockquote>
<ul>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-1-intro-and-topics-discussed/">第一章</a>: 主要列了一下这个系列的主要内容，相当于大纲</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/">第二章</a>: oom-killer机制及应用部署在kubernetes中如何处理oom事件</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/">第三章</a>: 非常详细的介绍了kubernetes中的内存相关的metrics items及常用收集工具的使用</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/">第四章</a>: kubernetes中驱逐机制及oom-killer如何协调响应</p>
</li>
</ul>
</blockquote>
<p>作者将对第2章及第4章进行翻译，本文是第4章: <strong>pod驱逐机制及oom流程</strong></p>
<p>以下是我【为什么翻译这个系列】前碰到的问题,先交代一下背景</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>某台node的kubelet中出现如下的错误日志:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394489] dotnet invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=1000</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394506]  oom_kill_process+0xe6/0x120</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394642] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=f90b24151029555d49a49d82159ec90c4fec53ba8515bd51a5633d1ff45d8f53,mems_allowed=0,oom_memcg=/kubepods,task_memcg=/kubepods/besteffort/pod5f3d2447-f535-4b3d-979c-216d4980cc3f/f90b24151029555d49a49d82159ec90c4fec53ba8515bd51a5633d1ff45d8f53,task=dotnet,pid=20341,uid=0</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394676] Memory cgroup out of memory: Killed process 20341 (dotnet) total-vm:172181696kB, anon-rss:4776416kB, file-rss:25296kB, shmem-rss:0kB, UID:0 pgtables:9620kB oom_score_adj:1000</span><br><span class="line">- Jan 14 18:02:17 aks-agentpool-20086390-vmss00003C kubelet[3044]: I0114 18:02:17.686538    3044 kuberuntime_container.go:661] <span class="string">&quot;Killing container with a grace period override&quot;</span> pod=<span class="string">&quot;alloc-tests/alloc-mem-leak&quot;</span> podUID=5f3d2447-f535-4b3d-979c-216d4980cc3f containerName=<span class="string">&quot;alloc-mem&quot;</span> containerID=<span class="string">&quot;containerd://d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616&quot;</span> gracePeriod=0</span><br><span class="line">- Jan 14 18:02:17 aks-agentpool-20086390-vmss00003C containerd[2758]: time=<span class="string">&quot;2022-01-14T18:02:17.687846041Z&quot;</span> level=info msg=<span class="string">&quot;Kill container \&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\&quot;&quot;</span></span><br><span class="line">- Jan 14 18:02:18 aks-agentpool-20086390-vmss00003C kubelet[3044]: I0114 18:02:18.923106    3044 kubelet_pods.go:1285] <span class="string">&quot;Killing unwanted pod&quot;</span> podName=<span class="string">&quot;alloc-mem-leak&quot;</span></span><br><span class="line">- Jan 14 18:02:18 aks-agentpool-20086390-vmss00003C kubelet[3044]: E0114 18:02:18.924926    3044 kuberuntime_container.go:691] <span class="string">&quot;Kill container failed&quot;</span> err=<span class="string">&quot;rpc error: code = NotFound desc = an error occurred when try to find container \&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\&quot;: not found&quot;</span> pod=<span class="string">&quot;alloc-tests/alloc-mem-leak&quot;</span> podUID=5f3d2447-f535-4b3d-979c-216d4980cc3f containerName=<span class="string">&quot;alloc-mem&quot;</span> containerID=&#123;Type:containerd ID:d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616&#125;</span><br><span class="line">- Jan 14 18:02:19 aks-agentpool-20086390-vmss00003C kubelet[3044]: E0114 18:02:19.001858    3044 kubelet_pods.go:1288] <span class="string">&quot;Failed killing the pod&quot;</span> err=<span class="string">&quot;failed to \&quot;KillContainer\&quot; for \&quot;alloc-mem\&quot; with KillContainerError: \&quot;rpc error: code = NotFound desc = an error occurred when try to find container \\\&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\\\&quot;: not found\&quot;&quot;</span> podName=<span class="string">&quot;alloc-mem-leak&quot;</span></span><br></pre></td></tr></table></figure>

<p>从日志其实很容易发现问题，就是有app的内存超限被oom了, 重点在于日志中的最后几行，<strong>为什么kubelet会提示Failed killing the pod， container Not Found</strong></p>
<p>问题: <strong>难道app被oom不是通过kubelet吗？</strong></p>
<p>由于篇幅较长，在翻译开始前，本人将对本章内容给出本人认为比较重要的几条结论</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="pod驱逐"><a href="#pod驱逐" class="headerlink" title="pod驱逐"></a>pod驱逐</h3><blockquote>
<ul>
<li><p>Kubernetes没有直接控制OOM-killer</p>
</li>
<li><p>pod驱逐和OOM-killer具有相同的目标：确保节点不会在没有任何剩余内存的情况下结束</p>
</li>
<li><p>如果一个或多个pod分配内存的速度太快，以至于Kubelet没有机会在默认的检查窗口(默认10s)中发现它，并且总的pod内存使用试图超过可分配内存加上硬逐出阈值的总和，那么内核的OOM killer将介入并终止pod容器中的一个或多个进程</p>
</li>
<li><p>Kubelet驱逐阈值的来源是从memory root group计算所有容器的<code>container_memory_working_set_bytes</code>的metrics计算而来</p>
</li>
<li><p>kubernetes的可分配资源(allocatable)无法控制运行在集群节点上、但在集群之外的应用使用资源</p>
</li>
<li><p>不被kubernetes管理的进程的<code>oom_score_adj</code>为0</p>
</li>
<li><p>OOM killer作用在容器级，而驱逐作用在pod级别</p>
</li>
</ul>
</blockquote>
<h3 id="信号及退出码"><a href="#信号及退出码" class="headerlink" title="信号及退出码"></a>信号及退出码</h3><blockquote>
<ul>
<li><p>如果容器的exit code大于或等于128,意味容器被它接收到的信号杀死</p>
</li>
<li><p>无论是Kubelet决定驱逐容器的父pod还是OOM-killer终止该容器的主进程，Kubernetes报告的退出码将是137，pod驱逐和OOM-killer两种情况发送的都是SIGKILL</p>
</li>
<li><p>Kubelet会监控内核发生OOM killer事件</p>
</li>
<li><p>不应该孤立地看待容器终止的exit code。不要盲目地认为容器的退出码是137就认为是被”OOMkilled”，同时需要检查”reason”字段</p>
</li>
</ul>
</blockquote>
<h2 id="Pod驱逐"><a href="#Pod驱逐" class="headerlink" title="Pod驱逐"></a>Pod驱逐</h2><p>Kubernetes中pod驱逐是什么？这是Kubernetes对资源不足的节点采取的自动操作，通过终止一个或多个Pod来减轻压力。由于本文涉及内存，我们将专门讨论内存，因为内存是节点面临短缺的资源。</p>
<p>在 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#oom-killer">OOM killer</a>和<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#cgroups-and-the-oom-killer">Cgroups以及OOM killer</a>中,我们已经看到OOM killer如何确保节点上的可用内存不低于临界水平。因此，很明显，这两种机制<strong>pod驱逐和OOM-killer-具有相同的目标：确保节点不会在没有任何剩余内存的情况下结束</strong>。那为什么他们两个同时存在呢？</p>
<p><strong>Kubernetes没有直接控制OOM-killer，请记住，OOM-killer是Linux内核的一个特性</strong>。Kubernetes所能做的(更确定的说是每个节点上的Kubelet),是OOM-killer的一种调节(knobs)：例如，通过为<code>OOM_Score_Adj</code>设置不同的值，它改变后者关于哪个牺牲者首先被选择的行为。</p>
<p>不过这仍然没有回答为什么需要这两种机制的，而只是告诉Kubernetes必须接受OOM-killer。</p>
<p>但是kubernetes是什么时候决定驱逐pod的呢？低内存(Low memory situation)是一个相当模糊的概念： 我们已经看到当系统内存低时<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#oom-killer">OOM-killer</a>的表现，因此得出结论，<strong>POD驱逐应该在此之前发生。但具体什么时候?</strong></p>
<p>按照 <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Kubernetes官方</a> : Kubernetes节点上的<strong>可分配(Allocatable)定义为可用于Pod的计算资源量</strong>.默认情况下，此功能通过设置 <code>--enforce-node-allocatable=pods</code>  参数来限定一旦pod的内存使用超过该值，Kubelet就会触发回收机制，即 “只要所有pod的总体使用量超过”可分配”，就通过驱逐pod执行强制执行”, 如文件所述<a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#enforcing-node-allocatable">这里</a>。</p>
<p>作者注:</p>
<p><code>--enforce-node-allocatable可取(pods,system-reserved,kube-reserved)</code>: 用于在当出现超量的情况下，从哪个对象(pod、system、kube)类型进行资源回收</p>
<p>我们可以通过检查<code>kubectl describe node</code>的输出很容易地看到这此值 </p>
<p>下面是本文中使用的Kubernetes集群的一个节点（7-GiB Azure DS2_v2节点）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Addresses:</span><br><span class="line">  Hostname:    aks-agentpool-20086390-vmss000043</span><br><span class="line">  InternalIP:  10.240.0.4</span><br><span class="line">Capacity:</span><br><span class="line">  attachable-volumes-azure-disk:  8</span><br><span class="line">  cpu:                            2</span><br><span class="line">  ephemeral-storage:              129900528Ki</span><br><span class="line">  hugepages-1Gi:                  0</span><br><span class="line">  hugepages-2Mi:                  0</span><br><span class="line">  memory:                         7120616Ki</span><br><span class="line">  pods:                           110</span><br><span class="line">Allocatable:</span><br><span class="line">  attachable-volumes-azure-disk:  8</span><br><span class="line">  cpu:                            1900m</span><br><span class="line">  ephemeral-storage:              119716326407</span><br><span class="line">  hugepages-1Gi:                  0</span><br><span class="line">  hugepages-2Mi:                  0</span><br><span class="line">  memory:                         4675304Ki</span><br><span class="line">  pods:                           110</span><br><span class="line">System Info:</span><br><span class="line">  Machine ID:                 a9f54faac15846a6866920a1010ee9d7</span><br></pre></td></tr></table></figure>

<p>让我们看看pod驱逐的实际情况：</p>
<p><del>视频1- kubernetes的pod驱逐表现</del></p>
<p>启动Kubernetes pod，运行内存泄漏工具的一个实例。不为运行应用的容器指定任何请求或限制。内存泄漏工具以100 MiB的块为单位分配内存，直到达到其设置的4600 MiB输入参数。左上方窗口是泄漏工具分配时的原始输出，右侧窗口跟踪pod的状态，而底部窗口跟踪Kubelet发出的消息。泄漏工具成功地完成了它的运行，这导致在RAM中使用了大约4700 MiB， 包含它分配的4600 MiB和用于底层的. NET运行时的大约100 MB的总和。但是，此值略大于节点的”allocatable”值，后者略低于4600 MiB，如 <code>kubectl describe node</code> 一样，自从 <code>--enforce-node-allocatable</code>默认标记由节点的Kubelet使用，稍后我们看到Kubelet正在驱逐pod，并显示了驱逐原因和方式的明确消息。</p>
<p>我们甚至可以用泄漏工具少分配几个MiB的内存–这样它的总RAM占用量就会低于”可分配”值，而且我们仍然会看到相同的结果。为什么？因为该节点上已经运行了其他Pod，占用的空间略高于200 MiB。您实际上可以在内核日志的逐出消息中看到它们的列表，其中包括Grafana、kube-state-metrics、Prometheus node exporter、几个调试容器和kube-system名称空间中的一些pod。</p>
<h2 id="Allocatable-可分配"><a href="#Allocatable-可分配" class="headerlink" title="Allocatable(可分配)"></a>Allocatable(可分配)</h2><p>我们已经看到了节点配置的“可分配”内存的值，并且知道它明显小于总容量。实际上，对于我们的D2s_v2 Azure Kubernetes Service（AKS）节点，“allocatable”值仅代表**此7 GB内存的大约65%**。乍看之下，这确实感觉像是一种严重的浪费，但我们先不要问为什么要使用这种机制，而是要关注“可分配”内存的价值是如何计算的。</p>
<p>Microsoft docs<a href="https://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads#resource-reservations">资源预留</a>说明了在留出不供pod使用的内存时需要考虑的2个值：</p>
<ul>
<li>750 MiB作为可用节点内存阈值(threshold)，如果达到该阈值，将导致pod驱逐</li>
<li>Kubernetes系统守护程序（包括Kubelet）预留多少内存。对于具有7 GiB节点的测试群集，此值总计为1.6 GiB（0.25 x 4 GiB + 0.2 x 3 GiB），相当于1638 MiB</li>
</ul>
<p>请记住，在撰写本文时（2021年12月），上面的值和公式对AKS有效，因为本文中使用的测试集群运行在AKS上。其他供应商（AWS、GCP等）使用自己的公式和值。看看<a href="https://learnk8s.io/allocatable-resources">Kubernetes节点中的可分配内存和CPU</a>，以比较主要提供商之间的差异</p>
<h2 id="–kube-reserved"><a href="#–kube-reserved" class="headerlink" title="–kube-reserved"></a>–kube-reserved</h2><p>官方文件中解释了<a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#kube-reserved">kube-reserved</a> 作为用于捕获kubernetes系统守护进程（如kubelet、container runtime、node problem detector等）的资源预留，而不是为作为pod运行的系统守护进程预留资源。保留的kube通常是节点上pod密度的函数</p>
<p>但是这个标志的值用在哪里呢？Kubernetes官方文档在此图像中明确指出：</p>
<p><img src="https://luckerbyhome.files.wordpress.com/2022/02/image-1.png?w=1024"></p>
<p>图1 -节点容量分布。</p>
<p>来源：<a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Kubernetes文档</a></p>
<p>由于AKS当前不使用 <code>--system-reserved</code>标志，内存的可分配值计算为总内存容量减去<code>--kube-reserved</code>和<code>--eviction-hard</code>标志的值</p>
<p>可以使用<code>--kube-reserved-cgroup</code>强制限制<code>--kube-reserved</code>以让Kubelet、容器运行时和友元都被禁止检查<code>--kube-reserved</code>值。这是用专门为它们创建的cgroup实现的，cgroup设置了相应的限制，就像为pod的父cgroup设置的限制一样。正如我们所看到的，当内存使用量超过cgroup的限制时（此时无法回收任何东西）,OOM-killer将介入，显然会给相应的Kubernetes组件带来灾难性的后果。- <code>-kube-reserved-cgroup flag用于保护pod的“可分配”区域，这样Kubernetes守护进程就不会消耗太多内存</code>。</p>
<h2 id="驱逐机制一览"><a href="#驱逐机制一览" class="headerlink" title="驱逐机制一览"></a>驱逐机制一览</h2><p>我们在上一节中看到的所有内容如何反映在驱逐机制中？<strong>只要所有pod的总体使用量小于可分配内存值，就允许pod使用内存</strong>。一旦超过了这个阈值，Kubelet就开始发挥作用</p>
<p>kubelet每隔10秒就会根据定义的阈值检查内存使用情况。如果Kubelet决定需要驱逐，则根据用于Kubelet驱逐的<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction">Pod selection for kubelet eviction</a>中描述的内部算法对Pod进行排序, 包括QoS等级和单个内存使用情况作为因素, 将计算后得到的列表中的第一个进行驱逐。只要没有降低阈值，Kubelet就会继续驱逐。<strong>如果一个或多个pod分配内存的速度太快，以至于Kubelet没有机会在其10s窗口中发现它，并且总的pod内存使用试图超过可分配内存加上硬逐出阈值的总和，那么内核的OOM killer将介入并终止pod容器中的一个或多个进程</strong>，正如我们在 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#cgroups-and-the-oom-killer">cgroup and OOM-killer</a>部分已经展示过</p>
<p>上面引用的Kubernetes官方文章充满了有趣的细节。例如，我们如何知道Kubelet默认每10s检查一次驱逐？该条明确指出：<strong>kubelet根据其配置的处理间隔（默认为10秒）评估驱逐阈值</strong>，此外，我们将看到Kubelet每10秒记录一次相关数据，我们将在下面进一步讨论这些场景。</p>
<p>一旦发生pod驱逐，发生此情况的节点将进入低内存状态(low-memory state)。这种状态会持续多久？文章明确指出：<code>eviction-pressure-transition-period</code>标志，其控制在将节点条件转变到不同状态之前，kubelet必须等待多长时间,默认值为5分钟</p>
<p>这个时间很重要，因为一旦pod被驱逐，您可能无法在接下来的5分钟内再次调度它或者其它pod。哪些pod会受到影响？<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition">Taints and Toleration</a>里提到，控制面会在那些比<em>BestEffort</em>更高Qos的pod添加<code>node.kubernetes.io/memory-pressure</code>标志，这是因为Kubernetes将尽量有限保证<strong>Guaranteed及Burstable</strong>这类QoS类别中的pod(即使这些pod没有指定memory request）而新的<code>BestEffort pod</code>不会调度到受影响的节点“</p>
<h2 id="Node-Allocatable图解"><a href="#Node-Allocatable图解" class="headerlink" title="Node Allocatable图解"></a>Node Allocatable图解</h2><p>总体pod内存使用情况的概念视图：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/node_allocatable_diagram.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/node_allocatable_diagram.png"></a></p>
<p>绿色箭头显示pod持续分配时内存填充的方向。请记住，该图显示了总体Pod内存使用量的变化情况。这里只有这个指标是相关的，您可以将其视为向右推（当pod的内存使用增加时）或向左拉（当pod的内存使用减少时）的垂直线。只要简单理解这个图即可不要过份解读，比如假设pod使用内存中的哪些区域，因为这会导致错误的结论。</p>
<p>物理上的Pod不会在“beginning”附近消耗内存，Kubelet只会在“end”时使用内存。</p>
<p>从防止过多的pod内存使用的角度来看，总体来说效果很好。让我们回顾一下DS2_v2 AKS节点的流程：只要POD的合计使用量在“可分配”量内，一切都很好。一旦Pod的总内存使用量超过4565 MiB，Pod将开始被逐出。在上图中，这将是任何时候pod的使用进入红色阴影区域。由于Kubelet仅每10秒检查一次回收阈值，因此总体pod内存使用量很可能在短时间内超过该限制，并达到5315 MiB阈值（图中的垂直红线）。当被命中时，OOM killer将终止一些pod容器中的一个或多个进程。无论pod在很短的时间内试图分配多少内存，这次都无关紧要，因为内核会做以下2件事:</p>
<blockquote>
<ol>
<li><p>时刻都在监视cgroup的限制</p>
</li>
<li><p>内核是第一个向进程分配内存的程序，它也是调用OOM killer的内核，OOM killer保证pod的内存使用永远不会超过5315 MiB的硬限制。因此，在图表中，pod的内存使用永远不会到达黑色交叉线区域。</p>
</li>
</ol>
</blockquote>
<h2 id="Pod驱逐时关心的metrics"><a href="#Pod驱逐时关心的metrics" class="headerlink" title="Pod驱逐时关心的metrics"></a>Pod驱逐时关心的metrics</h2><p>我们经常讨论内存使用量，到底是什么意思呢？以pod可以使用的“可分配”内存总量为例：对于我们的DS2_v2 AKS节点，该值为4565 MiB。当一个节点的RAM有4565 MiB充满了pod的数据时，是否意味着它就在开始驱逐的阈值附近？</p>
<p>换句话说，<strong>开始驱逐的度量标准是什么？</strong></p>
<p>回到 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/#metrics-values">Metrics values</a>部分，我们已经看到了很多跟踪每种对象类型的内存使用情况的指标。以容器对象为例，cAdvisor返回了一系列mertrics，如<code>container_memory_rss, container_memory_usage_bytes, container_memory_working_set_bytes</code>等。</p>
<p>那么，当Kubelet查看回收阈值时，它实际上是在比较什么内存指标呢？ <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#eviction-signals">Kubernetes文档</a>提供了此问题的答案：是<code>working set</code>,其中甚至包含一个小脚本，显示了在节点级别决定逐出的计算。本质上，它将节点的working_set度量计算为memory root cgroup的 <code>memory.usage_in_bytes</code> 减去<code>memory root cgroup中的memory.stat中的 inactive_file</code>字段。</p>
<p>这正是我们过去在研究如何通过资源度量API计算节点度量时遇到的公式，请参见 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/#cadvisor-table">cAdvisor metrics table</a>的最后几行，这是一个好消息，因为我们可以在下面几节的图表上绘制Kubelet的驱逐决策中使用的相同指标，方法是选择当前提供几乎所有内存指标的指标源：cAdvisor</p>
<p>顺便说一句，如果您想看到Kubelet的代码反映了上面所说的内容–对于<code>allocatable以及-eviction-hard</code>的阈值,请查看<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/#what-is-the-memory-metric-that-the-kubelet-is-using-when-making-eviction-decisions">Kubelet在做出驱逐决策时使用的内存指标是什么？</a>。</p>
<h3 id="OOM场景-2：Pod的内存使用量超过节点的“allocatable”值"><a href="#OOM场景-2：Pod的内存使用量超过节点的“allocatable”值" class="headerlink" title="OOM场景#2：Pod的内存使用量超过节点的“allocatable”值"></a>OOM场景#2：Pod的内存使用量超过节点的“allocatable”值</h3><p>让我们考虑一个pod，它不断地使用我们的内存泄漏工具分配内存，没有为运行应用程序的容器设置请求或限制。但是现在Kubelet的日志级别要增加（从默认值–v &#x3D;2增加到–v &#x3D;4），并且显示的系统日志要过滤掉那些同时包含”memory”和”evict”的条目。这样，我们将看到Kubelet每10秒根据定义的内存阈值执行一次驱逐和定期检查。</p>
<p><del>视频2 -另一个pod的驱逐，但这次列出了阈值检查</del></p>
<p>内存泄漏工具每6秒分配和接触100 MiB内存块（pod清单在<a href="https://gist.githubusercontent.com/luckerby/385a8b050121c3198d0f6e36e7fd44b8/raw/91bb09546f0eaa9938afdfcc6a8273318398486b/PodManifest_eviction_sample_2">这里</a>）。没有为工具提供目标内存值，因此它将尽可能长时间地运行。左上角的窗口是内存泄漏工具在分配时的原始输出，右侧窗口跟踪Pod的状态，而底部窗口跟踪感兴趣的系统消息。</p>
<p>启动过程很顺利，因为内存泄漏工具还没有启动。因此，每10秒从Kubelet-2发出的消息分别捕获pod可用可分配内存和可用节点内存的稳定值。值得注意的是，可分配内存值不是指kubectl describe node output中所指的“可分配”值（对于7-GiB AKS DS2_v2节点为4565 MiB），而是指为kubepod内存cgroup设置的总体限制（对于7-GiB AKS DS2_v2节点为5315 MiB），如–kube-reserved部分所述。</p>
<p>然后创建启动内存泄漏工具的pod，并在5分钟内以稳定的速度分配内存。在05：07，Kubelet检测到它为pod的所有可分配内存跟踪的值下降到750 MiB的硬驱逐阈值以下，并且因为节点的Kubelet使用<code>--enforce-node-allocatable=pod</code>默认标志–驱逐内存泄漏工具pod，并使用显式消息描述它正在做什么。</p>
<p>最后的日志显示该节点被标记为内存不足。</p>
<p>顺便说一句，与<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#pod-eviction-movie">前面</a>的pod驱逐不同，这一次您将不再看到为驱逐排序的pod列表，因为该特定消息不再匹配我们的grep过滤器。</p>
<p>让我们看看grafana：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/grafana_pod_evicted_allocatable_breached.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/grafana_pod_evicted_allocatable_breached.png"></a></p>
<p>图3-节点和pod的内存使用情况，沿着当前OOM场景的节点空闲内存</p>
<p>此图表上跟踪了3个指标：</p>
<ul>
<li>节点可用内存（绿色）：节点内存容量减去节点内存使用量。prometheus度量公式：<code>&lt;node_memory_capacity&gt; - container_memory_working_set_bytes&#123;id=&quot;/&quot;,instance=&lt;node&gt;&#125;</code></li>
<li>节点已用内存使用情况（黄色）：在该节点上运行的所有进程（包括容器内的进程）的工作集大小。prometheus度量公式： <code>container_memory_working_set_bytes&#123;id=&quot;/&quot;,instance=&lt;node&gt;&#125;</code>.有关详细信息，请参阅资源度量API端点表 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/#resource-metrics-endpoint-table">Resource Metrics API endpoint table</a></li>
<li>总pod内存使用情况（蓝色）：所有容器的工作集大小之和。不包括暂停容器的使用，但这是低无论如何（约500 KiB每pod）,prometheus度量公式：<code>sum(container_memory_working_set_bytes&#123;container!=&quot;&quot;,instance=&lt;node&gt;&#125;)</code></li>
</ul>
<p>红线是可分配内存值（4565 MiB），其上方的红色暗区（4565 MiB -&gt;5315 MiB）是在总体pod内存使用量进入该区域后发生逐出的区域。因此，标记的阈值仅与蓝色指标相关。它上面的黑色区域（5315 MiB -&gt;）是总的pod内存使用量永远不会达到的地方，因为OOM killer不允许任何pod的容器进程将其带到<code>kubepods</code>内存cgroup限制（5315 MiB）之上，正如我们在<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#kube-reserved">kube-reserved</a>部分所看到的。</p>
<p>由于此节点上已经有一些pod在运行，因此总体pod内存使用量从一个较小的非零值（大约130 MiB）开始。然后节点内存和整体pod内存使用率以相同的速率上升，这是正常的，因为节点上没有其他东西进行大量分配。一旦内存泄漏工具pod被驱逐，并且节点回收了它的内存,指标最终会返回到它们的原始值。</p>
<p>驱逐前蓝色指标的最后一个数据点应该在红色区域内吗？不太可能，因为Prometheus client每30秒（默认值）就擦除一次目标，而Kubelet每10秒运行一次驱逐检查。因此，尽管在上面的视频中的日志中看到总体pod内存使用确实超过了可分配值，但图表并没有显示它。</p>
<p>然而，从工具提示中可以明显看出，蓝色指标的下一个值将比列出的4.13 GiB高0.5 GiB，比可分配值（4565 MiB）高出约200 MiB。</p>
<p>但是在这个图表中有些东西看起来不太对。首先，在pod被逐出后，整个pod内存使用量保持不变约5分钟。这不可能是正确的，因为pod被驱逐得相当快。</p>
<p>原因何在？蓝色指标实际上是所有pod容器的总和。由于pod被逐出，它的容器被终止，为它们发出的内存值突然停止。但是，<strong>Prometheus会默认将突然消失的指标与其最后一个值保留5分钟</strong>（我们在Prometheus<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/#prometheus-tools-section">Prometheus</a>部分讨论过这一点.</p>
<p>为什么其他2个指标不受影响？它们都跟踪memory root cgroup统计信息，在被逐出的pod的容器停止后，memory root cgroup的值会立即更新。</p>
<p>第二，存在连续的时间戳，其中所有度量值在驱逐时间（22：03：25）前后保持完全相同。第三，总体pod内存使用的增长率有时显得“起伏不定”，但这可能与前一点有关。我不太确定这最后2点的原因，但我确实怀疑cAdvisor或Prometheus的bug</p>
<h2 id="–eviction-hard"><a href="#–eviction-hard" class="headerlink" title="–eviction-hard"></a>–eviction-hard</h2><p>Kubernetes自己的计划是只允许节点内存的有限数量给pod，这在保护任何流氓pod对其他pod或节点本身造成伤害方面效果很好。但计划并不总是如预期的那样。一个问题是，该节点上还有其他参与者，它们也使用节点上的有限内存总量。例如，假设OS开始消耗明显资源。尽管Kubelet设置了总体内存使用量的限制，但它并不能<em>保证</em>内存的使用。默认情况下，没有什么可以阻止操作系统或运行在其上的其他进程开始侵入Kubernetes的“可分配”领域。当这种情况发生时，pod将被驱逐得更早：尽管pod的整体内存使用率远远低于“allocatable”值，但还是会达到另一个阈值：eviction-hard。</p>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#kube-reserved">在官方文档</a> 中描述了<code>--eviction-hard</code>标志，它触发Kubelet尝试“每当节点上的内存可用性低于保留值时，就驱逐pod”。如果驱逐一个pod没有使可用内存远离<code>--eviction-hard</code>阈值，Kubelet将继续驱逐pod。</p>
<h3 id="OOM场景-3：节点可用内存降至-eviction-hard标志值以下"><a href="#OOM场景-3：节点可用内存降至-eviction-hard标志值以下" class="headerlink" title="OOM场景#3：节点可用内存降至--eviction-hard标志值以下"></a>OOM场景#3：节点可用内存降至<code>--eviction-hard</code>标志值以下</h3><p>一种方法是在节点本身上运行的常规进程中分配内存，将它作为独立的应用程序直接在节点上运行</p>
<p>我们将在目标节点上增加Kubelet的日志记录级别（从默认<code>的--v=2增加</code>到<code>--v=4</code>），这样我们就可以看到Kubelet每隔10秒针对节点可用内存和pod可分配内存的定义内存阈值执行的定期检查。针对系统日志运行的查询为</p>
<p><code>tail /var/log/syslog -f | grep -i &#39;(?=.*evict)(?=.*memory).*&#39; -P</code></p>
<p><del>视频3 -节点可用内存低于硬逐出阈值时的Pod逐出</del></p>
<p>我们来详细分析一下。内存泄漏工具-一旦启动-每6秒（参数-e以毫秒为单位）以100 MiB（参数-m）的块为单位分配内存。没有为工具提供目标内存值（用于分配最大内存量的参数-x为0表示无限），因此它将尽可能长时间地运行。左上角的窗口是内存泄漏工具直接在节点上分配内存时的原始输出，而底部的窗口跟踪感兴趣的系统消息。</p>
<p>一旦启动内存泄漏工具，它将以稳定的速度分配内存5分钟以上。</p>
<p>首先，请注意Kubelet每隔10秒报告的可分配内存(allocatable)并没有显著下降，因为内存泄漏工具没有作为pod运行，并且节点上运行的几个pod（Grafana&#x2F;Prometheus和kube-system中的几个）没有任何显著的内存活动。相反，报告的总可用内存(available)确实在持续下降–正如我们所料。</p>
<p>在05：52，Kubelet检测到它跟踪的节点<strong>可用内存</strong>值下降到750 MiB的硬驱逐阈值以下。它将节点标记为内存压力过大，并开始采取纠正措施。</p>
<p>prometheus node-exporter被驱逐两次。这个pod中的唯一容器没有设置任何请求或限制，这使得它从QoS的角度来看是一个BestEffort，Kubelet将它作为目标，然后将运行在同一节点上的Prometheus本身和Grafana进行驱逐。幸运的是，直接在节点上运行的内存泄漏工具消耗了太多的内存，以至于它被停止，这阻止了Kubelet驱逐更多的pod。</p>
<p>随着节点内存的消失，Kubelet的情况也不太好。在第一次驱逐之后，30s都没有看到Kubelet在日志中写入的内存统计信息。然后，它将逐出最近启动的node-exporter实例（之前已逐出），此后，超过1分钟kubelet都没有在系统日志中写入任何新内容。用于连接到节点并发回控制台输出（左上角窗口）的调试容器在给出OOM killer执行操作的提示之前停滞50秒。</p>
<p>您可能会认为，是OOM-killer将直接运行在节点上内存泄漏工具杀死，它将内存泄漏工具视为最庞大的任务。毕竟，我们在控制台输出中确实得到了相同的“被杀死”的信号，就像我们在OOM-killer时一样。但实际情况并非如此，因为内核日志显示最初终止的不是节点上运行的内存泄漏工具。相反，OOM killer首先选择我们用来从调试容器连接到节点本身的ssh客户端：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/oom_killer_ssh_client_targeted.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/oom_killer_ssh_client_targeted.png"></a></p>
<p>图4 - ssh客户端被OOM killer终止</p>
<p>然后，它选择在其中一个Kubernetes容器中生成的bash会话（很可能是调试会话）：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/oom_killer_bash_targeted.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/oom_killer_bash_targeted.png"></a></p>
<p>图5 -Bash进程接下来被OOM killer终止</p>
<p>您可能会觉得奇怪，几乎不消耗任何内存的进程会被杀死，而不是内存泄漏工具占用节点上几乎所有的内存。最后2个打印屏幕显示了泄漏工具的不成比例使用，与每次终止的相应过程相反：驻留集大小（RSS）是第5列，以内存页为单位（1页&#x3D; 4 KiB）。</p>
<p>那么，为什么OOM-killer要避开使用最大的进程呢？请注意，1000表示 <code>oom_score_adj</code> （最后2个打印屏幕中的任务列表中的最后一个数字列），其是具有BestEffort的QoS等级的调试容器的结果（没有请求，也没有为其唯一容器设置任何限制），根据 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-out-of-memory-behavior">文档</a>。这就是为会被OOM-killer吸引的原因，尽管它们的内存使用率相对较低。另一方面，由于在节点上运行的内存泄漏工具是一个标准进程，而不是通过Kubernetes启动的，因此其 <code>oom_score_adj</code>为0。</p>
<p>由于我们的内存泄漏工具启动，节点内存使用量开始上升，而整体pod内存使用量仍然保持不变。这是意料之中的，因为该工具作为独立于Kubernetes的进程直接在节点上运行。如果它作为一个pod运行，我们最终会遇到<code>--kube-reserved</code>的限制，实际上是在重复前面分析过的<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#oom-scenario-2-pods-memory-usage-exceeds-node-s-allocatable-value">OOM场景</a>。</p>
<p>请注意，在任何一个点上，pod的整体内存使用量都不会显著下降-node-exporter会被逐出，但它每次都会重新启动，因为它是由kubelet控制的。这也解释了为什么OOM-killer开始行动得相当快–因为在消耗大量内存的节点上运行的pod列表并不完整，Kubelet实际上无法回收太多内存（它所针对的pod无论如何都会重新启动，而且它似乎无法尝试进一步驱逐更多的pod）</p>
<h3 id="更改–eviction-hard阈值"><a href="#更改–eviction-hard阈值" class="headerlink" title="更改–eviction-hard阈值"></a>更改–eviction-hard阈值</h3><p>根据中的示意图 <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#node-memory-diagram">图二</a> 的值 <code>--eviction-hard</code>内存值的标志将对pod的可分配内存产生影响。我们将其从默认的750 MiB提升到1000 MiB。标志的值将变为 <code>-eviction-hard=memory.available&lt;1000Mi</code></p>
<p>可分配内存值已减少。让我们重做刚才的计算</p>
<p>由于AKS当前不使用 <code>--system-reserved</code>标志，内存的可分配值计算为总内存容量减去<code>--kube-reserved</code>和<code>--eviction-hard</code>标志的值。运行DS2_v2 AKS节点的编号，我们得到：</p>
<ul>
<li>7120616 KiB总可用内存，相当于6953 MiB， <code>kubectl describe node</code></li>
<li>减去1638 MiB，即<code>--kube-reserved</code></li>
<li>减去1000 MiB，即 <code>--eviction-hard</code></li>
</ul>
<p>为4315 MiB，大致相当于中看到的4419304 KiB值 <code>kubectl describe node</code>。</p>
<p>现在让我们看一下<code>kubepods cgroup</code>，它是在节点上运行的所有pod的cgroup的父节点。从相应节点上的调试容器：</p>
<ul>
<li>cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;kubepods</li>
<li>cat memory.limit_in_bytes</li>
<li>5573943296</li>
</ul>
<h2 id="Kubelet的pod回收机制和内核的OOM-killer之间的交互"><a href="#Kubelet的pod回收机制和内核的OOM-killer之间的交互" class="headerlink" title="Kubelet的pod回收机制和内核的OOM killer之间的交互"></a>Kubelet的pod回收机制和内核的OOM killer之间的交互</h2><p>值得指出的是，<strong>由于OOM killer和Kubelet的pod逐出机制的共存, 驱逐只在特定阈值内的指定间隔（默认为10秒）才开始，但OOM killer总是保持警惕</strong>，但会有一个硬限制–不时会出现令人惊讶的情况。因此，Kubelet和OOM-killer有时会表现为在某种竞争条件下为杀死行为不端的容器而“战斗”。</p>
<p>在OOM killer杀掉进程后，等Kubelet来到10s窗口时，就会提示试图杀死一个不再存在的容器。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394676] Memory cgroup out of memory: Killed process 20341 (dotnet) total-vm:172181696kB, anon-rss:4776416kB, file-rss:25296kB, shmem-rss:0kB, UID:0 pgtables:9620kB oom_score_adj:1000</span><br><span class="line"></span><br><span class="line">- Jan 14 18:02:19 aks-agentpool-20086390-vmss00003C kubelet[3044]: E0114 18:02:19.001858    3044 kubelet_pods.go:1288] <span class="string">&quot;Failed killing the pod&quot;</span> err=<span class="string">&quot;failed to \&quot;KillContainer\&quot; for \&quot;alloc-mem\&quot; with KillContainerError: \&quot;rpc error: code = NotFound desc = an error occurred when try to find container \\\&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\\\&quot;: not found\&quot;&quot;</span> podName=<span class="string">&quot;alloc-mem-leak&quot;</span></span><br></pre></td></tr></table></figure>

<p>作者注:</p>
<p>这就是作者在【背景】一栏中提出问题的回答:</p>
<p>当某个pod极速申请内存时超了limit，还没等到kubelet 驱逐周期前，会被OOM-killer杀掉，等kubelet再来kill时，container已经被kill了，因此container已不存在</p>
<h2 id="Kubelet杀container是因为OOM吗？"><a href="#Kubelet杀container是因为OOM吗？" class="headerlink" title="Kubelet杀container是因为OOM吗？"></a>Kubelet杀container是因为OOM吗？</h2><p>因此，我们知道当节点处于内存压力下时，Kubelet将开始驱逐pod（因为节点的pods allocatable(已分配)值变得太大，或者节点的整体内存不足）。但是，Kubelet终止container是因为它们超过了规定的限制吗？换句话说:</p>
<p><strong>Kubelet会比OOM-killer更快地介入（至少有时），以阻止超过其内存限制的容器吗？</strong></p>
<p>目前的文件（截至2021年12月）似乎在某种程度上表明：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/kubelet_killing_container_message_in_docs.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/kubelet_killing_container_message_in_docs.png"></a></p>
<p>图7 - Kubernetes文档显示Kubelet因超出其内存限制而终止容器</p>
<p>注意，相应的容器确实被OOM杀死了5次，但是父pod的event也提到了一个因为驱逐而被“杀死”的事件，然而，这个pod显然是运行，而不是被驱逐。我无法复现这种行为，但我仍然想知道Kubelet是否真的会终止违反其限制的容器?</p>
<p>但事实似乎并非如此，否则Kubelet将完全依赖于OS OOM killer来杀死超过其配置内存限制的容器。导致我得出这个结论的原因是：</p>
<ul>
<li>Kubelet正在检查是否只在指定的时间间隔（当前为10秒）执行驱逐，但这个时间间隔无法让kubelet立即发现内存被耗尽</li>
<li>内核记录了OOM日志，有专门的代码记录OOM事件，这篇<a href="https://engineering.linecorp.com/en/blog/prometheus-container-kubernetes-cluster/">博客</a>详细解释了这一点,但这听起来不太可信, 文章中，Kubelet在确保它能够捕捉到少数情况，当它错过时，容器反而被内核杀死，</li>
<li>即使OOM killer可以为特定的cgroup停止，这并不意味着它完全忽略了分配，如文档中的<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">第10节</a>,”如果OOM killer被禁用，cgroup下的任务在请求可负责内存时将挂起&#x2F;休眠在内存cgroup的OOM等待队列中，因此，由于OOM-killer处于休眠状态，这不会为Kubelet提供”捕获”使用超过其限制的容器的机会， 相反，OOM Killer首先不会允许分配，因为这些进程将简单地挂起</li>
<li>这篇文章中进行的所有测试都没有发现这种情况</li>
<li>我没有深入代码，我没有找到kubelet不支持通过监控每个容器的内存限制的论点</li>
<li>上面截图中看到的事件的最后一部分是“需要杀死Pod”。此消息不再出现在Kubernetes最新版本的代码中，但可以在页面测试完成时（~2019年）,然而，当底层节点不处于内存压力状态时，Kubelet监视OOM事件的方式似乎没有什么根本性的不同–再一次快速浏览一下</li>
<li>Kubelet设置了调整后的 <code>oom_score_adj</code> 用于它创建的<a href="https://github.com/kubernetes/kubernetes/blob/v1.23.1/pkg/kubelet/cm/container_manager_linux.go#L928-L930">容器</a>这似乎表明Kubelet对OOM-killer的依赖程度很高</li>
</ul>
<h2 id="关于pod驱逐的结论"><a href="#关于pod驱逐的结论" class="headerlink" title="关于pod驱逐的结论"></a>关于pod驱逐的结论</h2><p>让我们在这部分中总结一下关于OOM终止的容器和pod驱逐的发现：</p>
<ol>
<li><p><strong>Pod在总节点内存中的“可分配”百分比各不相同</strong>：到目前为止，我们看到的阈值是针对本文中测试AKS集群中使用的节点类型的，即DS2_v2机器。正如我们所看到的，“allocatable”值仅代表这个7 GB内存节点上内存容量的65%。但该百分比随节点的内存容量而变化，因为用于<code>--kube-reserved</code>值的<a href="(https://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads#resource-reservations">公式</a>是这么计算的，这意味着节点拥有更多内存时，<code>--kube-reserved</code>将消耗更少的内存。例如，在具有16 GiB RAM的Azure D4s_v3上，“allocatable”值跳至78%。</p>
</li>
<li><p><strong>OOM终止的容器和pod驱逐并不总是公平的</strong>：正如我们已经看到的，很可能泄漏或以其他方式恶意消耗内存的进程或容器不会受到“惩罚”，而是其他进程或容器在没有自身错误的情况下被终止。为pod设置一个“有保证的”QoS等级确实有帮助，但它不会阻止一个节点在操作系统组件的内存压力下最终驱逐它们。</p>
</li>
<li><p><strong>OOM killer与pod驱逐的作用范围</strong>：OOM killer在容器级别上起作用，终止那些容器中的进程（主进程可以是第一个退出的进程，也可以不是–正如我们在<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#cgroups-and-the-oom-killer">Cgroups和OOM killer</a>中详细看到的那样。另一方面，Pod驱逐将针对整个pod及其所有容器。</p>
</li>
<li><p><strong>OOM killer与pod驱逐的响应时间</strong>：OOM killer嵌入在内核中，它可以快速捕获任何试图使用超过限制的容器进程。相比之下，Kubelet（负责处理pod驱逐），在默认情况下检查驱逐间隔10秒。</p>
</li>
<li><p><strong>容器内存限制并不是银弹</strong>：并不是那些没有限制的pod才能成为驱逐的目标。设置限制不会阻止pod驱逐。您可以为内存设置一个相对较低的请求值-意味着pod被安排在特定节点上-但有一个极高的限制（实际上比节点的总容量高得多）。随后分配大量内存将导致pod被驱逐。有保证的pod不受这种情况的影响，因为它们比那些“鲁莽”分配的pod具有更好的QoS级别，所以Kubelet将从后一种类型中选择其驱逐目标。</p>
</li>
<li><p><strong>驱逐过程中涉及3个标志</strong> ： <code>--kube-reserved</code> 可保护Kubelet和其他Kubernetes守护程序不被pod分配过多内存。它通过规定”kubepods”内存cgroup的限制来实现这一点，cgroup是所有pod及其容器的父级。因此，如果pod试图使用超过【容量减去 <code>--kube-reserved</code>】的值时，则OOM killer介入，选择并终止这些容器中的一个进程。这保证了pod永远不会达到<code>--kube-reserved</code>的值。如果pod分配的容量小于【容量-<code>kube-reserved</code>】值，但大于<code>kubectl describe node</code>输出中的“allocatable”，则Kubelet将选择并逐出pod（如果kublet在10 s的检查周期内捕获到该值）,请注意，操作系统、其他守护进程或在Kubernetes外的进程不会被此标志停止，这仅仅是因为&#96;–kube-reserved&#96;&#96;转换为如上所示的设置，即只有pod会受到影响，而不是“常规”操作系统进程</p>
<p><code>--eviction-hard</code>标志确保一旦节点低于指定的内存量，pod就开始被逐出，同时它还间接指示pod的“可分配”内存的大小。–system-reserved标志是可用的，但是AKS目前没有为它设置默认值。。</p>
</li>
<li><p><strong>驱逐期间的宽限期</strong> ：在本节执行的整个测试中，所有pod均被驱逐，没有任何终止宽限期。尽管尚未讨论，但在启用所述软收回阈值时，可以有一个宽限期 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#soft-eviction-thresholds">这里</a>。</p>
</li>
<li><p><strong>OOM killed容器与pod驱逐行为</strong> ：默认情况下，OOM killer会导致容器重新启动（除非修改 <code>restartpolicy</code> ），除非它们由deployment或statefulset之类的东西管理-否则将永远对pod进行驱逐</p>
</li>
</ol>
<h2 id="kubectl-top-pod显示内存使用率为100-是否有问题？"><a href="#kubectl-top-pod显示内存使用率为100-是否有问题？" class="headerlink" title="kubectl top pod显示内存使用率为100%是否有问题？"></a>kubectl top pod显示内存使用率为100%是否有问题？</h2><p>作者注:</p>
<p>在这一小节中，原作者非常详细地解释了当使用<code>kubectl top pod</code>时内存使用率&gt;100%是否有问题?</p>
<p>这里只给出原作者结论，篇幅较长就不进行翻译了，对全文的理解没有影响</p>
<p>因此，为了回答最初的问题，只要您的pod具有“保证的”QoS等级（或较低等级，但您知道它们的长期内存使用），并且操作系统本身没有波动的内存使用，那么您完全可以使<code>kubectl top node </code>输出百分比大于100%，并处于正常情况。如果您不知道pod的内存使用行为，也不知道其QoS类别或节点的操作系统内存使用情况，则应将百分比超过100%视为严重问题，并开始为pod设置适当的请求和限制，观察操作系统内存使用行为等。</p>
<h2 id="信号及退出码-1"><a href="#信号及退出码-1" class="headerlink" title="信号及退出码"></a>信号及退出码</h2><p>当一个容器终止时，不管它是运行完成还是发生了什么不好的事情，Kubernetes都会记下它的退出代码。您可以在<code>kubectl describe pod</code>的输出中很容易地看到它的父pod。该退出代码提供了有关该容器内主进程发生了什么。<br>有一些很棒的文章讨论了退出代码和Kubernetes，比如[<a href="https://komodor.com/learn/exit-codes-in-containers-and-kubernetes-the-complete-guide/">Exit Codes in Containers and Kubernetes – The Complete Guide</a>。我们接下来要讨论的内容主要为内存不足情况。<br>Linux上的信号和退出代码是一个相当复杂的话题，所使用的各种shell之间存在差异，而且还有其他各种微妙之处。</p>
<p>但为了简化，主要思想是<strong>如果退出代码大于或等于128，那么它意味着我们的容器被它接收到的信号杀死了</strong>。<strong>从退出代码中减去128就是信号值</strong>。根据此编号，我们可以找到相应的信号，例如，通过查阅此处信号手册页上的[列表](Linux manual page](<a href="https://man7.org/linux/man-pages/man7/signal.7.html)%E3%80%82">https://man7.org/linux/man-pages/man7/signal.7.html)。</a><br>撇开软驱逐阈值不谈，<strong>无论是Kubelet决定驱逐容器的父pod还是OOM-killer终止该容器的主进程，Kubernetes报告的退出码将是137，pod驱逐和OOM-killer两种情况发送的都是SIGKILL</strong><br>如果容器是OOM终止的，则针对父pod运行的<code>kubectl describe pod</code>将在<code>reason</code>字段中列出”OOMKilled”。  当容器确实被OOM-killer终止时，<strong>Kubelet如何知道需要用OOMKilled标记容器的呢？因为Kubelet会在OOM killer执行操作时监视内核生成的事件，所以它知道发生了什么以及哪个被oom了</strong>(参考<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#is-kubelet-killing-containers-due-to-oom">is Kubelet killing containers due to OOM?</a><br>我们还关心另一个退出代码：在我们的内存泄漏工具中，如果它试图分配的内存超过. NET运行时允许的内存，那么进程将以退出代码139终止。这对应于SIGSEGV信号: 非常内存访问<br>不过，在查看退出代码时应小心。下面是kubectl describe pod的部分输出。发生了什么？</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">State:          Terminated</span><br><span class="line">      Reason:       Error</span><br><span class="line">      Exit Code:    143</span><br><span class="line">      Started:      Fri, 28 Jan 2022 23:12:28 +0200</span><br><span class="line">      Finished:     Fri, 28 Jan 2022 23:15:33 +0200</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Error</span><br><span class="line">      Exit Code:    137</span><br><span class="line">      Started:      Fri, 28 Jan 2022 23:11:33 +0200</span><br><span class="line">      Finished:     Fri, 28 Jan 2022 23:12:26 +0200</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  1</span><br></pre></td></tr></table></figure>

<p>在这里看到退出代码137， OOM killer是否对上个容器执行了kill呢？没有，因为容器运行得很好且没有stop的迹象。直到我使用htop的kill命令通过向主进程发送SIGKILL信号来终止主进程。这个信号的id在Linux上是9（不管CPU架构如何），因此退出代码的最终值是9 + 128 &#x3D; 137。</p>
<p>再试一次会怎么样？pod再次运行良好，直到我再次使用htop的kill命令以SIGTERM信号终止同一进程。这个实例的id是15，因此退出代码是15 + 128 &#x3D; 143。<br>因此，不应该孤立地看待容器终止的退出代码。不要盲目地认为137是”OOMCilled”，也要检查”reason”字段。<br>终止进程的信号是否可以通过某种方式进行监控？有很多方法可以做到这一点。</p>
<p>这是一个例子<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#how-to-obtain-data-about-who-sends-kill-signals-in-linux">How to obtain data about who sends kill signals in Linux</a></p>
<p>如何在Linux中获取有关谁发送了终止信号的数据。</p>
<p>The major drawback is that I couldn’t use it to see the actual SIGKILL sent by the OOM killer to a process that’s terminated inside a Kubernetes container</p>
<p>主要的缺点是我无法使用它来查OOM-killer发送给运行在kubernetes中某一个容器中被终止的进程的SIGKILL信号，我所看到的只是从containerd发送到containerd-shim的kill信号，很可能是Kubelet决定终止pod用以响应内存泄漏工具进程消失的结果。dotnet进程或其任何线程的id都与kill的目标进程的id不匹配（然而dotnet进程毕竟是containerd-shim进程的子进程）</p>
<h2 id="Metrics-Testing"><a href="#Metrics-Testing" class="headerlink" title="Metrics Testing"></a>Metrics Testing</h2><h2 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h2><h2 id="kubectl-top-node"><a href="#kubectl-top-node" class="headerlink" title="kubectl top node"></a>kubectl top node</h2><h2 id="Resource-Metrics-API-endpoint"><a href="#Resource-Metrics-API-endpoint" class="headerlink" title="Resource Metrics API endpoint"></a>Resource Metrics API endpoint</h2><h2 id="Summary-API-endpoint"><a href="#Summary-API-endpoint" class="headerlink" title="Summary API endpoint"></a>Summary API endpoint</h2><h2 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor"></a>cAdvisor</h2><h2 id="cgroups-pseudo-filesystem"><a href="#cgroups-pseudo-filesystem" class="headerlink" title="cgroups pseudo-filesystem"></a>cgroups pseudo-filesystem</h2><h2 id="htop"><a href="#htop" class="headerlink" title="htop"></a>htop</h2><h2 id="Why-is-there-a-difference-between-what-htop-shows-for-the-container-process-and-kubectl-top-pod"><a href="#Why-is-there-a-difference-between-what-htop-shows-for-the-container-process-and-kubectl-top-pod" class="headerlink" title="Why is there a difference between what htop shows for the container process and kubectl top pod?"></a>Why is there a difference between what htop shows for the container process and kubectl top pod?</h2><p>作者注:</p>
<p>以上小节，原作者做了相关测试来为上述结论提供相关证据，起辅助作用，篇幅太长，对结论影响不大，作者将不对这些节进行翻译</p>
<h2 id="Flows-leading-to-out-of-memory-situations"><a href="#Flows-leading-to-out-of-memory-situations" class="headerlink" title="Flows leading to out-of-memory situations"></a>Flows leading to out-of-memory situations</h2><p>我们已经在上面中看到了一个成功的运行，其中内存泄漏工具可以分配它所请求的内容。现在让我们分析一下事情是如何出错的，即<strong>当内存分配导致内存不足（OOM）错误时，反过来触发容器被杀死和pod被驱逐</strong><br>我们还讨论了运行容器所涉及的各种组件是如何组合在一起的-应用程序的运行时（如果存在）、Kubernetes本身和节点的底层操作系统。而且很明显，他们每个人都可以有自己的记忆极限，如果突破，可能会导致采取激烈的措施。<br>下图使用所有这些知识来描述如何处理内存分配请求，包括应用运行时（浅蓝色）、Kubernetes的回收机制（蓝灰色）和OOM killer（浅粉色）的决策，以及此请求如何因OOM错误而失败，从而导致严重后果（红色）。</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2022/02/allocate_flowchart_diagram.drawio.png"><img src="https://luckerbyhome.files.wordpress.com/2022/02/allocate_flowchart_diagram.drawio.png"></a>F21 – 可能的内存分配流程图</p>
<p>上面的流程捕获了当容器尝试分配和使用一定数量的内存时会发生什么。结束状态是内存分配成功（绿色框）或失败（任何红色框）。<br>每个失败的结束状态（任何红框）都有一个或多个与之关联的内存不足（OOM）方案。OOM场景代表了通过图导致特定结果的多个可能“路径”的结果。这些将在下一节OOM场景中描述。请注意，每个场景名称旁边的id（例如，OOM 3中的3）只是本文中使用的一种编号，以便轻松引用每个场景名称-它们在Kubernetes中没有任何意义。<br>此图中做了一些假设，以简化问题。例如，节点上的overcommit被认为是始终启用的（正如我们在过度提交部分所讨论的），这为我们节省了一些额外的决策块。操作系统是Linux，因为这是本文讨论的唯一操作系统。swap也被认为是关闭的–截至目前（2021年12月）是标准的，但它可能会在未来改变，特别是因为支持它的功能门已经在Kubernetes 1.22中引入。Kubernetes的软驱逐阈值也没有考虑在内。<br>如果图表只引用了一个容器在特定时间进行内存分配，那么流程中的一些操作就不必绑定到所述容器。图中有两个方框（粉色，红色轮廓），描述了可能影响的操作，而不仅仅是执行当前内存分配的容器：“已调用OS OOM killer”和“Kubelet驱逐一个或多个pod”。因此，请注意，图中至少有2个流通过这些框，因此分配内存并将底层节点置于内存压力之下的容器不会终止。它不仅“侥幸逃脱”并成功地分配和使用内存–至少可以使用一段时间–而且它会导致其他容器或pod在没有自身错误的情况下被终止。这一点在Cgroups、OOM killer和Pod逐出部分有详细介绍。</p>
<h2 id="OOM场景"><a href="#OOM场景" class="headerlink" title="OOM场景"></a>OOM场景</h2><p>让我们看看Kubernetes中涉及内存不足（OOM）的一些场景。它们最初是如何发生的，可以从上面导致内存不足情况的流程部分的图表中一目了然。将显示对每种情景的详细分析。<br>大多数场景都给出了如何重新创建场景的Pod清单，它们包含字段spec.nodeName，以显式地指示应该调度相应pod的节点。</p>
<h3 id="OOM1：容器在超出其内存限制时被OOMKilled"><a href="#OOM1：容器在超出其内存限制时被OOMKilled" class="headerlink" title="OOM1：容器在超出其内存限制时被OOMKilled"></a>OOM1：容器在超出其内存限制时被OOMKilled</h3><p>在OOM场景#1中分析了这一确切场景：<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#oom-scenario-1-container-is-oomkilled-when-it-exceeds-its-limit">OOM Scenario #1: Container is OOMKilled when it exceeds its limit</a>。展示了一旦容器超过极限，OOM-killer就将其kill掉<br>容器的退出代码是137，Kubernetes将”season”字段设置为”OOMGilled”。如果为父pod设置了默认的重新启动策略，则容器将无休止地重新启动。</p>
<h3 id="OOM2：Pod的内存使用量超过节点的”可分配-allocatable-”值"><a href="#OOM2：Pod的内存使用量超过节点的”可分配-allocatable-”值" class="headerlink" title="OOM2：Pod的内存使用量超过节点的”可分配(allocatable)”值"></a>OOM2：Pod的内存使用量超过节点的”可分配(allocatable)”值</h3><p>如果您没有对某些pod使用限制或者过度使用限制，就会遇到这种情况。</p>
<p>在此上下文中，我们将”overcommit”理解为某些pod的限制大于其请求值，并且计划pod的内存限制之和超过了节点可以支持的范围。注意，我们并不是指Linux内存过量使用，<br>回到OOM场景#2：Pod的内存使用超过了节点的”allocatable”值, 结果是pod被逐出，并且–如果它不受诸如deployment对象之类的任何对象控制–永远不会再次启动。<br>需要记住的一件事是，这个场景里，需要一部分的内存分配执行得相当慢，需要为运行在底层节点上的Kubelet提供了充足的时间，以便在测试pod开始使用过多内存时将其逐出。如果内存分配发生得更快–以至于Kubelet还没来到对”kubepods” cgroup是否达到其极限的定期检查(默认10s)，OOM-killer将启动</p>
<h3 id="OOM3：节点可用内存降至硬逐出阈值以下"><a href="#OOM3：节点可用内存降至硬逐出阈值以下" class="headerlink" title="OOM3：节点可用内存降至硬逐出阈值以下"></a>OOM3：节点可用内存降至硬逐出阈值以下</h3><p>以上流程图中没有打印错误：这种情况确实出现了两次-这就是为什么在它旁边放置了一个星号。内存泄漏工具作为一个公共进程直接在操作系统上启动。随着节点上的可用内存开始蒸发，Kubelet被触发以驱逐一些pod及其容器，由于这无法回收内存，随后OOM-killer终止了一些容器。这两种”终止”机制-pod驱逐和OOM-killer, 都没有杀掉实际的占用的进程，最终导致内存最终耗尽。<br>在此详细介绍此场景如何展开：<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#oom-scenario-3-node-available-memory-drops-below-the-eviction-hard-flag-value">OOM Scenario #3: Node available memory drops below the –eviction-hard flag value</a><br>您可能会争辩说，从技术上讲，在这个场景中进行内存分配的不是容器。因为它只作为常规进程运行。但它确实会以不止一种方式对pods造成损害，这一点在图表上很重要。</p>
<h3 id="OOM4：Pod的内存使用超过节点的”可分配-allocatable-”值（快速分配）"><a href="#OOM4：Pod的内存使用超过节点的”可分配-allocatable-”值（快速分配）" class="headerlink" title="OOM4：Pod的内存使用超过节点的”可分配(allocatable)”值（快速分配）"></a>OOM4：Pod的内存使用超过节点的”可分配(allocatable)”值（快速分配）</h3><p>等同于OOM场景2，但假设分配内存过快-如在分配内存时不等待下一个内存太长时间-这会触发OOM killer，而不是上面的OOM2中所示的pod逐出。</p>
<p>可以通过减少分配之间的时间或者显著增加每次分配的内存来实现。</p>
<h3 id="OOM5：容器设置了限制，应用程序内部分配内存，但应用程序运行时最终会在超出限制之前分配失败"><a href="#OOM5：容器设置了限制，应用程序内部分配内存，但应用程序运行时最终会在超出限制之前分配失败" class="headerlink" title="OOM5：容器设置了限制，应用程序内部分配内存，但应用程序运行时最终会在超出限制之前分配失败"></a>OOM5：容器设置了限制，应用程序内部分配内存，但应用程序运行时最终会在超出限制之前分配失败</h3><p>启动带有一个容器的pod。在容器内运行的应用程序是以使用运行时的语言（例如. NET）编写的。容器设置了内存限制(memory limit)。应用程序开始分配内存，但从到达limit的值，因为它在相当早的时候就报告内存不足。<br>一个实际的例子：您启动一个pod，该pod具有一个容器，该容器的内存限制设置为2000 MiB。您知道您的. NET应用程序需要大约1700 MiB，因此您认为应该是安全的。但是随着应用程序内存使用量的增长，你会发现你永远不会超过1700 MiB的内存分配。你的容器没有被OOMkilled，但是它奇怪地被重新启动了。<br>参考<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/#net">Runtime implications around OOM &#x2F; .NET</a>，在这种情况下，容器的退出代码为139，对应于SIGSEGV信号（更多详细信息，请参见<a href="http://signals-and-exit-codes/">Signals and exit codes</a>），而”原因”字段由Kubernetes设置为”错误”。<br>如果为父pod设置了默认的重新启动策略，则容器将无休止地重新启动。</p>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h3 id="Pod驱逐-1"><a href="#Pod驱逐-1" class="headerlink" title="Pod驱逐"></a>Pod驱逐</h3><p>Q：为什么在您显示的日志中可以看到”Killing container with a grace period override”消息？源代码显示这是一条–v&#x3D;3级消息，AKS Kubelet以–v&#x3D;2开头。怎么回事？<br>A：v1.21.2中就有一个bug，即使没有指定宽限期，上述消息也会被错误地记录–v&#x3D;2消息。</p>
<p>Q：被驱逐的Pod的状态是否为”OOMKilled”？<br>A：不是，<code>kubectl describe pod</code>的示例输出如下（已删除不相关部件），被逐出pod的状态设置为”<em>Failed</em>“，原因为”<em>Evicted</em>“</p>
<p>Q: 其中一个容器已被OOM kill的Pod的状态是什么？pod状态是否为”OOMkilled”？<br>A：在<code>kubectl describe pod</code>时，你不会看到pod的状态为”OOMkilled”。但如果运行<code>kubectl get pod</code>，则pod自身的状态将在”OOMkilled”状态之间循环，即使只有一个容器被OOM杀死，而不管其他容器是否正常</p>
<p>Q: 我的一个pod被驱逐，但是当我查看驱逐消息时，它指出其中的一个容器在一个有几GiB空闲可分配内存的节点上占用了不到100 MiB（如下）。这是怎么回事？</p>
<p>A: 截至2021年12月，使用cAdvisor检索使用指标，检索指标的间隔为10 - 15秒。上面看到的输出所对应的pod确实填满了它所运行的7-GiB节点上的整个可分配内存，这导致它被驱逐（没有触发OOM killer）。显然，从仅仅使用~50 MiB内存开始还可以用很久。但问题是它分配内存的速度相当快：它在分配的每100 MiB内存块之间仅暂停200毫秒，从启动到被美瞳需要大约12秒（从最后的消息中可以看出）。我怀疑打印的内存使用值是在创建容器时获得的，因此可以解释为什么这个数字很低，因为根本没有其他机会获得容器内部的新读数，因为它在pod驱逐后已经终止。</p>
<p>Q:我正在执行<code>kubectl describe pod</code>，并在输出的末尾看到以下事件。”杀死”列为原因是否意味着里面的容器被OOM kill？</p>
<p>A: 不，那是Kubelet在决定驱逐pod后终止了container。参见<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#is-kubelet-killing-containers-due-to-oom">Is Kubelet killing containers due to OOM?</a>。但是要注意，在Kubelet的pod机制和内核的OOM killer之间可能会出现罕见的交互情况，其中event可能不会告诉实际发生了什么（例如，reason中显示”Killing”，因为Kubelet试图驱逐pod，但OOM killer更快，杀死了里面的容器）。</p>
<p>Q：在pod内存使用时，指定<code>container!=&quot;&quot;</code>有什么意义？<br>答：过滤”container”字段不为空的容器可确保我们避免重复计数，因为 root cgroup包含所有内容，然后&#x2F;kubepods&#x2F;burstable包含 burstablepod的聚合统计信息等。实际上，我们只是在计数叶子容器。这就是<code>sum(container_memory_working_set_bytes&#123;container!=&quot;&quot;,instance=&lt;node&gt;&#125;)</code></p>
<p>Q：在一些视频中，我看到内存泄漏工具报告20 GiB的可见内存，但运行它的节点没有那么多的RAM。这是怎么回事？<br>A：使用的容器映像具有20 GiB的堆硬限制，以便. NET运行时在达到相应容器配置内存限制的75%时不阻止分配。换句话说，这保证了. NET运行时不会在AKS测试集群中的7-GiB节点上生成OOM情况。</p>
<p>Q：我可以使用什么Prometheus指标来查看OOM killer对节点执行操作的次数？<br>A：使用<code>node_vmstat_oom_kill</code></p>
<h3 id="signal-and-exit-code"><a href="#signal-and-exit-code" class="headerlink" title="signal and exit code"></a>signal and exit code</h3><p>Q: 可以发送一个SIGSEGV到一个进程使它崩溃吗？<br>A：在使用. NET的内存泄漏工具的情况下，完全可以做到， 而且在处理这个事件的工具代码中没有什么特别的东西。但与内核发送此信号不同，使用kill将其手动发送到另一个进程不会产生显著效果,<a href="https://stackoverflow.com/a/21367300/5853218">参考</a></p>
<h3 id="Flows-and-OOM-Scenarios"><a href="#Flows-and-OOM-Scenarios" class="headerlink" title="Flows and OOM Scenarios"></a>Flows and OOM Scenarios</h3><p>Q:  在分配流程图中，为什么说”突然”分配？<br>A：我所说的”突然”是指内存分配大到足以触发OOM-killer。”突然”分配意味着两种情况-（1）由于操作系统组件（或直接在操作系统上运行的东西）突然分配了大量内存，节点的内存总体非常低，或（2）整个”kubepod” cgroup内存不足（意味着pod试图使用超过”可分配”内存区域和硬逐出阈值的内存）。在这两种情况下，分配都不会是”缓慢的”分配：在（1）的情况下，慢速分配将使系统触发Kubernetes硬驱逐阈值，这将驱逐一些pod并回收一些内存。在（2）的情况下，在”可分配的”图表上的红色散列区域中徘徊的整体pod内存使用（参见节点可分配，图示）将导致Kubelet驱逐pod, 不给OOM-killer采取行动的机会。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h2><blockquote>
<ul>
<li><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/">https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(什么, 宿主机根目录被容器coredump打爆了?)</title>
    <url>/2023/07/29/Kubernetes-OutofDisk-CoreDump/</url>
    <content><![CDATA[<p>最近在生产Kubernetes集群容器中的进程频繁地出现coredump从而导致宿主机的根目录被打到100%, 在排查的过程中发现一些技术盲区, 记录一下</p>
<span id="more"></span>


<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>某晚突然收到磁盘告警, 提示某台机器的&#x2F;路径被占用100%(所有机器的根目录都是统一规格不算大),打开监控大图发现这台机器的根目录出现很有频率地打到100%降下后又100%的趋势.<br>由于计算集群将docker及kubelet的volume都配置到独立的disk上，因此除了根被打满的告警外，没有收到其它的报警, <strong>即不会引影响上面已有容器的正常运行</strong></p>
<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><p>由于计算节点只有集群管理员才能登录, 因此不存在是哪个开发同学直接在上面操作引起的，那么排查思路如下:</p>
<h4 id="df-x2F-du"><a href="#df-x2F-du" class="headerlink" title="df&#x2F;du"></a>df&#x2F;du</h4><p>根被打满，第一反应得查一下是不是在根目录下生成了大文件，du没发现问题,<strong>监控数据不会骗人</strong>, 多次执行df之后发现确认会到100%, 但是du又没找到是哪个大文件, 那么就可能是时间上很短，du没有抓到</p>
<h4 id="dmesg"><a href="#dmesg" class="headerlink" title="dmesg"></a>dmesg</h4><p>使用<code>Dmesg -T|less</code> 相关内核日志，发现有很多如下错误:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kernel: Pid 38693(python) over core_pipe_limit</span><br><span class="line">kernel: Skipping core dump</span><br><span class="line">kernel: systemd-coredump[11718]: Core file was truncated to 2147483648 bytes.</span><br></pre></td></tr></table></figure>

<p>显然，这跟一个Pid 为38693的python进程有关, 同时出现了coredump,dmesg里多次出现，那肯定不正常</p>
<h4 id="coredump"><a href="#coredump" class="headerlink" title="coredump"></a>coredump</h4><p>关于coredump在这里不展开描述，只提一下它的作用:</p>
<p>当程序发生内存越界访问等行为时，会触发OS的保护机制，此时OS会产生一个信号(signal)发送给对应的进程。当进程从内核态到用户态切换时，该进程会处理这个信号。此类信号（比如SEGV）的默认处理行为生成一个coredump文件<br>由于coredump会占用大量磁盘资源，所以计算节点应该都默认关闭了coredump功能, 通过以下命令确认:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ulimit</span> -c</span><br><span class="line"></span><br><span class="line"><span class="comment"># ulimit -c unlimited 不限制</span></span><br><span class="line"><span class="comment"># ulimit -c 1024      限制大小为1024</span></span><br><span class="line"><span class="comment"># ulimit -c 0         限制大小为0，即不输出core dump文件</span></span><br></pre></td></tr></table></figure>

<p>返回的是0, 说明coredump确认是关闭的,那奇怪为何会出现coredump呢? <strong>内核日志同样不会骗人</strong>, 从日志本身入手,<br>从日志中可以看到是个python进程，有pid,那么就可以查到是属于哪个容器的了，由于宿主机上跑的容器很多，所以更快的方法是: 使用coredumpctl命令查看发生coredump的进程信息,返回如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coredumpctl info</span></span><br><span class="line">PID: 38693 (python)</span><br><span class="line">UID: xxxx (xxxx)</span><br><span class="line">GID: xxxx (xxxx)</span><br><span class="line">Signal: 11 (SEGV)</span><br><span class="line">Timestamp: Wed 2023-07-28 19:58:05 CST</span><br><span class="line">Command Line: python xxx</span><br><span class="line">Executable: python </span><br><span class="line">Control Group: /kubepods.slice/...</span><br><span class="line"><span class="comment"># 省略...</span></span><br></pre></td></tr></table></figure>

<p>信息非常详细, 很容易拿到是哪个容器触发的coredump，<strong>由于都发生了coredump, 大概率这个python程序已经跑跪了</strong>，为了更快的地验证是不是这个容器导致宿主机根被打满，让相应的用户先将容器stop掉，发现宿主机的根目录果然降下来了且稳定在低水位。</p>
<h3 id="问题延伸"><a href="#问题延伸" class="headerlink" title="问题延伸"></a>问题延伸</h3><p>虽然快速地”解决了问题”,但产生了几个疑问:</p>
<ol>
<li>为什么宿主机上禁用了coredump还是发生了？</li>
<li>就算问题1成立，为什么占用的是宿主机上磁盘容量且是根目录?</li>
</ol>
<p>经过一篇追踪及搜索后, 领了几个技术盲区后能够解释上述问题</p>
<h4 id="为什么宿主机上禁用了coredump还是发生了？"><a href="#为什么宿主机上禁用了coredump还是发生了？" class="headerlink" title="为什么宿主机上禁用了coredump还是发生了？"></a>为什么宿主机上禁用了coredump还是发生了？</h4><p>首先, 虽然宿主机上禁用了coredump, 并不代表容器中就不能coredump,相反在容器中也是可以设置内核参数的,</p>
<p>所以, 给pod中的容器设置ulimit参数，可以通过以下方法:<br><strong>是在镜像中的初始化程序中调用setrlimit()系统调用来进行设置。子进程会继承父进程的ulimit参数</strong><br>这个很好排查，拿出现coredump问题的镜像，直接使用<code>docker run</code> 后发现<code>ulimit -c</code>返回的是<code>unlimited</code>,与宿主机不一样。</p>
<p>所以可以回答问题1, 即<strong>容器中能够使用coredump的原因是由于在容器中设置了<code>ulimit -c</code>参数</strong></p>
<p>根据问题1又衍生出了另一个问题: <strong>kubernetes中给容器设置某些内核参数，哪些是会与主机冲突？</strong></p>
<p>从上面的验证来看，至少<code>ulimit -c</code>不会覆盖宿主机的，要不然不存在问题1</p>
<p>其实，linux内核方面做了大量的工作，把一部分sysctl内核参数进行了namespace化(namespaced)。 也就是多个容器和主机可以各自独立设置某些内核参数。例如，可以通过<code>net.ipv4.ip_local_port_range</code>，在不同容器中设置不同的端口范围。<br>那如何判断一个参数是不是namespaced? </p>
<p>方式很简单: 运行一个具有privileged权限的容器，然后在容器中修改该参数，看一下在host上能否看到容器在中所做的修改。如果看不到， 那就是namespaced， 否则不是。</p>
<p>k8s还进一步把syctl参数分为safe和unsafe, safe的条件：</p>
<blockquote>
<ul>
<li>must not have any influence on any other pod on the node</li>
<li>must not allow to harm the node’s health</li>
<li>must not allow to gain CPU or memory resources outside of the resource limits of a pod.</li>
</ul>
</blockquote>
<p>非namespaced的参数肯定是unsafe。namespaced参数也只有一部分被认为是safe的。</p>
<p><strong>由于sysctl是由kubelet设置的</strong>，从kubelet的源码<code>pkg/kubelet/sysctl/namespace.go</code>来看，目前已经namespace化的sysctl内核参数如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kernel.shm*,</span><br><span class="line">kernel.msg*,</span><br><span class="line">kernel.sem,</span><br><span class="line">fs.mqueue.*,</span><br><span class="line">net.*.</span><br></pre></td></tr></table></figure>

<p>注意: vm并没有namespace化, 比如<code>vm.max_map_count</code> 在宿主机或者一个容器中设置它,其他所有容器都会受影响，都会看到最新的值。</p>
<p>在<code>pkg/kubelet/sysctl/safe_sysctls.go</code>中维护了safe sysctl参数的名单。目前只有三个参数被认为是safe的:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kernel.shm_rmid_forced,</span><br><span class="line">net.ipv4.ip_local_port_range,</span><br><span class="line">net.ipv4.tcp_syncookies</span><br></pre></td></tr></table></figure>

<h4 id="为什么占用的是宿主机上磁盘容量且是根目录"><a href="#为什么占用的是宿主机上磁盘容量且是根目录" class="headerlink" title="为什么占用的是宿主机上磁盘容量且是根目录?"></a>为什么占用的是宿主机上磁盘容量且是根目录?</h4><p>搞清了问题1, 那为什么是占用的是宿主机上的根目录容量呢, 接着查.<br>通过排查coredump发现, coredump可以有好几种方式, 在使用了systemd的系统上, 默认使用的是systemd-coredump,<br>关于systemd-coredump在这里不展开描述，只提一下它的作用:</p>
<p>systemd-coredump可收集并显示内核核心转储，用于分析应用程序崩溃问题。当某个进程（或属于应用程序的所有进程）崩溃时，此工具默认会将核心转储记录到 systemd 日记（如有可能还包括回溯），并将核心转储储存在<code>/var/lib/systemd/coredump</code>(默认路径) 中的某个文件内.</p>
<p>查看宿主机的<code>/var/lib/systemd/coredump</code>目录，果然发现有几个产生的core文件，且size很大.<br>systemd-coredump有如下几个重要配置文件,默认配置如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># /etc/systemd/coredump.conf</span><br><span class="line">[Coredump]</span><br><span class="line">#Storage=external</span><br><span class="line">#Compress=yes</span><br><span class="line">#ProcessSizeMax=2G</span><br><span class="line">#ExternalSizeMax=2G</span><br><span class="line">#JournalSizeMax=767M</span><br><span class="line">#MaxUse=</span><br><span class="line">#KeepFree=</span><br></pre></td></tr></table></figure>
<p>Storage可以有如下值:</p>
<blockquote>
<ul>
<li>none: 在日记中记录核心转储，但不储存。这样做有助于尽量减少敏感信息的收集与储存，例如，出于符合一般数据保护条例 (GDPR) 的目的。</li>
<li>external: 将核心储存在&#x2F;var&#x2F;lib&#x2F;systemd&#x2F;coredump中</li>
<li>journal: 将核心储存在 systemd 日记中</li>
</ul>
</blockquote>
<p>这里使用的是默认值external, 因此core文件存储在了&#x2F;var&#x2F;lib&#x2F;systemd&#x2F;coredump</p>
<p>另一个配置文件:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># /usr/lib/sysctl.d/50-coredusmp.conf</span><br><span class="line">kernel.core_pattern=|/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e</span><br><span class="line">kernel.core_pipe_limit=16</span><br></pre></td></tr></table></figure>
<p>第一行表明使用了<code>|</code>(管道,内核2.6.19就已支持)的方式.<br>第二行表示可以并发运行coredump的上限是16个, dmesg中看到的<code>over core_pipe_limit</code>就是超过了这个参数直接给skip了.</p>
<p>总结几个很重要的信息:</p>
<ol>
<li>使用管道的时候，内核会将 core 内容作为管道后程序的 stdin，然后通过新起一个内核线程来调用管道后的程序，此时，<strong>是以root用户来运行管道后的程序</strong></li>
<li><strong>使用管道时，dump的path目录解析是在系统初始化namespace中发生的，也就是宿主机的global ns 中，所以默认会将文件生成在宿主机的path目录中</strong></li>
<li><strong>同时由于kernel.core_pattern是一个未做隔离的变量，容器中和宿主机共享此值</strong></li>
</ol>
<p>所有配置结合起来就导致在容器中产生的coredump事件最后产生的core文件却生成了在宿主机上了<br>问题2的疑惑解决</p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>那需要不需要将在容器中产生的coredump直接保存在容器里呢?<br>答案是不建议，因为本身开启coredump的原因就是为了debug问题，在出现coredump时一般容器也可能就挂了，如果这时候coredump文件还保存在容器中,那也没办法拿到,开启coredump的初衷就不存在。<br>由于业务方需要开启coredump来定位问题这个情况客观存在, 也不能说一棒子打死就直接不给开放了,优雅一点通过以下几种方式解决:</p>
<h4 id="限制容器中ulimit参数"><a href="#限制容器中ulimit参数" class="headerlink" title="限制容器中ulimit参数"></a>限制容器中ulimit参数</h4><p>这次问题的本质是产生的coredump太大将宿主机根目录打满，那么可以在容器中限制一下ulimit的大小</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ulimit</span> -c <span class="comment"># 降低到128M</span></span><br></pre></td></tr></table></figure>

<h4 id="调整宿主机上systemd-coredump配置"><a href="#调整宿主机上systemd-coredump配置" class="headerlink" title="调整宿主机上systemd-coredump配置"></a>调整宿主机上systemd-coredump配置</h4><p>同时, 调整systemd-coredump的配置，限制并发数(<code>/usr/lib/sysctl.d/50-coredusmp.conf中的kernel.core_pipe_limit</code>参数), 同时将存储core文件的路径从根目录下迁移到大盘上或者直接设置为none</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># /etc/systemd/coredump.conf</span><br><span class="line">[Coredump]</span><br><span class="line">Storage=none # 或者是大盘路径</span><br><span class="line">#Compress=yes</span><br><span class="line">ProcessSizeMax=0</span><br><span class="line">#ExternalSizeMax=2G</span><br><span class="line">#JournalSizeMax=767M</span><br><span class="line">#MaxUse=</span><br><span class="line">#KeepFree=</span><br></pre></td></tr></table></figure>

<p>注意: 内核转储文件所占用的磁盘资源受两种不同方式的约束： (1)占用磁盘空间的大小受 &#x2F;etc&#x2F;systemd&#x2F;coredump.conf 配置文件以及对应的配置片段的约束； (2)占用磁盘时间的长短受 systemd-tmpfiles 配置的约束(对应的配置文件默认位于 &#x2F;usr&#x2F;lib&#x2F;tmpfiles.d&#x2F;systemd.conf)。</p>
<h3 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h3><p>这次出现的问题虽没有产生实质的影响,但还是发现不少可以提高稳定性的点, 这类问题不亲身遇到, 平时也没什么机会可以关注到,从点到线及面, <strong>出现问题不可怕, 解决能力很重要, 而且要快</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/</a></li>
<li><a href="https://www.jinbuguo.com/systemd/systemd-coredump.html">https://www.jinbuguo.com/systemd/systemd-coredump.html</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1583736">https://cloud.tencent.com/developer/article/1583736</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29135840">https://zhuanlan.zhihu.com/p/29135840</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/98106">https://github.com/kubernetes/kubernetes/issues/98106</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/3595">https://github.com/kubernetes/kubernetes/issues/3595</a></li>
<li><a href="https://cloud.redhat.com/blog/a-guide-to-core-dump-handling-in-openshift">https://cloud.redhat.com/blog/a-guide-to-core-dump-handling-in-openshift</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(rancher-local-path-provisioner)</title>
    <url>/2020/07/24/Kubernetes-Rancher-local-path-provisioner/</url>
    <content><![CDATA[<p>kubernetes提供的local path给持久化提供了相当大的便利, 但有一个问题是, 每次都需要手动提前在机器上创建相应的目录来做为被调度应用的持久化目录, 不是很方便, 有没有办法可以自动创建呢? </p>
<p>对于rancher平台，rancher也提供了相应的工具<a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner</a></p>
<span id="more"></span>

<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner-service-account</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner-role</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>, <span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;endpoints&quot;</span>, <span class="string">&quot;persistentvolumes&quot;</span>, <span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;*&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner-bind</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner-role</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner-service-account</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-provisioner</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">local-path-provisioner</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">local-path-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">local-path-provisioner-service-account</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">local-path-provisioner</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">rancher/local-path-provisioner:v0.0.14</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">local-path-provisioner</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--debug</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">start</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--config</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/etc/config/config.json</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/config/</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">local-path-config</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rancher.io/local-path</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">                &quot;nodePathMap&quot;:[</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                        &quot;node&quot;:&quot;DEFAULT_PATH_FOR_NON_LISTED_NODES&quot;,</span></span><br><span class="line"><span class="string">                        &quot;paths&quot;:[&quot;/data&quot;]</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">setup:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        mkdir -m 0777 -p $&#123;path&#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">teardown:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        rm -rf $&#123;path&#125;</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<p>yaml文件几乎可以不用动直接部署， 部署之后在集群中就会出现一个名为local-path的storeclass</p>
<p>这样，如果一个应用需要使用某台机器的本地目录做为持久化，需要二步操作</p>
<ol>
<li>调度到这台机器上</li>
<li>使用pvc时指定storeclass为local-path</li>
</ol>
<p>使用官方的例子</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-test</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volume-test</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:stable-alpine</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volv</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volv</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">local-path-pvc</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">local-path</span> <span class="comment"># 指定sc</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>kubectl apply -f </code></p>
<p>查看pv&#x2F;pvc绑定情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE</span><br><span class="line">pvc-bc3117d9-c6d3-11e8-b36d-7a42907dda78   2Gi        RWO            Delete           Bound     default/local-path-pvc   local-path               4s</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">local-path-pvc   Bound     pvc-bc3117d9-c6d3-11e8-b36d-7a42907dda78   2Gi        RWO            local-path     16s</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME          READY     STATUS    RESTARTS   AGE</span><br><span class="line">volume-test   1/1       Running   0          3s</span><br></pre></td></tr></table></figure>

<p>可以看到pv&#x2F;pvc绑定成功了且应用的持久化目录在机器的&#x2F;data目录下</p>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">                &quot;nodePathMap&quot;:[</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                        &quot;node&quot;:&quot;DEFAULT_PATH_FOR_NON_LISTED_NODES&quot;,</span></span><br><span class="line"><span class="string">                        &quot;paths&quot;:[&quot;/data&quot;]</span></span><br><span class="line"><span class="string">                &#125;,</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                        &quot;node&quot;:&quot;yasker-lp-dev1&quot;,</span></span><br><span class="line"><span class="string">                        &quot;paths&quot;:[&quot;/opt/local-path-provisioner&quot;, &quot;/data1&quot;]</span></span><br><span class="line"><span class="string">                &#125;,</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                        &quot;node&quot;:&quot;yasker-lp-dev3&quot;,</span></span><br><span class="line"><span class="string">                        &quot;paths&quot;:[]</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">setup:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        mkdir -m 0777 -p $&#123;path&#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">teardown:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        rm -rf $&#123;path&#125;</span></span><br></pre></td></tr></table></figure>

<p>配置里是可以对不同的node指定不同的path, 如果未指定node， 则使用默认值</p>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>通过这种方式我们可以不再需要生成pv, 直接使用pvc即可，但还是有些不足的地方,目前还不支持对持久化目录做到可以限制使用大小</p>
<p>另外，kubernetes sig也出了一个对于local volume的工具<a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">sig-storage-local-static-provisioner</a>相对来说更加通用，但是也有不足的地方,感兴趣的可以参考</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/rancher/local-path-provisioner/blob/master/README.md">https://github.com/rancher/local-path-provisioner/blob/master/README.md</a></li>
<li><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s的资源模板生成工具-boilr)</title>
    <url>/2020/12/26/Kubernetes-Resource-Templates-Boilr/</url>
    <content><![CDATA[<p>相信在使用k8s的过程中，对于各种资源的操作一定是最多的，社区包括kubernetes本身出了多种模板生成工具，比如helm, kustomize这两种是最常用的，本人也一直在用，但有的场景依然会觉得这两个工具相对较重，helm使用的是gotemplate，而kustomize则又依赖于kubectl，两者都有一定的学习成本且没有交互能力，最近在做的一个小需求是: 如何快速地让开发同学以最简单的方式根据提供的模板生成部署文件，github上发现了个<code>boilr</code>的工具，虽然工具老了点，但是足够地小，功能足够简单，重要的是有交互功能，比较贴合需求</p>
<span id="more"></span>

<p>boilr已经很久很久没更新了，很功能上看，boilr也引用了仓库的概念，本人相信boilr的作者是想把这个工具做的很NB</p>
<p>当然,从Readme看，也非常简洁，用法也相对简单</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>根据readme直接下载二进制文件即可使用</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>几乎所有的模板系统的生成思路都一样的，<strong>根据给定的k-v去替换相应的占位符</strong>，boilr也不例外</p>
<p>在使用前，先下载个模板文件，这里就使用官方的例子:</p>
<p>使用之前需要先初始化boilr的相关目录, 默认情况下，会生成&#x2F;root&#x2F;.config&#x2F;boilr&#x2F;templates目录，这个可以从源代码看到(configuration.go)[<a href="https://github.com/zhoushuke/boilr/blob/master/pkg/boilr/configuration.go]">https://github.com/zhoushuke/boilr/blob/master/pkg/boilr/configuration.go]</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">boilr init .</span><br></pre></td></tr></table></figure>

<p>然后就可以下载模板目录了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">boilr template download littlemanco/boilr-k8sservice k8sservice</span><br></pre></td></tr></table></figure>

<p>注意: 因为boilr很久都没有维护了，在download时可能会遇到<code>pkt-line 3: invalid capabilities</code>,这是由于boilr代码里使用的<code>go-git</code>相对于现在来说，版本太旧了，所以可以直接通过git clone 把模板下下来，是一样的效果</p>
<p>通过git clone下载下来后需要把<code>boilr-k8sservice</code>放到<code>/root/.config/boilr/templates</code>目录下</p>
<p>这里最主要的是有个<code>project.json</code>文件</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;Namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;product&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Service&quot;</span><span class="punctuation">:</span> <span class="string">&quot;product&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Application&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nginx&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;webserver&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ApplicationPortProtocol&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;TCP&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;UDP&quot;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ApplicationPortName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ApplicationPort&quot;</span><span class="punctuation">:</span> <span class="number">80</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;Type&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;ClusterIP&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;NodePort&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;LoadBalancer&quot;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;UsePrometheus&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;true&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;false&quot;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>通过解析这个文件里的字段进行模板文件的替换，文件中的value都是默认值，如果是列表的话，在交互时可提供选择,还是很方便的</p>
<p>使用以下命令指定使用该模板</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">boilr template use k8sservice /tmp/deploy</span><br></pre></td></tr></table></figure>

<p>后面的<code>/tmp/deploy</code>是最终生成的模板文件保存的目录，我这里修改了点东西</p>
<p>详细的交互输出如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">以下是详细输出内容:</span><br><span class="line">​```bash</span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;module_name&quot;</span> [default: <span class="string">&quot;test-frontend&quot;</span>]: product</span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;config_name&quot;</span> [default: <span class="string">&quot;config.yml&quot;</span>]: </span><br><span class="line">[✔] Created deploy/base/configmap.yml.j2</span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;registry_host&quot;</span> [default: <span class="string">&quot;&#123;&#123;registry_host&#125;&#125;&quot;</span>]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;registry_project&quot;</span> [default: <span class="string">&quot;&#123;&#123;registry_project&#125;&#125;&quot;</span>]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;registry_image&quot;</span> [default: <span class="string">&quot;&#123;&#123;registry_image&#125;&#125;&quot;</span>]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;registry_tag&quot;</span> [default: <span class="string">&quot;&#123;&#123;registry_tag&#125;&#125;&quot;</span>]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;port_number&quot;</span> [default: 8080]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;config_path&quot;</span> [default: <span class="string">&quot;/app/services&quot;</span>]: </span><br><span class="line">[✔] Created deploy/base/deployment.yml.j2</span><br><span class="line">[?] Please choose an option <span class="keyword">for</span> <span class="string">&quot;port_name&quot;</span> </span><br><span class="line">    1 - <span class="string">&quot;http&quot;</span></span><br><span class="line">    2 - <span class="string">&quot;grpc&quot;</span></span><br><span class="line">    Choose from 1..2 [default: 1]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;port_protocol&quot;</span> [default: <span class="string">&quot;TCP&quot;</span>]: </span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;service_type&quot;</span> [default: <span class="string">&quot;ClusterIP&quot;</span>]: </span><br><span class="line">[✔] Created deploy/base/service.yml.j2</span><br><span class="line">[?] Please choose a value <span class="keyword">for</span> <span class="string">&quot;namespace_name&quot;</span> [default: <span class="string">&quot;prod&quot;</span>]: </span><br><span class="line">[✔] Created deploy/vars/main.yml</span><br><span class="line">[✔] Successfully executed the project template sensespring <span class="keyword">in</span> /tmp/xpath</span><br></pre></td></tr></table></figure>



<h3 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h3><p>通过上面的使用，为了减少对环境的依赖，可以将模板文件打包到镜像中，这样开发同学就可以直接使用镜像进行生成，还是很方便的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM alpine:latest</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache tzdata bash ca-certificates</span><br><span class="line"></span><br><span class="line">ADD boilr /usr/local/bin/boilr</span><br><span class="line"></span><br><span class="line">RUN <span class="built_in">mkdir</span> -p /root/.config/boilr &amp;&amp; \</span><br><span class="line">    <span class="built_in">chmod</span> +x /usr/local/bin/boilr</span><br><span class="line"></span><br><span class="line">ADD templates /root/.config/boilr/templates</span><br><span class="line"></span><br><span class="line">WORKDIR /tmp</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [ <span class="string">&quot;boilr&quot;</span>, <span class="string">&quot;template&quot;</span>, <span class="string">&quot;use&quot;</span>, <span class="string">&quot;your-template&quot;</span>, <span class="string">&quot;/tmp/xpath&quot;</span> ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker build</span></span><br><span class="line"><span class="comment">#   docker build --no-cache -t xgenerator:v0.1 .</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># docker run</span></span><br><span class="line"><span class="comment">#   docker run -it -v /tmp/xpath:/tmp/xpath --rm 1f6d7ed63c0a</span></span><br></pre></td></tr></table></figure>

<p>通过这种方式，如果模板后续有变动，也可以通过调整镜像的方式推送到开发同学手中，很容易维护，在CI&#x2F;CD中又打通了一环</p>
<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>boilr的源码还是非常清楚的，可能是因为逻辑相对比较简单，本人不是专业开发，理解起来也不是很难.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/tmrts/boilr">https://github.com/tmrts/boilr</a></li>
<li><a href="https://github.com/littlemanco/boilr-k8sdeployment/blob/master/README.rst">https://github.com/littlemanco/boilr-k8sdeployment/blob/master/README.rst</a></li>
<li><a href="https://github.com/littlemanco/boilr-k8sservice/blob/master/README.rst">https://github.com/littlemanco/boilr-k8sservice/blob/master/README.rst</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kubernetes中的OOM-killer和应用程序运行时含义)</title>
    <url>/2023/02/09/Kubernetes-Out-Of-Memory-1/</url>
    <content><![CDATA[<p>最过在深入排查oom问题时有幸看到一个在kubernetes中探讨oom-killer问题的文章，我觉得写得非常详尽且解答了本人的诸多疑惑，遂决定翻译成中文，方便日后求解。</p>
<p>在翻译的过程中，我会尽可能地使用原文的意思，同时也会补充一些知识，同时会添加一些的本人的理解</p>
<span id="more"></span>

<h2 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h2><p>以下是原文信息，感兴趣且英文好的可直接阅读</p>
<p>原文: <a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/">Kubernetes中的内存不足（OOM）–第2部分：OOM-Killer和应用程序运行时含义</a></p>
<p>作者: <a href="https://mihai-albert.com/author/mihaialbert/">Mihai Albert</a></p>
<p>系列总共由四部分组成的文章，本人将对第2、4章节进行翻译，每章的大概内容如下:</p>
<blockquote>
<ul>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-1-intro-and-topics-discussed/">第一章</a>: 主要列了一下这个系统的主要内容，相当于大纲</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/">第二章</a>: oom-killer机制及在应用部署在kubernetes中如何处理oom事件</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/">第三章</a>: 非常详细的介绍了kubernetes中的内存相关的metrics items及常用收集工具的使用</p>
</li>
<li><p><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/">第四章</a>: kubernetes中驱逐机制及oom-killer如何协调响应</p>
</li>
</ul>
</blockquote>
<p>作者将对第2章及第4章进行翻译，本文是第2章: <strong>oom-killer及衣服洗程序运行态含义</strong></p>
<p>以下是我【为什么翻译这个系列】前碰到的问题,先交代一下背景</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>某台node的kubelet中出现如下的错误日志:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394489] dotnet invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=1000</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394506]  oom_kill_process+0xe6/0x120</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394642] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=f90b24151029555d49a49d82159ec90c4fec53ba8515bd51a5633d1ff45d8f53,mems_allowed=0,oom_memcg=/kubepods,task_memcg=/kubepods/besteffort/pod5f3d2447-f535-4b3d-979c-216d4980cc3f/f90b24151029555d49a49d82159ec90c4fec53ba8515bd51a5633d1ff45d8f53,task=dotnet,pid=20341,uid=0</span><br><span class="line">- Jan 14 18:01:55 aks-agentpool-20086390-vmss00003C kernel: [ 1432.394676] Memory cgroup out of memory: Killed process 20341 (dotnet) total-vm:172181696kB, anon-rss:4776416kB, file-rss:25296kB, shmem-rss:0kB, UID:0 pgtables:9620kB oom_score_adj:1000</span><br><span class="line">- Jan 14 18:02:17 aks-agentpool-20086390-vmss00003C kubelet[3044]: I0114 18:02:17.686538    3044 kuberuntime_container.go:661] <span class="string">&quot;Killing container with a grace period override&quot;</span> pod=<span class="string">&quot;alloc-tests/alloc-mem-leak&quot;</span> podUID=5f3d2447-f535-4b3d-979c-216d4980cc3f containerName=<span class="string">&quot;alloc-mem&quot;</span> containerID=<span class="string">&quot;containerd://d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616&quot;</span> gracePeriod=0</span><br><span class="line">- Jan 14 18:02:17 aks-agentpool-20086390-vmss00003C containerd[2758]: time=<span class="string">&quot;2022-01-14T18:02:17.687846041Z&quot;</span> level=info msg=<span class="string">&quot;Kill container \&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\&quot;&quot;</span></span><br><span class="line">- Jan 14 18:02:18 aks-agentpool-20086390-vmss00003C kubelet[3044]: I0114 18:02:18.923106    3044 kubelet_pods.go:1285] <span class="string">&quot;Killing unwanted pod&quot;</span> podName=<span class="string">&quot;alloc-mem-leak&quot;</span></span><br><span class="line">- Jan 14 18:02:18 aks-agentpool-20086390-vmss00003C kubelet[3044]: E0114 18:02:18.924926    3044 kuberuntime_container.go:691] <span class="string">&quot;Kill container failed&quot;</span> err=<span class="string">&quot;rpc error: code = NotFound desc = an error occurred when try to find container \&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\&quot;: not found&quot;</span> pod=<span class="string">&quot;alloc-tests/alloc-mem-leak&quot;</span> podUID=5f3d2447-f535-4b3d-979c-216d4980cc3f containerName=<span class="string">&quot;alloc-mem&quot;</span> containerID=&#123;Type:containerd ID:d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616&#125;</span><br><span class="line">- Jan 14 18:02:19 aks-agentpool-20086390-vmss00003C kubelet[3044]: E0114 18:02:19.001858    3044 kubelet_pods.go:1288] <span class="string">&quot;Failed killing the pod&quot;</span> err=<span class="string">&quot;failed to \&quot;KillContainer\&quot; for \&quot;alloc-mem\&quot; with KillContainerError: \&quot;rpc error: code = NotFound desc = an error occurred when try to find container \\\&quot;d3f3b2f7f02b832711593044c30a165bd991b4af5b1eadbb0c6d313d57660616\\\&quot;: not found\&quot;&quot;</span> podName=<span class="string">&quot;alloc-mem-leak&quot;</span></span><br></pre></td></tr></table></figure>

<p>从日志其实很容易发现问题，就是有app的内存超限被oom了, 重点在于日志中的最后几行，<strong>为什么kubelet会提示Failed killing the pod， container Not Found</strong></p>
<p>问题: <strong>难道app被oom不是通过kubelet吗？</strong></p>
<p>由于篇幅较长，在翻译开始前，本人将对本章内容给出本人认为比较重要的几条结论</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="overcommit"><a href="#overcommit" class="headerlink" title="overcommit"></a>overcommit</h3><blockquote>
<ul>
<li><p>设置overcommit后，系统对内存的处理是使用时分配，而不是申明时分配</p>
</li>
<li><p>即使关闭了overcommit，当系统内存不足时，OOM-killer仍将被调用。它的任务是在内存不足时杀死进程，它并不特别关心overcommit是否打开</p>
</li>
</ul>
</blockquote>
<h3 id="cgroups"><a href="#cgroups" class="headerlink" title="cgroups"></a>cgroups</h3><blockquote>
<ul>
<li><p>cgroups不限制进程可以<code>看到</code>多少资源，而是限制它可以<code>使用</code>多少资源</p>
</li>
<li><p>一旦启用分层计算(memory.use_hierarchy)，root cgroup包含该计算机上运行的所有进程的统计信息</p>
</li>
<li><p>容器中所有的进程都属于一个cgroup组，比如通过<code>kubectl exec</code>进入pod中运行一个<code>bash</code>，则bash运行的进程都将成为pod现有cgroup组的成员</p>
</li>
<li><p>当cgroup组整体超过其设置的limit时，内核首先会尝试从cgroup内部回收内存，如果回收不成功，将调用OOM程序来选择(打分)并终止cgroup内<code>最庞大</code>的任务</p>
</li>
<li><p>一旦突破了cgroup组超过limit，oom-killer一定会选择一个进程kill掉，但是被Kill掉的进程不一定是我们主客认为的最有可能被Kill的那个进程</p>
</li>
</ul>
</blockquote>
<h3 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h3><blockquote>
<ul>
<li><p>在Kubernetes中，只有当pid为1的程序为OOM-killer杀死时，Containers才会被标记为OOM killed, 有些应用程序可以容忍非init进程的OOM kill，因此现在kubernetes并不会跟踪非init进程OOM kill事件，目前认为是预期的现象</p>
</li>
<li><p>当您为Pod中的容器指定资源请求时，kube-schedule将使用此信息来决定将Pod放置在哪个节点上。当您为容器指定资源限制时，kubelet将强制执行这些限制，以便运行的容器不允许使用超过您设置的限制的资源</p>
</li>
</ul>
</blockquote>
<p>注意: </p>
<ol>
<li><p>原作者在原文中添加了一些视频演示来复现实现，由于时长较多，没有放到翻译中，请自行到原文中查看</p>
</li>
<li><p>在原文中，原作者为了更方便理解，举了一些生活中的例子来说明，为了减少篇幅，在不影响理解的情况下，去掉了一些例子的翻译</p>
</li>
<li><p>由于原作者在写该系列文章时出现的结论在现在看来已经过时，因此作者会进行适当的调整，以满足现阶段的状态</p>
</li>
</ol>
<h2 id="翻译全文"><a href="#翻译全文" class="headerlink" title="翻译全文"></a>翻译全文</h2><h3 id="overcommit-1"><a href="#overcommit-1" class="headerlink" title="overcommit"></a>overcommit</h3><p><strong>作者注</strong>: </p>
<ol>
<li><p>这里将overcommit翻译为<strong>超量</strong>提交,为了便于理解，以下内容都直接使用overcommit</p>
</li>
<li><p>overcommit是操作系统的一个参数，可进行配置,控制内存的分配</p>
</li>
</ol>
<p>如果大家像我一样熟悉Windows操作系统，首先我们需要讨论一下overcommit意味着什么，否则接下来的一些内容将毫无意义。简单地说，<strong>操作系统分配给进程的内存超过了它可以安全保证的内存</strong>。</p>
<p>举个例子：我们有一个运行着12 GB RAM和4 GB swap的Linux系统。该系统可以是裸机、VM、在Windows上运行的WSL发行版等，因为从overcommit的角度来看，操作系统并不重要。假设OS的内核和运行的各种组件使用1 GB。进程A出现并请求分配8 GB。OS（操作系统）欣然确认，进程A在其虚拟地址空间中获得了一个8GB的区域，可以使用。下一个进程B出现并请求9 GB内存。启用了overcommit（通常是默认设置）后，操作系统也很乐意成功处理此请求，因此现在进程B在其自己的虚拟地址空间中有一个9 GB的区域。当然，如果我们将1+8+9相加，这将超过操作系统知道它可以使用的内存总量（12+4），但只要进程A和B不需要一次性使用所有内存，一切都很好。</p>
<p>因为Linux（无论是否使用overcommit）不会急于为所有分配的内存创建内存页，并想着“如果我请求的内存实际上永远不会被占用呢？”。换句话说，<strong>操作系统对内存是使用时才分配</strong></p>
<p>为了说明overcommit，这里有一个Linux系统，它配置了大约12 GB的RAM和4 GB的swap，并设置为总是overcommit，很高兴地分配了1 TB并继续使用。左侧突出显示的两个值分别是RAM和交换的数量，而右下方的一个值是工具迄今为止成功分配的数量：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2021/11/linux_overcommit_1tb.png"><img src="https://luckerbyhome.files.wordpress.com/2021/11/linux_overcommit_1tb.png?w=1024"></a></p>
<p>图1–overcommit的表现</p>
<p>有三个值可以控制overcommit的行为，下面将对它们进行<a href="https://www.kernel.org/doc/Documentation/vm/overcommit-accounting.">解释</a></p>
<p>可以通过使用<code>sysctl vm.overcommit_memory=&lt;value&gt;</code>动态修改它的值：</p>
<blockquote>
<ul>
<li><p>0：启用了overcommit，但Linux会根据需要进行调整</p>
</li>
<li><p>1：在满足任何内存分配请求的意义上总是使用overcommit。上面看到的分配1 TB的系统是这样配置的</p>
</li>
<li><p>2：不要使用overcommit。有一个给定的提交限制(commit limit)，操作系统不会超过该限制</p>
</li>
</ul>
</blockquote>
<p>如果您使用Windows的时间足够长，您会发现上面的“模式2”与Linux的行为类似。我们已经超过了Windows中的提交限制(commit limit),<a href="https://mihai-albert.com/2019/04/21/out-of-memory-exception/#your-resevevation-now-is-confirmd%EF%BC%89%E3%80%82%E5%A6%82%E6%9E%9C%E6%82%A8%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E4%BA%86%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3[%E6%AD%A4%E5%A4%84](https://www.kernel.org/doc/Documentation/vm/overcommit-accounting">此处</a>，您注意到了&#96;vm.overcommit_rat提交限制(commit limit)定在“模式2”中计算提交限制时包含的物理RAM的百分比。默认值为50%，这意味着Linux将以swap大小加上物理RAM的50%来限制分配。</p>
<p>让我们通过设置<code>vm.overcommit_memory=2</code>不使用overcommit,这里的配置显示了相同数量的物理RAM（约12 GB）和swap（4 GB）以及我们的新的overcommit设置。</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2021/11/testnoovercommit_params.png"><img src="https://luckerbyhome.files.wordpress.com/2021/11/testnoovercommit_params.png?w=1017"></a>图2–不使用overcommit</p>
<p>所示的提交限制(commit limit)值计算为50%x ~12 GB+4 GB，略高于10 GB。当然，我们不期望能够使用我们的测试工具进行那么多的分配，因为内核和其他已经运行的程序已经进行了一些分配。它的价值体现在“Committed_AS”值中，该值接近4 GB。因此，我们希望能够在被拒绝额外内存之前分配大约6 GB：</p>
<p><img src="https://luckerbyhome.files.wordpress.com/2021/11/testnoovercommit_outcome1.png"></p>
<p>图3–不使用overcommit时内存分配失败速度更快</p>
<p>事实已经证明。</p>
<p>注意，在<code>vm.overcommit_ratio</code>低于100%–例如，将其设置为200%，尽管<code>vm.overcommit_ratio</code>设置为一个记录为禁用overcommit的值，但我们还是回到了overcommit。将比率从50%修改为200%后，测试：</p>
<p><a href="https://luckerbyhome.files.wordpress.com/2021/11/testnoovercommit_ratio_over_one.png"><img src="https://luckerbyhome.files.wordpress.com/2021/11/testnoovercommit_ratio_over_one.png?w=1024"></a></p>
<p>图4–修改使用overcommit参数</p>
<p>综上所述：从内存管理的角度来看，将overcommit设置为2，比率值为100%，以指定物理RAM+swap的提交限制(commit limit)时，Linux的工作方式与Windows分配多少内存一致。但这并不是默认情况下Linux的工作方式，在使用overcommit下，它将允许内存分配，而无需使用实际内存存储进行备份。如果所有进程都表现得很好，并且实际上没有尝试使用大量（成功）分配的内存，那么一切都很好；当游戏不再是这种情况时，应用程序开始被拒绝访问本应属于他们自己的内存（但自从他们分配了内存之后，还没有真正使用到这些内存）。</p>
<p>但为什么一开始就使用overcommit？至少有两个原因：</p>
<blockquote>
<ul>
<li><p>1）适应那些分配大量内存的应用程序，不完全使用所有内存，但如果不能首先分配内存块，则中断（显然在Linux下有相当多的应用程序）</p>
</li>
<li><p>2） 当fork（这是Linux系统上所有用户进程启动的唯一方式）并且各个进程已经分配了大量内存时，这又涉及到复制“源”进程的虚拟地址空间，进而有可能将系统的总提交大小增加到提交限制(commit limit)以上,<a href="https://stackoverflow.com/a/4597931/5853218">此处详细介绍</a>,不管新进程是单独使用任何新内存；如果启用并正确设置了overc提交限制(commit limit)t，那么就不会有超出提交限制的风险，因为这已经足够高了（或者没有考虑到）。</p>
</li>
</ul>
</blockquote>
<p>overcommit是好事吗？我可以看到赞成和反对的论点，但我可能在形成观点方面有偏见，因为我首先学习的Windows模型不允许这种机制。然而，围绕这一问题存在着激烈的<a href="%5Bhttps://lwn.net/Articles/104179/%5D(https://lwn.net/Articles/104179/">争论</a> </p>
<h3 id="OOM-killer"><a href="#OOM-killer" class="headerlink" title="OOM-killer"></a>OOM-killer</h3><p>我们需要解决的另一个重要问题是OOM-killer。<strong>OOM-killer是什么？这是一个Linux组件，它的任务是监视系统内存严重不足的情况，并在发生这种情况时采取严厉措施以摆脱这种状态。它如何释放内存？它只需根据特定的指标杀死一个进程，然后继续这样做，直到确定系统内存不再严重不足</strong>。有清晰简洁文档描述OOM-killer是如何<a href="https://www.kernel.org/doc/gorman/html/understand/understand016.html">运作</a></p>
<p>我们调用OOM-killer。非常简单，我们必须使其在目标Linux系统上的实际可用内存尽可能接近0。这反过来导致触发OOM-killer，试图释放一些内存并保持稳定。因此，我们的任务是用尽操作系统可以用来备份分配的内存：RAM和swap</p>
<p>首先，我们设置系统使用overcommit。通过这种方式，我们可以触发OOM-killer。接下来，我们将使用与之前相同的工具(一个内存泄漏程序)来分配内存，但这次我们将触及其中的很大一部分，只是为了确保减少可用内存。选择了50%，因此对于每个100 MB的块，我们将向其中的50 MB写入数据。让我们在下面的操作中看到这一点，并关注htop中的内存和swap使用情况: </p>
<p><del>视频1: 调用oom-killer</del></p>
<p>该进程正在写入正在分配的所有内存的一半。这一数量最终超过了物理RAM和swap容量，因此系统根本没有其他地方来分配更多内存。如果没有足够的内存来操作，内核就会调用OOM-killer，控制台收到一条“_Killed_”消息，而左边的内核日OOM-killer细地说明了OOM-killer执行的决策和操作。</p>
<p>关键是确定系统何时真正内存不足。OOM-killer会在[图1]中的场景中被调用吗？不会，至少不会很快，因为系统可以使用的实际内存仍然很高。我们确实分配了1 TB的内存——在使用<code>overcommit</code>模式下，<strong>这意味着每一个内存请求都会得到批准——但我们并没有真正使用这些内存</strong>，所以操作系统一开始就没有真正构建内存页面。因此，1 TB的进程既没有分配物理RAM也没有分配swap。</p>
<p>请记住，overcommit允许进程分配内存，但当需要使用这些内存时，它们都在为操作系统拥有的实际有限内存资源（物理RAM和swap）而斗争。</p>
<p><strong>即使关闭了overcommit（因此硬提交限制(commit limit)等于物理内存加交换的总和），当系统内存不足时，OOM-killer仍将被调用。请记住，它的任务是在内存不足时杀死进程，它并不特别关心过度提交是否打开</strong>。因此，即使关闭了提交限制(commit limit)ommit，但设置了提交限制，使得系统可能会遇到低内存问题（例如，<code>vm.overcommit_memory=2</code> and <code>vm.overcommit_ratio=100</code>），并且分配的块将触及其100%内存，让我们仔细检查一下：</p>
<p><del>视频2: 即使关闭了overcommit，也会调用OOM-killer</del></p>
<p>您可能会发现对显式禁用OOM killer(而不是简单地禁用overcommit)，例如<code>vm.oom_kill</code>。不幸的是，到目前为止（2021 11月），这似乎已经不存在了（至少在最新版本的Ubuntu上，尝试运行sysctl配置它时会出错），文档中也没有。然后，你可能会问，要禁用OOM-killer吗？这似乎不是一种直接的方法，尽管可以间接地做到这一点，方法是确保触发OOM杀手的条件在第一个地方永远不被满足。。</p>
<p>我们已经看到，将提交限制(commit limit)限制为RAM大小加上swap仍然可以让OOM-killer运行，因为本质上没有故障保提交限制(commit limit)；但您可以进一步降低提交限提交限制(commit limit)在实际使用分配的所有内存时提供可用空间。您应该将提交限制推到多低？可能不是很低，因为那时你会有足够的缓冲区——从大容量的RAM的意义上来说——你可能永远也不会使用上被你标记为缓冲区的内存。</p>
<p>下面是一个极端例子，当最大提交限制(commit limit)仅设置为swap的大小时。请注意，分配器工具只能使用与交换大小相等的RAM，而系统永远无法使用超过4GB的RAM。是什么把应用停止了呢？ 提交限制(commit limit)达到了（相对较小的）提交限制，操作系统不再分配内存块：</p>
<p><del>视频3–不再调用OOM-killer，但RAM被浪费</del></p>
<p>但总的来说，<strong>如果你遇到了OOM-killer，也许最好直接解决OOM问题的根本原因</strong></p>
<p>但是，假设系统内存不足同时OOM-killer不工作，那么系统会如何运行呢？</p>
<p>事实证明，有一种简单的方法可以进入这种状态：正常启动Linux系统（例如<code>vm.overcommit_memory=0</code>），消耗大量RAM，但不要过于极端——比如70%，然后切换<code>vm.overcommit_memory</code>设置为2，并使用足够低的<code>vm.overcommitratio</code>值默认值为50%。这实际上会使系统进入一种状态，即分配的内容（<code>/proc/meminfo</code>中的<code>Committed_AS </code>）大于提交限制(commit limit)（<code>/proc.meminfo</code>的<code>CommitLimit </code>）</p>
<p>尝试运行任何命令，您应该得到：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@DESKTOP-VOD028C:/var/log<span class="comment"># tail -f kern.log</span></span><br><span class="line">-bash: fork: Cannot allocate memory</span><br></pre></td></tr></table></figure>

<p>请记住，<a href="(https://man7.org/linux/man-pages/man2/fork.2.html)">fork</a>需要复制进程地址空间，父级的整个虚拟地址空间在子级中复制, 此外，在Linux下，fork是使用写页复制来实现的，因此它所带来的唯一代价是复制父页表所需的时间和内存”——这意味着最初不会消耗实际的新内存，</p>
<p>因为在我们实验场景中，我们基本上已经立即超过了提交限制，因为不能再分配了，</p>
<p>我们基本上已经立即超过了提交限制(commit limit)，现在Linux拒绝任何新的内存分配，因为提交大小突然变得太高。另一方面，OOM-killer也出现中——为什么会出现这样的情况，因为内存（RAM和swap）仍有很内存可用？</p>
<h3 id="cgroups-1"><a href="#cgroups-1" class="headerlink" title="cgroups"></a>cgroups</h3><p>请注意，本节不是对cgroups的介绍。有很多优秀的文章已经介绍了这一点，例如<a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups手册页</a>. 对于容器，请查看<a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction">这篇写得很好的文章</a>由Red Hat提供。</p>
<p>下面的段落试图通过这些cgroups概念来理解某些度量是如何提取的，并解释Kubernetes在内存不足的情况下，内部是如何工作处理的</p>
<p>什么是cgroup？我们为什么关心它们？</p>
<p>cgroups是Linux上用于限制和计算资源的机制 </p>
<p><strong>限制</strong>，如防止进程使用的资源多于分配给它的资源</p>
<p><strong>计算</strong>，如计算一个进程及其子进程在某个特定时间点从某个资源类型使用了多少资源。</p>
<p>为什么cgroups很重要？因为容器的整个概念都建立在它之上。容器的限制是通过cgroups在OS级别强制执行的，容器的内存使用情况信息是从cgroups获得的。</p>
<p>需要记住的一个重要方面是，<strong>cgroups不限制进程在资源方面可以“看到”什么，而是限制它可以使用什么</strong>。这篇精彩的文章:<a href="https://ops.tips/blog/why-top-inside-container-wrong-memory/">为什么顶部和自由内部容器不能显示正确的容器内存</a>解释了为什么从容器中查看空闲内存并不会报告它是否超出了其设置的限制，而是返回底层OS可用内存（加分：本文还讨论了当容器内运行的进程试图分配内存时，如何在后台工作，以及如何跟踪这些操作）。</p>
<p>让我们问一个相当奇怪的问题： <code>root cgroup 是显示所有进程的数据，还是只显示容器的数据？容器本身是由进程支持的</code>（请参阅上面引用的Red Hat文章了解容器简介），因此问题可以重新表述为<code>root cgroup是否显示所有进程的数据，还是仅显示属于容器的那些进程的数据？</code>。让我们试着回答这个问题。</p>
<p>根据<a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups</a>, cgroup文件系统最初包含一个root cgroup，即<code>/</code>。如果您检查root cgroup下的进程（对于任何cgroup控制器，如内存），您很可能会发现它们与 <code>ps aux</code>中列出的进程不同（使用根shell返回该主机上运行的所有进程）事实证明，进程只能属于特定的cgroup，而root cgroup并不特殊。因此，如果一个进程是cgroup控制器层次结构深处某个cgroup的一部分，那么该进程将不在相应控制器的root cgroup中。通过比较<code>ps-aux|wc-l</code>和<code>find /sys/fs/cgroup/memory-namecgroup</code>的输出，可以很容易地测试所有进程（&#x2F;threads）是否都在层次结构中。<code>cgroup.procs -exec cat &#39;&#123;&#125;&#39;\；|wc-l</code>，它将给出相同的值（当然，如果一个接一个执行得足够近）。</p>
<p>此时，您可能会认为问题的答案是“否”，因为毕竟root cgroup中的进程列表只是机器上运行的进程的子集。这就是cgroups分级核算的特点（cgroups文档的<a href="%5Bhttps://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt%5D(https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt)">第6节</a>. 事实证明，确实启用了分层计算)</p>
<p>事实证明，一旦启用（通过查看root cgroup中的<code>memory.use_hierarchy</code>文件并看到它的值为1，可以很容易地验证），值就不能轻易更改</p>
<p>如果cgroup下面已经创建了其他cgroup，或者父cgroup使用了_hierarchy enabled_，那么启用&#x2F;禁用将失败（如果确实要验证这一点，那么只需运行&#96;find &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory-name memory.use_hierarchy -exec cat ‘{}’ \；并生成一长串1），因此root cgroup内存值确实涉及所有进程）。因此，答案是，root cgroup确实包含该计算机上运行的所有进程的统计信息，只要启用了分层计算。</p>
<p>Kubernetes根据memory root cgroup的统计数据计算节点级别的节点内存使用情况。如上所述，memory root cgroup可以安全地用于获取有关在相应节点上运行的所有进程（包括操作系统和系统组件）的内存使用信息。</p>
<h3 id="cgroups及Kubernetes"><a href="#cgroups及Kubernetes" class="headerlink" title="cgroups及Kubernetes"></a>cgroups及Kubernetes</h3><p>让我们来谈谈涉及cgroups和Kubernetes的一些事情</p>
<p>第一, 在Kubernetes中创建的每个容器是否只有一个cgroup？事实上，只有一个,从<a href="https://stackoverflow.com/questions/62716970/are-the-container-in-a-kubernetes-pod-part-of-same-cgroup">这里</a>以获得完整的示例说明。</p>
<p>第二，有一个概念，<code>pause container</code>。这在本文中得到了很好的<a href="https://www.ianlewis.org/en/almighty-pause-container">解释</a>. 由于每个Kubernetes pod中都会有一个<code>pause container</code>，因此值得花时间去了解它，这样以后我们就会知道为什么每个pod都会看到一个额外容器的指标（其内存使用值会非常低）。</p>
<p>另一件有趣的事情是能够导航到内存控制器层次结构中的容器cgroup以检查统计信息。为此，您需要提取有关这些容器的详细信息。有几种方法可以解决这个问题。一个您应该使用CRI兼容的runtime，就是使用这里描述的crictl: [verify-pod-cgroup-limits](<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/#verify-pod-cgroup-limits%EF%BC%89%E3%80%82%E5%8F%A6%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%A6%82%E6%9E%9C%E6%82%A8%E4%BD%BF%E7%94%A8containerd%E4%BD%9C%E4%B8%BA%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6%EF%BC%88%E5%B0%B1%E5%83%8FAKS%E6%9A%82%E6%97%B6%E6%89%80%E5%81%9A%E7%9A%84%E9%82%A3%E6%A0%B7%EF%BC%89%E2%80%94%E2%80%94%E6%98%AF%E4%BD%BF%E7%94%A8_ctr%E2%80%93namespace">https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/#verify-pod-cgroup-limits）。另一个方法——如果您使用containerd作为容器运行时（就像AKS暂时所做的那样）——是使用_ctr–namespace</a> k8s列出容器。io容器列表（注意containerd本身具有名称空间，如本文所述<a href="https://github.com/containerd/containerd/issues/1815#issuecomment-347389634">https://github.com/containerd/containerd/issues/1815#issuecomment-347389634</a>). 获取pod内容器路径的另一种方法是仅使用cAdvisor的输出，我们将在本文中进一步介绍。</p>
<h3 id="cgroups-v2"><a href="#cgroups-v2" class="headerlink" title="cgroups v2"></a>cgroups v2</h3><p>如何检查Linux操作系统是否使用cgroups v2？在Kubernetes中启用cgroups v2的实际提案文本中看到的更简单的方式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">stat</span> -f --format <span class="string">&#x27;%T&#x27;</span> /sys/fs/cgroup</span><br></pre></td></tr></table></figure>

<p>现在，本文的某些部分——特别是围绕cgroup伪文件系统的低级讨论，以及一些代码分析，假设在Kubernetes节点上使用cgroups v1。本节详细介绍了当内部进程超过限制时，OOM-killer如何作用于cgroups，这也是基于使用cgroups v1节点的假设。为什么这很重要？因为cgroups v2修复了v1的一些缺OOM-killer不在乎它是否只杀死cgroup中的一个进程，并使相应的容器处于损坏状态,因此当cgroup v2成为标准时，该文章的部分内容将不再适用。</p>
<p>出于上述原因，除非另有说明, 我们将在整篇文章中假设使用cgroups v1</p>
<h3 id="cgroup及OOM-killer"><a href="#cgroup及OOM-killer" class="headerlink" title="cgroup及OOM-killer"></a>cgroup及OOM-killer</h3><p>我们之前已经看到OOM-killer在看到操作系统内存不足时会如何介入并采取行动。随着OOM-killer的引入，OOM-killer将与cgroups一起工作，如在<a href="https://lwn.net/Articles/761118/">Teaching the OOM killer about control groups</a>中所述的一样</p>
<p>但是，如果OOM-killer是由于系统内存严重不足才触发的，那对于cgroups来说触发的起因是什么呢？对于cgroup，特别是在Kubernetes中设置swap被禁用的cgroup的情况下，<strong>当cgroup的内存使用量超过设置的limit限制时</strong>，OOM-killer就会起作用。</p>
<p>OOM-killer是不是只要它的总内存使用量超过了定义的极限，哪怕是最小的数量，就会开始在cgroup内部造成破坏？不，我们从<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">memory</a>（第2.5节）中知道：“<strong>当cgroup超过其极限时，我们首先尝试从cgroup中回收内存，以便为cgroup所管理的新页面腾出空间。如果回收不成功，将调用OOM程序来选择并终止cgroup内最庞大的任务</strong>。最庞大的任务”实际上指的是消耗相应cgroup中最多内存的进程</p>
<p>所以现在我们知道OOM-killer何时决定对一个cgroup采取行动，以及它的行动是什么。让我们来看看这个展开。</p>
<h4 id="OOM场景-1：当容器超过其极限时，容器被OOM"><a href="#OOM场景-1：当容器超过其极限时，容器被OOM" class="headerlink" title="OOM场景#1：当容器超过其极限时，容器被OOM"></a>OOM场景#1：当容器超过其极限时，容器被OOM</h4><p><del>视频4–Kubernetes中调用的OOM-killer</del></p>
<p>您可以看到一个Kubernetes pod启动，它运行一个内存泄漏工具,</p>
<p>pod的清单为运行应用程序的容器指定了1 GiB的limit。该工具以100 MiB的块分配内存，直到OOM-killer介入，因为容器运行到1 GiB内存限制。左上方的窗口是泄漏工具分配时的原始输出，右侧的窗口跟踪pod的状态，而底部的窗口跟踪内核消息OOM-killer对于它分析的潜在受害者以及它决定最终杀死的过程都非常详尽。</p>
<p>到现在为止，一直都还不错。事情很简单：消耗最多的内存，在我们的例子中，内存泄漏工具中运行了.NET, 在cgroup使用过多内存时被终止。</p>
<h4 id="意外现象"><a href="#意外现象" class="headerlink" title="意外现象"></a>意外现象</h4><p>但是，让我们更进一步，考虑一个稍微不同的例子。启动一个容器，使用内存泄漏工具（下面的进程ID 15976）分配一定数量的内存（900MB）。一旦成功完成内存分配，将使用<code>kubectl exec</code>连接到容器并启动<code>bash</code>。在shell内部，启动了内存泄漏工具的一个新实例（下面的进程ID 14300），开始分配内存本身。</p>
<p>您可以在下面看到结果。请记住，PID是在节点级别看到的，在节点级别捕获了此日志。因此，容器启动的进程没有报告的PID为1，而是机器上的实际PID。<strong>rss数据以页为单位，因此需要乘以4 KB才能获得字节大小</strong>。OOM-killer发出的几条不重要的日志被省略了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895293] dotnet invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=-997</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895296] CPU: 0 PID: 14300 Comm: dotnet Not tainted 5.4.0-1059-azure <span class="comment">#62~18.04.1-Ubuntu</span></span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895297] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS Hyper-V UEFI Release v4.1 10/27/2020</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895298] Call Trace:</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895306]  dump_stack+0x57/0x6d</span><br><span class="line">.....</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895351] memory: usage 1048576kB, <span class="built_in">limit</span> 1048576kB, failcnt 30</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895352] memory+swap: usage 0kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895353] kmem: usage 5164kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895353] Memory cgroup stats <span class="keyword">for</span> /kubepods/pod3b5e63f1-b571-407f-be00-461ed99e968f:</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895397] anon 1068539904</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895397] file 0</span><br><span class="line">.....</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895398] Tasks state (memory values <span class="keyword">in</span> pages):</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895398] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895401] [  15885] 65535 15885      241        1    28672        0          -998 pause</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895403] [  15976]     0 15976 43026975   229846  2060288        0          -997 dotnet</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895404] [  16234]     0 16234      966      817    49152        0          -997 bash</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895406] [  14300]     0 14300 43012649    43356   581632        0          -997 dotnet</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895407] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=f038675297418b6357e95ea8ef45ee868cc97de6567a95dffa2a35d29db172bf,mems_allowed=0,oom_memcg=/kubepods/pod3b5e63f1-b571-407f-be00-461ed99e968f,task_memcg=/kubepods/pod3b5e63f1-b571-407f-be00-461ed99e968f/f038675297418b6357e95ea8ef45ee868cc97de6567a95dffa2a35d29db172bf,task=dotnet,pid=14300,uid=0</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.895437] Memory cgroup out of memory: Killed process 14300 (dotnet) total-vm:172050596kB, anon-rss:148368kB, file-rss:25056kB, shmem-rss:0kB, UID:0 pgtables:568kB oom_score_adj:-997</span><br><span class="line">Jan 16 21:33:51 aks-agentpool-20086390-vmss00003K kernel: [ 8334.906653] oom_reaper: reaped process 14300 (dotnet), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</span><br></pre></td></tr></table></figure>

<p>注意，被终止的进程不是容器最初启动的进程（pid 15976），尽管它使用的内存是我们随后运行的进程消耗的内存的4倍。反而是后者被杀。在这一点上，你可能会正确地得出结论，这与分配过程有关，当整个pod的cgroup超过极限时，这会触发OOM-killer。这看起来很公平，当有另一个进程在积极分配内存并最终将内存使用量推到极限时，为什么要kill掉一个没有做任何事情的进程？此外，日志确实指出了哪个进程被OOM-killer终止了。</p>
<p>让我们测试一下，看看当角色颠倒时会发生什么，而手动运行的泄漏工具（现在是进程id 31082）刚刚完成分配少量内存（总共60 MB，每4秒10 MB），而“主”容器进程（现在的进程id 30548）仍在分配内存（每3秒50 MB的块），直到耗尽。当整个团队超过1 GiB的限制时，谁会被kill？</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076861] dotnet invoked oom-killer: gfp_mask=0x400dc0(GFP_KERNEL_ACCOUNT|__GFP_ZERO), order=0, oom_score_adj=-997</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076865] CPU: 1 PID: 30548 Comm: dotnet Not tainted 5.4.0-1059-azure <span class="comment">#62~18.04.1-Ubuntu</span></span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076865] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS Hyper-V UEFI Release v4.1 10/27/2020</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076866] Call Trace:</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076874]  dump_stack+0x57/0x6d</span><br><span class="line">....</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076928] memory: usage 1048576kB, <span class="built_in">limit</span> 1048576kB, failcnt 19</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076929] memory+swap: usage 0kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076930] kmem: usage 5020kB, <span class="built_in">limit</span> 9007199254740988kB, failcnt 0</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076930] Memory cgroup stats <span class="keyword">for</span> /kubepods/pod794f73d1-9b08-4fc1-b8af-fed0810cc5c2:</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076941] anon 1068421120</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076941] file 0</span><br><span class="line">....</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076941] Tasks state (memory values <span class="keyword">in</span> pages):</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076942] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076944] [  30373] 65535 30373      241        1    24576        0          -998 pause</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076946] [  30548]     0 30548 43026962   233338  2101248        0          -997 dotnet</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076947] [  30617]     0 30617      966      799    45056        0          -997 bash</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076949] [  31082]     0 31082 43012630    39897   548864        0          -997 dotnet</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076951] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=35f19955ac068abba356c7fd9113b8e6fb1b54232564ac8d19316f80b4178fea,mems_allowed=0,oom_memcg=/kubepods/pod794f73d1-9b08-4fc1-b8af-fed0810cc5c2,task_memcg=/kubepods/pod794f73d1-9b08-4fc1-b8af-fed0810cc5c2/35f19955ac068abba356c7fd9113b8e6fb1b54232564ac8d19316f80b4178fea,task=dotnet,pid=31082,uid=0</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.076993] Memory cgroup out of memory: Killed process 31082 (dotnet) total-vm:172050520kB, anon-rss:134148kB, file-rss:25440kB, shmem-rss:0kB, UID:0 pgtables:536kB oom_score_adj:-997</span><br><span class="line">Jan 17 21:51:19 aks-agentpool-20086390-vmss00003M kernel: [ 1856.089185] oom_reaper: reaped process 31082 (dotnet), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</span><br></pre></td></tr></table></figure>

<p>还有另一个令人惊讶的是：手动启动的进程分配了最少的资源，在较小的区块中且暂停时间最大的进程被kill掉了。更重要的是，首先触发OOM-killer的是在cgroup中消耗最多内存的进程。您可能会正确地说：“但是，等等，最初由容器（在我们最新的示例中是pid 30548）启动的进程有其独特的方式，内核可能会以某种方式保护它不被杀死，只要它不是其cgroup中的最后一个”。可能是这样，事实是我根本不知道为什么我们会得到这样的结果。我所知道的是，除了选择文档告诉的“最庞大的任务”之外，还有其他代码可能会影响最终选择出来到底 哪个进程被Kill掉，可参考<a href="https://github.com/torvalds/linux/blob/v5.4/mm/oom_kill">oom_kill</a></p>
<p>尽管最后两个例子有点让人困惑，但这可能有一个好处：<strong>一旦突破了cgroup限制，oom-killer一定会选择一个进程kill掉，但是被Kill掉的进程可能会出乎意料</strong></p>
<p>查看内核日志，您可能发现当OOM-killer选择受害者时，<strong>内核日志中报告的任务（进程）引用了pod的cgroup。但事实是每个容器都创建了一个cgroup</strong>。如果您将<code>kubectl exec</code>进入容器中运行<code>bash</code>，则此新进程将成为容器的现有cgroup的成员。从该shell启动的任何新进程也将是同一cgroup的成员。</p>
<p>那么，为什么我们要得到pod cgroup的统计数据，而不是容器cgroup？后者包含内存泄漏工具实例。如果您仔细查看任务状态后的消息，您可以看到触发OOM的cgroup，这就是pod。所以我们得到了pod cgroup的统计数据，因为它作为一个整体超过了限制（<a href="https://www.ianlewis.org/en/almighty-pause-container">暂停容器</a>没有设置limit，内存泄漏容器的cgroup的limit与为pod本身创建的cgroup设置的limit相同）</p>
<h3 id="Kubernetes和OOM-killer"><a href="#Kubernetes和OOM-killer" class="headerlink" title="Kubernetes和OOM-killer"></a>Kubernetes和OOM-killer</h3><p>继前几节之后，需要注意的是，OOM-killer决定终止cgroups中的进程虽然发在Kubernetes容器内部，<strong>但OOM-killer是一个Linux内核组件</strong>。而不是kuberntes触发的，kubernetes无法控制内核oom的行为</p>
<p>考虑到OOM-killer的受害者被迅速终止，这有一个强有力的暗示：被杀进程没有时间进行任何清理或优雅的关闭。它永远没有机会做任何事情，因为内核永远不会给它再次运行的机会。这会让在Kubernetes中运行生产代码的人感到不安，因为您不希望任何服务的pods突然消失，特别是当它正在远程编写内容（例如，向数据库）时。你可以在这里找到这样的讨论<a href="https://github.com/kubernetes/kubernetes/issues/40157#issuecomment-867548699">当OOM不是SIGKILL</a>.</p>
<p>另一个重要的方面是<strong>OOM-killer不理解Kubernetes pod的概念</strong>。它甚至不知道什么是容器。<strong>容器是构建在cgroup和namespace之上的构造</strong>，因此oom-killer不<code>理解</code>容器，<strong>它只知道当一个cgroup超过其内存限制时，它必须杀死该cgroup中的至少一个任务（或进程）</strong>。这对于Kubernetes来说是一个问题，因为它在最底层处理容器。因此，让容器内的进程消失——尤其是任何不是主容器进程的进程——会让Kubernetes陷入困境：容器刚刚被条掉了进程，但还有可能pid 为1的进程还存在，但有可能它已经无法工作。这个确切的问题出现在这里的这个旧线程中<a href="https://github.com/kubernetes/kubernetes/issues/50632">具有多个进程的容器在OOM时未终止</a>告诉容器中的子进程被终止，整个pod没有标记为OOM。正如Kubernetes的一位委员所说：<strong>只有当pid为1的程序为OOM-killer杀死时，Containers才会被标记为OOM killed</strong>，有些应用程序可以容忍非init进程的OOM kills，因此我们选择不跟踪非init进程OOM kill事件，这是预期的方式。</p>
<p>我们在上一节的最后两个示例中看到了这个确切的场景，其中一个容器保持运行，尽管其中一个进程被OOM-killer杀死。</p>
<p>因此，如果OOM-killer独自操作，并且可以杀死可能导致容器停止的进程，那么Kubernetes如何应对这种情况？如果pod突然消失，它就不能成功地管理pod及其容器。正如我们将在本文后面看到的<a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-4-pod-evictions-oom-scenarios-and-flows-leading-to-them/#is-kubelet-killing-containers-due-to-oom">Is Kubelet killing containers due to OOM?</a></p>
<p>Kubernete可以很方便地拿到OOM-killer事件是何时生成的，因此可以将自己的状态与每个节点内发生的情况进行协调。</p>
<p>OOM-killer的名字让人认为，一旦它行动起来，它就会永远阻止受害者。但在处理Kubernetes时情况并非如此，因为默认情况下，如果容器终止（无论它是否成功退出），就会重新启动。在这种情况下，您将看到的结果是OOMKilled容器被无休止地重新启动（尽管有指数退避)</p>
<p>在我们完成本节之前的最后一件事：正如我们所注意到的，一旦超过了cgroup的内存限制（当然，如果什么都不能回收），OOM-killer就相当无情了。这自然会让人怀疑，是否存在某种软限制，以避免容器突然终止，并提前通知内存正在被耗尽。即使cgroups支持<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt">软限制</a>，Kubernetes目前还没有使用，注意，这里说的是cgroups的软限制,而不是kubelet驱逐机制中的软限制</p>
<h3 id="OOM的运行时含义"><a href="#OOM的运行时含义" class="headerlink" title="OOM的运行时含义"></a>OOM的运行时含义</h3><p>在容器中运行的应用程序很有可能是使用runtime编写的，例如.NET或Java。但从内存分配的角度来看，这又有什么关系呢？</p>
<p>有些语言没有运行时，比如C，它只有一个运行时库，但没有底层的“环境”运行时，你可以自由地将内存分配给你的核心内容。因此，正常情况下（通常在不使用限制最大内存的情况下，如cgroups），如果分配足够，只要操作系统允许这些分配，就可以使用所有RAM（包含swap）。</p>
<p>还有其他语言，如Java或基于.NET平台（C#）——依赖于运行时，其中的方法不同，应用程序不自行运行并分配虚拟内存，而是由运行时来处理，此外还有其他一些事情，如将分配给一代模型的对象映射到一代模型，调用垃圾收集器等。运行时将充当应用程序和操作系统之间的中间人，因为它需要跟踪使用的对象，如果GC可以回收这些对象周围的内存，因为它们不再使用，等等。</p>
<p>但现在我们不用关心<a href="https://mihai-albert.com/2019/12/15/boxing-performance-in-c-analysis-and-benchmark/#too-much-of-a-surperise">运行时如何分配内存的复杂性</a>。事实上，运行时可以有适当的设置来限制应用程序可以分配的最大内存。因此，在调查应用程序无法分配内存的原因时，我们必须添加一个潜在的罪魁祸首：仅确保底层Kubernetes节点的操作系统拥有所请求的内存并且没有cgroup限制应用程序在其容器中的内存是不够的，但运行时还必须允许分配内存。。</p>
<p>接下来的问题是，如果我们自己没有设置任何限制，为什么我们会关心运行时可以设置应用程序使用的最大内存限制这一事实？答案是，有时设置的默认值会阻止使用所有可用内存，我们将在下面看到。</p>
<h3 id="NET"><a href="#NET" class="headerlink" title=".NET"></a>.NET</h3><p>作者注:</p>
<p>这一节主要介绍.net程序在kubernetes中的表现，不是很重要，作者整节去掉</p>
<h3 id="Kubernetes资源请求和限制"><a href="#Kubernetes资源请求和限制" class="headerlink" title="Kubernetes资源请求和限制"></a>Kubernetes资源请求和限制</h3><p>了解Kubernetes中的资源请求和限制意味着什么，这一点很重要，因为在文章的后面，像容器获取OOM killed和pod驱逐这样的事情将大量使用资源请求(request)，特别是限制(limit)。</p>
<p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">pod及container的资源管理</a>是Kubernetes的官方文章——尤其是前半部分——对于资源请求和限制的概念解释地非常清楚。核心思想如下：<strong>当您为Pod中的容器指定资源请求时，kube-schedule将使用此信息来决定将Pod放置在哪个节点上。当您为容器指定资源限制时，kubelet将强制执行这些限制，以便运行的容器不允许使用超过您设置的限制的资源</strong>在下面那篇<a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resourc-limits-are-run">文章</a>,您还可以看到如何使用cgroups在后台实现这些限制。</p>
<p>一些超越资源请求和限制概念的好文章：</p>
<p><a href="https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits">Kubernetes最佳实践：资源请求和限制</a>清晰简洁地介绍了请求和限制的概念<br><a href="https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66">深入了解Kubernetes度量-第3部分容器资源度量</a>：有一节专门介绍请求和限制，很好地描述了这些概念。我强烈建议阅读整个系列的文章，因为它非常详细地讨论了其他主题</p>
<p>在这里，我们必须讨论pod的QoS。老实说，每次我遇到这个概念时，我只是挥挥手，想“不，我还不需要这个”；我以为我可以在不知道QoS意味着什么的情况下更快地学习OOM环境中涉及的组件是如何工作的，但最终我浪费的时间远远超过了学习这个概念所需的5分钟。它们之所以重要，有两个原因：</p>
<blockquote>
<ul>
<li><p>1）当节点上的内存变得稀缺时，它们决定了pod被逐出的顺序；</p>
</li>
<li><p>2）它们决定了在Linux节点的memory root cgroup: <code>/sys/fs/cgroup/memory/</code>层次结构中，pod的cgroup及其容器将被放置在何处（如果kubelet开启了<code>--cgroups-per-qos＝true</code>标志，默认值开启），这反过来又可以方便地浏览到伪文件系统中的正确目录，并查看属于特定pod的cgroups的各种详细信息。</p>
</li>
</ul>
</blockquote>
<p>pod的QoS不是手动附加到pod的。这只是Kubernetes根据向该pod中的容器分配的请求和限制来决定的。本质上，Kubernetes通过一个算法来查看pod内容器上设置的请求和限制，最后为pod分配一个相应的QoS类</p>
<blockquote>
<ul>
<li><p>Guaranteed_被分配给所有容器指定的资源值分别等于CPU和内存的限制值</p>
</li>
<li><p>Burstable_是指pod中至少有一个容器有CPU或内存请求，但不满足Guaranted_的“高”标准。</p>
</li>
<li><p>BestEffort_–是没有单个容器的pod的情况，它指定了至少一个CPU或内存限制或请求值。官方文章在这里<a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod">为Pods配置服务质量</a>.</p>
</li>
</ul>
</blockquote>
<h3 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h3><h4 id="overcommit-2"><a href="#overcommit-2" class="headerlink" title="overcommit"></a>overcommit</h4><p><strong>Q</strong>：假设overcommit设置为禁用，除了vm之外，还有其他参数控制提交限制(commit limit)及<code>vm.overcommit_ratio</code>吗？<br><strong>A</strong>：是的，可以通过<code>vm.overcommit_kbytes</code>。注意，使用此选项将禁用<code>vm.overcommit_ ratio</code>，并在读取时使该值为0。查看<a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">文档</a>了解更多详情。</p>
<p><strong>Q</strong>：Kubernetes是否在其Linux节点上使用overcommit？<br><strong>A</strong>：至少在AKS上是这样。截至目前（2021 11月），在AKS Linux节点上，overcommit策略被设置为盲目接受任何分配请求，而不检查提交限制(commit limit)–换句话说，总是过度提交,此处有<a href="https://www.kernel.org/doc/Documentation/vm/overcommit-accounting">详细信息</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@aks-agentpool-20086390-vmss00000E:/proc/sys/vm<span class="comment"># cat overcommit_memory</span></span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p><strong>Q</strong>：你不是在之前的帖子中说过，你还没有找到在Linux中显示承诺内存大小的方法吗？<code>/proc/meminfo</code>中的Committed_AS如何？<br><strong>A</strong>：这给出了系统范围内的提交量，相当于Windows上Process Explorer的系统信息窗口<code>commit charge</code>部分。但仍然不确定如何在流程级别获取这些信息。</p>
<p><strong>Q</strong>：如何使overcommit承诺的参数永久化？<br><strong>A</strong>：使用<code>systcl-p</code></p>
<h4 id="OOM-killer-1"><a href="#OOM-killer-1" class="headerlink" title="OOM-killer"></a>OOM-killer</h4><p><strong>Q</strong>：在Linux下，我在哪里可以找到一个很好的解释来解释日志中的内容？我很难找到我想要的信息。<br><strong>A</strong>：从这个优秀的<a href="https://askubuntu.com/questions/26237/difference-between-var-log-messages-var-log-syslog-and-var-log-kern-log">SO线程</a></p>
<p><strong>Q</strong>：在上面的视频中，OOM-killer被调用，您是否必须在运行“dmesg”之前启动底层Ubuntu WSL上的rsyslog服务？<br><strong>A</strong>：不需要启动rsyslog服务，因为内核消息显示得很好。</p>
<p><strong>Q</strong>：我并不真正关心OOM-killer的运行，但我只希望它不要kill我的进程。我该怎么做？<br><strong>A</strong>：您可能正在为流程设置<code>oom_score_adj</code>。看看这个<a href="https://serverfault.com/questions/762017/how-to-get-the-linux-oom-killer-to-not-kill-my-process?rq=1">例子</a>了解更多信息。</p>
<p><strong>Q</strong>：在[图1]中所示的过程必须至少使用一些RAM，对吗？<br><strong>A</strong>：当然，它本身及加载的各种模块及.NET运行时内部确实需要一些内存。</p>
<h4 id="cgroup"><a href="#cgroup" class="headerlink" title="cgroup"></a>cgroup</h4><p><strong>Q</strong>：在哪里可以找到详细讨论cgroups的文章？<br><strong>A</strong>：除了帖子中已经提到的那些：</p>
<ol>
<li><p>一篇关于cgroups相关多个主题的好<a href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">文章</a></p>
</li>
<li><p>cgroup的“内存”子系统中存在的文件的详细<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-memory">列表</a></p>
</li>
</ol>
<h4 id="cgroup及OOM-killer-1"><a href="#cgroup及OOM-killer-1" class="headerlink" title="cgroup及OOM-killer"></a>cgroup及OOM-killer</h4><p><strong>Q</strong>：我如何查看为pod里的容器组设置的限制？<br><strong>A</strong>：这里的文章[Pod overhead–Verify Pod cgroup limits](<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/#verify-pod">https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/#verify-pod</a> cgroup-limits)提供了一种提取pod的内存cgroup路径的方法，并展示了如何检查pod的cgroup内存限制。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h2><blockquote>
<ul>
<li><a href="https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-2-the-oom-killer-and-application-runtime-implications/">Out-of-memory (OOM) in Kubernetes; Part 2: The OOM killer and application runtime implications</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(使用PodSecurityPolicy)</title>
    <url>/2019/12/13/Kubernetes-PSP/</url>
    <content><![CDATA[<p>PodSecurityPolicy(psp)是kubernetes中提供的一种安全策略, 默认情况下，Kubernetes 允许创建一个有特权容器的 Pod，这些容器很可能会危机系统安全，而 Pod 安全策略（PSP）则通过确保请求者有权限按配置来创建 Pod，从而来保护集群免受特权 Pod 的影响</p>
<span id="more"></span>



<p><code>PodSecurityPolicy</code> 是 Kubernetes API 对象，你可以在不对 Kubernetes 进行任何修改的情况下创建它们，但是，默认情况下不会强制执行我们创建的一些策略，我们需要一个准入控制器、kube-controller-manager 配置以及 RBAC 权限配置</p>
<p>注: <strong>kubeadm搭建的集群默认情况下没有开启PodSecurityPolicy, 需要在api-erver的启动参数中开启, 直接修改api-erver的静态yaml文件, –enable-admission-plugins&#x3D;NodeRestriction,PodSecurityPolicy</strong></p>
<p>下面通过一个例子来说明<code>PodSecurityPolicy</code>的使用</p>
<p>在指定namespace下, 先增加一个禁止pod使用hostNetwork,然后添加一个serviceaccount,可以使用这个特定的serviceaccount能够使用hostNetwork, 其它pod则不可以</p>
<p>在默认情况下，所有pod可以直接指定 hostNetwork&#x3D;true,要想禁止使用,则需要增加一个psp</p>
<p>因此， 我们将创建2个策略，第一个是提供限制访问的“默认”策略，保证使用特权设置（例如使用 hostNetwork）无法创建 Pod。第二种是一个“提升”的许可策略，允许将特权设置用于某些 Pod</p>
<h3 id="Admission-Controller"><a href="#Admission-Controller" class="headerlink" title="Admission Controller"></a>Admission Controller</h3><p>Admission Controller（准入控制器）<code>拦截</code>对 kube-apiserver 的请求，拦截发生在请求的对象被持久化之前，但是在请求被验证和授权之后.</p>
<p> 将<code>PodSecurityPolicy</code>添加到 kube-apiserver 上的<code>--enabled-admission-plugins</code>参数中，然后重启 kube-apiserver, 目前参数如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,PersistentVolumeLabel</span><br></pre></td></tr></table></figure>



<h3 id="用于禁止使用hostNetwork的PodSecurityPolicy"><a href="#用于禁止使用hostNetwork的PodSecurityPolicy" class="headerlink" title="用于禁止使用hostNetwork的PodSecurityPolicy"></a>用于禁止使用hostNetwork的PodSecurityPolicy</h3><h4 id="新建PodSecurityPolicy"><a href="#新建PodSecurityPolicy" class="headerlink" title="新建PodSecurityPolicy"></a>新建PodSecurityPolicy</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">restrictive</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">defaultAllowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">hostIPC:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;configMap&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;downwardAPI&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;emptyDir&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;persistentVolumeClaim&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;secret&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;projected&#x27;</span></span><br><span class="line">  <span class="attr">allowedCapabilities:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;*&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="创建clusterrole"><a href="#创建clusterrole" class="headerlink" title="创建clusterrole"></a>创建clusterrole</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-restrictive</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">extensions</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">podsecuritypolicies</span></span><br><span class="line">  <span class="attr">resourceNames:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">restrictive</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">use</span></span><br></pre></td></tr></table></figure>

<h4 id="创建rolebinding"><a href="#创建rolebinding" class="headerlink" title="创建rolebinding"></a>创建rolebinding</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-default</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:serviceaccounts</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-restrictive</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<p>上述的对象实现restrictive策略和系统中所有的控制器（对象加在system:serviceaccounts下） ServiceAccount 进行绑定， 这样，所有pod在使用sa时都将会禁止使用hostNetwork. </p>
<p>kubectl apply 之后，对于新生成的pod对象(如果是deployment对象, deployment对象可以创建成功，但是pod的event log会提示以下错误信息)，是无法指定hostNetwork, 对于已经使用的pod不影响</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#提示的错误信息如下: </span></span><br><span class="line"><span class="attr">Error creating:</span> <span class="string">pods</span> <span class="string">&quot;nginx-hostnetwork-deploy-6647d65899-&quot;</span> <span class="attr">is forbidden:</span> <span class="string">unable</span> <span class="string">to</span></span><br><span class="line"><span class="attr">validate against any pod security policy:</span> [<span class="attr">spec.securityContext.hostNetwork: Invalid value: true:</span> <span class="string">Host</span> <span class="string">network</span> <span class="string">is</span> <span class="string">not</span> <span class="string">allowed</span> <span class="string">to</span> <span class="string">be</span> <span class="string">used</span>]</span><br></pre></td></tr></table></figure>



<h3 id="用于允许使用hostNetwork的PodSecurityPolicy"><a href="#用于允许使用hostNetwork的PodSecurityPolicy" class="headerlink" title="用于允许使用hostNetwork的PodSecurityPolicy"></a>用于允许使用hostNetwork的PodSecurityPolicy</h3><h4 id="新建PodSecurityPolicy-1"><a href="#新建PodSecurityPolicy-1" class="headerlink" title="新建PodSecurityPolicy"></a>新建PodSecurityPolicy</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">permissive</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostIPC:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">hostPorts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">min:</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&#x27;*&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="创建sa"><a href="#创建sa" class="headerlink" title="创建sa"></a>创建sa</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">create</span> <span class="string">sa</span> <span class="string">psp-sa</span></span><br></pre></td></tr></table></figure>

<h4 id="创建clusterrole-1"><a href="#创建clusterrole-1" class="headerlink" title="创建clusterrole"></a>创建clusterrole</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-permissive</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">extensions</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">podsecuritypolicies</span></span><br><span class="line">  <span class="attr">resourceNames:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">permissive</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">use</span></span><br></pre></td></tr></table></figure>

<h4 id="创建clusterrolebinding"><a href="#创建clusterrolebinding" class="headerlink" title="创建clusterrolebinding"></a>创建clusterrolebinding</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">specialsa-psp-permissive</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-permissive</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br></pre></td></tr></table></figure>



<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>pod中使用该psp-sa的就可指定hostNetwork,没有使用该sa的无法使用hostNetwork</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-hostnetwork-deploy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.15.4</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">psp-sa</span> <span class="comment">#指定sa</span></span><br></pre></td></tr></table></figure>

<p>如果没有指定serviceAccount&#x3D;psp-sa, 则会提示以下错误</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe rs nginx-hostnetwork-deploy-74c8fbd687</span><br><span class="line">Name:           nginx-hostnetwork-deploy-74c8fbd687</span><br><span class="line">......</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason        Age                   From                   Message</span><br><span class="line">  ----     ------        ----                  ----                   -------</span><br><span class="line">  Warning  FailedCreate  80s (x15 over 2m42s)  replicaset-controller  Error creating: pods <span class="string">&quot;nginx-hostnetwork-deploy-74c8fbd687-&quot;</span> is forbidden: unable to validate against any pod security policy: [spec.securityContext.hostNetwork: Invalid value: <span class="literal">true</span>: Host network is not allowed to be used]</span><br></pre></td></tr></table></figure>



<p>从这个例子可以了现, psp虽然能够在pod之间做一些限制, 但是力度还是非常粗糙的，如果需要更加细粒度地对流量进行控制,psp做不到, 可以考虑服务网格.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.qikqiak.com/post/setup-psp-in-k8s/">https://www.qikqiak.com/post/setup-psp-in-k8s/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#policy-reference">https://kubernetes.io/docs/concepts/policy/pod-security-policy/#policy-reference</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/">https://kubernetes.io/docs/admin/authorization/</a></li>
<li><a href="https://kubernetes.io/docs/admin/admission-controllers/">https://kubernetes.io/docs/admin/admission-controllers/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kubernetes中的反射模式(DownwardAPI))</title>
    <url>/2020/05/12/Kubernetes-Reflection-Pattern/</url>
    <content><![CDATA[<p>该文章为翻译文档, 主要在阐述kubernetes中的DownwardAPI的作用， 这里是<a href="https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern">原文地址</a></p>
<span id="more"></span>



<h1 id="什么是“反射”？"><a href="#什么是“反射”？" class="headerlink" title="什么是“反射”？"></a>什么是“反射”？</h1><p>反射是大多数（不是全部）编程语言中都可以使用的概念。它只是指某种对象揭示其自身重要信息的能力。例如，它的名称，它的父类以及它包含的任何元数据。在Cloud和DevOps领域中具有相同的概念。例如，如果您登录到AWS EC2实例，则可以通过从以下位置发出GET请求到<a href="http://169.254.169.254/latest/meta-data/%E6%9D%A5%E8%BD%BB%E6%9D%BE%E8%8E%B7%E5%8F%96%E6%9C%89%E5%85%B3%E8%AF%A5%E7%89%B9%E5%AE%9A%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%A4%A7%E9%87%8F%E4%BF%A1%E6%81%AF%E3%80%82">http://169.254.169.254/latest/meta-data/来轻松获取有关该特定实例的大量信息。</a></p>
<h1 id="为什么我们需要反射？"><a href="#为什么我们需要反射？" class="headerlink" title="为什么我们需要反射？"></a>为什么我们需要反射？</h1><p>对象的能用术语指的是工作单元。因此，在编程语言中，对象是类的实例，在内部部署基础结构中，对象可以是物理或虚拟主机，在云环境中，它是实例，而在Kubernetes中，它是Pod 。</p>
<p>在本文中，我们对Kubernetes感兴趣，因此Pod和object可以互换使用。</p>
<p>在许多情况下，您需要Pod的元数据，尤其是在该Pod是无状态应用程序的一部分的情况下，其中Pod本质上是动态的。让我们看一些可能的情况：</p>
<ul>
<li>您需要Pod的IP地址，以识别它是否是网络上检测到的可疑流量的来源。</li>
<li>在容器内运行的应用程序需要知道Pod在其中运行的名称空间，这可能是因为该程序被编程为根据名称空间所传达的运行环境而有所不同。</li>
<li>您需要知道限制容器当前资源限制（CPU和内存）。例如，您可以进一步使用此数据在启动Java应用程序时自动调整其堆大小。</li>
</ul>
<p>幸运的是，Kubernetes通过使用Downward API使这项任务相对容易。</p>
<h1 id="DownwardAPI是如何工作？"><a href="#DownwardAPI是如何工作？" class="headerlink" title="DownwardAPI是如何工作？"></a>DownwardAPI是如何工作？</h1><p>Downward API通过环境变量和文件将元数据注入到容器中。它们的使用方式与configMaps和Secrets一样，用于处理将外部信息传递给应用程序的情况。但是，Downward API不会将所有可用的元数据注入到容器中。相反，我们选择需要对容器可用的变量。</p>
<p>让我们举个例子以便更好地理解。以下定义文件创建一个Pod，该Pod从bash映像运行容器。我们使用Downward API注入三个可用变量：Pod的IP地址，此Pod运行所在的名称空间以及对其施加的当前内存限制。下图可以描述这种情况：</p>
<p><img src="https://www.magalix.com/hs-fs/hubfs/reflection.png?width=720&name=reflection.png" alt="反射"></p>
<p><a href="https://www.magalix.com/cs/c/?cta_guid=18dc390a-3729-410b-a0ec-41819020a462&placement_guid=963b2ba4-df32-4a3b-8781-f8fa4ff38750&portal_id=3487587&canon=https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern&redirect_url=APefjpFCTwHTF_0D7xGnKEa3OyhUHUrxPjuzbaM6DJbCtG7cCui55tcC3IEiq2qMyBoWSCtpP2YORW-hQeacfrxSibzeMsSHiUV67Y_bPVzca7Oq0nDy4FJtbLNajeOMnwmSMHL-HZOs&click=f4df9f45-b688-40bc-a4b9-bfd9cfcbde54&hsutk=33479dc673383b321023ea8e6a151cb1&signature=AAH58kGwChv9446MJBtnkLTaNmfCXRWI0g&utm_referrer=https://www.google.com/&__hstc=197029363.33479dc673383b321023ea8e6a151cb1.1589190812305.1589190812305.1589190812305.1&__hssc=197029363.1.1589190812306&__hsfp=441927329&contentType=blog-post">了解如何持续优化您的k8s集群</a></p>
<p>定义文件如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">bash</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mycontainer</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;bash&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;sleep 1000000&#x27;</span>]</span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_IP</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">status.podIP</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_NAMESPACE</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_MEMORY_LIMIT</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">resourceFieldRef:</span></span><br><span class="line">          <span class="attr">containerName:</span> <span class="string">mycontainer</span></span><br><span class="line">          <span class="attr">resource:</span> <span class="string">limits.memory</span></span><br><span class="line">          <span class="attr">divisor:</span> <span class="string">1Mi</span></span><br></pre></td></tr></table></figure>

<p>让我们使用kubectl apply -f <em>filename</em>运行此定义，然后访问Pod内的容器，看看我们的元数据是否可用：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">exec</span> <span class="string">-it</span> <span class="string">mypod</span> <span class="string">bash</span></span><br><span class="line"><span class="string">bash-5.0#</span> <span class="string">echo</span> <span class="string">MY_IP</span></span><br><span class="line"><span class="string">MY_IP</span></span><br><span class="line"><span class="string">bash-5.0#</span> <span class="string">echo</span> <span class="string">$MY_IP</span></span><br><span class="line"><span class="number">172.17</span><span class="number">.0</span><span class="number">.4</span></span><br><span class="line"><span class="string">bash-5.0#</span> <span class="string">echo</span> <span class="string">$MY_NAMESPACE</span></span><br><span class="line"><span class="string">default</span></span><br><span class="line"><span class="string">bash-5.0#</span> <span class="string">echo</span> <span class="string">$MY_MEMORY_LIMIT</span></span><br><span class="line"><span class="number">1900</span></span><br></pre></td></tr></table></figure>

<p>因此，我们可以通过查询相应的环境变量MY_IP，MY_NAMESPACE和MY_MEMORY_LIMIT来获取运行此Pod的IP地址和命名空间。</p>
<h1 id="FieldRef"><a href="#FieldRef" class="headerlink" title="FieldRef"></a>FieldRef</h1><p>在第一个示例中，我们使用fieldRef参数来选择需要通过fieldPath注入到Pod中的信息。以下是通过fieldRef可用的可能值的列表：</p>
<table>
<thead>
<tr>
<th><strong>Name</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td>spec.nodename</td>
<td>The name of the node where the Pod is running.</td>
</tr>
<tr>
<td>status.hostIP</td>
<td>The IP address of the node where the Pod us running.</td>
</tr>
<tr>
<td>metadata.name</td>
<td>The Pod name (notice that this is different than the container’s name. A Pod may have more than one container)</td>
</tr>
<tr>
<td>metadata.namespace</td>
<td>The namespace of the Pod</td>
</tr>
<tr>
<td>status.podIP</td>
<td>The IP address of the Pod</td>
</tr>
<tr>
<td>spec.serviceAccountName</td>
<td>The service account that was used with the Pod.</td>
</tr>
<tr>
<td>metadata.uid</td>
<td>The UID of the running Pod</td>
</tr>
<tr>
<td>metadata.labels[<em>‘label</em>‘]</td>
<td>The value of the label put on the Pod. For example, if a Pod is labeled env&#x3D;prod, then metadata.labels[‘env’] returns ‘prod’.</td>
</tr>
<tr>
<td>metadata.annotations[‘annotation’]</td>
<td>Similar to labels, it gets the value of the specified annotation.</td>
</tr>
</tbody></table>
<h1 id="ResourceFieldRef"><a href="#ResourceFieldRef" class="headerlink" title="ResourceFieldRef"></a>ResourceFieldRef</h1><p>fieldRef参数允许您注入有关Pod的元数据。如果需要有关Pod消耗的资源（即CPU和内存）的数据，则应使用resourceFieldRef。以下是可用于获取此数据的可用选项的列表：</p>
<table>
<thead>
<tr>
<th><strong>Name</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td>requests.cpu</td>
<td>The amount of CPU specified in the requests field of the Pod definition</td>
</tr>
<tr>
<td>requests.memory</td>
<td>The amount of memory specified in the requests field of the Pod definition</td>
</tr>
<tr>
<td>limits.cpu</td>
<td>The CPU limit of the Pod</td>
</tr>
<tr>
<td>limits.memory</td>
<td>The memory limit of the Pod</td>
</tr>
</tbody></table>
<p>可以在其定义文件内的Pod上添加requests和limits。它们都允许您控制给定Pod可以消耗多少资源的硬性限制和软性限制。它们还帮助调度程序根据其资源请求和限制将Pod分配给适当的节点。有关此主题的更多信息，您可以参考我们的容量规划文章。</p>
<h1 id="在修改它们之后获取Pod元数据"><a href="#在修改它们之后获取Pod元数据" class="headerlink" title="在修改它们之后获取Pod元数据"></a>在修改它们之后获取Pod元数据</h1><p>在Pod运行时，允许Kubernetes用户更改Pod的某些元数据。因此，否则无法更改诸如资源请求和限制之类的字段是无法修改的, 除非删除Pod, 但可以更改Pod标签。如果用户发出了kubectl edit pod pod_name命令，则只要该字段接受，他们就可以对Pod定义进行动态修改。</p>
<p>如果Pod的元数据已动态更改，则无法通过环境变量将它们重新注入到容器中，因为这需要重新启动容器。但是，您仍然可以通过使用将数据注入到容器中的另一种方法（卷）来监视和捕获这些更改。</p>
<p>以下定义演示了如何使用与configMaps和Secrets一起使用卷的方式相同的卷，以允许容器访问数据：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">prod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">bash</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mycontainer</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;bash&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;sleep 1000000&#x27;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mypod-vol</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/mypod-metadata</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mypod-vol</span></span><br><span class="line">    <span class="attr">downwardAPI:</span></span><br><span class="line">      <span class="attr">items:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">labels</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.labels</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">annotations</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.annotations</span></span><br></pre></td></tr></table></figure>

<p>应用此定义并登录到Pod，我们可以看到已经为我们安装了包含相关数据的卷：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">title: Kubernetes学习(使用PodSecurityPolicy)</span><br><span class="line">comments: flase</span><br><span class="line"><span class="built_in">date</span>: 2019-12-13 17:10:53</span><br><span class="line">tags: Kubernetes</span><br><span class="line">categories: Kubernetes</span><br></pre></td></tr></table></figure>

<p>如您所见，我们通过存储在卷上的文件而不是环境变量来获取数据。这使我们能够在信息变化的任何地方动态地检索信息，而无需重新启动容器。但是，仍需要对正在运行的应用程序进行配置，以检测标签或注释文件中的更改（如果正在使用其值），并在进行修改时采取相应的措施。</p>
<ul>
<li>在很多情况下，您需要应用程序了解运行它的基础结构的某些元数据。应用程序可以使用此信息来做出明智的决定或使手动任务自动化。</li>
<li>Kubernetes提供了Downward API，该API允许您将其中一些元数据注入Pod，并使其中的容器可以访问它们。</li>
<li>Downward API允许您在API服务器上查询许多元数据项以及资源请求和限制。</li>
<li>您可以通过环境变量或安装卷将所需的信息注入Pod。</li>
<li>使用环境变量的不利之处在于它们无法反映对Pod的元数据（如标签和注释）的动态更改。您可以将卷用作解决方法。</li>
<li>尽管Downward API是实现Pod反射的一种优雅方法，但它仍然可以提供的数据量受到限制。还有其他Pod方面可能需要通过Downward API提供，而其他方面则不提供。解决此缺点的方法是让应用程序直接查询API服务器以获取丢失的数据。许多编程语言中都有许多客户端库，使您可以通过代码查询API服务器。</li>
</ul>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes学习(Kubernetes集群运行Spark-Job提示Invalid-DER:-object-is-not-integer错误的解决办法)</title>
    <url>/2020/02/20/Kubernetes-Spark-Job-InvalidDER/</url>
    <content><![CDATA[<p>最近有个同事说他在测试kubernetes中跑spark，在运行spark-job时提示<code>Invalid DER: object is not integer</code>，但是如果在线上环境跑同样的任务就没问题, 束手无措，想让我帮看一下什么问题</p>
<span id="more"></span>



<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>在测试Kubernetes执行spark-job时提示的错误完整的如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200309123610.png"></p>
<h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>从上面的报错来看, 很明显涉及kubernetes.client的问题, spark用的当然是java的client</p>
<p>首先, 可以排除是spark, 同事可以确认spark版本及spark-job完全一样.</p>
<p>那么很容易就联想到两个环境下的Kubernetes是有区别的, 开先, 我的同事怀疑是用户认证的问题, 这个基本可以排除, 我觉得是kubernetes集群上的区别关系很大, 那么很自然, 从差别着手</p>
<p>首先，发现集群的版本不一致, 测试环境下的kubernetes的版本较低, 那版本的差异很有问题. </p>
<p><strong>上kubernetes的github查找issue</strong>, 发现一个相近的问题.</p>
<p><a href="https://github.com/fabric8io/kubernetes-client/issues/1314">https://github.com/fabric8io/kubernetes-client/issues/1314</a></p>
<p>kubernetes的java client 在github这个项目<a href="https://github.com/fabric8io/kubernetes-client">https://github.com/fabric8io/kubernetes-client</a>上的<a href="https://github.com/fabric8io/kubernetes-client/issues/1314">issue</a></p>
<p>再根据提交的代码记录发现问题:</p>
<p>在kubernetes的java client早版本的代码里</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200309141145.png"></p>
<p>在ClientCertificateAuthentication.java这个文件中, 是写死了证书的加密类型只能是RSA, 而测试的kubernetes使用的是minikube搭建的, 默认情况下使用的证书加密类型为EC(Eliptical Curve), 而测试的kubernetes并不支持, 如何查看证书的类型呢, 很简单, cat ca-key.pem, 第一行都标注了证书加密类型. </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200310103152.png"></p>
<p>线上环境没问题是因为kubernetes使用的版本已经支持了EC这种加密类型</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>所以，问题就很清晰了,有两个解决方案, **要么升级java client, 要么使用RSA的证书类型.**由于spark已经是最新版本的了, 因此只能切换到RSA证书，这其实还是比较麻烦的, 我建议直接使用新版本的kubernetes重新搭建, 也比换证书要快, 比较minikube、kubeadm一键式还是很爽的.</p>
<p>所以, <strong>多多查找对应project的issue, 你遇到的问题, 大概率别人都遇到过.</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/fabric8io/kubernetes-client/issues/1314">https://github.com/fabric8io/kubernetes-client/issues/1314</a></li>
<li><a href="https://github.com/fabric8io/kubernetes-client">https://github.com/fabric8io/kubernetes-client</a></li>
<li><a href="https://github.com/fabric8io/kubernetes-client/issues/1314">https://github.com/fabric8io/kubernetes-client/issues/1314</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(volume挂载情况汇总)</title>
    <url>/2020/02/04/Kubernetes-Volumes-Mount/</url>
    <content><![CDATA[<p>在kubernetes中, 如果某个pod需要使用到volume, 且要将该volume挂载到宿主机上, 则需要注意存在被覆盖的情况，在这做一个汇总.</p>
<span id="more"></span>

<p>当挂载一个文件或者目录，宿主机或者容器存在与否, 所有情况如下:</p>
<table>
<thead>
<tr>
<th><strong>宿主机</strong></th>
<th><strong>容器</strong></th>
<th><strong>运行结果</strong></th>
</tr>
</thead>
<tbody><tr>
<td>文件存在</td>
<td>文件不存在</td>
<td>挂载成功，容器中新建文件</td>
</tr>
<tr>
<td>目录存在</td>
<td>目录不存在</td>
<td>挂载成功，容器中新建目录</td>
</tr>
<tr>
<td>文件不存在</td>
<td>文件存在</td>
<td>挂载成功，宿主机新建该文件，容器中的文件被覆盖为空</td>
</tr>
<tr>
<td>文件存在</td>
<td>文件存在</td>
<td>挂载成功，容器内的文件内容被覆盖</td>
</tr>
<tr>
<td>目录存在</td>
<td>目录存在</td>
<td>挂载成功，容器内目录内容被覆盖</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>文件存在</td>
<td>目录存在</td>
<td>容器启动失败</td>
</tr>
<tr>
<td>目录存在</td>
<td>文件存在</td>
<td>容器启动失败</td>
</tr>
<tr>
<td>——</td>
<td>——</td>
<td>——</td>
</tr>
<tr>
<td>目录不存在</td>
<td>目录不存在</td>
<td>挂载成功，docker会自动在宿主机和容器内新建此目录</td>
</tr>
<tr>
<td>目录不存在</td>
<td>文件存在</td>
<td>容器启动失败</td>
</tr>
<tr>
<td>目录不存在</td>
<td>目录存在</td>
<td>挂载成功，容器内目录内容被覆盖（空）</td>
</tr>
</tbody></table>
<p>除了使用volume之外，还经常使用到cofigmap对象，需要特别注意的是，如果使用configmap提示以下错误，</p>
<p>说明这次的挂载将一个路径挂载到了一个文件中, 这里需要使用subpath.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2060F5C5-4E0F-4EC6-A4CD-1CF7AC3CCBAA.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">Kubernetes.io</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(apiserver证书中新增IP/DNS)</title>
    <url>/2020/04/29/Kubernetes-add-new-cert-into-apiserver/</url>
    <content><![CDATA[<p>主要用于修改由kubeadm部署的集群需要修改apiserver中证书信息的DNS. 比如apiserver的vip突然变动就会需要这个操作,防范于未然, 到真遇到的时候也不至于手忙脚乱不知所措, 有预案就好办. </p>
<span id="more"></span>



<p>注: 以下步骤是假定集群部署时使用kubeadm, 且使用的默认的CA签署的证书.</p>
<p>###Kubeadm config配置修改 </p>
<h4 id="查看证书中包含的dns"><a href="#查看证书中包含的dns" class="headerlink" title="查看证书中包含的dns"></a>查看证书中包含的dns</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -<span class="keyword">in</span> apiserver.crt -noout -text</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200507144124.png"></p>
<p>可以看到当前证书中已经存在的认证的域名</p>
<p>查看当前存在的kubeadm config.yaml</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm config view &gt; /root/kubeadmconf.yml</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">v1.15.4</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="string">&quot;172.20.25.26:16443&quot;</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">&quot;registry.aliyuncs.com/google_containers&quot;</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">podSubnet:</span> <span class="string">&quot;10.244.0.0/16&quot;</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">certSANs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;172.20.25.26&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;k8s.cluster.local&quot;</span> <span class="comment"># 新增域名</span></span><br></pre></td></tr></table></figure>

<h4 id="删除证书"><a href="#删除证书" class="headerlink" title="删除证书"></a>删除证书</h4><p>删除k8s证书目录中的apiserver.crt 、apiserver.key</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/pki</span><br><span class="line"><span class="built_in">mv</span> apiserver.crt apiserver.key ~</span><br></pre></td></tr></table></figure>

<h4 id="生成新证书"><a href="#生成新证书" class="headerlink" title="生成新证书"></a>生成新证书</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm init phase certs apiserver --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>

<h4 id="更新kubeadm-ConfigMap"><a href="#更新kubeadm-ConfigMap" class="headerlink" title="更新kubeadm ConfigMap"></a>更新kubeadm ConfigMap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm config upload from-file --config /root/kubeadmconf.yml</span><br></pre></td></tr></table></figure>

<h4 id="重启kubelet"><a href="#重启kubelet" class="headerlink" title="重启kubelet"></a>重启kubelet</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<h4 id="重启apiserver"><a href="#重启apiserver" class="headerlink" title="重启apiserver"></a>重启apiserver</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker <span class="built_in">rm</span> -f</span><br></pre></td></tr></table></figure>

<p>再次查看证书后会发现新增域名已经包含在证书里面了.</p>
<h3 id="certSANs的作用"><a href="#certSANs的作用" class="headerlink" title="certSANs的作用"></a>certSANs的作用</h3><p>Kubernetes apiserver使用数字证书来加密去往&#x2F;来自API服务器的流量以及对与apiserver的连接进行身份验证。这样，如果您尝试使用诸如kubectl之类的命令行客户端连接到APIserver，并且使用的证书Subject Alternative Names（SAN）列表中未包含的主机名或IP地址，就会得到一个错误，指示证书对于指定的IP地址或主机名无效。要解决此问题，需要更新证书，以便SAN列表中包含用于访问API服务器的所有IP地址或主机名</p>
<p>使用kubeadm默认部署的情况下, kubeadm会将一些默认的dns比如<code>kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local</code>等加入到证书列表中</p>
<p>这个从kubeadm的部署日志中可以看到</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">I0326 02:57:52.228780    2975 version.go:237] remote version is much newer: v1.14.0; falling back to: stable-1.13</span><br><span class="line">[certs] Generating <span class="string">&quot;apiserver&quot;</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.148.0.39]</span><br></pre></td></tr></table></figure>

<p>因此也建议在部署时, <strong>将所有master节点的hostname、ip、127.0.0.1、apiserver的VIP等加入到certSANs列表中</strong></p>
<h3 id="rancher是如何实现的"><a href="#rancher是如何实现的" class="headerlink" title="rancher是如何实现的"></a>rancher是如何实现的</h3><p>因为生产环境使用的是rancher做为k8s集群管理平台, 看到上面的结论让我想到一个疑问:</p>
<p><strong>既然在证书列表中的ip才可以通过apiserver的认证, 为何从rancher上导出的kubeconfig file入到本地机器(ip不在apiserver的证书列表中, 与rancher平台网络是想通的)也可以访问集群？</strong></p>
<p>其实想想也好解释, 我们在rancher上的操作，其实都不是直接访问的apiserver，而是通过rancher去连接的apiserver, 想想在用rancher托管k8s集群时, rancher是不是需要在k8s集群中安装agent?</p>
<p>答案就在agent上，rancher web上的所有操作都被转到了rancher server上而被下发到rancher agent上, 所有的认证及权限或者其它跟apiserver通信的操作都由rancher agent上完成</p>
<p>而rancher agent是以daemonset的方式部署在master上, 这样，master的ip自然是在apiserver的认证列表中.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200507164258.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">https://kubernetes.io</a></li>
<li><a href="https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/">https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(关于deployment中的port定义)</title>
    <url>/2019/12/27/Kubernetes-about-deployment-ports/</url>
    <content><![CDATA[<p>不知大家在写deployment yaml文件的时候会不会主动地定义port字段, 相信在最开始的时候有很多同学会跟我一样对deployment中的port字段有些疑惑, <strong>不是有service吗? 在deployment中为何还存在port的定义呢?</strong></p>
<span id="more"></span>



<p>最简的deployment</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kua100201</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">strategy:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">16379</span> <span class="comment"># here</span></span><br><span class="line">        <span class="attr">resources:</span> &#123;&#125;</span><br><span class="line"><span class="attr">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>service</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">service:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ratings</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>deployment中的port对应用端口有什么影响呢?, 如果这里的端口与service的端口不一样又该如何呢?</p>
<p>首先来看一下deployment关于port部分的<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#container-v1-core">说明</a>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port <span class="built_in">which</span> is listening on the default <span class="string">&quot;0.0.0.0&quot;</span> address inside a container will be accessible from the network. Cannot be updated</span><br></pre></td></tr></table></figure>

<p>大概的意思就是: 列举容器中需要暴露的端口列表, 但是只做起着信息说明的作用, 并不能影响端口真正暴露端口.</p>
<p>我们来测试一下如果deployment中的端口与service中定义的端口不一样会有什么影响?</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl clusterip port(6379) -- 正常</span></span><br><span class="line"><span class="comment"># curl podip + port(6379) -- 正常</span></span><br><span class="line"><span class="comment"># curl podip + 16379 --curl: (7) Failed to connect to 10.244.0.104 port 16379: Connection refused</span></span><br></pre></td></tr></table></figure>

<p>所以可以看到, **虽然在deployment中指定了port， 但这个端口只是用于起着提示类作用,也就是说deployment可以随便指定端口, 但是为了友好的告诉其它人这个应用的监听端口,所以会在deployment中指定端口列表 **</p>
<p>这个跟<code>dockerfile中使用expose</code>来暴露端口一样, 虽然是叫暴露,也只是起着告诉别人这个应用对外监听的端口,实际上对应用真正暴露端口并没有直接关系.</p>
<p>在rancher中部署应用的时, 很人性化的有多种类型让我们选择</p>
<blockquote>
<ul>
<li>NodePort:  所有node节点上都会开启这个端口来接收请求然后转发到这个应用上</li>
<li>HostPort: 跟使用–network&#x3D;host类似, 通过应用最终部署到的host上接收请求然后转发到应用上</li>
<li>CLUSTERIP: service模式</li>
<li>layer-4 load balancer: 这种不常用</li>
</ul>
</blockquote>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200507173634.png"></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200507173601.png"></p>
<p>rancher上部署应用会直接创建对应的service, 而且会在deployment中记录ports的相关信息, 还是很方便的.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span>      </span><br><span class="line">       <span class="attr">ports:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8232</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">8232tcp2</span></span><br><span class="line">         <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#container-v1-core">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#container-v1-core</a></li>
<li><a href="https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/">https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(又是bridge-nf-call-iptables惹的祸)</title>
    <url>/2021/08/18/Kubernetes-bridge-nf-call-iptables/</url>
    <content><![CDATA[<p>使用kubernetes遇到最多的70%问题都可以归于网络问题，最近发现如果内核参数: <code>bridge-nf-call-iptables</code>设置不当的话会影响kubernetes中Node节点上的Pod通过ClusterIP去访问<strong>同Node</strong>上的其它pod时会有超时现象，复盘记录一下排查的前因后因.</p>
<span id="more"></span>



<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>集群环境为K8s v1.15.9，cni指定了flannel-vxlan跟portmap, kube-proxy使用mode为ipvs</p>
<p>问题现象是，<strong>某个Node节点上的pod通过service访问其它服务时，有些能通，有些不通，不通的都提示timeout</strong></p>
<p>异常的请求:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -v http://panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080</span><br><span class="line">* Rebuilt URL to: http://panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080/</span><br><span class="line">*   Trying 10.233.53.172...</span><br><span class="line">* TCP_NODELAY <span class="built_in">set</span></span><br><span class="line"><span class="comment"># 这里一直等待直到超时</span></span><br></pre></td></tr></table></figure>

<p>这个节点上的很多pod都存在这个问题， 其它节点未发现异常.</p>
<p>正常请求:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -v http://panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080</span><br><span class="line">* Rebuilt URL to: http://panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080/</span><br><span class="line">*   Trying 10.233.53.172...</span><br><span class="line">* TCP_NODELAY <span class="built_in">set</span></span><br><span class="line">* Connected to panorama-v2-frontend-service.spring-prod.svc.cluster.local (10.233.53.172) port 8080 (<span class="comment">#0)</span></span><br><span class="line">&gt; GET / HTTP/1.1</span><br><span class="line">&gt; Host: panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080</span><br><span class="line">&gt; User-Agent: curl/7.52.1</span><br><span class="line">&gt; Accept: */*</span><br><span class="line">&gt; </span><br><span class="line">&lt; HTTP/1.1 200 OK</span><br></pre></td></tr></table></figure>



<h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>先对异常Node进行排查，因为其它节点是正常的,通过对异常Node的网络、kube-proxy、 iptables、ipvs规则、kubelet等排查后未发现有可疑的地方，就暂时排除嫌疑了.</p>
<p>第二个可疑的是DNS, coreDNS负责对service解析成ClusterIP, 在出问题的Node上经过多次测试均能正确解析，<strong>coreDNS排除嫌疑</strong></p>
<p>可疑的地方都筛了一遍,无果, 那就只能再从现象来发现看看有没有相同点.</p>
<p>首先，访问路径为: <strong>Service – &gt; ClusterIP – &gt; PodIP</strong></p>
<p>对service访问异常，coreDNS已经被排除了，那先绕过service，直接使用ClusterIP访问呢? 测试后现象依旧</p>
<p>那再绕过ClusterIP，直接使用PodIP呢? Bingo，之前会出问题的访问都是正常的了.</p>
<p>那么问题就出在CluterIP – &gt; PodIP上, 那么又有以下可能:</p>
<blockquote>
<ul>
<li><ol>
<li>ClusterIP没有正确转发到PodIP上可能导致超时</li>
</ol>
</li>
<li><ol start="2">
<li>如果正确转发，响应没有返回也可能导致超时</li>
</ol>
</li>
</ul>
</blockquote>
<p>第一种可能性很容易排查，之前已经确认了ipvs规则、iptables规则都是没有问题的，且通过ClusterIP发起的请求可以到达PodIP上, 基本就排除了可能性一</p>
<p>另外，对比正常跟异常请求会发现，异常的请求原Pod跟目标pod都是在同一个Node上，而正常的请求则处于不同的Node，会是这个影响吗？</p>
<p>上面的可能性二，只能祭出抓包神器了<strong>tcpdump</strong>, 通过抓包发现(<strong>抓包过程见文未</strong>)会发现请求中出现了<strong>Reset</strong></p>
<p>那么问题转换一下: 为什么相同Node上podA通过service&#x2F;ClusterIP访问PodB响应会不返回呢，而通过PodIP访问就没问题？</p>
<p>补充一句就是，<strong>相同Node上的pod相互访问是不需要经过Flannel的，因此Flannel可以排除嫌疑</strong></p>
<p>so, 问题在哪?</p>
<p>回到tcpdump的抓包数据, 可以发现，响应的数据没有按照请求的路径返回，嗯，Interesting</p>
<h3 id="罪魁祸首"><a href="#罪魁祸首" class="headerlink" title="罪魁祸首"></a>罪魁祸首</h3><p>不管是 iptables 还是 ipvs 模式，Kubernetes 中访问 service都会进行 DNAT，将原本访问 ClusterIP:Port 的数据包 <strong>DNAT</strong> 成 service 的某个 Endpoint (PodIP:Port)，然后内核将连接信息插入 conntrack 表以记录连接，目的端回包的时候内核从 conntrack 表匹配连接并<strong>SNAT</strong>，这样原路返回形成一个完整的连接链路.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210818235131.png"></p>
<p>从tcpdump看到请求被reset了, 没错, <strong>bridge-nf-call-iptables</strong>(如果是ipv6的话则是<strong>net.bridge.bridge-nf-call-ip6tables</strong>)参数</p>
<p>但是不对，这个参数linux默认开启的呢?难道是有人修改了么？</p>
<p>使用命令查看该参数是否开启:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line"><span class="comment"># 0</span></span><br></pre></td></tr></table></figure>

<p>返回0，说明确实没有开启(后来被证实是被同事修改了)，那这个参数是如何影响的返回路径的呢?</p>
<p>那就不得不说<strong>linux bridge</strong>了</p>
<p>虽然CNI使用的是flannel, 但flannel封装的也是linux bridge，<strong>linux bridge是虚拟的二层转发设备</strong>，而 iptables conntrack 是在三层上，所以如果直接访问同一网桥内的地址(ip同一网段)，就会直接走二层转发，不经过 conntrack:</p>
<p>结合上面的图来看，同Node通过service访问pod的访问路径如下:</p>
<blockquote>
<ul>
<li><ol>
<li>PodA 访问 service, 经过coreDNS解析成Cluster IP，不是网桥内的地址(ClusterIP一般跟PodIP不在一个网段)，走Conntrack,进行DNAT，将ClusterIP转换成PodIP:Port</li>
</ol>
</li>
<li><ol start="2">
<li>DNAT 后发现是要转发到了<strong>同节点</strong>上的 PodB，PodB 回包时发现目的 IP(此时是PodA的IP) 在同一网桥上(PodA与PodB的IP段一致)，就直接走二层转发了，不会去调 conntrack，这样就导致回包时没有原路返回</li>
</ol>
</li>
</ul>
</blockquote>
<p>没有返回包就导致请求方一直等直到超时退出.</p>
<p>这样也解释了为何访问在其它节点的应用的ClusterIP没有问题，因为目标PodIP与源PodIP不在同一个网段上,肯定要走conntrack.</p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>总述，开启参数后问题解决</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;net.bridge.bridge-nf-call-iptables=1&quot;</span> &gt;&gt; /etc/sysctl.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;net.bridge.bridge-nf-call-ip6tables=1&quot;</span> &gt;&gt; /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line">sysctl -p /etc/sysctl.conf</span><br></pre></td></tr></table></figure>



<h3 id="linux-conntrack"><a href="#linux-conntrack" class="headerlink" title="linux conntrack"></a>linux conntrack</h3><p>关于conntrack其实也是个值得好好研究一番的知识点, 各个发行版都有工具可以看到conntrack里的记录,格式如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># conntrack -L</span></span><br><span class="line">tcp      6 119 SYN_SENT src=10.224.1.34 dst=10.233.53.172 sport=56916 dport=8080 [UNREPLIED] src=10.224.1.56 dst=10.224.1.34 sport=8080 dport=56916 mark=0 use=1</span><br></pre></td></tr></table></figure>

<p>那个著名的<a href="https://izsk.me/2020/06/10/Kubernetes-coredns-5s-timeout/">DNS 5s timeout</a>的问题就跟conntrack机制有关，由于篇幅有限，就不在这里展开.</p>
<h3 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a>tcpdump</h3><p>在容器中的抓包命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tcpdump -vvv host 10.224.1.34 or 10.233.53.172 or 10.224.1.56</span><br></pre></td></tr></table></figure>

<p>其中的三个ip分别对应podA IP, podB的ClusterIP, podB的PodIP</p>
<p>这里由于篇幅的关系，只保存有关键信息，同时使用注释是作者加入的，方便理解.</p>
<p>对于异常请求的tcpdump，如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># podA 请求 PodB</span></span><br><span class="line">panorama-frontend-deploy-c8f6fd4b6-52tvf.45954 &gt; panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080: Flags [S], <span class="built_in">cksum</span> 0x4cc5 (incorrect -&gt; 0xba1b), <span class="built_in">seq</span> 1108986852, win 28200, options [mss 1410,sackOK,TS val 1345430037 ecr 0,nop,wscale 7], length 0</span><br><span class="line"><span class="comment"># 10-224-1-56是PodB的podIP, 这里省略了解析过程,可以看到返回数据给PodA</span></span><br><span class="line">10-224-1-56.panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080 &gt; panorama-frontend-deploy-c8f6fd4b6-52tvf.45954: Flags [S.], <span class="built_in">cksum</span> 0x1848 (incorrect -&gt; 0x99ac), <span class="built_in">seq</span> 3860576650, ack 1108986853, win 27960, options [mss 1410,sackOK,TS val 2444502128 ecr 1345430037,nop,wscale 7], length 0</span><br><span class="line"><span class="comment"># 重点: podA直接reset了请求.</span></span><br><span class="line">panorama-frontend-deploy-c8f6fd4b6-52tvf.45954 &gt; 10-224-1-56.panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080: Flags [R], <span class="built_in">cksum</span> 0xb6b5 (correct), <span class="built_in">seq</span> 1108986853, win 0, length 0</span><br></pre></td></tr></table></figure>

<p>最后会发现PodA给PodB发送了个<strong>R Flags</strong>, 也就是<strong>reset</strong>, 就是因为当PodB返回<strong>握手确认</strong>给到PodA，PodA根本不认识这个请求，所以直接给reset掉了, <strong>三手握手都没有建立</strong>，this is why!</p>
<p>而对于<code>net.bridge.bridge-nf-call-iptables=1</code>的正常请求的tcpdump如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 能看到正常的三次握手， 这里省略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启传输数据</span></span><br><span class="line">panorama-frontend-deploy-c8f6fd4b6-52tvf.36434 &gt; panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080: Flags [P.], <span class="built_in">cksum</span> 0x4d3c (incorrect -&gt; 0x6f84), <span class="built_in">seq</span> 1:128, ack 1, win 221, options [nop,nop,TS val 1346139372 ecr 2445211463], length 127: HTTP, length: 127</span><br><span class="line">    GET / HTTP/1.1</span><br><span class="line">    Host: panorama-v2-frontend-service.spring-prod.svc.cluster.local:8080</span><br><span class="line">    User-Agent: curl/7.52.1</span><br><span class="line">    Accept: */*</span><br><span class="line"></span><br><span class="line">panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080 &gt; panorama-frontend-deploy-c8f6fd4b6-52tvf.36434: Flags [.], <span class="built_in">cksum</span> 0x4cbd (incorrect -&gt; 0xe8a6), <span class="built_in">seq</span> 1, ack 128, win 219, options [nop,nop,TS val 2445211463 ecr 1346139372], length 0</span><br><span class="line"></span><br><span class="line">panorama-v2-frontend-service.spring-prod.svc.cluster.local.8080 &gt; panorama-frontend-deploy-c8f6fd4b6-52tvf.36434: Flags [P.], <span class="built_in">cksum</span> 0x4dac (incorrect -&gt; 0x0421), <span class="built_in">seq</span> 1:240, ack 128, win 219, options [nop,nop,TS val 2445211463 ecr 1346139372], length 239: HTTP, length: 239</span><br><span class="line">    HTTP/1.1 200 OK</span><br><span class="line">    Server: nginx/1.17.1</span><br><span class="line">    Date: Wed, 18 Aug 2021 15:10:17 GMT</span><br><span class="line">    Content-Type: text/html</span><br><span class="line">    Content-Length: 1540</span><br><span class="line">    Last-Modified: Fri, 09 Jul 2021 06:36:53 GMT</span><br><span class="line">    Connection: keep-alive</span><br><span class="line">    ETag: <span class="string">&quot;60e7ee85-604&quot;</span></span><br><span class="line">    Accept-Ranges: bytes</span><br></pre></td></tr></table></figure>

<p>相信这个请求路径还是很清晰的，就不再啰嗦.</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>禁用<strong>net.bridge.bridge-nf-call-ip6tables</strong>这个参数当然也有好外，那就是考虑同网段的IP访问没必要走conntrack，一定程度有助于性能.</p>
<p>kubernetes的<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">官方文档</a>中<strong>明确</strong>提及Node节点上需要开启这个参数,不然碰到各种诡异的现象也只是时间问题，所以还是不要随意调整。</p>
<p>以防后患的话可以对该参数是否开启进行监控,防止被人误修改.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2020/06/10/Kubernetes-coredns-5s-timeout">https://izsk.me/2020/06/10/Kubernetes-coredns-5s-timeout</a></li>
<li><a href="https://wiki.libvirt.org/page/Net.bridge.bridge-nf-call_and_sysctl.conf">https://wiki.libvirt.org/page/Net.bridge.bridge-nf-call_and_sysctl.conf</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1761367">https://cloud.tencent.com/developer/article/1761367</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></li>
<li><a href="https://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/">https://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/85422#issuecomment-714629138">https://github.com/kubernetes/kubernetes/issues/85422#issuecomment-714629138</a></li>
<li><a href="https://www.stackrox.com/post/2020/01/kubernetes-networking-demystified/">https://www.stackrox.com/post/2020/01/kubernetes-networking-demystified/</a></li>
<li><a href="https://blog.csdn.net/u010278923/article/details/81735970">https://blog.csdn.net/u010278923/article/details/81735970</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践四)</title>
    <url>/2021/07/06/Kubernetes-Infinitband-SRIOV-network-4-knows/</url>
    <content><![CDATA[<p>前面记录的都是在现有的集群中如何基于infiniband实现集群中的pod同时能够拥有两个网络, 其中一些实现细节并没有展开，个人觉得还是有必要再总结总结.</p>
<span id="more"></span>

<h3 id="Infinitband"><a href="#Infinitband" class="headerlink" title="Infinitband"></a>Infinitband</h3><p>由于infinitband涉及到硬件, 是一种比较复杂的高速网络，个人能力有限，在这之前也没有接触过该技术，如果想全方位的了解的话，网上的有些资料还是很有帮助的，感兴趣的可以参考下列连接</p>
<p><a href="https://zhuanlan.zhihu.com/p/74238082">关于InfiniBand架构和知识点漫谈</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/163104439">谈谈InfiniBand和Ethernet网络的差异</a></p>
<p><a href="https://blog.csdn.net/swingwang/article/details/72935461">InfiniBand主流厂商和产品分析</a></p>
<p>通过个人的实践及网上的资料，有以下结论:</p>
<ol>
<li>infiniband网络与以太网之间是无法直接通信的，但目前有些交换机可以进行协议的转换，最直接便捷的方法是带有IB网卡的机器也配置以太网网卡即可</li>
<li>infiniband设置现在只有少量的厂家有能力生产， intel跟Mellanox是佼佼者. Mellanox占据龙头位置，同时Mellanox能够为infiniband技术提供全套服务</li>
<li>Infinitband一般用于连接海量数据交换场景，比如直连存储, AI节点GPU数据交换等，但Infinitband毕竟是需要硬件支撑，非常昂贵，也有DPDK,RoCE等方案也能提供高速网络.</li>
</ol>
<h3 id="SR-IOV"><a href="#SR-IOV" class="headerlink" title="SR-IOV"></a>SR-IOV</h3><p>SR-IOV (Single Root Input&#x2F;Output Virtualization) 的主要作用是将一个物理设备模拟成多个虚拟设备,在虚拟化技术中使用非常频繁，</p>
<p>比如一台物理机上可以虚拟出多台虚机，虚机中的网卡就可以通过SR-iov技术虚拟出来的</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210711203304.png"></p>
<p><strong>sr-iov需要硬件上的支持，前提条件就是硬件需要支持SRIOV，主板要支持VT-d技术，好在目前大部分服务器都支持了该技术</strong></p>
<p>但是sr-iov跟传统的虚拟化技术有啥区别呢:</p>
<p>以前的虚拟化技术大多都是通过软件模拟的方式模拟出相关的硬件功能，<strong>而sriov是硬件级别的虚拟化，也就是说在使用方来看，这跟真实的硬件几乎没有差别，因此性能大大的提升</strong></p>
<p>通过sriov虚拟出来的硬件之间是隔离的,在某一时刻，一个 VF 只能被分配给一个虚机。一个虚机可以拥有多个 VF</p>
<p>SR-IOV 驱动是在内核中实现的.</p>
<p>总结以下知识点:</p>
<ol>
<li><p>PF就是物理网卡所支持的一项PCI(元件扩展接口)功能，PF可以扩展出若干个VF</p>
</li>
<li><p>VF是支持SRIOV的物理网卡所虚拟出的一个“网卡”或者说虚出来的一个实例，它会以一个独立网卡的形式呈现出来，每一个VF有它自己独享的PCI配置区域，并且可能与其他VF共享着同一个物理资源（公用同一个物理网口）</p>
</li>
<li><p>PF miniport driver在VF之前最先加载,VF及PF之间是隔离的，任何经由VF驱动或所执行的结果都不会影响到其他的VF或PF</p>
</li>
<li><p>Network Interface Card（NIC）即物理网卡，在启用SRIOV之后会生成若干vport，物理NIC所要做的就是转发physical port与vport之间的流量</p>
</li>
<li><p>physical port顾名思义就是物理网口，在SRIOV场景中physical port充当一个面向对外的网络媒介</p>
</li>
<li><p>VPort是个抽象出来的接口，类似于物理网口，它们被映射给每一个VF或者PF，供parentOS或guestOS来使用</p>
<p>通过以上架构的描述就可以看出，启用SRIOV之后，物理NIC将通过VF与虚拟机（VF driver）进行数据交互，反之亦然。那么这样一来即可跳过中间的虚拟化堆栈（即VMM层），以达到近乎于纯物理环境的性能；这一点也是SRIOV最大的价值所在，它有别于以往虚拟机通过仿真设备和虚拟化层进行流量传递的情况</p>
<p>同样，sr-iov也是一种很复杂的技术，这里也只能挑一些重点记录，毕竟，我们也只是用.</p>
</li>
</ol>
<h3 id="sriov-network-cni"><a href="#sriov-network-cni" class="headerlink" title="sriov-network-cni"></a>sriov-network-cni</h3><p>如果pod想使用基于物理网通过sriov虚拟出的网络，那这个时候就需要实现network-cni逻辑,用于给pod分配ip.</p>
<h3 id="sriov-network-device-plugin"><a href="#sriov-network-device-plugin" class="headerlink" title="sriov-network-device-plugin"></a>sriov-network-device-plugin</h3><p>同理，虚拟出来的网卡的数据是有限的，不可能一个ip可以分配给多个pod，那这个时候虚拟IP就是一种device资源，自然要通过device-plugin来进行增删改查,在pod新建时网卡的库存减1，销毁时加1.</p>
<h3 id="sriov-network-operator"><a href="#sriov-network-operator" class="headerlink" title="sriov-network-operator"></a>sriov-network-operator</h3><p>而sriov-network-operator则是基于operator的机制次上述两种方案组合在一起, 方便部署，其它里面启动的也就上述两个pod</p>
<h3 id="multus"><a href="#multus" class="headerlink" title="multus"></a>multus</h3><p>在大多数情况下，一个k8s集群中只会有一种容器网络，如果想同时使用多种的话，k8s本身是没有方案的，而multus则可以处理这个场景，<strong>multus其实就是个代理</strong>，它会获取所有pod创建前请求的cni，然后根据配置文件将请求转发到真实的CNI插件上，比如flannel，比如sriov-cni等等，这些插件的路径默认在&#x2F;opt&#x2F;cni&#x2F;bin，在multus中，可以指定一种容器网络为默认的容器网络，通过解析pod的annotation来是否存在<code>k8s.v1.cni.cncf.io/networks</code>来判断该pod需要请求的cni。</p>
<h3 id="whereabouts"><a href="#whereabouts" class="headerlink" title="whereabouts"></a>whereabouts</h3><p>pod请求cni二进制后会从ip池中选择ip，那IP池要怎么管理呢?如果只是一台机器的话，则通过host-local这种最简单的方案也可以实现对ip增删改查来实现管理，host-local会在本地的某个路径下，如果是已经分配出去的ip,则会以ip为主新建一个文件，如果释放就删除，这样的话，就能知道哪些ip已经分配，哪些还没有，但是如果在多个node共享一个ip段的话，host-local显然是无法做到分布式，比如flannel是通过etcd来实现分布式ip分配不重复的，而<strong>whereabouts也是一种代理工具,用于解决ip分配问题</strong>，它支持etcd做为后端的存储，而kubernetes本身就用etcd做为数据库，因此whereabouts可以很人性化的直接指定Kubernetes来使用集群的etcd，这样就完美地解决了分布式ip不重复的问题.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/347204232">https://zhuanlan.zhihu.com/p/347204232</a></li>
<li><a href="https://blog.csdn.net/wangdd_199326/article/details/90476728">https://blog.csdn.net/wangdd_199326/article/details/90476728</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/74238082">https://zhuanlan.zhihu.com/p/74238082</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/163104439">https://zhuanlan.zhihu.com/p/163104439</a></li>
<li><a href="https://blog.csdn.net/swingwang/article/details/72935461">https://blog.csdn.net/swingwang/article/details/72935461</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(capsule实现多租户模型)</title>
    <url>/2023/07/21/Kubernetes-capsule/</url>
    <content><![CDATA[<p>capsule做为kubernetes的多租户模型的另一实现, 与hnc有异曲同工之处,由于Capsule的声明性，以及所有的配置都可存储在Git中，因此Capsule原生具备GitOps特征</p>
<span id="more"></span>

<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>在单个集群中，Capsule Controller将多个命名空间聚合在一个名为Tenant的轻量级抽象中，基本上是一组Kubernetes命名空间。在每个租户中，用户可以自由地创建他们的名称空间并共享所有分配的资源。<br>另一方面，<strong>capsule 策略引擎保持不同租户彼此隔离。网络和安全策略、资源配额、限制范围、RBAC以及在租户级别定义的其他策略会自动由租户中的所有命名空间继承</strong>。然后，用户可以自由地自主地操作他们的租户，而不需要集群管理员的干预</p>
<p><img src="https://capsule.clastix.io/assets/static/capsule-operator.d853076.ce76636fce3e3130134a6f768700f6f7.svg"></p>
<h3 id="租户"><a href="#租户" class="headerlink" title="租户"></a>租户</h3><p>在Capsule中，租户是一种抽象，用于在集群管理员定义的一组边界内将多个命名空间分组到单个实体中。然后将租户分配给称为租户所有者的用户或用户组<br><strong>只有集群管理员才有权限操作Tenant对象</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Tenant</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">oil</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">owners:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">alice</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">system:serviceaccount:tenant-system:robot</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br></pre></td></tr></table></figure>

<p>声明了一个oil的租户, 它的租户管理员是User为alice的人或者是sa, 具有相应权限的sa也可以是租户管理员<br>每个租户都有一个委派的用户或一组用户作为租户管理员。在capsule中这被称为租户所有者。其他用户可以在租户内部使用由租户所有者直接分配的不同级别的权限和授权进行操作。<br><strong>Capsule不关心集群中使用的身份验证策略，并且支持所有Kubernetes身份验证方法</strong>。使用Capsule的唯一要求是将租户用户分配到由<code>--capsule-user-group</code>选项定义的组(该参数很重要，可以传入多个值)，默认安装时这个参数的值为capsule.clastix.io。</p>
<p>具体的身份验证策略详见下文</p>
<p>一般在集群管理员创建完租户后, 会将相应的集群认证文件(常见的如kubeconfig文件，相当还可以是token等)发送给owners列表中的用户， 他们便可使用以下命令查看是否有创建namespace的权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 具有创建ns的权限</span></span><br><span class="line">kubectl auth can-i create namespaces</span><br><span class="line">Warning: resource <span class="string">&#x27;namespaces&#x27;</span> is not namespace scoped</span><br><span class="line"><span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 具有删除ns的权限,只能删除由它创建的ns</span></span><br><span class="line">kubectl auth can-i delete ns -n oil-production</span><br><span class="line"><span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是没有get ns的权限</span></span><br><span class="line">kubectl auth can-i get namespaces</span><br><span class="line">no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 没有get 其它集群级别资源的权限</span></span><br><span class="line">kubectl auth can-i get nodes</span><br><span class="line">no</span><br><span class="line">kubectl auth can-i get persistentvolumes</span><br><span class="line">no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也不能创建及get租户资源</span></span><br><span class="line">kubectl auth can-i get tenants</span><br><span class="line">no</span><br></pre></td></tr></table></figure>

<p><strong>Owner的用户可以创建&#x2F;删除ns,但无法获取除ns之个的集群级别的资源权限, 相关的权限都限制在它创建的ns中</strong><br>默认情况下，租户所有者将通过RoleBinding API被授予两个ClusterRole资源：</p>
<blockquote>
<ul>
<li>Kubernetes admin(<strong>可以理解为是限制在namespace中的admin而不是真的集群admin</strong>)，由它授权大多数命名空间范围内的资源</li>
<li>由Capsule创建名为capsule-namespace-deleter的权限，允许删除创建的命名空间，所以Owner有删除ns的权限</li>
</ul>
</blockquote>
<p><strong>要注意的一点是，由于Owner支持serviceaccount，需要确保不能使用groupsystem：serviceaccounts或groupsystem：serviceaccounts：{capsule-namespace}作为Capsule组，否则您将在Capsule控制器中出现短路，因为Capsule本身由serviceaccount控制</strong></p>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><h4 id="Tenant"><a href="#Tenant" class="headerlink" title="Tenant"></a>Tenant</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Tenant</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">oil</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">owners:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">alice</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">additionalRoleBindings:</span> <span class="comment"># 可以为相关的用户指定额外的权限，与rbac进行联动</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">clusterRoleName:</span> <span class="string">&#x27;argoproj-provisioner&#x27;</span></span><br><span class="line">      <span class="attr">subjects:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">alice</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">joe</span></span><br><span class="line">  <span class="attr">namespaceOptions:</span> <span class="comment"># 可以创建的最大namespace数量</span></span><br><span class="line">    <span class="attr">quota:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">additionalMetadata:</span>  <span class="comment"># 给ns添加annotation及label</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">storagelocationtype:</span> <span class="string">s3</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">capsule.clastix.io/backup:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">resourceQuotas:</span> <span class="comment"># ns的resourcequota, 最终转换成ns的resourcequota对象, 也支持自定义的资源quota</span></span><br><span class="line">    <span class="attr">scope:</span> <span class="string">Tenant</span> <span class="comment"># 可以是tenant级别(该tenant下的所有的ns有效), 也可以是namespace级别(针对ns有限, ns可通过label selector选定)</span></span><br><span class="line">    <span class="attr">items:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">hard:</span></span><br><span class="line">        <span class="attr">limits.cpu:</span> <span class="string">&quot;8&quot;</span></span><br><span class="line">        <span class="attr">limits.memory:</span> <span class="string">16Gi</span></span><br><span class="line">        <span class="attr">requests.cpu:</span> <span class="string">&quot;8&quot;</span></span><br><span class="line">        <span class="attr">requests.memory:</span> <span class="string">16Gi</span></span><br><span class="line">        <span class="attr">pods:</span> <span class="string">&quot;10&quot;</span></span><br><span class="line">  <span class="attr">limitRanges:</span>  <span class="comment"># 容器的limitrange，最终转换成ns的limitrange对象</span></span><br><span class="line">    <span class="attr">items:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">limits:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">default:</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">        <span class="attr">defaultRequest:</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">Container</span></span><br><span class="line">  <span class="attr">priorityClasses:</span> <span class="comment"># 限制该tenant下的所有pod的优先级</span></span><br><span class="line">    <span class="attr">allowed:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">custom</span> <span class="comment"># 自定义的PriorityClass</span></span><br><span class="line">    <span class="attr">allowedRegex:</span> <span class="string">&quot;^tier-.*$&quot;</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">env:</span> <span class="string">&quot;production&quot;</span></span><br><span class="line">  <span class="attr">nodeSelector:</span>  <span class="comment"># 给tenant下的所有pod指定调度策略，当然也包含亲和及非亲和属性</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">oil</span></span><br><span class="line">    <span class="attr">kubernetes.io/os:</span> <span class="string">linux</span></span><br><span class="line">  <span class="comment"># ... 更多属性</span></span><br></pre></td></tr></table></figure>

<p>tenant支持的属性非常多，这里不一一列出，可以看出，实现这些属性的机制是通过了策略引擎的方式</p>
<h4 id="GlobalTenantResource"><a href="#GlobalTenantResource" class="headerlink" title="GlobalTenantResource"></a>GlobalTenantResource</h4><p>GlobalTenantResource被设计用于在选定的tenant对象中共享资源</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">GlobalTenantResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fossil-pull-secrets</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tenantSelector:</span>  <span class="comment"># 通过label选择tenant, 只有tenant对象上具有该label才能继承resources中指定的对象</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">energy:</span> <span class="string">fossil</span></span><br><span class="line">  <span class="attr">resyncPeriod:</span> <span class="string">60s</span>  <span class="comment"># 资源是否已复制并同步的时间间隔</span></span><br><span class="line">  <span class="attr">resources:</span>  <span class="comment"># 指定需要共享的资源对象列表，同样，这里支持任一对象，包含CRD</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespacedItems:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line">          <span class="attr">namespace:</span> <span class="string">harbor-system</span> <span class="comment"># 在harbor-system中带有tenant: fossil label的secret都将被共享</span></span><br><span class="line">          <span class="attr">selector:</span></span><br><span class="line">            <span class="attr">matchLabels:</span></span><br><span class="line">              <span class="attr">tenant:</span> <span class="string">fossil</span></span><br></pre></td></tr></table></figure>

<p>由于GlobalTenantResource是群集范围内的资源，因此只有群集管理员才能操作该对象, 如果想在某一tenant中的所有ns共享资源，又如何实现呢？</p>
<h4 id="TenantResource"><a href="#TenantResource" class="headerlink" title="TenantResource"></a>TenantResource</h4><p>TenantResource则是用于实现上述目的，tenant的Owner用户可以直接操作该对象</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TenantResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">solar-db</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">solar-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resyncPeriod:</span> <span class="string">60s</span> <span class="comment"># 资源是否已复制并同步的时间间隔</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span> <span class="comment"># 选择namespace</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">environment:</span> <span class="string">production</span></span><br><span class="line">      <span class="attr">rawItems:</span>  <span class="comment"># 选择资源对象列表</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">postgresql.cnpg.io/v1</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">Cluster</span></span><br><span class="line">          <span class="attr">metadata:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">postgresql</span></span><br><span class="line">          <span class="attr">spec:</span></span><br><span class="line">            <span class="attr">description:</span> <span class="string">PostgreSQL</span> <span class="string">cluster</span> <span class="string">for</span> <span class="string">the</span> <span class="string">Solar</span> <span class="string">project</span></span><br><span class="line">            <span class="attr">instances:</span> <span class="number">3</span></span><br><span class="line">            <span class="attr">postgresql:</span></span><br><span class="line">              <span class="attr">pg_hba:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">hostssl</span> <span class="string">app</span> <span class="string">all</span> <span class="string">all</span> <span class="string">cert</span></span><br><span class="line">            <span class="attr">primaryUpdateStrategy:</span> <span class="string">unsupervised</span></span><br><span class="line">            <span class="attr">storage:</span></span><br><span class="line">              <span class="attr">size:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>

<p>有了这些属性，Tenant几乎可以实现任何的多租户需求<br>参考<a href="https://capsule.clastix.io/docs/general/tutorial">官网</a></p>
<h3 id="身份验证"><a href="#身份验证" class="headerlink" title="身份验证"></a>身份验证</h3><h4 id="X509"><a href="#X509" class="headerlink" title="X509"></a>X509</h4><p>官方给出的case是基于x509证书的方式实现的Owner身份认证，主要是hack&#x2F;create-user.sh脚本，这里讲一下流程<br>首先通过openssl创建证书: key及csr</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl genrsa -out <span class="variable">$&#123;USER&#125;</span>-<span class="variable">$&#123;TENANT&#125;</span>.key 2048</span><br><span class="line">openssl req -new -key <span class="variable">$&#123;USER&#125;</span>-<span class="variable">$&#123;TENANT&#125;</span>.key -subj <span class="string">&quot;/CN=<span class="variable">$&#123;USER&#125;</span><span class="variable">$&#123;MERGED_GROUPS&#125;</span>&quot;</span> -out <span class="variable">$&#123;TMPDIR&#125;</span>/<span class="variable">$&#123;USER&#125;</span>-<span class="variable">$&#123;TENANT&#125;</span>.csr</span><br></pre></td></tr></table></figure>

<p>重点关注-subj参数, CN表示common Name, USER是脚本传入的用户，即tenant的Owner中指定的用户, MERGED_GROUPS为默认的capsule.clastix.io,然后生成CertificateSigningRequest对象提交给集群，集群自动approve及签发相关证书并生成USER的kubeconfig文件.由于CN中已经传入了USER, 这个USER也属于capsule.clastix.io组， 正好与–capsule-user-group参数capsule.clastix.io(默认值)一致,因此在创建完成后, USER使用kubeconfig文件即可拥有Tenant Owner权限,整个流程跟正常搭建kubernetes证书的生成是一个道理。</p>
<p>但是生产中不可能一个一个手工去创建用户证书, 前面说过，<strong>Capsule不关心集群中使用的身份验证策略，并且支持所有Kubernetes身份验证方法</strong><br>所以只要是支持了kubernetes的身份验证策略的系统或者是平台，都可以对接上capsule, 以下再以rancher为例进行说明</p>
<h4 id="对接Rancher"><a href="#对接Rancher" class="headerlink" title="对接Rancher"></a>对接Rancher</h4><p>rancher做为一款常用的集群管理平台, 支持对接多种账号体系,如AD&#x2F;LDAP, 这里以LDAP为例(rancher如何配置AD不属于本篇内容，不再赘述),<br>这里简单说一下原理: <strong>当使用LDAP账号登陆rancher时，rancher会从LDAP中验证是否为合法用户，如果是则将该用户保存在自身的CRD对象中,同时生成该用户的唯一ID,同时这个用户会有一个用户组，也由rancher生成。<br>rancher在与kubernetes集群交互时，使用kubernetes提供的<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation"><code>user impersontion</code></a>机制进行通信，将ldap中的用户信息(比如cn&#x3D;zsk)封装成自身的用户机制(id&#x3D;u-32ipck5o7k)去与kubernetes交互</strong></p>
<p>上述机制很重要,所以这种情况下，在指定Tenant的Owener时 User应该是LDAP中的用户吗？答案是否定的，而应该要是rancher中的用户</p>
<p>rancher中的User对象信息如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">management.cattle.io/v3</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">displayName:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">controller.cattle.io/mgmt-auth-users-controller</span></span><br><span class="line">    <span class="attr">cattle.io/creator:</span> <span class="string">norman</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">u-32ipck5o7k</span></span><br><span class="line"><span class="attr">principalIds:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">openldap_user://CN=zsk,OU=xxx,OU=xxx,DC=xxx,DC=xxx</span>  <span class="comment"># ldap相关的用户组</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">local://u-32ipck5o7k</span>  <span class="comment"># 每一个ldap用户都会对应rancher的一个本地用户</span></span><br></pre></td></tr></table></figure>

<p>所以capsule正确的设置userGroups的配置文件应该是, 与–capsule-user-group参数是等价的:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CapsuleConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">capsule-default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">enableTLSReconciler:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">forceTenantPrefix:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">protectedNamespaceRegex:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">capsule.clastix.io</span>  <span class="comment"># 这里保留默认的用户组</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">openldap_group://CN=zsk,OU=yyy,DC=zzz,DC=com</span></span><br></pre></td></tr></table></figure>

<p>openldap_group是rancher基于ldap机制时写入到kubernetes中用户对应的group信息,这个参数就是我们需要的. 在集群中查找某个用户的信息可使用以下方法:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrole </span><br><span class="line"><span class="comment"># 查看以cattle-impersonation-u-32ipck5o7k(为例) 开头的对象, 其中32ipck5o7k可以在rancher UI中的用户管理中看到)</span></span><br></pre></td></tr></table></figure>

<p>要注意的是，<strong>查询用户信息需要在rancher自身的集群中查询，而不是在rancher托管的集群</strong></p>
<p>相关issue:</p>
<ol>
<li><a href="https://github.com/clastix/capsule/issues/761">how capsule integrate with rancher under openLDAP</a></li>
</ol>
<h3 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps"></a>GitOps</h3><p>capsule的所有属性都可以定义在git中, 那么与gitops的衔接可谓是非常便捷, 如flux, argoCD等gitops工具, 这一块不再赘述<br>可参考官网的详细步聚<a href="https://capsule.clastix.io/docs/guides/flux2-capsule">flux2-capsule</a></p>
<h3 id="capsule-proxy"><a href="#capsule-proxy" class="headerlink" title="capsule-proxy"></a>capsule-proxy</h3><p>这里也简单提一下capsule-proxy项目,从上面的实践可以看出，虽然capsule通过webhook机制可以将权限限定在特定的namespaces中,但还是会存在下列一类的问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get namespaces</span><br><span class="line">Error from server (Forbidden): namespaces is forbidden:</span><br><span class="line">User <span class="string">&quot;alice&quot;</span> cannot list resource <span class="string">&quot;namespaces&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;&quot;</span> at the cluster scope</span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是在get时又会提示有权限</span></span><br><span class="line">$ kubectl auth can-i [get|list|watch|delete] ns oil-production</span><br><span class="line"><span class="built_in">yes</span></span><br></pre></td></tr></table></figure>

<p>原因是RBAC操作仅在Cluster-Scope中可用，并且不授予没有适当权限的用户。<br>为了解决这类问题，capsule引入了自己的解决方案, 即capsule-proxy.capsule-proxy实现了一个简单的反向代理，它只拦截对API服务器的<a href="https://capsule.clastix.io/docs/general/proxy">特定请求</a>,<br>然后capsule完成请求的转换、响应、返回数据,这样就可解决上述问题<br>相关的issue:</p>
<ol>
<li><a href="https://github.com/clastix/capsule-proxy/issues/303">can list all pod in all namespace under a tenant?</a></li>
<li><a href="https://github.com/clastix/capsule/issues/756">Feature: additional pod metadata</a></li>
</ol>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li><p>当Tenant Owner用户使用命令创建ns时: <code>kubectl create ns zsk</code>, 提示以下错误:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error from server (You <span class="keyword">do</span> not have any Tenant assigned: please, reach out to the system administrators): admission webhook <span class="string">&quot;owner.namespace.capsule.clastix.io&quot;</span> denied the request: You <span class="keyword">do</span> not have any Tenant assigned: please, reach out to the system administrators</span><br></pre></td></tr></table></figure>

<p>原因: 出现这个报错的原因是在tenant中的owners.name设置成了ldap中的cn(zsk), 会与CapsuleConfiguration中的userGroups对应不上, 导致通过userinfo无法找到绑定的tenant.<br>解决: 修改为rancher中的用户组即可(openldap_group:&#x2F;&#x2F;CN&#x3D;zsk,OU&#x3D;yyy,DC&#x3D;zzz,DC&#x3D;com),参考对接Rancher一节</p>
</li>
<li><p>如何解决Tenant下ns的resourcequota自定义问题: </p>
<p>解决: 由于需要为该租户下的每个ns设定resourcequota, tenant对象支持设置resourcequota会统一下发到所有ns, 但考虑到后续业务上会出现ns的resourcequota的大小不一, 因此resourcequota不能设置在tenant上，<br>可以通过tenantresource这个对象将resourcequota进行传播，同时使用namespaceselector来指定传播到ns范围,<br>tenant在创建ns时支持给ns添加label或者是annotation，因此可以与tenantresource中的namespaceselector对应起来，这样处理后就能实现tenant下的ns可以自定义resourcequota<br>不是必须: 这里最好有一个单独的ns(如ns-robot)专门用于传播对象，这样可以放心使用namespaceSelector选择ns而不用当心会重复选择到tenantresource所在的ns.如果没有namespaceSelector则默认是该租户下的所有ns<br><strong>当resourcequota期间变小后, 不会对现在ns里的pod产生影响，但会出现已经使用的quota超过了ns的resourcequota的现象</strong></p>
<p>例:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">capsule.clastix.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TenantResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tnt-default-rq</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">notebook</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resyncPeriod:</span> <span class="string">120s</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">tenant-rq.k2.com/use-default:</span> <span class="string">enable</span> </span><br><span class="line">      <span class="attr">rawItems:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line">          <span class="attr">metadata:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">tnt-default-rq-notebook</span></span><br><span class="line">          <span class="attr">spec:</span></span><br><span class="line">            <span class="attr">hard:</span></span><br><span class="line">              <span class="attr">limits.cpu:</span> <span class="string">&quot;120&quot;</span></span><br><span class="line">              <span class="attr">limits.memory:</span> <span class="string">&quot;928Gi&quot;</span></span><br><span class="line">              <span class="attr">requests.cpu:</span> <span class="string">&quot;120&quot;</span></span><br><span class="line">              <span class="attr">requests.memory:</span> <span class="string">&quot;928Gi&quot;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://capsule.clastix.io/">https://capsule.clastix.io/</a></li>
<li><a href="https://capsule.clastix.io/docs/general/proxy">https://capsule.clastix.io/docs/general/proxy</a></li>
<li><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation">https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(operator部署高可用Clickhouse)</title>
    <url>/2020/05/30/Kubernetes-ck-operator-install/</url>
    <content><![CDATA[<p>ClickHouse是一种用于在线分析处理的面向列的开源DBMS。 由俄罗斯IT公司Yandex为Yandex.Metrica网站分析服务开发的, 多用于OLAP分析需求. 今天记录下clickhouse-operator的搭建部署.</p>
<span id="more"></span>

<p>这里只会分享ck operator的部署流程, 至于它的原理及operator的原理，不会详细说明.</p>
<p>Ck以2*2为例， 使用表复制.</p>
<h3 id="部署zookeeper"><a href="#部署zookeeper" class="headerlink" title="部署zookeeper"></a>部署zookeeper</h3><p>由于表复制需要zk的协调, 这里直接使用ck github上的zk容器部署方案, 3节点statefulset，yaml文件大家可参考<a href="https://github.com/Altinity/clickhouse-operator/blob/master/deploy/zookeeper/quick-start-volume-emptyDir/zookeeper-3-nodes.yaml">这里</a>，当然在生产环境下量好使用pv来保障可用性, 由于只是测试环境这里直接使用的是emptyDir.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">-n</span> <span class="string">clickhouse-operator</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">zk-node-3.yaml</span></span><br></pre></td></tr></table></figure>



<h3 id="node打taints"><a href="#node打taints" class="headerlink" title="node打taints"></a>node打taints</h3><p>考虑到ck的资源消耗, 因此需要使用taint来控制其调度</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加taint</span></span><br><span class="line">kubectl taint nodes k8s-node-249 clickhouse=<span class="built_in">enable</span>:NoExecute</span><br><span class="line"><span class="comment"># 添加label</span></span><br><span class="line">kubectl label nodes kube-node-249 clickhouse=<span class="built_in">enable</span></span><br></pre></td></tr></table></figure>



<h3 id="local-PV"><a href="#local-PV" class="headerlink" title="local PV"></a>local PV</h3><h3 id="配置local-PV"><a href="#配置local-PV" class="headerlink" title="配置local PV"></a>配置local PV</h3><p>配置ck使用的存储, 这里没有使用共享存储, 直接使用宿主机的systemFile做为PV</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bigdata-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/no-provisioner</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ck-pv-0</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span>  </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/bigdata</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">enable</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ck-pv-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span>  </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/bigdata</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">enable</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ck-pv-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span>  </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/bigdata</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">enable</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ck-pv-3</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span>  </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/bigdata</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">enable</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f pv-pvc.yaml</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619144317.png"></p>
<h3 id="部署operator"><a href="#部署operator" class="headerlink" title="部署operator"></a>部署operator</h3><p>使用<code>OPERATOR_NAMESPACE=clickhouse-operator</code>变量指定需要部署的ns，这里为<code>clickhouse-operator</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -s https://raw.githubusercontent.com/Altinity/clickhouse-operator/master/deploy/operator-web-installer/clickhouse-operator-install.sh| OPERATOR_NAMESPACE=clickhouse-operator bash</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619144254.png"></p>
<h3 id="部署Clickhouse"><a href="#部署Clickhouse" class="headerlink" title="部署Clickhouse"></a>部署Clickhouse</h3><h4 id="yaml文件"><a href="#yaml文件" class="headerlink" title="yaml文件"></a>yaml文件</h4><p>这里以<code>ck 2*2</code>为例子, 即 <code>2shard+ 2replicas</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&#x27;clickhouse.altinity.com/v1&#x27;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&#x27;ClickHouseInstallation&#x27;</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&#x27;1box&#x27;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">defaults:</span></span><br><span class="line">    <span class="attr">templates:</span></span><br><span class="line">      <span class="attr">serviceTemplate:</span> <span class="string">service-template</span></span><br><span class="line">      <span class="attr">podTemplate:</span> <span class="string">pod-template</span></span><br><span class="line">      <span class="attr">dataVolumeClaimTemplate:</span> <span class="string">volume-claim</span></span><br><span class="line">  <span class="attr">configuration:</span></span><br><span class="line">    <span class="attr">settings:</span></span><br><span class="line">      <span class="attr">timezone:</span> <span class="string">&quot;Asia/Shanghai&quot;</span></span><br><span class="line">    <span class="attr">zookeeper:</span></span><br><span class="line">      <span class="attr">nodes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">zookeeper.clickhouse-operator</span></span><br><span class="line">          <span class="attr">port:</span> <span class="number">2181</span></span><br><span class="line">    <span class="attr">clusters:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&#x27;realty-ck&#x27;</span></span><br><span class="line">        <span class="attr">layout:</span></span><br><span class="line">          <span class="attr">shardsCount:</span> <span class="number">2</span></span><br><span class="line">          <span class="attr">replicasCount:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">templates:</span></span><br><span class="line">    <span class="attr">serviceTemplates:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">service-template</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">8123</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tcp</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">9000</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">          <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">    <span class="attr">podTemplates:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pod-template</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">tolerations:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">              <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">Equal</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">enable</span></span><br><span class="line">          <span class="attr">affinity:</span></span><br><span class="line">            <span class="attr">nodeAffinity:</span></span><br><span class="line">              <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">                <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">                      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&#x27;clickhouse&#x27;</span></span><br><span class="line">                        <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                        <span class="attr">values:</span></span><br><span class="line">                          <span class="bullet">-</span> <span class="string">&#x27;enable&#x27;</span></span><br><span class="line">            <span class="attr">podAntiAffinity:</span></span><br><span class="line">              <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">                    <span class="attr">matchExpressions:</span></span><br><span class="line">                      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&#x27;clickhouse.altinity.com/app&#x27;</span></span><br><span class="line">                        <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                        <span class="attr">values:</span></span><br><span class="line">                          <span class="bullet">-</span> <span class="string">&#x27;chop&#x27;</span></span><br><span class="line">                  <span class="attr">topologyKey:</span> <span class="string">&#x27;kubernetes.io/hostname&#x27;</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">clickhouse</span></span><br><span class="line">              <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">              <span class="attr">image:</span> <span class="string">yandex/clickhouse-server:latest</span></span><br><span class="line">              <span class="attr">ports:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">                  <span class="attr">containerPort:</span> <span class="number">8123</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">client</span></span><br><span class="line">                  <span class="attr">containerPort:</span> <span class="number">9000</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">interserver</span></span><br><span class="line">                  <span class="attr">containerPort:</span> <span class="number">9009</span></span><br><span class="line">              <span class="attr">volumeMounts:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volume-claim</span></span><br><span class="line">                  <span class="attr">mountPath:</span> <span class="string">/var/lib/clickhouse</span></span><br><span class="line">              <span class="attr">resources:</span></span><br><span class="line">                <span class="attr">limits:</span></span><br><span class="line">                  <span class="attr">memory:</span> <span class="string">&#x27;4Gi&#x27;</span></span><br><span class="line">                  <span class="attr">cpu:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line">                <span class="attr">requests:</span></span><br><span class="line">                  <span class="attr">memory:</span> <span class="string">&#x27;4Gi&#x27;</span></span><br><span class="line">                  <span class="attr">cpu:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volume-claim</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">accessModes:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">          <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>说明: </p>
<ol>
<li>由于节点上打了taint， 因此需要使用tolerations</li>
<li>由于使用了local PV, 因此使用nodeAffinity来限制pod调度到合适的节点，同时使用podAntiAffinity来限制相同的clickhouse pod不调度到相同节点.</li>
<li>可以限制使用的cpu&#x2F;memory, 以免ck对整个集群造成影响.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n clickhouse-operator apply -f ck-2x2-cluster.yaml</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>默认情况下, ck-operator的默认用户名密码为cli, 可通过clickhouse-operator&#x2F;deploy&#x2F;operator&#x2F;clickhouse-operator-install-template.yaml&#96;修改</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改 01-clickhouse-user.xml， 大约在1682行</span></span><br><span class="line"><span class="string">&lt;password_sha256_hex&gt;716b36073a90c6fe1d445ac1af85f4777c5b7a155cea359961826a030513e448&lt;/password_sha256_hex&gt;</span></span><br><span class="line"><span class="comment"># 修改</span></span><br><span class="line"><span class="attr">chUsername:</span> <span class="string">clickhouse_operator</span></span><br><span class="line"><span class="attr">chPassword:</span> <span class="string">clickhouse_operator_password</span> <span class="comment"># 密码， 大约在1480行</span></span><br><span class="line"><span class="attr">chPort:</span> <span class="number">812</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="验证pod运行"><a href="#验证pod运行" class="headerlink" title="验证pod运行"></a>验证pod运行</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619145058.png"></p>
<p>可以看到, 出现了4个pod刚好对应<code>2*2</code>且pv都由avaliable变成bound状态，说明按照预期的方式调度</p>
<p><em>创建的pod名称例如chi-{metadata}-{clusterName}-0-0-0等格式</em></p>
<h4 id="查看configmap"><a href="#查看configmap" class="headerlink" title="查看configmap"></a>查看configmap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get cm -n clickhouse-operator</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619145156.png"></p>
<h4 id="查看svc-x2F-ep对象"><a href="#查看svc-x2F-ep对象" class="headerlink" title="查看svc&#x2F;ep对象"></a>查看svc&#x2F;ep对象</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619145611.png"></p>
<h4 id="登录集群"><a href="#登录集群" class="headerlink" title="登录集群"></a>登录集群</h4><p>最后可以通过<code>clickhouse-1box</code>这个service来登录ck集群</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">clickhouse-client -h clickhouse-1box --port 9000</span><br></pre></td></tr></table></figure>



<h3 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h3><p>在pv对应的目录下，会发现ck operator生成的目录结构如下:</p>
<h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── access</span><br><span class="line">│   ├── quotas.list</span><br><span class="line">│   ├── roles.list</span><br><span class="line">│   ├── row_policies.list</span><br><span class="line">│   ├── settings_profiles.list</span><br><span class="line">│   └── users.list</span><br><span class="line">├── data</span><br><span class="line">│   ├── default</span><br><span class="line">│   └── system</span><br><span class="line">│       ├── metric_log</span><br><span class="line">│       └── trace_log</span><br><span class="line">├── dictionaries_lib</span><br><span class="line">├── flags</span><br><span class="line">├── format_schemas</span><br><span class="line">├── metadata</span><br><span class="line">│   ├── default</span><br><span class="line">│   ├── default.sql</span><br><span class="line">│   └── system</span><br><span class="line">│       ├── metric_log.sql</span><br><span class="line">│       └── trace_log.sql</span><br><span class="line">├── metadata_dropped</span><br><span class="line">├── preprocessed_configs</span><br><span class="line">│   ├── config.xml</span><br><span class="line">│   └── users.xml</span><br><span class="line">├── status</span><br><span class="line">├── tmp</span><br><span class="line">└── user_files</span><br></pre></td></tr></table></figure>



<h4 id="启动进程"><a href="#启动进程" class="headerlink" title="启动进程"></a>启动进程</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619153737.png"></p>
<p>clickhouse启动时的主要配置文件为&#x2F;etc&#x2F;clickhouse-server&#x2F;config.xml, 但其它的配置文件如<code>/etc/clickhouse-server/config.d</code>等里面的配置可以覆盖主配置. <a href="%60https://clickhouse.tech/docs/en/operations/configuration-files/%60">参考</a></p>
<h4 id="容器挂载"><a href="#容器挂载" class="headerlink" title="容器挂载"></a>容器挂载</h4><p>看一下pod中挂载情况, </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /var/lib/clickhouse</span><br><span class="line">      name: volume-claim</span><br><span class="line">    - mountPath: /etc/clickhouse-server/config.d/</span><br><span class="line">      name: chi-1box-common-configd</span><br><span class="line">    - mountPath: /etc/clickhouse-server/users.d/</span><br><span class="line">      name: chi-1box-common-usersd</span><br><span class="line">    - mountPath: /etc/clickhouse-server/conf.d/</span><br><span class="line">      name: chi-1box-deploy-confd-realty-ck-0-0</span><br><span class="line">         </span><br><span class="line">volumes:</span><br><span class="line">  - name: volume-claim</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: volume-claim-chi-1box-realty-ck-0-0-0</span><br><span class="line">  - configMap:</span><br><span class="line">      defaultMode: 420</span><br><span class="line">      name: chi-1box-common-configd</span><br><span class="line">    name: chi-1box-common-configd</span><br><span class="line">  - configMap:</span><br><span class="line">      defaultMode: 420</span><br><span class="line">      name: chi-1box-common-usersd</span><br><span class="line">    name: chi-1box-common-usersd</span><br><span class="line">  - configMap:</span><br><span class="line">      defaultMode: 420</span><br><span class="line">      name: chi-1box-deploy-confd-realty-ck-0-0</span><br><span class="line">    name: chi-1box-deploy-confd-realty-ck-0-0</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<p>可以发现, 有2个common配置, common配置是4个pod都相同的配置, 也就是跟集群相关的配置，另外每个pod还有特定的配置，每个pod都不相同，如macros.xml</p>
<h4 id="配置文件分析"><a href="#配置文件分析" class="headerlink" title="配置文件分析"></a>配置文件分析</h4><p>部署完，生成的operator的配置如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200622105300.png"></p>
<p>其中, 下面几个带operator相关的配置是operator本身的配置, 可以直接用官方的，也可以根据实际情况微调，但并不是所有的配置文件都会用上, 上面的那几个则为clickhouse集群相关的配置, 不需要关注， 这里重点看以下文件.</p>
<h5 id="config-xml"><a href="#config-xml" class="headerlink" title="config.xml"></a>config.xml</h5><p>这个是clickhouse启动时指定的主配置文件, 该配置文件可以被config.d下的配置文件覆盖或者合并</p>
<h5 id="config-d"><a href="#config-d" class="headerlink" title="config.d"></a>config.d</h5><p>Path to folder where ClickHouse configuration files common for all instances within CHI are located.</p>
<p>集群相关的配置文件, 分片相关的配置就在该目录下</p>
<h5 id="conf-d"><a href="#conf-d" class="headerlink" title="conf.d"></a>conf.d</h5><p>Path to folder where ClickHouse configuration files unique for each instance (host) within CHI are located.</p>
<p>每个实际独有的配置, 如实例名</p>
<h5 id="users-d-x2F-users-xml"><a href="#users-d-x2F-users-xml" class="headerlink" title="users.d&#x2F;users.xml"></a>users.d&#x2F;users.xml</h5><p>Path to folder where ClickHouse configuration files with users settings are located.</p>
<p>Files are common for all instances within CHI</p>
<p>用户权限相关的配置</p>
<p>配置文件相关的大家可自行查看内容，比较好理解.</p>
<h4 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200619155956.png"></p>
<p>如果不写端口，域名中默认端口9000</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200622122841.png"></p>
<p>至此, 整个clickhouse-operator就部署完了, github上docs&#x2F;chi-example有非常多的例子适用各种场合,非常有参考价值.</p>
<p>后续还有操作比如分片扩容, ck版本升级等操作, github上也有详细的说明.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://clickhouse.tech/">https://clickhouse.tech/</a></li>
<li><a href="https://clickhouse.tech/docs/en/">https://clickhouse.tech/docs/en/</a></li>
<li><a href="https://github.com/Altinity/clickhouse-operator/blob/master/deploy/zookeeper/quick-start-volume-emptyDir/zookeeper-3-nodes.yaml">https://github.com/Altinity/clickhouse-operator/blob/master/deploy/zookeeper/quick-start-volume-emptyDir/zookeeper-3-nodes.yaml</a></li>
<li><a href="https://blog.csdn.net/weixin_39992480/article/details/104938427">https://blog.csdn.net/weixin_39992480/article/details/104938427</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(daemonset)</title>
    <url>/2020/01/24/Kubernetes-daemonset/</url>
    <content><![CDATA[<p>daemonset是kubernetes常用且重要的资源对象, DaemonSet 可以保证集群中所有的或者部分的节点都能够运行同一份 Pod 副本，每当有新的节点被加入到集群时，Pod 就会在目标的节点上启动，如果节点被从集群中剔除，节点上的 Pod 也会被垃圾收集器清除, 比如xx-agent， fluentd, node-exporter等, daemonset在kubernetes中随处可见.</p>
<span id="more"></span>



<h3 id="daemonset创建"><a href="#daemonset创建" class="headerlink" title="daemonset创建"></a>daemonset创建</h3><p>daemonset无法通过kubectl命令行直接进行创建, 可以先–dry-run先创建deployment后再修改</p>
<p>这里以kube-proxy的daemonset yaml文件为例:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/usr/local/bin/kube-proxy</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--config=/var/lib/kube-proxy/config.conf</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--hostname-override=$(NODE_NAME)</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_NAME</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/google_containers/kube-proxy:v1.17.0</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">        <span class="attr">resources:</span> &#123;&#125;</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">        <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/lib/kube-proxy</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">beta.kubernetes.io/os:</span> <span class="string">linux</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">      <span class="attr">securityContext:</span> &#123;&#125;</span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">kube-proxy</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">kube-proxy</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">CriticalAddonsOnly</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">FileOrCreate</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/lib/modules</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">lib-modules</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">	<span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>跟deployment非常相似, 可以看到，yaml文件里没有replicas字段.</p>
<p>当然daemonset也是会受调度器影响的, 也就是说, 不一定会在所有的机器都会部署,也可能是符合条件的部分.</p>
<h3 id="daemonset调度"><a href="#daemonset调度" class="headerlink" title="daemonset调度"></a>daemonset调度</h3><p>在kubernetes的v1.12之前的版本, daemonset是直接由daemonset controller进行调度, 不会通过 kube-scheduler,但直接使用daemonset controller会造成一些<a href="https://draveness.me/kubernetes-daemonset">问题</a>, 因此在v1.12之后改成了由默认的kube-scheduler进行调度, 可以从上面的yaml文件中<code>schedulerName</code>看出</p>
<p>特别地，kube-proxy中使用了</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">CriticalAddonsOnly</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line"><span class="comment">#上面两个条件是并行的</span></span><br></pre></td></tr></table></figure>

<p>这里使用了tolerations, 也就是对节点上的taint进行容忍, 特别注意第二个条件</p>
<p><code>- operator: &quot;Exists&quot;</code></p>
<p>即这个 toleration 能容忍任意 taint，所以添加任何地taint都无法驱逐kube-proxy</p>
<p>kube-proxy做为kubernetes中service的实现方式,对服务的转发至关重要, 因此必须要在所有节点上进行部署</p>
<p>从kubernetes的官方文档中可以看到daemonset 还会默认增加的toleration.</p>
<table>
<thead>
<tr>
<th>Toleration Key</th>
<th>Effect</th>
<th>Version</th>
<th>Alpha Features</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>node.kubernetes.io&#x2F;not-ready</td>
<td>NoExecute</td>
<td>1.8+</td>
<td>TaintBasedEvictions</td>
<td>when TaintBasedEvictions is enabled,they will not be evicted when there are node problems such as a network partition.</td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;unreachable</td>
<td>NoExecute</td>
<td>1.8+</td>
<td>TaintBasedEvictions</td>
<td>when TaintBasedEvictions is enabled,they will not be evicted when there are node problems such as a network partition.</td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;disk-pressure</td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>TaintNodesByCondition</td>
<td></td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;memory-pressure</td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>TaintNodesByCondition</td>
<td></td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;unschedulable</td>
<td>NoSchedule</td>
<td>1.11+</td>
<td>ScheduleDaemonSetPods, TaintNodesByCondition</td>
<td>When ScheduleDaemonSetPodsis enabled, TaintNodesByConditionis necessary to make sure DaemonSet pods tolerate unschedulable attributes by default scheduler.</td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;network-unavailable</td>
<td>NoSchedule</td>
<td>1.11+</td>
<td>ScheduleDaemonSetPods, TaintNodesByCondition, hostnework</td>
<td>When ScheduleDaemonSetPodsis enabled, TaintNodesByConditionis necessary to make sure DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.</td>
</tr>
<tr>
<td>node.kubernetes.io&#x2F;out-of-disk</td>
<td>NoSchedule</td>
<td>1.8+</td>
<td>ExperimentalCriticalPodAnnotation(critical pod only), TaintNodesByCondition</td>
<td></td>
</tr>
</tbody></table>
<p>出现以上情况是不会对daemonset造成影响的, 打个比如,假如某台机器出现了设定的磁盘不足, 这个时候kube-scheduler会将改节点打上taint: node.kubernetes.io&#x2F;out-of-disk, effect&#x3D;NoSchedule,就是说出现这个taint后不再允许新pod调度到该机器上, 但对daemonset的调度是不影响的, 因为daemonset默认会加上对应的tolerations.</p>
<h3 id="daemonset驱逐"><a href="#daemonset驱逐" class="headerlink" title="daemonset驱逐"></a>daemonset驱逐</h3><p>daemonset既然会默认有这么多tolerations，那有没有办法进行驱逐呢?</p>
<p>在kubernetes中经常使用drain来做节点的问题处理, 在使用drain命令时，对于四处种pod不会进行驱逐</p>
<blockquote>
<ul>
<li>daemonset pod</li>
<li>Static pod</li>
<li>不被ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet管理的pod</li>
<li>pod中使用了emptyDir的存储, drain也不会直接驱逐, 存在丢数据的风险,可使用–delete-local-data&#x3D;true</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Set configuration context </span></span><br><span class="line">kubectl config use-context ek8s</span><br><span class="line"><span class="comment">#Set the node labelled with name=ek8s-node-1 as unavailable and reschedule all the pods running on it</span></span><br><span class="line">kubectl cordon node</span><br><span class="line">kubectl drain node --ignore-daemonsets=<span class="literal">true</span> --delete-local-data=<span class="literal">true</span></span><br><span class="line"><span class="comment">#drain节点时会提示驱逐pod的明细</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果使用drain作用于存在daemonset的节点但是又没有使用ignore-daemonsets=true标志, 则drain命令会被忽略,不会对节点上的pod进行驱逐，节点变成SchedulingDisabled状态，同时提示以下错误</span></span><br><span class="line"><span class="comment">#error: unable to drain node &quot;instance-node1&quot;, aborting command...</span></span><br></pre></td></tr></table></figure>

<p>如果使用drain而又没有ignore-daemonsets&#x3D;true时，则drain命令会被忽略，这是因为如果不被忽略的话, drain对pod进行驱逐,而调度器又会将daemonset调度过来,造成死循环的.</p>
<h3 id="daemonset更新"><a href="#daemonset更新" class="headerlink" title="daemonset更新"></a>daemonset更新</h3><p>多kube-proxy的yaml文件来看, daemonset的更新也属于rollingUpdate， 机制跟deployment一样.</p>
<h3 id="daemonset删除"><a href="#daemonset删除" class="headerlink" title="daemonset删除"></a>daemonset删除</h3><p>这个很简单, 一条命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl delete  DaemonSet [Name]</span><br></pre></td></tr></table></figure>





<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">https://kubernetes.io</a></li>
<li><a href="https://www.cnblogs.com/tylerzhou/p/11007427.html">https://www.cnblogs.com/tylerzhou/p/11007427.html</a></li>
<li><a href="https://www.bmc.com/blogs/kubernetes-daemonset/">https://www.bmc.com/blogs/kubernetes-daemonset/</a></li>
<li><a href="https://draveness.me/kubernetes-daemonset">https://draveness.me/kubernetes-daemonset</a></li>
<li><a href="https://banzaicloud.com/blog/drain/">https://banzaicloud.com/blog/drain/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(在configmap中引用secret)</title>
    <url>/2020/06/28/Kubernetes-configmap-reference-var-from-secret/</url>
    <content><![CDATA[<p>使用kubernetes做为底层平台, 对于配置文件的部署形式, 如果没有统一的配置管理, 使用最多的就是configmap及secret</p>
<p>但有时候只会将敏感的信息放在secret中，而大多数的配置会放在configmap, 这就造成要么把secret以env的形式存在于容器中,要么就以volumn的形式挂载到容器中, 需要从两个源来引用配置， 从本质上, configmap与secret是一个东西, 有时候会觉得这种场景其实可以优化到一个配置中， 官方到目前为止并不打算支持在configmap中引用secret, 这是在github上讨论的<a href="https://github.com/kubernetes/kubernetes/issues/79224">issue</a></p>
<span id="more"></span>



<p>某天随便逛github时，发现一个可以实现该功能的开源库<a href="https://github.com/machinezone/configmapsecrets">configmapsecrets</a>,它能够在configmap中引用secret中的变量, 最后生成一个完整的secret. 这样， 应用直接把这个secret挂载到指定路径即可, 同时，支持热更新.</p>
<p>经过一番研究后，发现用在GitOps流程中还是非常方便的, 结合<a href="https://github.com/bitnami-labs/sealed-secrets">SealedSecrets</a>对secret的加密， 两者结合更好的支持了配置安全，方便统一管理</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>原理非常简单，通过CRD来监控集群中特定资源(configmapSecret), 分析该资源引用的secret, 填充变量生成secret. </p>
<p>同时为了支持多个实例，多实例之间的竞争关系, 使用了<code>sync.RWMutex</code></p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="安装CRD"><a href="#安装CRD" class="headerlink" title="安装CRD"></a>安装CRD</h4><p>按照github上的说明, 先生成CRD, 这里需要注意的是, 如果只想监视特定的ns中有效, 修改deployment.yaml中的容器的启动参数，</p>
<p>添加<code>all-namespaces=false</code>，这样，configmapsecret只会处理这个ns中的对象</p>
<p><code>kubectl apply -f manifest</code></p>
<h4 id="secret"><a href="#secret" class="headerlink" title="secret"></a>secret</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">alertmanager-keys</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">monitoring</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">alertmanager</span></span><br><span class="line"><span class="attr">stringData:</span> <span class="comment"># 使用stringData，不需要事先使用base64加密</span></span><br><span class="line">  <span class="attr">opsgenieKey:</span> <span class="string">9eccf784-bbad-11e9-9cb5-2a2ae2dbcce4xxxxyyyyyyzzzzz</span></span><br><span class="line">  <span class="attr">slackURL:</span> <span class="string">https://hooks.slack.com/services/EFNPN1/EVU44X/J51NVTYSKwuPtCz3yyyy</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure>

<h4 id="ConfigmapSecret"><a href="#ConfigmapSecret" class="headerlink" title="ConfigmapSecret"></a>ConfigmapSecret</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">secrets.mz.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMapSecret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">alertmanager-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">monitoring</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">alertmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">alertmanager-config</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">alertmanager</span></span><br><span class="line">    <span class="attr">data:</span></span><br><span class="line">      <span class="attr">alertmanager.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          global:</span></span><br><span class="line"><span class="string">            resolve_timeout: 5m</span></span><br><span class="line"><span class="string">            opsgenie_api_key: $(OPSGENIE_API_KEY)  # 在 Var中定义</span></span><br><span class="line"><span class="string">            slack_api_url: $(SLACK_API_URL)</span></span><br><span class="line"><span class="string">            special_how: $(CONFIGMAP_HOW)</span></span><br><span class="line"><span class="string">            special_type: $(CONFIGMAP_TYPE)</span></span><br><span class="line"><span class="string">          route:</span></span><br><span class="line"><span class="string">            receiver: default</span></span><br><span class="line"><span class="string">            group_by: [&quot;alertname&quot;, &quot;job&quot;, &quot;team&quot;]</span></span><br><span class="line"><span class="string">            group_wait: 30s</span></span><br><span class="line"><span class="string">            group_interval: 5m</span></span><br><span class="line"><span class="string">            repeat_interval: 12h</span></span><br><span class="line"><span class="string">            routes:</span></span><br><span class="line"><span class="string">              - receiver: foobar-sre</span></span><br><span class="line"><span class="string">                match:</span></span><br><span class="line"><span class="string">                  team: foobar-sre</span></span><br><span class="line"><span class="string">              - receiver: widget-sre</span></span><br><span class="line"><span class="string">                match:</span></span><br><span class="line"><span class="string">                  team: widget-sre</span></span><br><span class="line"><span class="string">          receivers:</span></span><br><span class="line"><span class="string">            - name: default</span></span><br><span class="line"><span class="string">              slack_configs:</span></span><br><span class="line"><span class="string">                - channel: unrouted-alerts</span></span><br><span class="line"><span class="string">            - name: foobar-sre</span></span><br><span class="line"><span class="string">              opsgenie_configs:</span></span><br><span class="line"><span class="string">                - responders:</span></span><br><span class="line"><span class="string">                    - name: foobar-sre</span></span><br><span class="line"><span class="string">                      type: team</span></span><br><span class="line"><span class="string">              slack_configs:</span></span><br><span class="line"><span class="string">                - channel: foobar-sre-alerts</span></span><br><span class="line"><span class="string">            - name: widget-sre</span></span><br><span class="line"><span class="string">              opsgenie_configs:</span></span><br><span class="line"><span class="string">                - responders:</span></span><br><span class="line"><span class="string">                    - name: widget-sre</span></span><br><span class="line"><span class="string">                      type: team</span></span><br><span class="line"><span class="string">              slack_configs:</span></span><br><span class="line"><span class="string">                - channel: widget-sre</span></span><br><span class="line"><span class="string"></span>  <span class="attr">vars:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">OPSGENIE_API_KEY</span></span><br><span class="line">      <span class="attr">secretValue:</span> <span class="comment"># 引用secret </span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">alertmanager-keys</span> <span class="comment"># secret 名字</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">opsgenieKey</span> <span class="comment"># secret key</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">SLACK_API_URL</span></span><br><span class="line">      <span class="attr">secretValue:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">alertmanager-keys</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">slackURL</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CONFIGMAP_HOW</span> <span class="comment"># 引用configmap</span></span><br><span class="line">      <span class="attr">configMapValue:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">special-config</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">special.how</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CONFIGMAP_TYPE</span></span><br><span class="line">      <span class="attr">configMapValue:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">special-config</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">special.type</span></span><br></pre></td></tr></table></figure>

<p>vars中内容即是引用的变量, 上面引用的是secret, 当然也可以引用configmap.</p>
<p><strong>注意: 引用的必须是同一ns中secret&#x2F;configmap</strong></p>
<p>使用命令生成以上2个对象</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f secret.yaml</span><br><span class="line">kubectl apply -f confsec.yaml</span><br></pre></td></tr></table></figure>

<p>最终会在同一ns中生成一个secret，名字跟ConfigMapSecret对象的名字相同， 当然secret的data数据是被base64加密的</p>
<h3 id="其它测试"><a href="#其它测试" class="headerlink" title="其它测试"></a>其它测试</h3><ol>
<li>如果修改引用的secret，会提示以下日志</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;INFO&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;2020-06-28T03:52:57.386Z&quot;</span>,<span class="string">&quot;source&quot;</span>:<span class="string">&quot;controllers.controller.ConfigMapSecret&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;controllers/configmapsecret_controller.go:271&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;Updating Secret&quot;</span>,<span class="string">&quot;configmapsecret&quot;</span>:<span class="string">&quot;monitoring/alertmanager-config&quot;</span>,<span class="string">&quot;secret&quot;</span>:<span class="string">&quot;monitoring/alertmanager-config&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>修改configmapsecret本身,会提示以下错误, 但并不影响</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;ERROR&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;2020-06-28T03:50:08.555Z&quot;</span>,<span class="string">&quot;source&quot;</span>:<span class="string">&quot;controllers.controller.ConfigMapSecret&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;controllers/configmapsecret_controller.go:462&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;Unable to update status&quot;</span>,<span class="string">&quot;configmapsecret&quot;</span>:<span class="string">&quot;monitoring/alertmanager-config&quot;</span>,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;Operation cannot be fulfilled on configmapsecrets.secrets.mz.com \&quot;alertmanager-config\&quot;: the object has been modified; please apply your changes to the latest version and try again&quot;</span></span><br></pre></td></tr></table></figure>

<p>1、2两种情况最终的secret会跟着被修改</p>
<ol start="3">
<li>而如果直接修改最终生成的secret，修改完之后会立即被复原.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;INFO&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;2020-06-28T04:23:43.699Z&quot;</span>,<span class="string">&quot;source&quot;</span>:<span class="string">&quot;controllers.controller.ConfigMapSecret&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;controllers/configmapsecret_controller.go:271&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;Updating Secret&quot;</span>,<span class="string">&quot;configmapsecret&quot;</span>:<span class="string">&quot;monitoring/alertmanager-config&quot;</span>,<span class="string">&quot;secret&quot;</span>:<span class="string">&quot;monitoring/alertmanager-config&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>删除configmapsecret, 则最终生成的secret也会被删除, 引用的secret则不会.</li>
</ol>
<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>关键代码在<code>/pkg/controllers/configmapsecret_controller.go</code>中</p>
<p>首先通过<code>SetupWithManager</code>启动manager， 该manager用于监控configmapsecret及secret</p>
<p>关键函数<code>sync</code>用于同步相关资源的变更，通过对比OwnerReferences的UID来判断是否有变更，它调用<code>renderSecret</code>根据configmapsecret及引用的secret&#x2F;configmap来渲染生成最终的secret</p>
<p>而处理引用变量的逻辑则是在<code>makeVariables</code>函数</p>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>如果使用容器平台，如rancher, UI上不支持显示crd， 因此configmapsecret无法显示在页面上.</p>
<p>如果最终生成的形式为secret， 被base64编码，不能够直观地显示配置, 如果能够直接生成configmap，我觉得更合理</p>
<p>但生成的secret也不影响这个库用于GitOps中，结合<a href="https://github.com/bitnami-labs/sealed-secrets">SealedSecrets</a>对secret的加密处理, 至此可以实现整个资源端到端的闭环.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/issues/79224">https://github.com/kubernetes/kubernetes/issues/79224</a></li>
<li><a href="https://github.com/machinezone/configmapsecrets">https://github.com/machinezone/configmapsecrets</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(configmap及secret更新后自动重启POD)</title>
    <url>/2020/05/10/Kubernetes-deploy-hot-reload-when-configmap-update/</url>
    <content><![CDATA[<p>在Kubernetes中, 更新configmap及secret是无法自动更新POD的, 如果需要配置生效, 需要重新部署,这无疑会增加多余操作, 刚好最近在测试gitlab的CI&#x2F;CD, 需要这个自动能力,调研了以下几种可用方式.</p>
<span id="more"></span>



<h3 id="为何configmap-x2F-secret更新后POD不自动重启"><a href="#为何configmap-x2F-secret更新后POD不自动重启" class="headerlink" title="为何configmap&#x2F;secret更新后POD不自动重启"></a>为何configmap&#x2F;secret更新后POD不自动重启</h3><p>configmap&#x2F;secret更新后大约10s后会更新到pod中, 但是pod无法感应到configmap&#x2F;secret的更新， 这其实是由于configmap&#x2F;secret机制的, 因为获取configmap是否更新的操作是由kubelet是定时（以一定的时间间隔）同步Pod和缓存中的configmap内容的，且三种Manager更新缓存中的configmap内容可能会有延迟，所以，当我们更改了configmap的内容后，真正反映到Pod中可能要经过syncFrequency + delay这么长的时间, 因此无法做到立即重启pod, 所以把重启的操作交给操作人员来决定.</p>
<p>由于configmap与secret的机制相差无几, 因此下面的例子以configmap为例, secret只不过加密的configmap，原理一样.</p>
<p>要实现cm&#x2F;secret更新后自动重启Pod, 一般分为以下几种方法:</p>
<blockquote>
<ul>
<li>配置文件inotifywatch</li>
<li>发送重启信号</li>
<li>Sidecar</li>
<li>reloader</li>
</ul>
</blockquote>
<p><strong>选型的话最好就是不需要业务端支持.不可能实现这个还需要改造一堆代码，不现实</strong></p>
<h3 id="发送信号"><a href="#发送信号" class="headerlink" title="发送信号"></a>发送信号</h3><p>这个一般都需要应用程序本身支持接收信号量机制, 比如nginx就支持使用<code>HUP</code>来做热重启</p>
<p>这个因为涉及到需要修改业务代码, 一般就不会考虑</p>
<h3 id="Inotifywatch检查"><a href="#Inotifywatch检查" class="headerlink" title="Inotifywatch检查"></a>Inotifywatch检查</h3><p>可以使用脚本来watch配置文件是否存在更新, </p>
<p>Github有现成的可用，主要实现用了inotifywatch 参考这里<a href="https://github.com/William-Yeh/configmap-auto-reload">configmap-auto-reload</a></p>
<h3 id="sidecar"><a href="#sidecar" class="headerlink" title="sidecar"></a>sidecar</h3><p>这个github也有现成的方案, prometheus跟alertmanager的热重启用的就是这个sidecar</p>
<p>可以参考<a href="https://github.com/jimmidyson/configmap-reload">configmap-reload</a></p>
<h3 id="reloader"><a href="#reloader" class="headerlink" title="reloader"></a>reloader</h3><p>这个开源库使用的是kubernetes的list&#x2F;watch机制来监听指定configmap的变化来执行滚动升级操作</p>
<p>实现可以参考<a href="https://github.com/stakater/Reloader">reloader</a></p>
<p>最终经过一番测试之后，选择了reloader, 用这个实现pod的自动重启对业务来说影响最小.</p>
<h3 id="reloader使用"><a href="#reloader使用" class="headerlink" title="reloader使用"></a>reloader使用</h3><p>这里使用一个例子: 使用了两个configmap，一个挂载到了容器的env， 一个挂载到了容器的配置文件.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">stage</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">READ_TIMEOUT_SECONDS:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">WRITE_TIMEOUT_SECONDS:</span> <span class="string">&quot;10&quot;</span></span><br><span class="line">  <span class="attr">NAME:</span> <span class="string">&quot;elithrar&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">stage</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">nginx.conf:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    user  nginx;</span></span><br><span class="line"><span class="string">    worker_processes  auto;</span></span><br><span class="line"><span class="string">    #worker_processes  4;</span></span><br><span class="line"><span class="string">    error_log  /var/log/nginx/error.log debug;</span></span><br><span class="line"><span class="string">    pid        /var/run/nginx.pid;</span></span><br><span class="line"><span class="string">    events &#123;</span></span><br><span class="line"><span class="string">        worker_connections  1024;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    http &#123;</span></span><br><span class="line"><span class="string">        include       /etc/nginx/mime.types;</span></span><br><span class="line"><span class="string">        default_type  application/octet-stream;</span></span><br><span class="line"><span class="string">        log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span></span><br><span class="line"><span class="string">                          &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span></span><br><span class="line"><span class="string">                          &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span></span><br><span class="line"><span class="string">        access_log  /var/log/nginx/access.log  main;</span></span><br><span class="line"><span class="string">        sendfile        on;</span></span><br><span class="line"><span class="string">        keepalive_timeout  65;</span></span><br><span class="line"><span class="string">        include /etc/nginx/conf.d/*.conf;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">conf.d__default.conf:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    server &#123;</span></span><br><span class="line"><span class="string">        listen       80;</span></span><br><span class="line"><span class="string">        server_name  localhost;</span></span><br><span class="line"><span class="string">        location / &#123;</span></span><br><span class="line"><span class="string">            root   /usr/share/nginx/html;</span></span><br><span class="line"><span class="string">            index  index.html index.htm;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        location /nginx_status &#123;</span></span><br><span class="line"><span class="string">            stub_status;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        error_page   500 502 503 504  /50x.html;</span></span><br><span class="line"><span class="string">        location = /50x.html &#123;</span></span><br><span class="line"><span class="string">            root   /usr/share/nginx/html;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-deployment</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">stage</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">configmap.reloader.stakater.com/reload:</span> <span class="string">&quot;nginx-config, demo-config&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">configmap-volume</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">nginx-config</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">nginx.conf</span></span><br><span class="line">                <span class="attr">path:</span> <span class="string">nginx.conf</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">mime.types</span></span><br><span class="line">                <span class="attr">path:</span> <span class="string">mime.types</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">conf.d__default.conf</span></span><br><span class="line">                <span class="attr">path:</span> <span class="string">conf.d/default.conf</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-demo-app</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:latest</span> </span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">configmap-volume</span></span><br><span class="line">            <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">envFrom:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">configMapRef:</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>更新configmap后会发现pod很快就重启了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200512114134.png"></p>
<p>从reloader中看到以下日志:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200512114255.png"></p>
<p>进入容器看到env更新了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200512114215.png"></p>
<p>configmap配置文件也对应更新了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200512121303.png"></p>
<p>完美</p>
<h3 id="Reloader源码分析"><a href="#Reloader源码分析" class="headerlink" title="Reloader源码分析"></a>Reloader源码分析</h3><p>再来看deployment的yaml文件，会发现在容易的env中新增了两个环境变量(因为有两个configmap)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200512122205.png"></p>
<p>核心思路就是通过计算新的configmap与旧的configmap的哈希值是否有变化，源代码核心的就是这一段了:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">updateContainers</span><span class="params">(upgradeFuncs callbacks.RollingUpgradeFuncs, item <span class="keyword">interface</span>&#123;&#125;, config util.Config, autoReload <span class="type">bool</span>)</span></span> constants.Result &#123;</span><br><span class="line">	<span class="keyword">var</span> result constants.Result</span><br><span class="line">  <span class="comment">// 在这里指定添加的环境变量的前缀，就是上图中看到的新增的环境变量的格式</span></span><br><span class="line">	envar := constants.EnvVarPrefix + util.ConvertToEnvVarName(config.ResourceName) + <span class="string">&quot;_&quot;</span> + config.Type</span><br><span class="line">	container := getContainerToUpdate(upgradeFuncs, item, config, autoReload)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> container == <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> constants.NoContainerFound</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//update if env var exists</span></span><br><span class="line">	result = updateEnvVar(upgradeFuncs.ContainersFunc(item), envar, config.SHAValue)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// if no existing env var exists lets create one</span></span><br><span class="line">	<span class="keyword">if</span> result == constants.NoEnvVarFound &#123;</span><br><span class="line">		e := v1.EnvVar&#123;</span><br><span class="line">			Name:  envar,</span><br><span class="line">			Value: config.SHAValue,</span><br><span class="line">		&#125;</span><br><span class="line">		container.Env = <span class="built_in">append</span>(container.Env, e)</span><br><span class="line">		result = constants.Updated</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">updateEnvVar</span><span class="params">(containers []v1.Container, envar <span class="type">string</span>, shaData <span class="type">string</span>)</span></span> constants.Result &#123;</span><br><span class="line">	<span class="keyword">for</span> i := <span class="keyword">range</span> containers &#123;</span><br><span class="line">		envs := containers[i].Env</span><br><span class="line">		<span class="keyword">for</span> j := <span class="keyword">range</span> envs &#123;</span><br><span class="line">			<span class="keyword">if</span> envs[j].Name == envar &#123;</span><br><span class="line">				<span class="keyword">if</span> envs[j].Value != shaData &#123;</span><br><span class="line">					envs[j].Value = shaData</span><br><span class="line">					<span class="keyword">return</span> constants.Updated</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">return</span> constants.NotUpdated</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> constants.NoEnvVarFound</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="gitlab-CI-x2F-CD中实现重启"><a href="#gitlab-CI-x2F-CD中实现重启" class="headerlink" title="gitlab CI&#x2F;CD中实现重启"></a>gitlab CI&#x2F;CD中实现重启</h3><p>当然，如果结合gitlab的CI&#x2F;CD功能，因为可以在<code>gitlab-ci.yml</code>中写脚本，就很容易用以下两种方式同样可以实现</p>
<ol>
<li>当更新configmap时，只进行pod重启, 可以使用patch增加一个环境变量:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl patch deployment &lt;deployment-name&gt; -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;name&quot;:&quot;&lt;container-name&gt;&quot;,&quot;env&quot;:[&#123;&quot;name&quot;:&quot;RESTART_TIME&quot;,&quot;value&quot;:&quot;&#x27;</span>$(<span class="built_in">date</span> +%s)<span class="string">&#x27;&quot;&#125;]&#125;]&#125;&#125;&#125;&#125;&#x27;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>或者直接在CI计算一个configmap文件的md5或者hash值, 然后将这个值传入deployment， 这样也会使pod重启.</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">com.aylei.configmap/hash:</span> <span class="string">$&#123;CONFIGMAP_HASH&#125;</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>





<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">https://kubernetes.io</a></li>
<li><a href="https://github.com/William-Yeh/configmap-auto-reload">https://github.com/William-Yeh/configmap-auto-reload</a></li>
<li><a href="https://github.com/stakater/Reloader">https://github.com/stakater/Reloader</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57570231">https://zhuanlan.zhihu.com/p/57570231</a></li>
<li><a href="https://william-yeh.net/post/2019/06/autoreload-from-configmap/">https://william-yeh.net/post/2019/06/autoreload-from-configmap/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(关于CoreDNS的5s超时问题)</title>
    <url>/2020/06/10/Kubernetes-coredns-5s-timeout/</url>
    <content><![CDATA[<p>今天同事反映了一个很奇怪的问题: 在k8s环境里的容器中curl另一个服务时会出现断断续续的超时, 问题现象很简单, 但问题根源很复杂</p>
<span id="more"></span>



<h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Kubernetes-1.15.1</span><br><span class="line"></span><br><span class="line">serviceA: 10.244.3.45</span><br><span class="line">servieB:</span><br><span class="line">	ClusterIP: 10.105.224.130</span><br><span class="line"></span><br><span class="line">CoreDNS-1.3.1:</span><br><span class="line">	ClusterIP: 10.96.0.10</span><br><span class="line">	instanceA: 10.244.3.43</span><br><span class="line">	instanceB: 10.244.0.45</span><br></pre></td></tr></table></figure>



<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200610105313.png"></p>
<h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><h4 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h4><p>首先很容易想到, 因为现象是时好时坏，是不是有可能服务中有某一个节点不正常, 排查一圈后，涉及的服务都未见异常.</p>
<h4 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h4><p>既然服务都正常, 那会不会是解析有问题, 因为coreDNS也是多实例的, 因此直接通过ClusterIP访问, 未见超时，因此可以判断是<strong>解析环节出现问题</strong></p>
<h4 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h4><p>在确认是解析环节的问题后, 就从<code>coreDNS</code>入手了, <code>coreDNS</code>的运行情况未见异常, 日志也不见错误输出, 打开<code>debug</code>日志后，再次观察</p>
<p>也未见任何错误日志，这就奇怪了, 没办法，只能<code>tcpdump</code>了</p>
<h4 id="Tcpdump"><a href="#Tcpdump" class="headerlink" title="Tcpdump"></a>Tcpdump</h4><p>只需要看53端口的流量即可, 通过<code>tcpdump</code>抓到包后,也并未发现在任何可疑的地方. 本来寄希望于神器, 却什么都没发现，这就尴尬了</p>
<h4 id="Kube-proxy"><a href="#Kube-proxy" class="headerlink" title="Kube-proxy"></a>Kube-proxy</h4><p>由于dns的实现是通过kube-proxy实现的, 因此kube-proxy的差异也会引起转发异常, 但是通过比对两台宿主机的<code>iptables</code>，也一样,  这就排除了<code>kube-proxy</code>的问题</p>
<h3 id="Why-5s"><a href="#Why-5s" class="headerlink" title="Why 5s"></a>Why 5s</h3><p>其实从现象来看, 我第一想到的是这个5s很熟悉, 因为之前优化过DNS服务器重试的问题, 记得默认的超时时间就是5s, 所以很容易想到了</p>
<p>这里需要简单地说一下k8s中的服务名字 –&gt;实例IP的一个的流程:</p>
<p><strong>ServiceName –&gt; coreDNS ClusterIP –&gt; coreDNS instance –&gt; ClusterIP –&gt; Service instance</strong></p>
<p>测试了从<code>coreDNS ClusterIP --&gt; coreDNS instance</code>的解析是没有问题的，因此问题出现<code>coreDNS instance --&gt; ClusterIP</code></p>
<p>那么单独通过coreDNS的两个实例去解析, 会发现有一个实例确实会出现<code>5s Timeout的情况</code>,另一个实例没有问题.</p>
<p>对比了两个实例所在的宿主机, 跟dns相关的<code>/etc/resolv.conf</code>，<code>/etc/sysctl.conf</code>等有关系的文件，没有差别</p>
<p>这就很奇怪了, 几乎相关的组件都排查了，并没有发现可疑的地方, 没办法，只能网上查看是否有相关的<code>issue</code>.</p>
<h3 id="Conntrack"><a href="#Conntrack" class="headerlink" title="Conntrack"></a>Conntrack</h3><p>经过一番搜索, 还真发现是踩到了一个坑, 倒不是k8s的问题, 而是conntrack的一个bug.</p>
<p>关于<code>conntrack</code>, 涉及到内核的一些知识，没办法搞的太深，太深了也看不太懂, 大家可自行了解</p>
<p>这里就参考<a href="https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/">这里</a>总结一下:</p>
<p>当加载内核模块<code>nf_conntrack</code>后，conntrack机制就开始工作，如上图，椭圆形方框<code>conntrack</code>在内核中有两处位置(PREROUTING和OUTPUT之前)能够跟踪数据包。对于每个通过<code>conntrack</code>的数据包，内核都为其生成一个conntrack条目用以跟踪此连接，对于后续通过的数据包，内核会判断若此数据包属于一个已有的连接，则更新所对应的conntrack条目的状态(比如更新为ESTABLISHED状态)，否则内核会为它新建一个conntrack条目。所有的conntrack条目都存放在一张表里，称为连接跟踪表</p>
<p>连接跟踪表存放于系统内存中，可以用<code>cat /proc/net/nf_conntrack</code>, ubuntu则是<code>conntrack</code>命令查看当前跟踪的所有conntrack条目</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200612162959.png"></p>
<h3 id="Prombles"><a href="#Prombles" class="headerlink" title="Prombles"></a>Prombles</h3><p>经过一番参考之后, 发现问题的根源在于:</p>
<p>DNS client (glibc 或 musl libc) 会并发请求 A (ipv4地址) 和 AAAA (ipv6地址)记录，跟 DNS Server 通信自然会先 connect (建立fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，这时它们源 Port 相同，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们<strong>分别创建 conntrack 表项</strong>，而集群内请求 kube-dns 或 coredns 都是访问的CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包被 DNAT 成同一个 IP，最终它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时， </p>
<p>netfilter conntrack 模块为每个连接创建 conntrack 表项时，表项的创建和最终插入之间还有一段逻辑，没有加锁，是一种乐观锁的过程。conntrack 表项并发刚创建时五元组不冲突的话可以创建成功，但中间经过 NAT 转换之后五元组就可能变成相同，第一个可以插入成功，后面的就会插入失败，因为已经有相同的表项存在。比如一个 SYN 已经做了 NAT 但是还没到最终插入的时候，另一个 SYN 也在做 NAT，因为之前那个 SYN 还没插入，这个 SYN 做 NAT 的时候就认为这个五元组没有被占用，那么它 NAT 之后的五元组就可能跟那个还没插入的包相同</p>
<p>所以总结来说，根本原因是内核 conntrack 模块的 bug，DNS client(在linux上一般就是resolver)会并发地请求A 和 AAAA 记录netfilter 做 NAT 时可能发生资源竞争导致部分报文丢弃</p>
<p>这篇post有非常详细的解释，建议大家都好好地读一读<a href="https://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts">racy conntrack and dns lookup timeouts</a></p>
<p>post的相关结论：</p>
<ul>
<li>只有多个线程或进程，并发从同一个 socket 发送相同五元组的 UDP 报文时，才有一定概率会发生</li>
<li>glibc, musl(alpine linux的libc库)都使用 “parallel query”, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃</li>
<li>由于 ipvs 也使用了 conntrack, 使用 kube-proxy 的 ipvs 模式，并不能避免这个问题</li>
</ul>
<h3 id="Resolve"><a href="#Resolve" class="headerlink" title="Resolve"></a>Resolve</h3><h4 id="Use-TCP"><a href="#Use-TCP" class="headerlink" title="Use TCP"></a>Use TCP</h4><p>默认情况下, dns的请求一般都是使用UDP请求的, 因为力求效率，不需要三握四挥</p>
<p>由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加<code>options use-vc</code>, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">use-vc (since glibc 2.14)</span><br><span class="line">                     Sets RES_USEVC <span class="keyword">in</span> _res.options.  This option forces the</span><br><span class="line">                     use of TCP <span class="keyword">for</span> DNS resolutions.</span><br></pre></td></tr></table></figure>

<p>缺点是使用tcp会比udp稍慢</p>
<h4 id="single-request-reopen-sing-request"><a href="#single-request-reopen-sing-request" class="headerlink" title="single-request-reopen/sing-request"></a><code>single-request-reopen/sing-request</code></h4><p>resolv.conf还有另外两个相关的参数：</p>
<ul>
<li>single-request-reopen (since glibc 2.9)</li>
<li>single-request (since glibc 2.10)</li>
</ul>
<p>man resolv.conf中解释如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">single-request-reopen (since glibc 2.9)                     </span><br><span class="line"></span><br><span class="line">Sets RES_SNGLKUPREOP <span class="keyword">in</span> _res.options.  The resolver                     uses the same socket <span class="keyword">for</span> the A and AAAA requests.  Some                     hardware mistakenly sends back only one reply.  When                     that happens the client system will sit and <span class="built_in">wait</span> <span class="keyword">for</span>                     the second reply.  Turning this option on changes this                     behavior so that <span class="keyword">if</span> two r</span><br><span class="line">equests from the same port are                     not handled correctly it will close the socket and open                     a new one before sending the second request.</span><br><span class="line"></span><br><span class="line">single-request (since glibc 2.10)                     </span><br><span class="line"></span><br><span class="line">Sets RES_SNGLKUP <span class="keyword">in</span> _res.options.  By default, glibc                     performs IPv4 and IPv6 lookups <span class="keyword">in</span> parallel since                     version 2.9.  Some appliance DNS servers cannot handle                     these queries properly and make the requests time out.                     This option disables the behavior and makes glibc                     perform the IPv6 and IPv4 requests sequentially (at the                     cost of some slowdown of the resolving process).</span><br></pre></td></tr></table></figure>

<ul>
<li><code>single-request-reopen</code>: 发送 A 类型请求和 AAAA 类型请求使用不同的源端口，这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突</li>
<li><code>single-request</code>: 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突</li>
</ul>
<p>所以需要将参数写入到容器的<code>/etc/resolv.conf</code>中, 那么也有几种办法</p>
<p>对于已经上线运行的容器可以直接修改yaml定义</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">template:</span><br><span class="line">  spec:</span><br><span class="line">    dnsConfig:</span><br><span class="line">      options:</span><br><span class="line">        - name: single-request-reopen</span><br></pre></td></tr></table></figure>

<p>也可以直接写到启动命令中</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span></span><br><span class="line">  <span class="attr">postStart:</span></span><br><span class="line">    <span class="attr">exec:</span></span><br><span class="line">      <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-c</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span></span><br></pre></td></tr></table></figure>

<p>再或者直接通过<code>Entrypoint/CMD</code>写入</p>
<p><code>/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf</code></p>
<h4 id="musl-libc"><a href="#musl-libc" class="headerlink" title="musl libc"></a>musl libc</h4><p>前几种方案是 glibc 支持的，而基于 alpine 的镜像底层库是 musl libc 不是 glibc，所以即使加了这些 options 也没用，因此如果是通过alpine构建的镜像，当然也有解决办法，网上推荐使用本地DNS的方式，也就是每个节点都部署一个本地的dns, 容器直接请求本的dns，不需要走 DNAT，也不会发生 conntrack 冲突。另外还有个好处，就是避免 DNS 服务成为性能瓶颈</p>
<p>NodeLocal DNScache（k8s v1.15中的beta），这已在k8s官方文档中进行了详细介绍。该解决方案旨在通过作为DaemonSet在每个节点上运行DNS缓存代理来提高群集上的总体DNS性能，以便Pod可以与在同一节点上运行的这些代理联系，从而减少仍然使用conntrack发生竞争的可能性</p>
<p>当然，这个需要集群版本在1.15以上，如果低于这个版本使用其它的dns加速工具如dnsmasq nscd等也可以实现本地dns缓存，但部署起来会比较复杂</p>
<p>当然也可以直接设置 POD.spec.dnsPolicy 为 “Default”， 意思是POD里面的 &#x2F;etc&#x2F;resolv.conf是从宿主机上的 &#x2F;etc&#x2F;resolv.conf继承而来，这样宿主机上的所有pod都会将宿主机上的&#x2F;etc&#x2F;resolv.conf直接挂载到容器内.这样也能解决问题</p>
<ul>
<li>“<code>Default</code>“: The Pod inherits the name resolution configuration from the node that the pods run on. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node">related discussion</a> for more details.</li>
<li>“<code>ClusterFirst</code>“: Any DNS query that does not match the configured cluster domain suffix, such as “<code>www.kubernetes.io</code>“, is forwarded to the upstream nameserver inherited from the node. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#effects-on-pods">related discussion</a> for details on how DNS queries are handled in those cases.</li>
</ul>
<p>如果想尽快解决问题的话，个人还是建议直接更换alpine镜像, 基于alpine构建的镜像还是有挺多坑的.</p>
<h3 id="getaddrinfo-、gethostbyname"><a href="#getaddrinfo-、gethostbyname" class="headerlink" title="getaddrinfo()、gethostbyname()"></a>getaddrinfo()、gethostbyname()</h3><p>关于A与AAAA, 有两个很常用的函数, 之前也有用过, 但是从来没有关注过这两者的区别</p>
<p><code>getaddrinfo()</code>会返回A与AAAA的记录, 如果其中任一个没有返回，都将超时重试</p>
<p>而<code>gethostbyname()</code>只会返回A记录</p>
<p>参考<a href="https://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds">stackoverflow</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://tencentcloudcontainerteam.github.io/">https://tencentcloudcontainerteam.github.io/</a></li>
<li><a href="https://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts">https://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts</a></li>
<li><a href="https://www.bookstack.cn/read/kubernetes-practice-guide/troubleshooting-cases-dns-lookup-5s-delay.md">https://www.bookstack.cn/read/kubernetes-practice-guide/troubleshooting-cases-dns-lookup-5s-delay.md</a></li>
<li><a href="https://tencentcloudcontainerteam.github.io/">https://tencentcloudcontainerteam.github.io/</a></li>
<li><a href="https://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds">https://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds</a></li>
<li><a href="https://blog.codacy.com/dns-hell-in-kubernetes/">https://blog.codacy.com/dns-hell-in-kubernetes/</a></li>
<li><a href="https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/">https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(Docker)</title>
    <url>/2019/11/15/Kubernetes-docker-arch/</url>
    <content><![CDATA[<p>在使用kubernetes的时候，常常不太会关注docker这层, 在Node节点查看docker的进程, 会发现在docker-client、dockerd、containerd、containerd-shim、runC, 傻傻分不清楚这几个跟docker相关的进程到底扮演着什么角色.</p>
<span id="more"></span>



<h3 id="CRI"><a href="#CRI" class="headerlink" title="CRI"></a>CRI</h3><p>在docker的最初版本, docker就是client+daemonset架构, 非常简洁</p>
<p>只不过到后来Kubernetes的兴起, 容器厂商的技术对战也变得非常激烈, 再到后来Kubernetes取得完全的胜利, 再到后来几家大佬公司一起制定了CRI(container runtime interface), docker为了适配才成了今天所看到的架构</p>
<p>关于容器时代几家大佬公司之间的竞争, 有兴趣的可以看下<a href="https://zhuanlan.zhihu.com/p/87602649">这篇文章</a>.</p>
<p>CRI在Kubernetes的架构中也是个很重要的部分, 至于为何需要这个, 不再这里详细, 今天只学一学docker(这里说的docker会存在歧义)做为CRI</p>
<p>先来看看CRI在Kubernetes的位置</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422155644.png"></p>
<p>Kubernetes中的docker那层的架构分为以下几层:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422163423.png"></p>
<p>而ContainerRuntime(docker)的结构如下</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200414141636.png"></p>
<h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><p>最上面那一层是个client，就是我们经常使用的docker ps等命令行的时候, 相当于我们做为client去请求docker的服务端, 那必须存在一个daemonset的进程一直来监听主机上发出的命令</p>
<h4 id="dockerEngine"><a href="#dockerEngine" class="headerlink" title="dockerEngine"></a>dockerEngine</h4><p>这个其实就相当于是docker daemon, 负责监听主机上docker相关的命令,将相关命令发送给containerd.</p>
<h4 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h4><p>先看下官方的说明:</p>
<p><code>containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.</code></p>
<p>翻译一下: <strong>containerd</strong>在linux跟windows是一个守护进程我, 通过它管理容器的生命周期, 从图像传输和存储到容器执行和监督，再到低级存储再到网络附件等等</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422164159.png"></p>
<p>containerd 向上为 Docker Daemon 提供了 gRPC 接口，使得 Docker Daemon 屏蔽下面的结构变化，确保原有接口向下兼容。向下通过 containerd-shim 结合 runC，使得引擎可以独立升级，避免之前 Docker Daemon 升级会导致所有容器不可用的问题</p>
<p>其实 <code>containerd</code> 就包含了我们常用的 <code>docker</code> 的命令， 所以可以通过Client不经过docker daemon直接与containerd进行通信</p>
<h4 id="containerd-shim"><a href="#containerd-shim" class="headerlink" title="containerd-shim"></a>containerd-shim</h4><p>containerd 通过 shim 调用 runc 的包函数来启动容器， 也就是说containerd通过shim操作runc，runc真正控制容器生命周期</p>
<p>启动一个容器就会启动一个shim进程，因此会在Node上看到大量的shim进程</p>
<p>引入shim, 主要有以下几点考虑.</p>
<ul>
<li>允许runc在创建&amp;运行容器之后退出</li>
<li>用shim作为容器的父进程，而不是直接用containerd作为容器的父进程，是为了防止这种情况：当containerd挂掉的时候，shim还在，因此可以保证容器打开的文件描述符不会被关掉</li>
<li>依靠shim来收集&amp;报告容器的退出状态，这样就不需要containerd来wait子进程</li>
</ul>
<h3 id="runC"><a href="#runC" class="headerlink" title="runC"></a>runC</h3><p> runc是OCI(Open Container Initiative, 开放容器标准)的一个参考实现. 于是, containerd-shim 在这一步需要调用 runc 这个命令行工具, 来启动容器;</p>
<p>runc 启动完容器后本身会直接退出, containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程</p>
<p>平时也不怎么去关注docker， 都不知道一个docker居然涉及到这么多的东西</p>
<p>不过由于containerd的发展, 现在已经完全有能力跟client直接交互,</p>
<p>相信不久以后,containerd很可能马上就会替代docker，成为Kubernetes首选的容器运行时, 这样架构上相对简洁一点.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://containerd.io/">https://containerd.io/</a></li>
<li><a href="https://www.pyfdtic.com/2018/03/16/docker-%E5%86%85%E9%83%A8%E7%BB%84%E4%BB%B6%E7%BB%93%E6%9E%84-docker-daemon-container-runC/">https://www.pyfdtic.com/2018/03/16/docker-内部组件结构-docker-daemon-container-runC/</a></li>
<li><a href="https://blog.csdn.net/u013812710/article/details/79001463">https://blog.csdn.net/u013812710/article/details/79001463</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/87602649">https://zhuanlan.zhihu.com/p/87602649</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(hnc实现namesapce分层模型)</title>
    <url>/2023/07/20/Kubernetes-hnc/</url>
    <content><![CDATA[<p>namespace在kubernetes中是一个很重要的基础构件，它构成了几乎所有 Kubernetes 控制平面安全和共享策略的骨干。命名空间有两个关键属性，使其成为策略执行的理想选择</p>
<span id="more"></span>

<ul>
<li>首先，命名空间可以用来<strong>代表所有权</strong>。大多数 Kubernetes 对象资源必须在某一个命名空间中，所以如果使用命名空间来代表所有权，那么命名空间中的所有对象都隶属于同一个所有者。</li>
<li>其次，命名空间的<strong>创建和使用需要授权</strong>。只有超级管理员才能创建命名空间，其他用户需要明确的权限才能使用这些命名空间（包括创建、查看和修改命名空间中的资源对象）。可以设置恰当的安全策略，防止非特权用户创建某些资源对象。</li>
</ul>
<p>namespace是个硬限制，在很多场景下会有诸多问题, 因此层级命名空间(hierarchical namespaces)出现了，它是 Kubernetes<a href="https://github.com/kubernetes-sigs/multi-tenancy">多租户工作组(Working Group for Multi-Tenancy)</a>而提出的新概念</p>
<p>在最简单的形式下，层级命名空间就是一个常规的命名空间，它标识了一个单一的、可选的父命名空间；</p>
<p><strong>更复杂的形式下，父命名空间还可以继承出子空间。这样就建立了跨命名空间的所有权概念，而不是局限于命名空间内。</strong></p>
<p>这种层级命名空间的所有权可以在命名空间的基础上实现额外的两种功能：</p>
<ul>
<li><strong>策略继承</strong>: 如果一个命名空间是另一个命名空间的子空间，那么权限策略（例如<code>RBAC RoleBindings</code>）将会<strong>从父空间直接复制到子空间</strong></li>
<li><strong>继承创建权限</strong>: 通常情况下，需要管理员权限才能创建命名空间。但层级命名空间提供了一个新方案：子命名空(subnamespaces),只需要使用父命名空间中的部分权限即可操作子命名空间。</li>
</ul>
<p>有了这两个功能后，集群管理员就可以为团队创建一个『根』命名空间，以及所有必要的权限策略，然后将创建子命名空间的权限赋予该团队的成员。这样团队内的成员就可以在不违反集群策略的情况下创建自己的子命名空间。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><p>这里忽略安装流程，比较简单，可直接参考github的<a href="https://github.com/kubernetes-sigs/hierarchical-namespaces/tree/master/docs/user-guide">user-guide</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hns可以以plugin的形式安装到Kubectl中，之后便可以直接使用kubectl hns</span></span><br><span class="line"></span><br><span class="line">kubectl create ns acme-org <span class="comment"># 创建root ns</span></span><br><span class="line">kubectl hns create team-c -n acme-org <span class="comment"># 使用hns 直接创建一个subns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以通过创建ns一样创建</span></span><br><span class="line">kubectl create ns acme-org</span><br><span class="line">kubectl create ns team-a</span><br><span class="line">kubectl hns <span class="built_in">set</span> team-a --parent acme-org <span class="comment"># 指定父子关系</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用hns tree查看关系树</span></span><br><span class="line">kubectl hns tree acme-org</span><br><span class="line"></span><br><span class="line">acme-org</span><br><span class="line">├── team-a</span><br><span class="line">│ └── service-1</span><br><span class="line">├── team-b</span><br><span class="line">└── [s] team-c</span><br></pre></td></tr></table></figure>

<p>有[s]的表明是通过第一种方式创建subns, 这种叫subns</p>
<p>没有[s]表明是先创建的ns，然后set –parent的方式创建, 这种叫fullns, </p>
<p>单独拎出来说是因为这两种创建方式创建出来的sub ns会有一些区别:</p>
<p>subns与fullns(直接通过k create ns创建的ns)的区别:</p>
<blockquote>
<ul>
<li>subns的生命周期跟随rootns， 删除fullns的rootns，不会影响fullns， 而subns的rootns删除,subns也会被删除</li>
<li>subns的parent的rootns不能修改，而fullns可以随时调整</li>
</ul>
</blockquote>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>在安装hnc时，存在两个配置文件</p>
<ol>
<li>hierarchyconfigurations: 为每个namespace的分层信息,记录某个ns是否存在subns及rootns是谁</li>
<li>HNCConfiguration: 为hnc全局的配置文件，里面记录了需要传播的资源对象及资源个数等信息，<strong>只有集群管理员才有权限修改这个配置文件</strong></li>
</ol>
<h3 id="propagation-传播"><a href="#propagation-传播" class="headerlink" title="propagation(传播)"></a>propagation(传播)</h3><p>hnc中一个很重要的特性就是propagation, 当以root ns创建了多层的subns, 对于一些通用的属性需要从root ns传播到subns, 这样可以实现<strong>一处维护，随处传播的效果</strong></p>
<p><strong>hnc支持所有资源类型，不仅包括kubernetes常用的类型，还包括CRD, 同时还可以控制传播的资源个数</strong></p>
<p><strong>默认情况下， parent namespace下的所属的Role 和 RoleBinding 对象会传递给 child 命名空间对象</strong></p>
<p>首先查看现在的配置中哪些开启了传播</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看配置</span></span><br><span class="line">kubectl get hncconfiguration config -o yaml</span><br></pre></td></tr></table></figure>

<p>有两种方式实现:</p>
<blockquote>
<ul>
<li>a. 通过全局配置文件<br>如果是k8s自带资源(可省略–group)：<br><code>kubectl hns config set-resource secrets --mode Propagate</code><br>如果是crd资源：<br><code>kubectl hns config set-resource podpresets --group redhatcop.redhat.io --mode Propagate</code></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>b. 通过annotaions(也需要hncconfiguration中设置该配置annotation方可生效)<br>在需要propagation的对象添加如下的annotaions, 不同的–mode,使用的annotaions效果不一样, 详见how-to.md:<br><code>kubectl annotate secret my-secret -n parent propagate.hnc.x-k8s.io/all=true</code></li>
</ul>
</blockquote>
<p>所有被传播的对象的label都会被添加上以下内容:<br><code>app.kubernetes.io/managed-by: hnc.x-k8s.io</code><br><code>hnc.x-k8s.io/inherited-from: acme-org</code></p>
<p>注意: 如果使用–mode Propagate, 在subns无法修改由rootns传播过来的对象,会提示<br><code>admission webhook &quot;objects.hnc.x-k8s.io&quot; denied the request: configmaps &quot;team-b/cm-test&quot; is forbidden: cannot modify object propagated from namespace &quot;acme-org&quot;</code></p>
<p>格式:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl-hns config set-resource RESOURCE [--group GROUP] [--force] --mode &lt;Propagate|Remove|Ignore|AllowPropagate&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 举个例子</span></span><br><span class="line"><span class="comment"># 给所有的ns传播一个configmap，很简单，在HNCConfiguration中开启configmap的默认传播, 然后在parent ns中发布这个configmap，即可传播到subns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给所有的ns传播一个crd，同样，需要先在hnc的配置中打开该对象需要被传播的开关</span></span><br><span class="line"><span class="comment"># 这里以gvk为redhatcop.redhat.io/v1/podpresets这个crd对象为例</span></span><br><span class="line">kubectl hns config set-resource podpresets --group redhatcop.redhat.io --mode Propagate</span><br><span class="line"><span class="comment"># 然后在parent ns中发布这个crd对象即可</span></span><br></pre></td></tr></table></figure>

<h4 id="传播策略"><a href="#传播策略" class="headerlink" title="传播策略"></a>传播策略</h4><p>对资源的传播，可指定 3 种模式(即在上述命令中的–mode参数):</p>
<ol>
<li><p><strong>Propagate</strong>(传播):将对象从祖先传播到后代,并删除过时的后代。</p>
</li>
<li><p><strong>AllowPropagate</strong>(允许传播): opt-in传播-当且仅当对象上至少有一个标签选择器时将对象从你对象传播到子对象并删除过时的后代。</p>
</li>
<li><p><strong>Remove</strong>(删除):删除所有现有的传播副本,但不会修改源对象。</p>
</li>
<li><p><strong>Ignore</strong>(忽略):停止修改该资源。不会传播新的或更改的对象,也不会删除过时的对象。这是<strong>默认</strong>模式</p>
</li>
</ol>
<h3 id="HRQ"><a href="#HRQ" class="headerlink" title="HRQ"></a>HRQ</h3><p>HRQ(HierarchicalResourceQuota)是hnc中的一种CRD, 旨在解决resourcequota的传播问题, HRQ最终会转换成ns的resourcequota对象</p>
<p><strong>rootns的HierarchicalResourceQuota虽然会传播到所有subns下生成相同的resourcequota，但HierarchicalResourceQuota指定的资源为所有subns下的资源总和</strong></p>
<h3 id="Q-x2F-A"><a href="#Q-x2F-A" class="headerlink" title="Q&#x2F;A"></a>Q&#x2F;A</h3><ol>
<li><p>Q: 创建一个rootns及HierarchicalResourceQuota， 再创建一个没有设置HierarchicalResourceQuota的subns, resourcequota如何传播??<br>A: subns与rootns下都会生成resourcequota且与HierarchicalResourceQuota相同，同时在这两个ns下的资源不会超过HierarchicalResourceQuota</p>
</li>
<li><p>Q: 创建一个rootns及HierarchicalResourceQuota, 再创建一个subns，正常情况下rootns会传播到subns,<br>此时在subns下再创建一个HierarchicalResourceQuota且规格大于rootns的HierarchicalResourceQuota, 会发生什么??<br>A: 测试发现不管这个在subns上创建一个HierarchicalResourceQuota是否与rootns同名，都可以创建成功,但是查看生成的resourcequota会发现此时的resourcequota还是以rootns下的HierarchicalResourceQuota为准，<br>如果此时在subns下发布一个job, 资源比rootns的hrq大，但比subns的hrq小，状态会pending, describe的原因是不能超时rootns的资源上限, 这里提示不能超过rootns的原因在于subns的quota比rootns的大，所以先超过了rootns</p>
</li>
<li><p>Q: 接Q2问题，如果此时在subns下创建的HierarchicalResourceQuota小于rootns的HierarchicalResourceQuota，情况又如何?？<br>A: 如果此时在subns下发布一个job，资源比subns的hrq大但比rootns的hrq小，状态会pending, describe的原因是不能超时subns的资源上限</p>
<p>总结: 经过Q2及Q3可以得出结论: subns的HierarchicalResourceQuota可以比rootns的大,subns下的资源不能大于subns的HierarchicalResourceQuota或者rootns的HierarchicalResourceQuota, 如果出现pending，提示的原因是先超过了哪个ns的quota则提示哪个ns的quota超出，这样符合原生的resourcequota限制，即<strong>一个namespace可以同时存在多个对某一类资源(如CPU)的限制，会以小规格为准</strong></p>
</li>
<li><p>Q: 在rootns开启HierarchicalResourceQuota的propagation情况下，subns的HierarchicalResourceQuota是否可覆盖rootns的HierarchicalResourceQuota??<br>A: 开启–enable-hrq后，HierarchicalResourceQuota默认会propagation到所有subns<br>如果此时subns中没有hrq资源,则在subns中会直接生成一条同等规格的resoucequota对象, 同时这个rq会跟着rootns的hrq修改而修改<br>但如果此时subns有自己的hrq，则不会跟随rootns的修改而变化(见Q5)，<br>如果此时修改subns的hrq对象，是否会覆盖rootns的hrq对象?<br>会覆盖, 但是subns的resourcequota还是以subns及rootns两者的小值为准, 如果此时将rootns的HierarchicalResourceQuota删除后，subns的resourcequota的规格会变成它自己的HierarchicalResourceQuota所对应的规格</p>
</li>
<li><p>Q: 接Q4问题，如果只是resourcequota, 是否也会propagation??<br>A: 不会， 在subns下创建一个名为notebook-user-quota的resourcequota对象，然后在rootns中发布HierarchicalResourceQuota<br>在subns中会发现多了一个hrq.hnc.x-k8s.io的resourcequota，notebook-user-quota的resourcequota保存不变(一个namespace可以同时存在多个对某一类资源(如CPU)的限制，会以小规格为准)<br>经测试，无法在subns下直接创建名为hrq.hnc.x-k8s.io的resourcequota,则会直接删除</p>
</li>
<li><p>Q: propagation能否支持crd对象??<br>A: 支持，任何对象都可传播，可使用以下命令进行，详情可看关于propagation小节</p>
</li>
<li><p>Q: 由上可知<strong>最终生成的resourcequota会以较小值为依赖进行资源控制</strong>, 但如果想subns的resourcequota比rootns的大如何限制？<br>A: 通过rootns不propagation HierarchicalResourceQuota到subns， subns即可单独设置resourcequota<br>经测试发现，无法通过rootns的HierarchicalResourceQuota达到此目的, 但是可以通过原生的控制resourcequota propagation来实现<br>HierarchicalResourceQuota无法达到目的原因在于HierarchicalResourceQuota默认是传播的,因此没办法通过annotations进行控制哪些不传播哪些传播,比如要开启resourcequota的传播:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先开启resourcequota实现传播</span></span><br><span class="line"><span class="comment"># kubectl hns config set-resource resourcequota --mode Propagate</span></span><br></pre></td></tr></table></figure>

<p>然后在rootns中创建resourcequota, 可通过在annotations中排除不需要传播的ns.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">root-rq</span> </span><br><span class="line"> <span class="attr">namespace:</span> <span class="string">root-notebook</span></span><br><span class="line"> <span class="attr">annotations:</span></span><br><span class="line">   <span class="attr">propagate.hnc.x-k8s.io/treeSelect:</span> <span class="string">&#x27;!notebook-user-zsk&#x27;</span> <span class="comment"># 这里指定哪些ns不需要传播</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">hard:</span></span><br><span class="line">   <span class="attr">requests.cpu:</span> <span class="string">&quot;5&quot;</span></span><br><span class="line">   <span class="attr">requests.memory:</span> <span class="string">&quot;5Gi&quot;</span></span><br><span class="line">   <span class="attr">limits.cpu:</span> <span class="string">&quot;5&quot;</span></span><br><span class="line">   <span class="attr">limits.memory:</span> <span class="string">&quot;5G&quot;</span></span><br></pre></td></tr></table></figure>

<p>动态修改annotation,会动态生效<br>这样即可实现HQR控制所有subns的资源总量，一部分继承rootns的resourcequota,同时不需要继承的subns则可自定义resourcequota<br>另外，如果是某些subns的资源比rootns的hrq小的话，则可直接在subns中设置HierarchicalResourceQuota即可覆盖rootns中的quota(以较小值为准)</p>
</li>
</ol>
<p>hnc中使用了mutating webhook的方式拦截了权限的请求，将权限限制在合适的subns中</p>
<p>同时使用策略引擎达到资源可以向下传播，很大程度解决了kubernetes中namespace的诸多限制</p>
<p>在一些其它的开源项目如capsule中看到hnc的思路，相信hnc会成为kubernetes关于资源、权限等维度的标准分层实现</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/hierarchical-namespaces">https://github.com/kubernetes-sigs/hierarchical-namespaces</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/188457275">https://zhuanlan.zhihu.com/p/188457275</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(hostPort劫持了我的请求)</title>
    <url>/2021/08/01/Kubernetes-hostport/</url>
    <content><![CDATA[<p>最近排查了一个kubernetes中使用了hostport后遇到比较坑的问题，奇怪的知识又增加了.</p>
<span id="more"></span>



<h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>集群环境为K8s v1.15.9，cni指定了flannel-vxlan跟portmap, kube-proxy使用mode为ipvs，集群3台master,同时也是node，这里以node-1,node-2,node-3来表示。</p>
<p>集群中有2个mysql, 部署在两个ns下，mysql本身不是问题重点，这里就不细说，这里以mysql-A,mysql-B来表示。</p>
<p>mysql-A落在node-1上，mysql-B落在node-2上， <strong>两个数据库svc名跟用户、密码完全不相同</strong></p>
<p>出现诡异的现象这里以一张图来说明会比较清楚一些:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210729220711.png"></p>
<p>其中绿线的表示访问没有问题，红线表示连接Mysql-A提示用户名密码错误</p>
<p>特别诡异的是，当在Node-2上通过svc访问Mysql-A时，输入Mysql-A的用户名跟密码提示密码错误，密码确认无疑，但当输入Mysql-B的用户名跟密码，居然能够连接上，看了下数据，连上的是Mysql-B的数据库，给人的感觉就是请求转到了Mysql-A, 最后又转到了Mysql-B，当时让人大跌眼镜</p>
<p>碰到诡异的问题那就排查吧，排查的过程倒是不费什么事，最主要的是要通过这次踩坑机会挖掘一些奇怪的知识出来。</p>
<h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>既然在Node-1上连接Mysql-A&#x2F;Mysql-B都没有问题，那基本可以排查是Mysql-A的问题</p>
<p>经实验，在Node-2上所有的服务想要连Mysql-A时，都有这个问题，但是访问其它的服务又都没有问题，说明要么是mysql-A的3306这个端口有问题，通过上一步应该排查了mysql-A的问题，那问题只能出在Node-2上</p>
<p>在k8s中像这样的请求转发出现诡异现象，当排除了一些常见的原因之外，最大的嫌疑就是iptables了，作者遇到过多次</p>
<p>这次也不例外，虽然当前集群使用的ipvs， 但还是照例看下iptables规则，查看Node-2上的iptables与Node-1的iptables比对，结果有蹊跷, 在Node-2上发现有以下的规则在其它节点上没有</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">-A</span> <span class="string">CNI-DN-xxxx</span> <span class="string">-p</span> <span class="string">tcp</span> <span class="string">-m</span> <span class="string">tcp</span> <span class="string">--dport</span> <span class="number">3306</span> <span class="string">-j</span> <span class="string">DNAT</span> <span class="string">--to-destination</span> <span class="number">10.224</span><span class="number">.0</span><span class="number">.222</span><span class="string">:3306</span></span><br><span class="line"><span class="string">-A</span> <span class="string">CNI-HOSTPORT-DNAT</span> <span class="string">-m</span> <span class="string">comment</span> <span class="string">--comment</span> <span class="attr">&quot;dnat name&quot;:</span> <span class="string">\&quot;cni0\&quot;</span> <span class="attr">id:</span> <span class="string">\&quot;xxxxxxxxxxxxx\&quot;&quot;</span> <span class="string">-j</span> <span class="string">CNI-DN-xxx</span></span><br><span class="line"><span class="string">-A</span> <span class="string">CNI-HOSTPORT-SNAT</span> <span class="string">-m</span> <span class="string">comment</span> <span class="string">--comment</span> <span class="attr">&quot;snat name&quot;:</span> <span class="string">\&quot;cni0\&quot;</span> <span class="attr">id:</span> <span class="string">\&quot;xxxxxxxxxxxxx\&quot;&quot;</span> <span class="string">-j</span> <span class="string">CNI-SN-xxx</span></span><br><span class="line"><span class="string">-A</span> <span class="string">CNI-SN-xxx</span> <span class="string">-s</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">/32</span> <span class="string">-d</span> <span class="number">10.224</span><span class="number">.0</span><span class="number">.222</span><span class="string">/32</span> <span class="string">-p</span> <span class="string">tcp</span> <span class="string">-m</span> <span class="string">tcp</span> <span class="string">--dport</span> <span class="number">80</span> <span class="string">-j</span> <span class="string">MASQUERADE</span></span><br></pre></td></tr></table></figure>

<p>其中10.224.0.222为Mysql-B的pod ip, xxxxxxxxxxxxx经查实为Mysql-B对应的pause容器的id</p>
<p>从上面的规则总结一下就是目的为3306端口的请求都会转发到10.224.0.222这个地址，即Mysql-B</p>
<p>看到这里，作者明白了为什么在Node-2上去访问Node-1上Mysql-A的3306会提示密码错误而输入Mysql-B的密码却可以正常访问</p>
<p>虽然两个mysql的svc名不一样，但上面的iptables只要目的端口是3306就转发到Mysql-B了，当请求到达mysql后，使用正确的用户名密码自然可以登录成功</p>
<p>原因是找到了，但是又引出来了更多的问题? </p>
<ol>
<li>这几条规则是谁入到iptables中的？</li>
<li>怎么解决呢，是不是删掉就可以?</li>
</ol>
<h3 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h3><p>同样是Mysql，为何Mysql-A没有呢? 那么比对一下这两个Mysql的部署差异</p>
<p>比对发现, 除了用户名密码，ns不一样外，Mysql-B部署时使用了hostPort&#x3D;3306, 其它的并无异常</p>
<p>难道是因为hostPort？</p>
<p>作者日常会使用NodePort，倒却是没怎么在意hostPort,也就停留在hostPort跟NodePort的差别在于NodePort是所有Node上都会开启端口，而hostPort只会在运行机器上开启端口，由于hostPort使用的也少，也就没太多关注，网上短暂搜了一番，描述的也不是很多，看起来大家也用的不多</p>
<p>那到底是不是因为hostPort呢?</p>
<p><strong>Talk is cheap, show me the code</strong></p>
<p>通过实验来验证，这里简单使用了三个nginx来说明问题, 其中两个使用了hostPort，这里特意指定了不同的端口，其它的都完全一样，发布到集群中，yaml文件如下</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-hostport2</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">nginx-hostport2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">nginx-hostport2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">nginx-hostport2</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">nodeName:</span> <span class="string">spring-38</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx:latest</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">              <span class="attr">hostPort:</span> <span class="number">31123</span></span><br></pre></td></tr></table></figure>

<p>Finally,问题复现:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210728133243.png"></p>
<p>可以肯定，这些规则就是因为使用了hostPort而写入的,但是<strong>由谁写入的这个问题还是没有解决?</strong></p>
<h3 id="罪魁祸首"><a href="#罪魁祸首" class="headerlink" title="罪魁祸首"></a>罪魁祸首</h3><p>作者开始以为这些iptables规则是由kube-proxy写入的, 但是查看kubelet的源码并未发现上述规则的关键字</p>
<p>再次实验及结合网上的探索，可以得到以下结论:</p>
<p>首先从kubernetes的官方发现以下描述:</p>
<p>The CNI networking plugin supports <code>hostPort</code>. You can use the official <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap">portmap</a> plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.</p>
<p>If you want to enable <code>hostPort</code> support, you must specify <code>portMappings capability</code> in your <code>cni-conf-dir</code>. For example:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;k8s-pod-network&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;cniVersion&quot;:</span> <span class="string">&quot;0.3.0&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;plugins&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># ...其它的plugin</span></span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;portmap&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;capabilities&quot;:</span> &#123;<span class="attr">&quot;portMappings&quot;:</span> <span class="literal">true</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</a></p>
<p><strong>也就是如果使用了hostPort， 是由portmap这个cni提供portMapping能力，同时，如果想使用这个能力，在配置文件中一定需要开启portmap，这个在作者的集群中也开启了，这点对应上了</strong></p>
<p>另外一个比较重要的结论是:</p>
<p>The CNI ‘portmap’ plugin, used to setup HostPorts for CNI, inserts rules at the front of the iptables nat chains; which take precedence over the KUBE- SERVICES chain. Because of this, the HostPort&#x2F;portmap rule could match incoming traffic even if there were better fitting, more specific service definition rules like NodePorts later in the chain</p>
<p>参考: <a href="https://ubuntu.com/security/CVE-2019-9946">https://ubuntu.com/security/CVE-2019-9946</a></p>
<p><strong>翻译过来就是使用hostPort后，会在iptables的nat链中插入相应的规则，而且这些规则是在KUBE- SERVICES规则之前插入的，也就是说会优先匹配hostPort的规则，我们常用的NodePort规则其实是在KUBE- SERVICES之中，也排在其后</strong></p>
<p>从portmap的源码中果然是可以看到相应的代码</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210729225847.png"></p>
<p>感兴趣的可以的<a href="https://github.com/containernetworking/plugins.git">plugins</a>项目的meta&#x2F;portmap&#x2F;portmap.go中查看完整的源码</p>
<p>所以，<strong>最终是调用portmap写入的这些规则.</strong></p>
<h3 id="端口占用"><a href="#端口占用" class="headerlink" title="端口占用"></a>端口占用</h3><p>进一步实验发现，hostport可以通过iptables命令查看到， 但是无法在ipvsadm中查看到</p>
<p><strong>使用lsof&#x2F;netstat也查看不到这个端口,这是因为hostport是通过iptables对请求中的目的端口进行转发的，并不是在主机上通过端口监听</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210728133203.png"></p>
<p>既然lsof跟netstat都查不到端口信息，那这个端口相当于没有处于listen状态?</p>
<p>如果这时再部署一个hostport提定相同端口的应用会怎么样呢?</p>
<p>结论是: <strong>使用hostPort的应用在调度时无法调度在已经使用过相同hostPort的主机上，也就是说，在调度时会考虑hostport</strong></p>
<p>如果强行让其调度在同一台机器上，那么就会出现以下错误，如果不删除的话，这样的错误会越来越多，吓的作者赶紧删了.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210728132630.png"></p>
<p>如果这个时候创建一个nodePort类型的svc， 端口也为31123,结果会怎么样呢?</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-nodeport2</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">nginx-nodeport2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">nginx-nodeport2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">nginx-nodeport2</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">nodeName:</span> <span class="string">spring-38</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx:latest</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-nodeport2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">31123</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">nginx-nodeport2</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210728133425.png"></p>
<p>可以发现，NodePort是可以成功创建的，同时监听的端口也出现了.</p>
<p><strong>从这也可以说明使用hostposrt指定的端口并没有listen主机的端口，要不然这里就会提示端口重复之类</strong></p>
<p>那么问题又来了，同一台机器上同时存在有hostPort跟nodePort的端口，这个时候如果curl 31123时， 访问的是哪一个呢?</p>
<p>经多次使用curl请求后，均是使用了hostport那个nginx pod收到请求</p>
<p><strong>原因还是因为KUBE-NODE-PORT规则在KUBE-SERVICE的链中是处于最后位置，而hostPort通过portmap写入的规则排在其之前</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210728142725.png"></p>
<p>因此会先匹配到hostport的规则，自然请求就被转到hostport所在的pod中，这两者的顺序是没办法改变的，因此无论是hostport的应用发布在前还是在后都无法影响请求转发</p>
<p>另外再提一下，<strong>hostport的规则在ipvsadm中是查询不到的，而nodePort的规则则是可以使用ipvsadm查询得到</strong></p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>要想把这些规则删除，可以直接将hostport去掉，那么规则就会随着删除,比如下图中去掉了一个nginx的hostport</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210727230931.png"></p>
<p>另外使用较多的port-forward也是可以进行端口转发的，它又是个什么情况呢? 它其实使用的是socat及netenter工具，网上看到一篇文章，原理写的挺好的，感兴趣的可以看一看</p>
<p>参考: <a href="https://vflong.github.io/sre/k8s/2020/03/15/how-the-kubectl-port-forward-command-works.html">https://vflong.github.io/sre/k8s/2020/03/15/how-the-kubectl-port-forward-command-works.html</a></p>
<h3 id="生产建议"><a href="#生产建议" class="headerlink" title="生产建议"></a>生产建议</h3><p>一句话，生产环境除非是<strong>必要且无他法</strong>，不然<strong>一定不要使用hostport</strong>，除了会影响调度结果之外，还会出现上述问题，可能造成的后果是非常严重的</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.qikqiak.com/post/how-to-use-ipvs-in-kubernetes/">https://www.qikqiak.com/post/how-to-use-ipvs-in-kubernetes/</a></li>
<li><a href="https://serenafeng.github.io/2020/03/26/kube-proxy-in-iptables-mode/">https://serenafeng.github.io/2020/03/26/kube-proxy-in-iptables-mode/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/94418251">https://zhuanlan.zhihu.com/p/94418251</a></li>
<li><a href="https://ronaknathani.com/blog/2020/07/kubernetes-nodeport-and-iptables-rules/">https://ronaknathani.com/blog/2020/07/kubernetes-nodeport-and-iptables-rules/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</a></li>
<li><a href="https://vflong.github.io/sre/k8s/2020/03/15/how-the-kubectl-port-forward-command-works.html">https://vflong.github.io/sre/k8s/2020/03/15/how-the-kubectl-port-forward-command-works.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(flannel深入学习)</title>
    <url>/2020/01/05/Kubernetes-flannel-details/</url>
    <content><![CDATA[<p>得益于社区的繁荣, 包括部署工具的完善, 现在几乎没人会从0去搭建kubernetes集群, 随便拿起一个开源工具, 几乎都是一条命令, 喝个咖啡, 然后整个集群就能用了, 非常快捷, 但是方便的时候, 也容易遗漏掉其中的一些知识点, 有些对整个kubernetes的了解及生态还是非常有必要的.</p>
<p>由于社区可选的CNI成熟方案非常的多, 而flannel是kubernetes默认的容器网络模型, 还是要重点深入学习下</p>
<p>由于flannel涉及到CNI相关的知识, CNI也不会在这里深入探讨, 有机会跟CRI一起更.</p>
<span id="more"></span>



<p>比如有以下涉及容器网络的问题? </p>
<ol>
<li>kubernetes如何知道集群中采用了哪种CNI, 处理逻辑是什么?</li>
<li>使用ip addr看到一个docker0、cni0、flannel.1它们有什么作用, docker0在flannel中扮演什么角色?</li>
<li>flannel的通信方式？</li>
<li>从使用kubectl创建了一个pod后，期间发生了什么？</li>
</ol>
<h3 id="如何知道使用了哪种CNI-处理逻辑是什么？"><a href="#如何知道使用了哪种CNI-处理逻辑是什么？" class="headerlink" title="如何知道使用了哪种CNI,处理逻辑是什么？"></a>如何知道使用了哪种CNI,处理逻辑是什么？</h3><p>这里并不会对CNI的规范做一个说明, 简单总结来说就是:</p>
<p><code>CNI旨在为容器平台提供网络的标准化，它 本身并不是实现或者代码，可以理解成一个协议，这个协议连接了两个组件：容器管理系统和网络插件。它们之间通过 JSON 格式的文件进行通信，实现容器的网络功能。具体的事情都是插件来实现的，包括：创建容器网络空间（network namespace）、把网络接口（interface）放到对应的网络空间、给网络接口分配 IP 等等</code>。</p>
<p>要知道一个集群使用了哪种CNI, 最好的办法就是顺序数据流向一步一步分析.</p>
<p>从上面对CNI的总结看, CNI的相关配置一定是在容器运行时这一端, 因为要创建容器网络空间, 自然是要在client端，那么对于kubernetes来说就是kubelet, 查看kubelet参数(这里kubelet是以容器启动的)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413131106.png"></p>
<p>这3个参数决定了集群使用的哪种CNI, 其中第2个参数指定了使用CNI来组件容器网络, 现在来逐一的查看这些目录</p>
<h4 id="x2F-etc-x2F-cni-x2F-net-d"><a href="#x2F-etc-x2F-cni-x2F-net-d" class="headerlink" title="&#x2F;etc&#x2F;cni&#x2F;net.d"></a>&#x2F;etc&#x2F;cni&#x2F;net.d</h4><p>该文件下只有一个文件<code>10-flannel.conflist</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200413131329691.png"></p>
<p>注意<code>type: flannel</code>, 那么这里就指定了使用的是flannel, 而不是calico, macvlan等其它CNI</p>
<h4 id="x2F-opt-x2F-cni-x2F-bin"><a href="#x2F-opt-x2F-cni-x2F-bin" class="headerlink" title="&#x2F;opt&#x2F;cni&#x2F;bin"></a>&#x2F;opt&#x2F;cni&#x2F;bin</h4><p>这个目录下放置着CNI的二进制文件, 因为CNI协议规则，所有实现了CNI的都必须提交二进制的文件供kubelet调用, 并且会调用后端的网络插件</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413131222.png"></p>
<p>因此通过指定<code>type: flannel</code>来这个目录下调用<code>flannel</code></p>
<p>这里要区分一下flannel跟flanneld这两个</p>
<h4 id="flannel"><a href="#flannel" class="headerlink" title="flannel"></a>flannel</h4><p>平时大家说的flannel指的是&#x2F;opt&#x2F;cni&#x2F;bin&#x2F;flannel, 但是这个二进制的github源码是在<a href="https://github.com/containernetworking/plugins">plugins</a>, 并不是指的flanneld.</p>
<p>这个flannel才是真正做为CNI插件实现了<code>CNI_COMMAND ADD/DEL</code>方法</p>
<h4 id="flanneld"><a href="#flanneld" class="headerlink" title="flanneld"></a>flanneld</h4><p>这个flanneld的github源码在<a href="https://github.com/coreos/flannel">flannel</a>, 这个flanneld主要实现overlay网络, 包含给所有node分配subset段, 同步节点间的网络信息等.</p>
<p>flanneld可以以容器的方式(daemonset)运行, 查看下配置文件</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413131838.png"></p>
<p>从配置文件可以到有两个配置文件</p>
<h5 id="cni-conf-json"><a href="#cni-conf-json" class="headerlink" title="cni-conf.json"></a>cni-conf.json</h5><p>这个文件就是&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;10-flannel.conflist， 指定backend使用flannel</p>
<h5 id="net-conf-json"><a href="#net-conf-json" class="headerlink" title="net-conf.json"></a>net-conf.json</h5><p>这个文件指定了整个kubernetes网络能够使用的ip地址段(在部署时可以被覆盖), 且指定了flannel使用vxlan模式(flannel本身提供多种模式), 最终这个文件会挂载到<code>/etc/kube-flannel/net-conf.json</code>, 提供给cni使用</p>
<h5 id="subset-env"><a href="#subset-env" class="headerlink" title="subset.env"></a>subset.env</h5><p>还有一个很关键的文件就是<code>/run/flannel/subnet.env</code>, 该文件记录了某个node上的flanneld启动后在etcd端分配的ip子段, 也就是说, 这个node上的所有pod只能在这个ip段内分配</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413131511.png"></p>
<p>从这里可以看出, 每台机器上最多能够使用的ip有254个, 这个是远远够的, 一般情况下一台机器的pod数不会过百</p>
<h5 id="flannel-cni容器"><a href="#flannel-cni容器" class="headerlink" title="flannel-cni容器"></a>flannel-cni容器</h5><p>细心的可能会发现flannel的yaml文件还包含了install-cni.sh的容器, 这个容器的逻辑比较简单, 大家可自行查看源码<a href="https://github.com/coreos/flannel-cni">github</a></p>
<h5 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h5><p>flanneld的启动参数中指定了etcd, etcd在flannel充当数据的角色, 同时使用etcd做为服务注册、 发现, 所有pod&#x2F;node的ip&#x2F;mac信息都存储在etcd中, 所有flanneld节点共享， 通过这种方式来避免ip重复分配.</p>
<h4 id="处理逻辑"><a href="#处理逻辑" class="headerlink" title="处理逻辑"></a>处理逻辑</h4><p>因此, 如果集群中使用了flannel做为CNI, 完整的处理逻辑如上分析, 简单说的话就是:</p>
<blockquote>
<ul>
<li>指定整个集群pod的ip CIDR</li>
<li>每个node上都会启动flanneld进行, 通过etcd分配该node上所有pod的使用地址, 后续会在整个集群内共享各个node上的pod ip信息, 生成路由信息.</li>
<li>所有node通过flannel overlay网络进行通信</li>
</ul>
</blockquote>
<h3 id="docker0、cni0、flannel-1"><a href="#docker0、cni0、flannel-1" class="headerlink" title="docker0、cni0、flannel.1"></a>docker0、cni0、flannel.1</h3><p>如果在一台node上使用<code>ip addr</code>, 会发现有docker0、cni0、flannel.1这些网络接口</p>
<p>这三者充当什么角色.</p>
<p>首先docker0这个跟flannel及docker的部署顺序有关系, 如果是docker先部署了, 后再部署flannel,部署完成之后也没有重启docker, 那么这种情况docker0是不做为node与pod之间的bridege, 而是使用了默认的cni0来组成veth对</p>
<p>可以使用<code>brctl show</code>来查看, 会发现该node上所有的pod都是绑定在cni0上</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413121335.png"></p>
<p>而如何在部署flannel时修改了docker的启动参数, docker使用了flannel的bip参数的话,那就会默认使用docker0来组vth对, 即pod都会通过docker0通信.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413173855.png"></p>
<p>而flannel.1则是flanneld组成的overlay网络的通道, 它本质上是个VXLAN Tunnel End Point，如上图所示</p>
<p>具体的细节下面使用flannel的通信来说明</p>
<h3 id="flannel通信方式"><a href="#flannel通信方式" class="headerlink" title="flannel通信方式"></a>flannel通信方式</h3><h4 id="Kube-dns"><a href="#Kube-dns" class="headerlink" title="Kube-dns"></a>Kube-dns</h4><p>这里简单提一下kube-dns, kube-dns负责集群内的域名解析, 因此在集群内访问时才能使用<code>xxx.namespace.svc.cluster.local</code>这种形式访问.</p>
<p>kube-dns也支持上游dns, kube-dns集群内解析不了的可交由上游dns解析. </p>
<p>如在kube-dns的pod中挂载宿主要的&#x2F;etc&#x2F;resolv.conf文件.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200411211019.png"></p>
<h3 id="路由规则"><a href="#路由规则" class="headerlink" title="路由规则"></a>路由规则</h3><p>kubernets的网络，从设计上来讲是“扁平、直接”的，要实现kubernetes的cni模型, 需要实现3点要求:</p>
<blockquote>
<ul>
<li>所有容器可以不使用NAT技术就可以与其他容器通信</li>
<li>所有节点(物理机 虚拟机 容器)都可以不使用NAT同容器通信</li>
<li>容器看到的IP地址和别的机器看到的IP是一致的</li>
</ul>
</blockquote>
<p>那么一条网络报文是怎么从一个容器发送到另外一个容器的呢(跨主机) ？</p>
<p>这里以<code>vxlan</code>为例</p>
<ol>
<li>容器直接使用目标容器的 ip 访问，默认通过容器内部的 eth0 发送出去</li>
<li>报文通过 veth pair 被发送到 vethXXX</li>
<li>vethXXX 是直接连接到虚拟交换机 cni0 的，报文通过虚拟 bridge cni0 发送出去</li>
<li>查找路由表，外部容器 ip 的报文都会转发到 flannel.1 虚拟网卡，这是一个 P2P 的虚拟网卡,然后报文就被转发到监听在另一端的 flanneld</li>
<li>flanneld 通过 etcd 维护了各个节点之间的路由表，把原来的报文 UDP 封装一层，通过配置的 <code>iface</code> 发送出去</li>
<li>报文通过主机之间的网络找到目标主机</li>
<li>报文继续往上，到传输层，交给监听在 8285 端口的 flanneld 程序处理</li>
<li>数据被解包，然后发送给 flannel.1 虚拟网卡</li>
<li>查找路由表，发现对应容器的报文要交给 cni0</li>
<li>cni0 找到连到自己的容器，把报文发送过去</li>
</ol>
<p>如果是同一node上pod通信, 则直接通过cni0就可以, </p>
<p>查看node上的路由信息:</p>
<p><code>route -n</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200411215227.png"></p>
<p>从这里可以看到, <code>10.42.xx</code>的使用flannel.1 iface转发, 这也是集群pod的网段.</p>
<p>而如果是访问的其它ip,则是通过eth0 iface</p>
<h4 id="VXLAN"><a href="#VXLAN" class="headerlink" title="VXLAN"></a>VXLAN</h4><p>vxlan是Linux上支持的一个隧道的技术，隧道也就是点到点，端到端的两个设备的通信，其实vxlan实现是有一个vtep的设备做数据包的封装与解封装，而现在已经封装到flannel.1这个进程里面了，也就是这个虚拟网卡包含了veth对使用对这个vxlan进行封装和解封装</p>
<h4 id="为何需要封装"><a href="#为何需要封装" class="headerlink" title="为何需要封装"></a>为何需要封装</h4><p>为什么需要封装呢? 从上面的图可以看到</p>
<p>假如现在有如下请求.podA –&gt; podB(<code>如果是serviceB的clusterIP,则会通过kube-proxy机制随机得到一个后端ip</code>)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413192311.png"></p>
<p>则根据路由信息, 知道需要从flannel.1网口转发, 但是flannel.1要发送到哪呢？按照常规逻辑, 这个请求包中只包含有如下信息：</p>
<p><code>src: podA地址</code></p>
<p><code>dst: podB地址</code></p>
<p>通过flannel生成的路由信息可以知道需要通过NodeA上的flannel.1进行转发.</p>
<p>如果仅仅知道上面这些信息是无法将请求转发到NodeB的, 因为现在还不知道NodeB的IP,就无法路由</p>
<p>因此, 需要一种机制把NodeB的IP封装到原始包中, 进行路由, 同时NodeB需要支持解封装</p>
<p>这就是需要封装的原因, 而vxlan是内核原生支持的特性.性能也比较高</p>
<p>当然也许有人会问不是可以通过api-server根据podip来查找所属的NodeIP吗？</p>
<p>这完全是没问题的, 但这会交付网络寻址的问题融入到kubernetes的代码中,将会变得异常复杂,同时也违背了<code>插件化</code>的理念, 因此路由寻址问题必须在容器网络插件层面解决.</p>
<h4 id="封包格式"><a href="#封包格式" class="headerlink" title="封包格式"></a>封包格式</h4><p>如上图所示，当NodeB加入flannel网络时，和其他所有backend一样，它会将自己的subnet 10.1.16.0&#x2F;24和Public IP 192.168.0.101写入etcd中，和其他backend不一样的是，它<code>还会将vtep设备flannel.1的mac地址也写入etcd中</code>。</p>
<p>之后，NodeA会得到EventAdded事件，并从中获取NodeB添加至etcd的各种信息。这个时候，它会在本机上添加三条信息：</p>
<ol>
<li><p>路由信息：所有通往目的地址10.1.16.0&#x2F;24的封包都通过vtep设备flannel.1设备发出，发往的网关地址为10.1.16.0，即NodeB中的flannel.1设备</p>
</li>
<li><p>fdb信息：MAC地址为MAC B的封包，都将通过vxlan首先发往目的地址192.168.0.101，即NodeB</p>
</li>
<li><p>arp信息：网关地址10.1.16.0的地址为MAC B</p>
</li>
</ol>
<p>最终生成的封装包如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413193513.png"></p>
<p>可以查看其它机器的flannel.1设备的mac地址</p>
<p><code>bridge fdb show  dev flannel.1</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413181732.png"></p>
<p>通过mac地址查找ip地址</p>
<p><code>bridge fdb show  dev flannel.1|grep &#39;46:2f:09:f1&#39;</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413181908.png"></p>
<h4 id="完整流程"><a href="#完整流程" class="headerlink" title="完整流程"></a>完整流程</h4><p>有一个容器网络封包要从A发往容器B，封包首先通过网桥转发到NodeA中。此时通过，查找路由表，该封包应当通过设备flannel.1发往网关10.1.16.0。通过进一步查找arp表，我们知道目的地址10.1.16.0的mac地址为MAC B。到现在为止，vxlan负载部分的数据已经封装完成。由于<code>flannel.1是vtep设备</code>，会对通过它发出的数据进行vxlan封装（这一步是由内核完成的），通过查询fdb获取目的mac地址为MAC B的ip地址为192.168.0.101</p>
<p>封包到达主机B的eth0，通过内核的vxlan模块解包，容器数据封包将到达vxlan设备flannel.1，封包的目的以太网地址和flannel.1的以太网地址相等，三层封包最终将进入主机B并通过路由转发达到目的容器</p>
<p><code>先通过NodeB的IP在三层网络进行路由, 到达NodeB后, 再在二层网络使用MAC地址发现NodeB上的flannel.1设备, 最终通过PodB的IP进行转发完成整个流程</code></p>
<p>整个流程如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413185816.png"></p>
<p><strong>flannel不需要Nodes之间在同一个子网, 只需要能够互通就行.</strong></p>
<h4 id="host-gw"><a href="#host-gw" class="headerlink" title="host-gw"></a>host-gw</h4><p>hostgw是最简单的backend，它的原理非常简单，直接添加路由，将目的主机当做网关，直接路由原始封包。例如，我们从etcd中监听到一个EventAdded事件：subnet为10.1.15.0&#x2F;24被分配给主机Public IP 192.168.0.100，hostgw要做的工作非常简单，在本主机上添加一条目的地址为10.1.15.0&#x2F;24，网关地址为192.168.0.100，输出设备为上文中选择的集群间交互的网卡即可。对于EventRemoved事件，删除对应的路由即可</p>
<p><strong>Host-gw有一定的限制，就是要求所有的主机都在一个子网内，即二层可达(不需要经过路由)，否则就无法将目的主机当做网关，直接路由</strong></p>
<p>至于选择vxlan还是host-gw, 根据实际情况吧, 性能上host-gw肯定是高一点, 但网络上有限制, 而vxlan如果集群规则不是很大的话, 基本跟host-gw的性能差不多.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">https://kubernetes.io</a></li>
<li><a href="https://github.com/containernetworking/plugins">https://github.com/containernetworking/plugins</a></li>
<li><a href="https://github.com/coreos/flannel-cni">https://github.com/coreos/flannel-cni</a></li>
<li><a href="https://blog.51cto.com/14143894/2462379">https://blog.51cto.com/14143894/2462379</a></li>
<li><a href="https://www.cnblogs.com/YaoDD/p/7681811.html">https://www.cnblogs.com/YaoDD/p/7681811.html</a></li>
<li><a href="https://blog.51cto.com/14143894/2462379">https://blog.51cto.com/14143894/2462379</a></li>
<li><a href="https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e">https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(深入理解emptyDir)</title>
    <url>/2022/12/30/Kubernetes-emptyDir/</url>
    <content><![CDATA[<p>emptyDir这种local storage在Kubernetes中使用频率还是比较高的，今天就来详细说一说emptyDir中使用共享内存的相关知识，同时也会结合源码，希望可以帮助大家更好的理解emptyDir</p>
<span id="more"></span>

<h3 id="共享目录"><a href="#共享目录" class="headerlink" title="共享目录"></a>共享目录</h3><p>先来看看emptyDir的常规使用方法，相信大家也非常熟悉了，简单举几个例子</p>
<p>应用场景：在一个pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂到两个容器的目录中，然后nginx容器负责想Volume中写日志，busybox中通过命令将日志内容读取到控制台， 多个容器间共享路径，这个应该是使用最为频繁的</p>
<p>yaml如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-emptydir</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:1.17.1</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/log/nginx</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox:1.30</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;tail -f /logs/access.log&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/logs</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>emptyDir在被创建之初是一个空目录，并且会一直存在于Pod的生命周期中，当Pod被移除(node重启时)时，emptyDir中的数据也会被随之删除，emptyDir可以被挂载到Pod的任意容器下的任意路径下，并且被Pod中的所有容器所共享</p>
<p>这种只适用于数据不重要的场景:</p>
<p><strong>问题1: 这个共享形式其实是个目录，但这个目录存在于哪里?跟node有什么关联?</strong></p>
<p>在kubelet的工作目录(root-dir参数控制)，默认为&#x2F;var&#x2F;lib&#x2F;kubelet, 会为每个使用了<code>emptyDir: &#123;&#125;</code>的pod创建一个目录，格式如<code>/var/lib/kubelet/pods/&#123;podid&#125;/volumes/kubernetes.io~empty-dir/</code>, 所有放在emptyDir中数据，最终都是落在了node的上述路径中， 从这里可以看出，<strong>由于默认情况下这个路径是挂载在根目录上的</strong>，因此建议还是对kubernetes集群中使用<code>emptyDir: &#123;&#125;</code>的使用情况进行监控，以免某些pod写入大量的数据导致node根目录被打爆，从而引发Evicted</p>
<p>这里有个因<code>emptyDir: &#123;&#125;</code>使用不当导致集群异常的案例<a href="https://juejin.cn/post/7005942213255381000">skywalking-agent使用emptyDir导致磁盘空间不足 - 掘金</a>)</p>
<p>总结一下: <code>emptyDir: &#123;&#125;</code>占用的是node的<strong>存储资源</strong></p>
<h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>先解释一下共享内存与内存的区别:</p>
<blockquote>
<ul>
<li>内存一般指node的物理内存,就是我们常说的某某机器内存多大</li>
<li>共享内存是进程单通信的一种方式，多个进程都可访问同一块内存空间.</li>
</ul>
</blockquote>
<p>emptyDir可以指定memory做为存储介质, 这个是重点要讨论的地方，看yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-emptydir</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:1.17.1</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1024Mi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1024Mi</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/log/nginx</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span></span><br><span class="line">      <span class="attr">medium:</span> <span class="string">Memory</span></span><br><span class="line">      <span class="attr">sizeLimit:</span> <span class="string">512Mi</span></span><br></pre></td></tr></table></figure>

<p>ps: 以上yaml只是举个例子，真实场景并不会把昂贵的内存资源用于存储日志</p>
<p>这里要特别说明的是，由于是内存，所以在pod或者是node重启时，也会导致存储丢失</p>
<p>这里指定了medium&#x3D;Memory(没有指定medium时如第一个case), 且指定sizeLimit为500Mi</p>
<p>在这里引出第2个问题: </p>
<p><strong>问题2: 这个Memory占用的是谁的内存，是pod的resource中limit内存的一部分还是会额外占用Node的内存? 对于使用sizeLimit参数，设置与不设置有什么不同?</strong></p>
<p>先说结论，<strong>medium&#x3D;Memory指定的内存不能超过pod中所有容器limit.memory资源之和</strong>，其中的大小关系由sizeLimit限定</p>
<h4 id="linux共享内存"><a href="#linux共享内存" class="headerlink" title="linux共享内存"></a>linux共享内存</h4><p>这里不得不先说一说linux中的共享内存(这里不考虑swap分区)，在linux中，常常能看到<code>tmpfs</code>类型的文件系统， tmpfs是一种<strong>虚拟内存</strong>文件系统,最常用的虚拟内存就是&#x2F;dev&#x2F;shm了，经常会被来做为内存使用，可以提高数据的读写速度，在linux中，声明一块共享内存非常简单，如下命令即可将&#x2F;tmp挂载到共享内存上，大小为20m</p>
<p><code>mount -t tmpfs -o size=20m tmpfs /tmp</code></p>
<p>tmpfs有一个特点就是， 虽然size&#x3D;20m, 但<strong>它是按照实际情况计算内存的，只是说最大能用到20m</strong>,这个特性很重要.</p>
<h4 id="medium-x3D-Memory的实现"><a href="#medium-x3D-Memory的实现" class="headerlink" title="medium&#x3D;Memory的实现"></a>medium&#x3D;Memory的实现</h4><p>回到问题2，kubernetes在处理medium&#x3D;Memory类型的emptyDir时，就是使用了tmpfs，可以从代码看到: </p>
<p><code>kubernetes/pkg/volume/emptydir/empty_dir.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 省略其它代码</span></span><br><span class="line"><span class="keyword">switch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> ed.medium == v1.StorageMediumDefault:</span><br><span class="line">        err = ed.setupDir(dir)</span><br><span class="line">    <span class="keyword">case</span> ed.medium == v1.StorageMediumMemory:</span><br><span class="line">        err = ed.setupTmpfs(dir)</span><br><span class="line">    <span class="keyword">case</span> v1helper.IsHugePageMedium(ed.medium): <span class="comment">// 不讨论这种情况</span></span><br><span class="line">        err = ed.setupHugepages(dir)</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        err = fmt.Errorf(<span class="string">&quot;unknown storage medium %q&quot;</span>, ed.medium)</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 省略其它代码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// setupTmpfs creates a tmpfs mount at the specified directory.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ed *emptyDir)</span></span> setupTmpfs(dir <span class="type">string</span>) <span class="type">error</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> ed.mounter == <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;memory storage requested, but mounter is nil&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err := ed.setupDir(dir); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Make SetUp idempotent.</span></span><br><span class="line">    medium, isMnt, _, err := ed.mountDetector.GetMountMedium(dir, ed.medium)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// If the directory is a mountpoint with medium memory, there is no</span></span><br><span class="line">    <span class="comment">// work to do since we are already in the desired state.</span></span><br><span class="line">    <span class="keyword">if</span> isMnt &amp;&amp; medium == v1.StorageMediumMemory &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> options []<span class="type">string</span></span><br><span class="line">    <span class="comment">// Linux system default is 50% of capacity.</span></span><br><span class="line">    <span class="keyword">if</span> ed.sizeLimit != <span class="literal">nil</span> &amp;&amp; ed.sizeLimit.Value() &gt; <span class="number">0</span> &#123;</span><br><span class="line">        options = []<span class="type">string</span>&#123;fmt.Sprintf(<span class="string">&quot;size=%d&quot;</span>, ed.sizeLimit.Value())&#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;pod %v: mounting tmpfs for volume %v&quot;</span>, ed.pod.UID, ed.volName)</span><br><span class="line">    <span class="keyword">return</span> ed.mounter.MountSensitiveWithoutSystemd(<span class="string">&quot;tmpfs&quot;</span>, dir, <span class="string">&quot;tmpfs&quot;</span>, options, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重点就是最后一句话，最终转化成执行 mount tmpfs命令创建了一个共享内存挂载到pod中</p>
<h4 id="是否会影响调度"><a href="#是否会影响调度" class="headerlink" title="是否会影响调度"></a>是否会影响调度</h4><p>再来看看如下的yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 省略其它字段</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">logs-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span></span><br><span class="line">      <span class="attr">medium:</span> <span class="string">Memory</span></span><br><span class="line">      <span class="attr">sizeLimit:</span> <span class="string">512Mi</span></span><br></pre></td></tr></table></figure>

<p>pod即申请了resource资源，又指定的emptyDir.medium，那么pod真正占用的资源是1024Mi还是1024Mi+512Mi?  medium.sizeLimit如何影响调度器调度？</p>
<p>答案是1024Mi, 因为medium.memory指定的内存会包含在limit的指定的内存数值中，这个很好理解，举个例子，我们常说一个node的内存是4G, 那么如果有共享内存，那么共享内存的大小最大也只能在4Gi这个范围(同样这里不考虑swap分区)</p>
<p>源码中定义了sizeLimit的使用范围</p>
<p><code>kubernetes/pkg/volume/emptydir/empty_dir.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">calculateEmptyDirMemorySize</span><span class="params">(nodeAllocatableMemory *resource.Quantity, spec *volume.Spec, pod *v1.Pod)</span></span> *resource.Quantity &#123;</span><br><span class="line">    <span class="comment">// if feature is disabled, continue the default behavior of linux host default</span></span><br><span class="line">    sizeLimit := &amp;resource.Quantity&#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> !utilfeature.DefaultFeatureGate.Enabled(features.SizeMemoryBackedVolumes) &#123;</span><br><span class="line">        <span class="keyword">return</span> sizeLimit</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// size limit defaults to node allocatable (pods can&#x27;t consume more memory than all pods)</span></span><br><span class="line">    sizeLimit = nodeAllocatableMemory</span><br><span class="line">    zero := resource.MustParse(<span class="string">&quot;0&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// determine pod resource allocation</span></span><br><span class="line">    <span class="comment">// we use the same function for pod cgroup assignment to maintain consistent behavior</span></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> this could be nil on systems that do not support pod memory containment (i.e. windows)</span></span><br><span class="line">    podResourceConfig := cm.ResourceConfigForPod(pod, <span class="literal">false</span>, <span class="type">uint64</span>(<span class="number">100000</span>), <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">if</span> podResourceConfig != <span class="literal">nil</span> &amp;&amp; podResourceConfig.Memory != <span class="literal">nil</span> &#123;</span><br><span class="line">        podMemoryLimit := resource.NewQuantity(*(podResourceConfig.Memory), resource.BinarySI)</span><br><span class="line">        <span class="comment">// ensure 0 &lt; value &lt; size</span></span><br><span class="line">        <span class="keyword">if</span> podMemoryLimit.Cmp(zero) &gt; <span class="number">0</span> &amp;&amp; podMemoryLimit.Cmp(*sizeLimit) &lt; <span class="number">1</span> &#123;</span><br><span class="line">            sizeLimit = podMemoryLimit</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// volume local size is  used if and only if less than what pod could consume</span></span><br><span class="line">    <span class="keyword">if</span> spec.Volume.EmptyDir.SizeLimit != <span class="literal">nil</span> &#123;</span><br><span class="line">        volumeSizeLimit := spec.Volume.EmptyDir.SizeLimit</span><br><span class="line">        <span class="comment">// ensure 0 &lt; value &lt; size</span></span><br><span class="line">        <span class="keyword">if</span> volumeSizeLimit.Cmp(zero) &gt; <span class="number">0</span> &amp;&amp; volumeSizeLimit.Cmp(*sizeLimit) &lt; <span class="number">1</span> &#123;</span><br><span class="line">            sizeLimit = volumeSizeLimit</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sizeLimit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码中存在一句很重要的注释，即</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// volume local size is  used if and only if less than what pod could consume</span></span><br></pre></td></tr></table></figure>

<p><strong>也就是说使用的volume size不能超过pod所能消耗的资源总和</strong></p>
<p>上述代码指定了如下的等量关系:</p>
<ol>
<li>当 Pod 并没有设置memory limit时，此时 shm大小为node的Allocateable Memory大小</li>
<li>当Pod 设置了Memory Limit 但是在medium的emptyDir未设置sizeLimit时，shm 大小为Pod 的memory Limit</li>
<li>当Pod的medium emptyDir设置sizeLimit时，shm大小为sizeLimit, 注意，如果设置的值超过pod的的memory Limit，则会被自动设置等于pod的的memoryLimit</li>
</ol>
<p>正因为如此，<strong>medium.memory并不会影响调度器的调度，调度器还是会以pod requests中的资源进行调度，然后cgroup以按limits中的资源设置</strong></p>
<p><strong>问题3: medium.memory受不受cgroup的限制?</strong></p>
<p>是受限制的，由于这部分内存本就是resource.limits中的一部分，当medium.memory对应的目录使用超过了sizelimit时，也会被kubelet Evicted，但不是立即被Evicted， 而是会等kubelet Evicte周期探测后发生，默认大概1-2分钟。</p>
<p>总结一下: <code>emptyDir: medium.memory</code>占用的是node的<strong>内存资源</strong></p>
<h3 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h3><p>因此在需要Pod中使用共享内存的场景，建议如下:</p>
<ol>
<li><p>务必配置pod的memory limits</p>
</li>
<li><p>挂载emptyDir.medium.memory挂载到pod的&#x2F;dev&#x2F;shm(其它路径也可)</p>
</li>
<li><p>设置sizeLimit为limits的50%</p>
</li>
<li><p>限制emptyDir的使用并且做好emptyDir的监控措施以免发生集群雪崩</p>
</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.sobyte.net/post/2022-04/k8s-pod-shared-memory/">Setting up the shared memory of a kubernetes Pod - SoByte</a></li>
<li><a href="https://juejin.cn/post/7005942213255381000">skywalking-agent使用emptyDir导致磁盘空间不足 - 掘金</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38359775">https://zhuanlan.zhihu.com/p/38359775</a></li>
<li><a href="https://damonyi.cc/2021/03/31/k8s-pod-%E9%85%8D%E7%BD%AEshareMemory/">k8s pod 配置shareMemory | 云里雾里</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kubectl的工作机制分析)</title>
    <url>/2021/01/10/Kubernetes-how-does-kubectl-work/</url>
    <content><![CDATA[<p>看到一篇关于kubectl运行的机制，觉得写得非常不错，图文并茂很形象，就翻译成了中文记录一下，原文地址: <a href="https://erkanerol.github.io/post/how-kubectl-exec-works/">https://erkanerol.github.io/post/how-kubectl-exec-works/</a></p>
<span id="more"></span>



<p>上周五，我的一位同事问了一个有关如何使用go-client在pod中执行命令的问题。我不知道答案，我注意到我从未想过“ kubectl exec”中的机制。我有一些想法，但是我不100％确定。我需要通过实践来找到答案，在阅读了一些博客，文档和源代码后，我学到了很多东西。在这篇博客中，我将分享我的理解和发现。</p>
<p>如果出现问题，可在以下频道找到我 :<a href="https://twitter.com/erkan_erol">https://twitter.com/erkan_erol</a>_</p>
<h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>我克隆了<a href="https://github.com/ecomm-integration-ballerina/kubernetes-cluster%EF%BC%8C%E4%BB%A5%E4%BE%BF%E5%9C%A8MacBook%E4%B8%AD%E5%88%9B%E5%BB%BAk8s%E9%9B%86%E7%BE%A4%E3%80%82%E6%88%91%E4%BF%AE%E6%94%B9kubelet%E9%85%8D%E7%BD%AE%E4%B8%AD%E8%8A%82%E7%82%B9%E7%9A%84IP%E5%9C%B0%E5%9D%80%EF%BC%8C%E5%9B%A0%E4%B8%BA%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E4%B8%8D%E5%85%81%E8%AE%B8%E6%88%91%E8%BF%90%E8%A1%8C%60kubectl">https://github.com/ecomm-integration-ballerina/kubernetes-cluster，以便在MacBook中创建k8s集群。我修改kubelet配置中节点的IP地址，因为默认配置不允许我运行`kubectl</a> exec&#96;。您可以在<a href="https://medium.com/@joatmon08/playing-with-kubeadm-in-vagrant-machines-part-2-bac431095706">这里</a>找到根本原因。</p>
<p>下文提到的关于机器的对应关系如下:</p>
<ul>
<li>any machine &#x3D; MacBook</li>
<li>master IP &#x3D; 192.168.205.10</li>
<li>work IP &#x3D; 192.168.205.11</li>
<li>API server端口 &#x3D; 6443</li>
</ul>
<h2 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h2><p><img src="https://erkanerol.github.io/img/kubectl-exec/components.png" alt="组件"></p>
<ul>
<li>kubectl exec：当我们在机器上运行“ kubectl exec…”时，可以在任何有权限访问k8s api服务器上运行。</li>
<li><a href="https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver">api server</a>：<a href="https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver">api server</a>上的组件，用于公开Kubernetes API。它是Kubernetes控制平面的前端。</li>
<li><a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet</a>：在集群中每个节点上运行的代理。确保容器在容器中运行。</li>
<li><a href="https://kubernetes.io/docs/concepts/overview/components/#container-runtime">container runtime</a>：负责运行容器的软件。例如：docker，cri-o，containerd…</li>
<li>kernel：工作节点中操作系统的内核，负责管理进程。</li>
<li>target container：作为Pod的一部分并在其中一个工作程序节点上运行的容器。</li>
</ul>
<h2 id="Findings"><a href="#Findings" class="headerlink" title="Findings"></a>Findings</h2><h3 id="1-Client端"><a href="#1-Client端" class="headerlink" title="1. Client端"></a>1. Client端</h3><ul>
<li><p>在默认名称空间中创建容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ kubectl run exec-test-nginx --image=nginx</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后运行exec命令并<code>sleep 5000</code>进行观察</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ kubectl exec -it exec-test-nginx-6558988d5-fgxgg -- sh</span><br><span class="line"># sleep 5000</span><br></pre></td></tr></table></figure>
</li>
<li><p>我们可以观察到kubectl过程（在这种情况下为pid &#x3D; 8507）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ ps -ef |grep kubectl</span><br><span class="line">501  8507  8409   0  7:19PM ttys000    0:00.13 kubectl exec -it exec-test-nginx-6558988d5-fgxgg -- sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>当我们检查该进程的网络活动时，我们可以看到它与api-server（192.168.205.10.6443）有连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ netstat -atnv |grep 8507</span><br><span class="line">tcp4       0      0  192.168.205.1.51673    192.168.205.10.6443    ESTABLISHED 131072 131768   8507      0 0x0102 0x00000020</span><br><span class="line">tcp4       0      0  192.168.205.1.51672    192.168.205.10.6443    ESTABLISHED 131072 131768   8507      0 0x0102 0x00000028</span><br></pre></td></tr></table></figure>
</li>
<li><p>让我们检查一下代码。kubectl使用子资源创建一个POST请求,<code>exec</code>并发送一个rest请求。</p>
</li>
</ul>
<p><img src="https://erkanerol.github.io/img/kubectl-exec/rest-request.png" alt="休息请求"></p>
<h3 id="2-apiserver端"><a href="#2-apiserver端" class="headerlink" title="2. apiserver端"></a>2. apiserver端</h3><ul>
<li><p>我们可以在api服务器端观察请求。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">handler.go:143] kube-apiserver: POST &quot;/api/v1/namespaces/default/pods/exec-test-nginx-6558988d5-fgxgg/exec&quot; satisfied by gorestful with webservice /api/v1</span><br><span class="line">upgradeaware.go:261] Connecting to backend proxy (intercepting redirects) https://192.168.205.11:10250/exec/default/exec-test-nginx-6558988d5-fgxgg/exec-test-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1</span><br><span class="line">Headers: map[Connection:[Upgrade] Content-Length:[0] Upgrade:[SPDY/3.1] User-Agent:[kubectl/v1.12.10 (darwin/amd64) kubernetes/e3c1340] X-Forwarded-For:[192.168.205.1] X-Stream-Protocol-Version:[v4.channel.k8s.io v3.channel.k8s.io v2.channel.k8s.io channel.k8s.io]]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>请注意，http请求包括协议升级请求。<a href="https://www.wikiwand.com/en/SPDY">SPDY</a>允许将单独的stdin &#x2F; stdout &#x2F; stderr &#x2F; spdy-error流通过单个TCP连接进行多路复用。</p>
</blockquote>
</li>
<li><p>Api服务器收到请求并将其绑定到 <code>PodExecOptions</code></p>
</li>
<li><p>为了能够采取必要的措施，api-server需要知道应该请求哪个node。</p>
<p>当然，端点是从节点信息派生的。</p>
<p>发现了，KUBELET具有可<code>node.Status.DaemonEndpoints.KubeletEndpoint.Port</code>连接API服务器的端口。</p>
<blockquote>
<p><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/#apiserver-to-kubelet">master&#x2F;node之间的通信&gt; master集群通信 &gt; apiserver到kubelet</a></p>
<p>这些连接在kubelet的HTTPS端点处终止。默认情况下，API服务器不验证kubelet的服务证书，这使得连接容易遭受中间人攻击，不安全的公共网络。</p>
</blockquote>
</li>
<li><p>现在，api服务器知道了端点并打开了连接。</p>
</li>
<li><p>让我们检查master上发生了什么。</p>
</li>
</ul>
<p>首先，得到worker节点的ip。正是<code>192.168.205.11</code>在这种情况下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ kubectl get nodes k8s-node-1 -o wide</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME</span><br><span class="line">k8s-node-1   Ready    &lt;none&gt;   9h    v1.15.3   192.168.205.11   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-159-generic   docker://17.3.3</span><br></pre></td></tr></table></figure>

<p>然后获取kubelet端口。在这个例子中是<code>10250</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// any machine</span><br><span class="line">$ kubectl get nodes k8s-node-1 -o jsonpath=&#x27;&#123;.status.daemonEndpoints.kubeletEndpoint&#125;&#x27;</span><br><span class="line">map[Port:10250]</span><br></pre></td></tr></table></figure>

<p>然后检查网络。是否存在到工作节点（192.168.205.11）的连接？可以看到，当我杀死exec进程时，它消失了，所以我知道它正是由于exec命令而由api-server设置的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// master node</span><br><span class="line">$ netstat -atn |grep 192.168.205.11</span><br><span class="line">tcp        0      0 192.168.205.10:37870    192.168.205.11:10250    ESTABLISHED</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><img src="https://erkanerol.github.io/img/kubectl-exec/api-server-to-kubelet.png" alt="api服务器到kubelet"></p>
<ul>
<li>现在，kubectl和api-server之间的连接仍然打开，并且api-server和kubelet之间还有另一个连接。</li>
</ul>
<h3 id="3-work节点"><a href="#3-work节点" class="headerlink" title="3. work节点"></a>3. work节点</h3><ul>
<li>让我们通过连接到work节点并检查正在发生的事情。</li>
</ul>
<p>首先，我们也可以在此处观察连接。第二行。<code>192.168.205.10</code>是master的IP。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// worker node</span><br><span class="line">$ netstat -atn |grep 10250</span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN</span><br><span class="line">tcp6       0      0 192.168.205.11:10250    192.168.205.10:37870    ESTABLISHED</span><br></pre></td></tr></table></figure>

<p>那我们的sleep命令呢？可以通过ps命令找到！！！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// worker node</span><br><span class="line">$ ps -afx</span><br><span class="line">...</span><br><span class="line">31463 ?        Sl     0:00      \_ docker-containerd-shim 7d974065bbb3107074ce31c51f5ef40aea8dcd535ae11a7b8f2dd180b8ed583a /var/run/docker/libcontainerd/7d974065bbb3107074ce31c51</span><br><span class="line">31478 pts/0    Ss     0:00          \_ sh</span><br><span class="line">31485 pts/0    S+     0:00              \_ sleep 5000</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ul>
<li>kubelet如何做到的？</li>
<li>kubelet有一个守护程序，该守护程序通过10250端口与api server通信。</li>
<li>kubelet计算执行请求的响应端点。</li>
</ul>
<p>不要混淆。它不返回命令的结果。它返回一个通信端点。</p>
<p>kubelet实现的<code>RuntimeServiceClient</code>接口是Container Runtime Interface的一部分。</p>
<p>它仅使用gRPC通过Container Runtime Interface调用方法。</p>
<p>container runtime 负责实施 <code>RuntimeServiceServer</code></p>
<p><img src="https://erkanerol.github.io/img/kubectl-exec/kubelet-to-container-runtime.png" alt="Kubelet到容器的运行时"></p>
<ul>
<li>如果是这样，我们需要观察kubelet与容器运行时之间的联系。对？让我们检查。</li>
</ul>
<p>在运行exec命令之前和之后运行此命令，并检查diff。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// worker node</span><br><span class="line">$ ss -a -p |grep kubelet</span><br><span class="line">...</span><br><span class="line">u_str  ESTAB      0      0       * 157937                * 157387                users:((&quot;kubelet&quot;,pid=5714,fd=33))</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>嗯 在kubelet（pid &#x3D; 5714）与某个组件通过UNIX套接字建立了新连接。正是DOCKER（pid &#x3D; 1186）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// worker node</span><br><span class="line">$ ss -a -p |grep 157387</span><br><span class="line">...</span><br><span class="line">u_str  ESTAB      0      0       * 157937                * 157387                users:((&quot;kubelet&quot;,pid=5714,fd=33))</span><br><span class="line">u_str  ESTAB      0      0      /var/run/docker.sock 157387                * 157937                users:((&quot;dockerd&quot;,pid=1186,fd=14))</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>这是运行我们的命令的docker守护进程（pid &#x3D; 1186）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// worker node.</span><br><span class="line">$ ps -afx</span><br><span class="line">...</span><br><span class="line"> 1186 ?        Ssl    0:55 /usr/bin/dockerd -H fd://</span><br><span class="line">17784 ?        Sl     0:00      \_ docker-containerd-shim 53a0a08547b2f95986402d7f3b3e78702516244df049ba6c5aa012e81264aa3c /var/run/docker/libcontainerd/53a0a08547b2f95986402d7f3</span><br><span class="line">17801 pts/2    Ss     0:00          \_ sh</span><br><span class="line">17827 pts/2    S+     0:00              \_ sleep 5000</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="4-Docker-runtime"><a href="#4-Docker-runtime" class="headerlink" title="4. Docker runtime"></a>4. Docker runtime</h3><ul>
<li>让我们检查cri-o的源代码以了解它如何发生。逻辑在docker中相似。</li>
</ul>
<p>它具有一个实现RuntimeServiceServer的服务器。</p>
<p>在链的最后, container runtime在work 节点中执行命令。</p>
<p><img src="https://erkanerol.github.io/img/kubectl-exec/container-runtime-to-kernel.png" alt="容器运行时到内核"></p>
<p>最后，kernel执行命令</p>
<p><img src="https://erkanerol.github.io/img/kubectl-exec/kernel-puts.png" alt="内核输入"></p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ul>
<li>api-server也可以初始化与kubelet的连接。</li>
<li>这些连接将一直持续到交互式执行程序结束。<ul>
<li>kubectl和api server之间的连接</li>
<li>api server和kubelet之间的连接</li>
<li>kubelet与container runtime之间的连接</li>
</ul>
</li>
<li>kubectl或api-server无法在work节点中运行任何内容。kubelet可以运行，但也可以与container runtime时交互以进行此类操作。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://groups.google.com/forum/#!topic/kubernetes-dev/Cjia36v39vM">https://groups.google.com/forum/#!topic/kubernetes-dev/Cjia36v39vM</a></li>
<li><a href="https://medium.com/@joatmon08/playing-with-kubeadm-in-vagrant-machines-part-2-bac431095706">https://medium.com/@joatmon08/playing-with-kubeadm-in-vagrant-machines-part-2-bac431095706</a></li>
<li><a href="https://serverfault.com/questions/252723/how-to-find-other-end-of-unix-socket-connection">https://serverfault.com/questions/252723/how-to-find-other-end-of-unix-socket-connection</a></li>
</ul>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(k3d实践)</title>
    <url>/2021/03/02/Kubernetes-k3d/</url>
    <content><![CDATA[<p>之前进行了<a href="https://izsk.me/2020/08/07/Kubernetes-Kind/">kind实践</a>,可以很方便地快速组件在k8s中的验证，今天发现一款由rancher开源的相同工具, <a href="https://k3d.io/">k3d</a>，也是非常的牛逼，照样也实践一把</p>
<span id="more"></span>

<p>k3d的底层使用的是rancher开发的<a href="https://k3s.io/">k3s</a>, 一款轻量级的k8s发行版，普遍用于端边协同领域</p>
<p>k3d在快速开发中,因此有时官方的文档更新的不太及时，就容易出现看文档无法得到想要的结果，好在，一切都是那么方便 </p>
<p>docker做为先决条件，是需要提前安装好的</p>
<p>同样，需要安装k3d 的client，这里不再详说，可参考<a href="https://github.com/rancher/k3d">git k3d</a></p>
<p>这里要注意的是，最好使用新版本的k3d, k3d版本之间的差异还是比较大的，以下都是以v3.3为例</p>
<p>k3d同样能够实现以node做为虚拟node进行k8s的模拟</p>
<p>想要创建一个集群, 最简单的1master+1node</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">k3d cluster create demo</span><br></pre></td></tr></table></figure>

<p>如果需要指定多节点的话，则</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">k3d cluster create demo --servers 3 --agents 3</span><br></pre></td></tr></table></figure>

<p>demo为集群名,因为k3d支持创建多个集群</p>
<p>–servers 为控制面节点</p>
<p>–agents为node节点</p>
<p>这里一个3master + 3node的k8s集群就创建出来了，当然，k3d也跟Kind一样，除了cli方式之外也支持配置文件指定参数.</p>
<p>可以使用以下命令进行list</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">k3d cluster list</span><br></pre></td></tr></table></figure>

<p>在创建完集群之后，会自动地生成config文件，这个跟正常使用k8s是一样的，因此可以使用kubectl命令</p>
<p>查看一下pod情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get pod --all-namespaces</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210302232345.png"></p>
<p>可以看到几个比较常用见的组件</p>
<p>比如:</p>
<blockquote>
<ul>
<li>local-path-provisioner: 这个提供本地存储pv动态创建</li>
<li>metrics-server: 用于pod metrics的收集</li>
<li>traefik: 这个用于请求转发</li>
<li>coredns: 集群域名解析</li>
</ul>
</blockquote>
<p>从上面的图可以看到，其它的如Kube-proxy, kube-scheduler是没有，这个可以理解，用k3d创建出来的集群不管多少个agent，本质上还是一台实体机</p>
<p>经过一番实践后，体验还是比较不错的，但还是有点欠缺，比如，如果使用了hostpath时，因为是虚拟出来的node节点，是没有hostpath的，那要如何使用呢, 这个翻了很多的文档也没找到说明，最后还是在issue中找到答案，方法如下</p>
<p>使用host path时:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">k3d cluster create deom -v /data/middleware-data:/data/middleware-data</span><br></pre></td></tr></table></figure>

<p>需要在创建集群里将hostpath使用到的目录进行挂载即可</p>
<p>另外如果要使用nodeport，同样也是如此，需要在创建集群时就进行操作</p>
<p>k3d跟kind相比，感觉还是k3d体验好一些，kind之前用的时候出现过几次用kind创建出来的集群直接就不能用，kubectl config不起作用</p>
<p>k3d倒没有出现这种情况, 但是就是文档写的不怎么样，比如要从私有仓库中拉取镜像，这个就麻烦一点，但官方文档写的也不是很好，需要自己琢磨</p>
<p>不过，不得不承认，k3d确实是个很实用的效率工具，对于需要快速地创建k8s集群进行验证，用完即焚的你可能，本人也打算在团队中推广大家使用起来</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://k3s.io/">https://k3s.io</a></li>
<li><a href="https://k3d.io/">https://k3d.io</a></li>
<li><a href="https://github.com/rancher/k3d">https://github.com/rancher/k3d</a></li>
<li><a href="https://izsk.me/2020/08/07/Kubernetes-Kind/">https://izsk.me/2020/08/07/Kubernetes-Kind/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes(Nginx Ingress实践)</title>
    <url>/2019/07/27/Kubernetes-ingress-nginx/</url>
    <content><![CDATA[<p>kubernetes集群内的服务, 在集群外是无法访问的, 想访问的话, 必须通过一种方式将请求代理进集群内部, ingress就是最为常用的手段, ingress也有许多开源方案,这里以ingress-nginx为例.Nginx Ingress 也是本人最常用集群外对访问方式.</p>
<span id="more"></span>

<p>首先会将nginx ingress部署到集群中，然后会聊了聊它的使用方式.</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><strong>Nginx ingress 本质上就是nginx+Lua</strong>, 所以使用过nginx的，其实对它的原理一看就明白.</p>
<p>首先看一下Nginx 的 反向代理模式</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200701183352.png"></p>
<p><strong>在 k8s 系统中，后端服务的变化是十分频繁的，单纯依靠人工来更新nginx 的配置文件几乎不可能，nginx-ingress 由此应运而生。Nginx-ingress 通过监视 k8s 的资源状态变化实现对 nginx 配置文件的自动更新</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200701183553.png"></p>
<p>同样一看便知， 对于使用nginx ingress来说，知道它是怎么样的工作方式感觉就可以了，源码之类的其实没必要过多去解读.</p>
<p>总结 : Ingress-nginx分为3部分:</p>
<blockquote>
<ul>
<li>nginx-ingress-controller:  通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段 Nginx 配置，再写到 Nginx-ingress-control的 Pod 里，这个 Ingress Contronler 的pod里面运行着一个nginx服务，控制器会把生成的nginx配置写入&#x2F;etc&#x2F;nginx.conf文件中，然后 reload 一下 使用配置生效。以此来达到动态更新问题</li>
<li>Nginx: 反向代理服务</li>
<li>Ingress: ingress资源对象</li>
</ul>
</blockquote>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">prometheus.io/port:</span> <span class="string">&quot;10254&quot;</span></span><br><span class="line">        <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">beta.kubernetes.io/os</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">NotIn</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">windows</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/nginx-ingress-controller</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--default-backend-service=$(POD_NAMESPACE)/default-http-backend</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--configmap=$(POD_NAMESPACE)/nginx-configuration</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--udp-services-configmap=$(POD_NAMESPACE)/udp-services</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--annotations-prefix=nginx.ingress.kubernetes.io</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">rancher/nginx-ingress-controller:0.21.0-rancher3</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">10254</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">443</span></span><br><span class="line">          <span class="attr">hostPort:</span> <span class="number">443</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">10254</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">host-time</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">host-time</span></span><br></pre></td></tr></table></figure>

<h4 id="Default-http-backend"><a href="#Default-http-backend" class="headerlink" title="Default http backend"></a>Default http backend</h4><p>在新版的nginx ingress中，强制需要部署一个默认的后端, 用于处理无法匹配上的请求，就是一个nginx即可.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">default-http-backend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">default-http-backend</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">20Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">20Mi</span></span><br></pre></td></tr></table></figure>

<h4 id="Ingress-Rule"><a href="#Ingress-Rule" class="headerlink" title="Ingress Rule"></a>Ingress Rule</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fxxk</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">fxxk.com</span> <span class="comment"># 1. 指定绑定的域名</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">auth</span> <span class="comment"># 3. 则转发到这个service的8082端口</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8082</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/fxxk</span> <span class="comment"># 2. 如果匹配到该条规则</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">auth2</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8899</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/client/v1</span></span><br></pre></td></tr></table></figure>

<p>规则使用还是比较清晰的, 一看就明白, ingress支持很多个域名, 最终都会转换成nginx里的server的标准配置.</p>
<p>最后将上述资源apply 到集群中即可.</p>
<h3 id="支持TCP-x2F-UDP"><a href="#支持TCP-x2F-UDP" class="headerlink" title="支持TCP&#x2F;UDP"></a>支持TCP&#x2F;UDP</h3><p>其实在Nginx 1.9之后的版本就已经支持了TCP转发了, 因此Nginx Ingress默认也是可以直接使用TCP转发的, 使用了Nginx Ingress这么久，从来都没有关注过这个点</p>
<p>查看了官方文档，使用也比较简单</p>
<p>首先，需要在nginx ingress controller 的daemonset中容器的启动参数增加对TCP&#x2F;UDP的配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span>  </span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/nginx-ingress-controller</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--default-backend-service=$(POD_NAMESPACE)/default-http-backend</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--configmap=$(POD_NAMESPACE)/nginx-configuration</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--udp-services-configmap=$(POD_NAMESPACE)/udp-services</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--annotations-prefix=nginx.ingress.kubernetes.io</span></span><br></pre></td></tr></table></figure>

<p><code>--tcp-services-configmap</code> 指定包含tcp的规则的cm， tcp-services这个名字也可以自定义，具体格式下面再说</p>
<p><code>--udp-services-configmap</code>指定包含udp的规则的cm</p>
<p>当然，这个configmap可以在其它的namespace下, 比如放在 default的话，则是defalt&#x2F;tcp-services</p>
<p>来看一下这个tcp-services的cm的内容，非常简单:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">&quot;9000&quot;:</span> <span class="string">clickhouse-operator/clickhouse-1box:9000</span></span><br><span class="line">  <span class="comment"># &quot;9001&quot;: clickhouse-operator/clickhouse-1box:9001 # 可以同时暴露多个tcp端口</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tcp-services</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure>

<p>要关注的只有<code>data</code>字段，格式为: <code>out_port: namespaces/svc_name:port</code>, 即out_port为需要对外暴露的tcp端口, 这个例子表示，需要对外暴露9000端口，通过9000端口进来的TCP请求请转发到ns: clickhouse-operator下的svc: clickhouse-1box的9000端口上</p>
<p>总共就需要这2步，就能够实现对外接收tcp请求了，是不是非常方便.</p>
<p>最后来看看在nginx pod中生成的配置，其实就是转换成了nginx 的stram</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 省略...</span></span><br><span class="line"><span class="comment"># TCP services</span></span><br><span class="line"><span class="string">server</span> &#123;</span><br><span class="line">        <span class="string">preread_by_lua_block</span> &#123;</span><br><span class="line">                <span class="string">ngx.var.proxy_upstream_name=&quot;tcp-clickhouse-operator-clickhouse-1box-9000&quot;;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="string">listen</span>                  <span class="number">9000</span><span class="string">;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">proxy_timeout</span>           <span class="string">600s;</span></span><br><span class="line">        <span class="string">proxy_pass</span>              <span class="string">upstream_balancer;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># UDP services</span></span><br></pre></td></tr></table></figure>

<h3 id="Annotation-x2F-Configmap"><a href="#Annotation-x2F-Configmap" class="headerlink" title="Annotation&#x2F;Configmap"></a>Annotation&#x2F;Configmap</h3><p>nginx本身强大的原因之一在于它的很多参数都是可以设置的, 在nginx ingress中也是如此，通过增加configmap及annotation可以自定义非常多的选项, 甚至可以使用自定义模板</p>
<p>比如: 我需要自定义nginx的日志，或者调整一些参数配置，那么就可以使用一个nginx-config的cm, 然后在daemonset中指定使用这个cm即可.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">client-body-buffer-size:</span> <span class="string">10m</span></span><br><span class="line">  <span class="attr">error-log-level:</span> <span class="string">error</span></span><br><span class="line">  <span class="attr">http-redirect-code:</span> <span class="string">&quot;301&quot;</span></span><br><span class="line">  <span class="attr">log-format-upstream:</span> <span class="string">&#x27;&#123;&quot;time&quot;: &quot;$time_iso8601&quot;, &quot;remote_addr&quot;: &quot;$proxy_protocol_addr&quot;,</span></span><br><span class="line"><span class="string">    &quot;x-forward-for&quot;: &quot;$proxy_add_x_forwarded_for&quot;, &quot;request_id&quot;: &quot;$request_id&quot;, &quot;remote_user&quot;:</span></span><br><span class="line"><span class="string">    &quot;$remote_user&quot;, &quot;bytes_sent&quot;: $bytes_sent, &quot;request_time&quot;: $request_time, &quot;status&quot;:</span></span><br><span class="line"><span class="string">    $status, &quot;vhost&quot;: &quot;$host&quot;, &quot;request_proto&quot;: &quot;$server_protocol&quot;, &quot;path&quot;: &quot;$uri&quot;,</span></span><br><span class="line"><span class="string">    &quot;request_query&quot;: &quot;$args&quot;, &quot;request_length&quot;: $request_length, &quot;request_method&quot;:</span></span><br><span class="line"><span class="string">    &quot;$request_method&quot;, &quot;http_referrer&quot;: &quot;$http_referer&quot;, &quot;http_user_agent&quot;: &quot;$http_user_agent&quot;</span></span><br><span class="line"><span class="string">    &#125;&#x27;</span></span><br><span class="line">  <span class="attr">proxy-body-size:</span> <span class="string">10m</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-configuration</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure>

<p>其它的一些常用配置可参考以下:</p>
<p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">annotations列表</a></p>
<p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/">configmap列表</a></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>controller启动时提示以下提示:</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428180840.png"></p>
<p>解决:</p>
<p>nginx-ingress-controller的daemonset中的启动命令中指定了ingress-class, 所以需要相对应</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428205558.png"></p>
<p>所以最好不要指定ingress-class, 不然以后想增加的时候都会需要与这个对应</p>
<ol start="2">
<li>关于<code>--annotations-prefix=nginx.ingress.kubernetes.io</code></li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428210203.png"></p>
<p>在编写ingress规则时经常需要定义一些annotations, annotations-prefix支持使用自定义前缀， 默认值为<code>nginx.ingress.kubernetes.io</code>,一般情况下不建议修改, 官方支持的annotations列表可<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">参考</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/user-guide">https://kubernetes.github.io/ingress-nginx/user-guide</a></li>
<li><a href="https://www.jianshu.com/p/1ef0bb822fcc">https://www.jianshu.com/p/1ef0bb822fcc</a></li>
<li><a href="https://www.servicemesher.com/blog/kubernetes-ingress-controller-deployment-and-ha/">https://www.servicemesher.com/blog/kubernetes-ingress-controller-deployment-and-ha/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kubeadm源码修改证书有效期)</title>
    <url>/2020/04/22/Kubernetes-kubeadm/</url>
    <content><![CDATA[<p>相信很多同学在搭建kubernetes时都会选择使用工具进行安装, 现在开源的工具真是层出不穷，本人也体验过多种部署工具, 如rke、kubespray、kubeadm、minikube甚至手工安装等, kubeadm是本人第一个尝试的工具, 因此也使用的最多, kubeadm默认的证书是1年到期, client能够到期后自动进行续签,但是server端却不能, 如果在到期时没有及时进行证书的延期, 则会造成整个集群异常,今天主要记录下kubeadm通过修改源码来改变集群证书的相关操作</p>
<span id="more"></span>

<h3 id="修改kubeadm源码"><a href="#修改kubeadm源码" class="headerlink" title="修改kubeadm源码"></a>修改kubeadm源码</h3><h4 id="切换分支"><a href="#切换分支" class="headerlink" title="切换分支"></a>切换分支</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/kubernetes/kubernetes.git</span><br><span class="line"><span class="built_in">cd</span> kubernetes</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428155835.png"></p>
<h4 id="修改kubeadm源码-1"><a href="#修改kubeadm源码-1" class="headerlink" title="修改kubeadm源码"></a>修改kubeadm源码</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// kubernetes/cmd/kubeadm/app/constants/constants.go</span></span><br><span class="line"><span class="comment">// 修改以下常量</span></span><br><span class="line">CertificateValidity = time.Hour * <span class="number">24</span> * <span class="number">365</span></span><br><span class="line"><span class="comment">// 变成10年</span></span><br><span class="line"><span class="comment">// CertificateValidity = time.Hour * 24 * 3650</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CertificateValidity常量在kubernetes/cmd/kubeadm/app/util/pkiutil/pki_helpers.go中引用</span></span><br><span class="line"><span class="comment">// 当然， 在这里修改也是可以的</span></span><br><span class="line">NotAfter:     time.Now().Add(kubeadmconstants.CertificateValidity).UTC(),</span><br><span class="line"><span class="comment">// NotAfter:     time.Now().Add(kubeadmconstants.CertificateValidity * 100.UTC(),</span></span><br></pre></td></tr></table></figure>

<p>这两个地方都可以修改, 只要最后这两个值能跟期望值对应上即可，修改完之后对kubeadm进行编译</p>
<h4 id="二进制编译"><a href="#二进制编译" class="headerlink" title="二进制编译"></a>二进制编译</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubeadm GOFLAGS=-v GOGCFLAGS=<span class="string">&quot;-N -l&quot;</span></span><br></pre></td></tr></table></figure>

<p>最终会在kubernetes&#x2F;_output生成kubeadm, 如果大家不想麻烦的话, 这里有个现成的版本对应v1.15, 证书有效期100年.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://rj-bai.oss-cn-beijing.aliyuncs.com/kubeadm -O /usr/bin/kubeadm &amp;&amp; <span class="built_in">chmod</span> +x /usr/bin/kubeadm</span><br></pre></td></tr></table></figure>

<p>或者进入cmd&#x2F;kubeadm<br>执行go build -v命令,如果没出错,会生成可执行文件kubeadm</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">go build -v</span><br></pre></td></tr></table></figure>

<p>生成的可执行文件在当前文件夹下面</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> cmd/kubeadm/</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428163928.png"></p>
<h4 id="容器编译"><a href="#容器编译" class="headerlink" title="容器编译"></a>容器编译</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用官方的kube-cross镜像进行编译.</span></span><br><span class="line">docker pull us.gcr.io/k8s-artifacts-prod/build-image/kube-cross:v1.12.17-1</span><br><span class="line">build/run.sh make kubeadm</span><br><span class="line"><span class="comment"># 经过几分钟后，最终会在以下目录生成kubeadm</span></span><br><span class="line">_output/dockerized/bin/linux/amd64</span><br><span class="line"><span class="comment"># 可以查看下kubeadm版本</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428161521.png"></p>
<h3 id="搭建新集群"><a href="#搭建新集群" class="headerlink" title="搭建新集群"></a>搭建新集群</h3><p>如果是新集群的话就比较容器了, 直接使用新的kubeadm进行部署, 那么部署出来的集群证书就是100年.p</p>
<p>kubeadm如何进行集群部署, 这里不再详细记录了.这里只写主要步骤</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt install -y kubelet=1.15.1-00 kubectl=1.15.1-00 kubeadm=1.15.1-00 --allow-change-held-packages</span><br><span class="line"></span><br><span class="line">kubeadm init --config=kubeadm-config.yaml --upload-certs</span><br><span class="line"></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml</span><br><span class="line"></span><br><span class="line">kubeadm init --config=kubeadm-config.yaml --upload-certs</span><br></pre></td></tr></table></figure>

<p>将新版的kubeadm替换默认安装的kubeadm</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mv</span> /usr/bin/kubeadm /usr/bin/kubeadm_bak</span><br><span class="line"><span class="built_in">cp</span> _output/dockerized/bin/linux/amd64/kubeadm /usr/bin/kubeadm</span><br><span class="line"><span class="built_in">chmod</span> +x /usr/bin/kubeadm</span><br></pre></td></tr></table></figure>

<p>kubeadm执行完了按照提示执行命令</p>
<p>最终部署起来的集群证书有效期为100年</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428163448.png"></p>
<h3 id="对现有集群进行证书更新"><a href="#对现有集群进行证书更新" class="headerlink" title="对现有集群进行证书更新"></a>对现有集群进行证书更新</h3><p>那如果是对存在的集群进行证书重签，如何操作呢?</p>
<p>首先, 查看当前集群的证书有效期，这里使用的kubeadm是未修改源码的kubeadm</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm alpha certs check-expiration</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428122925.png"></p>
<p>在没有使用kubeadm搭建的情况下,可以使用以下命令查看证书过期时间</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -<span class="keyword">in</span> front-proxy-client.crt   -noout -text  |grep Not</span><br></pre></td></tr></table></figure>

<p>然后使用证书100年的kubeadm执行以下操作即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用100年的kubeadm更新证书</span></span><br><span class="line">./kubeadm alpha certs renew all</span><br><span class="line"><span class="comment"># 再次查看证书有效期， 对比前后会发现证书已经从1年变成了100年了.</span></span><br><span class="line">./kubeadm alpha certs check-expiration</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200428161640.png"></p>
<p>至此, 不管是搭建集群之前操作证书有效期还是对存在的集群延长证书有效期进行操作都不怕了,完美.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/">https://github.com/kubernetes/kubernetes/</a></li>
<li><a href="https://www.kubernetes.org.cn/5777.html">https://www.kubernetes.org.cn/5777.html</a></li>
<li><a href="https://blog.rj-bai.com/post/160.html">https://blog.rj-bai.com/post/160.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(核心模块)</title>
    <url>/2021/03/27/Kubernetes-kubebatch-action-plugins/</url>
    <content><![CDATA[<p>接上篇，主要介绍了kube-batch中两个重要的对象，queue及podgroup，这次主要讲讲kube-batch的核心的几个模块</p>
<span id="more"></span>

<h3 id="默认配置"><a href="#默认配置" class="headerlink" title="默认配置"></a>默认配置</h3><p>以下是kube-batch默认的配置，如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">actions:</span> <span class="string">&quot;allocate, backfill&quot;</span></span><br><span class="line"><span class="attr">tiers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">priority</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gang</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">drf</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">predicates</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proportion</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nodeorder</span></span><br></pre></td></tr></table></figure>

<p>可以看到分为actions及tiers，其中actions指定了kube-batch在调度时需要执行的操作，同时，这些操作会关联一些plugins(简单来说就是一些算法)来实现相关功能，比如说，actions是allocate，allocate意为分配资源，但是在分配资源时有时也会有一些要求，比如优先级高的先分配，或者把某个任务当成一个整体进行分配(gang)等等，同时，不同的actions可能关联同一个plugins，比如对于资源回收时，也可能存在先回收优先级低的pod的资源，这就是actions及plugins之间的关系</p>
<p>kube-batch支持多种的actions及plugins,这个则需要根据实际的情况按需选择</p>
<p>列表如下:</p>
<ul>
<li>actions<ul>
<li>allocate</li>
<li>backfill</li>
<li>preempt</li>
<li>reclaim</li>
</ul>
</li>
<li>plugins<ul>
<li>conformance</li>
<li>drf</li>
<li>gang</li>
<li>nodeorder</li>
<li>predicates</li>
<li>priority</li>
<li>proportion</li>
</ul>
</li>
</ul>
<p>要注意的是，配置文件中actions及plugins的指定顺序非常重要，<strong>kube-batch不会考虑action及plugins列表之间的顺序是不是合理，它只会按照指定的顺序依次执行</strong>，因此，要根据实际情况谨慎、实操地对配置进行优化，最后才能达到合理效果</p>
<p>tiers下的plugins是列表结构，对于actions来说，如果遇到了优先级更高的plugins，则不会再进行应用低优先级的plugins，比如上面的配置中，如果一个优先级高的跟一个优先级低的任务同时到来，则优先级高的获取到它需要的所有资源，而不会再考虑其它的plugins，如果两者优先级一样且都是以podgroup存在，则就会按drf plugins进行处理.</p>
<p>这张图很形象地说明kube-batch的工作原因</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210324234022.png"></p>
<p>它通过 list-watch 监听 Pod, Queue, PodGroup 和 Node 等资源，在本地维护一份集群资源的全局缓存，依次通过如下的策略（reclaim, allocate, preemption，predict） 完成资源的调度</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210328221253.png"></p>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>Cache模块封装了对API Server的节点、容器等对象的数据同步逻辑。Kubernetes的数据保存在分布式存储etcd中，所有对数据的查询和操作都通过调用API Server的接口，而非直接操作etcd。在调度时，需要集群中的节点和容器的使用资源和状态等信息。Cache模块通过调用Kubernetes的SDK，通过watch机制监听集群中的节点、容器的状态变化，将信息同步到自己的数据结构中。</p>
<p>Cache模块还封装了对API server的接口的调用。比如Cache.Bind这个接口，会去调用API Server的Bind接口，将容器绑定到指定的节点上。在kube-batch中只有cache模块需要和API Server交互，其他模块只需要调用Cache模块的接口,一方面减少api-server的压力，另一方面加快kube-batch本身的调度性能</p>
<h3 id="session"><a href="#session" class="headerlink" title="session"></a>session</h3><p>Session模块是将其他三个模块串联起来的一个模块。Kube-batch在每个调度周期开始时，都会新建一个Session对象，这个Session的初始化时，会做以下操作：</p>
<ul>
<li>调用Cache.Snapshot接口，将Cache中节点、任务和队列的信息拷贝一份副本，之后在这个调度周期中使用这份副本进行调度。因为Cache的数据会不断变化，为了保持同个调度周期中的数据一致性，在一开始就拷贝了一份副本。</li>
<li>将配置中的各个Plugin初始化，然后调用plugin的OnSessionOpen接口。Plugin在OnSessionOpen中，会初始化自己需要的数据，并将一些回调函数注册到session中。Plugin可以向Session中注册的函数是：<ol>
<li>jobOrderFns： 决定哪个训练任务优先被处理（调度、回收、抢占）</li>
<li>queueOrderFns：决定哪个训练队列优先被处理</li>
<li>taskOrderFns：决定任务中哪个容器优先被处理</li>
<li>predicateFns： 判断某个节点是否满足容器的基本调度要求。比如容器中指定的节点的标签</li>
<li>nodeOrderFns： 当多个节点满足容器的调度要求时，优先选择哪个节点</li>
<li>preemptableFns： 决定某个容器是否可以被抢占</li>
<li>reclaimableFns ：决定某个容器是否可以被回收</li>
<li>overusedFns： 决定某个队列使用的资源是否超过限额，是的话不再调度对队列中的任务</li>
<li>jobReadyFns：判断某个任务是否已经准备好，可以调用API Server的接口将任务的容器调度到节点</li>
<li>jobPipelinedFns ： 判断某个任务是否处于Pipelined状态</li>
<li>jobValidFns： 判断某个任务是否有效</li>
</ol>
</li>
</ul>
<p>注意Plugin不需要注册上面所有的函数，而是可以根据自己的需要，注册某几个函数。比如Predict plugin就只注册了predicateFns这个函数到Session中。</p>
<p>初始化成功后，Kube-batch会依次调用不同的Action的Execute方法，并将Session对象作为参数传入。在Execute中，会调用Session的各种方法。这些方法，有些最终会调用到Cache的方法， 有些是调用Plugin注册的方法。</p>
<h3 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h3><p>因为业务中只使用到了默认的action，因此就简单说下allocate跟backfill这两个，其它的如reclaim跟preempt可参考<a href="http://dockone.io/article/9021">这里</a></p>
<h4 id="allocate"><a href="#allocate" class="headerlink" title="allocate"></a>allocate</h4><p>allocate用于将有明确资源需求的pod（task）分配到某个节点，在k8s对象中，pod或者container有没有指定request及limit，对应不同的Qos，Qos表示为服务质量，在这里不展开说明，感兴趣的可参考<a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">这里</a>,对于明确指定了requests跟limit且相等的这类pod，对应的Qos为guaranteed，同等情况下会被优先保障资源。</p>
<p>如果不额外配置，allocate在kube-batch是默认的选项</p>
<p>那如果有pod没有指定资源怎么办呢？</p>
<h4 id="backfill"><a href="#backfill" class="headerlink" title="backfill"></a>backfill</h4><p>这个时候就需要靠backfill(回填)这个action来处理了,这个模块主要实现当podgroup中有pod没有被绑定成功时，其它的绑定的pod的资源会释放,在podgroup中非常关键.</p>
<h3 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h3><h4 id="gang"><a href="#gang" class="headerlink" title="gang"></a>gang</h4><p>gang翻译成中文是帮派的意思，电影中的帮派大多都是共进退的，因为gang插件实现的就是帮派调度，gang-scheduler是podgroup中的核心，实现All or Nothing的效果</p>
<p>那么Kube-batch如何Gang-Scheduler，逻辑如下:</p>
<blockquote>
<ul>
<li>增加一个PodGruop的CRD。调度以PodGroup为单位。同时对应还有一个PodGroupController进行PodGroup的管理</li>
<li>整个调度过程采用延迟创建Pod的过程。只有当PodGroup中的所有Pod都有合适的Node绑定时，才开始创建</li>
<li>c. 定义了一种新的Action-BackFill.当PodGroup还有Pod没绑定时，之前绑定Pod的资源会释放</li>
</ul>
</blockquote>
<p>gang注册3个function，分别是：</p>
<blockquote>
<ul>
<li><code>preemptableFn</code> 为避免gang的策略被preempt和reclaiｍ干扰，定义了preemptableFn,排除那些还未准备就绪的job，避免其被抢占。（虽然实际上这些job未真正调度到node上去，但是确实从逻辑上把资源分配给它了）</li>
<li><code>jobOrderFn</code> 为让已经就绪的job尽快被调度到节点，定义了jobOrderFn ,让已经就绪的job拥有更高的优先级</li>
<li><code>jobReadyFn</code> 用来判断一个job是否已经就绪。<code>jobReady</code>会调用所有注册了的 plugin的Ready判定函数，只有都判定为ready ,才返回true</li>
</ul>
</blockquote>
<h4 id="DRF"><a href="#DRF" class="headerlink" title="DRF"></a>DRF</h4><p>实现了Dominant Resouce Fairenss算法，这个算法能够有效对多种资源（CPU、Memory、GPU）进行调度</p>
<p>Drf目的是尽量避免集群内某一类资源使用比例偏高，而其他类型资源使用比例却很低的不良状态。在调度时，让具有最低资源占用比例的任务具有高优先级,主要关注<code>onSessionOpen</code>函数：</p>
<blockquote>
<ul>
<li>统计集群中所有node可分配资源总量</li>
<li>统计Job资源申请，计算资源占比（资源申请&#x2F;资源总量）</li>
<li>注册Job排序函数，根据资源占比进行排序，主要资源占比越低job优先级越高</li>
<li>注册事件处理函数，包括分配函数以及驱逐函数，函数实现比较简单，就是当task发生变化时，增加（分配）&#x2F;减少（驱逐）Job资源申请总量，并且更新资源占比。</li>
</ul>
</blockquote>
<p>　drf注册2个function，分别是:</p>
<blockquote>
<ul>
<li><code>jobOrderFn</code> 是job的排序函数，会让share值越小的job排在最前面，即拥有最高的优先级，这个是实现drf的关键。</li>
<li><code>preemptableFn</code>返回可抢占的job列表，job的筛选规则是：如果待选job的share值大于将被调度的job的share值，则选中该待选job</li>
</ul>
</blockquote>
<p>kub-batch的session跟cache机制是固定的，主要通过action跟plugins机制来实现多样化调度算法.</p>
<p>虽然目前业务上运行的很ok，但根据本人的实践情况来看，不太建议生产使用，原因如下:</p>
<p>本人因为历史原因，接入kube-batch是目前最好的选择，由于kube-batch项目基本已不再维护，其中的有些bug已经有人提issue，但并没有在kube-batch这个项目中fix，而是直接在volcano的fix列表中了，所以如果需要fix,还需要从volcano中cherry-pick过来，但是虽说volcano是基于kube-batch而来，但volcano的版本迭代非常快，真的很难判断cherry-pick过来之后会不会有其它问题，如果有问题再来看volcano，如没有特殊原因还不如直接从一开始就使用上volcano，多快好省.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
<li><a href="https://www.jianshu.com/p/042692685cf4">https://www.jianshu.com/p/042692685cf4</a></li>
<li><a href="http://dockone.io/article/9021">http://dockone.io/article/9021</a></li>
<li><a href="http://www.ririshihaori.com/posts/2020/1/kube-batch/">http://www.ririshihaori.com/posts/2020/1/kube-batch/</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(核心模块)</title>
    <url>/2021/03/27/Kubernetes-kubebatch-actions-plugins/</url>
    <content><![CDATA[<p>接上篇，主要介绍了kube-batch中两个重要的对象，queue及podgroup，这次主要讲讲kube-batch的核心的几个模块</p>
<span id="more"></span>

<h3 id="默认配置"><a href="#默认配置" class="headerlink" title="默认配置"></a>默认配置</h3><p>以下是kube-batch默认的配置，如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">actions:</span> <span class="string">&quot;allocate, backfill&quot;</span></span><br><span class="line"><span class="attr">tiers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">priority</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gang</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">drf</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">predicates</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proportion</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nodeorder</span></span><br></pre></td></tr></table></figure>

<p>可以看到分为actions及tiers，其中actions指定了kube-batch在调度时需要执行的操作，同时，这些操作会关联一些plugins(简单来说就是一些算法)来实现相关功能，比如说，actions是allocate，allocate意为分配资源，但是在分配资源时有时也会有一些要求，比如优先级高的先分配，或者把某个任务当成一个整体进行分配(gang)等等，同时，不同的actions可能关联同一个plugins，比如对于资源回收时，也可能存在先回收优先级低的pod的资源，这就是actions及plugins之间的关系</p>
<p>kube-batch支持多种的actions及plugins,这个则需要根据实际的情况按需选择</p>
<p>列表如下:</p>
<ul>
<li>actions<ul>
<li>allocate</li>
<li>backfill</li>
<li>preempt</li>
<li>reclaim</li>
</ul>
</li>
<li>plugins<ul>
<li>conformance</li>
<li>drf</li>
<li>gang</li>
<li>nodeorder</li>
<li>predicates</li>
<li>priority</li>
<li>proportion</li>
</ul>
</li>
</ul>
<p>要注意的是，配置文件中actions及plugins的指定顺序非常重要，<strong>kube-batch不会考虑action及plugins列表之间的顺序是不是合理，它只会按照指定的顺序依次执行</strong>，因此，要根据实际情况谨慎、实操地对配置进行优化，最后才能达到合理效果</p>
<p>tiers下的plugins是列表结构，对于actions来说，如果遇到了优先级更高的plugins，则不会再进行应用低优先级的plugins，比如上面的配置中，如果一个优先级高的跟一个优先级低的任务同时到来，则优先级高的获取到它需要的所有资源，而不会再考虑其它的plugins，如果两者优先级一样且都是以podgroup存在，则就会按drf plugins进行处理.</p>
<p>这张图很形象地说明kube-batch的工作原因</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210324234022.png"></p>
<p>它通过 list-watch 监听 Pod, Queue, PodGroup 和 Node 等资源，在本地维护一份集群资源的全局缓存，依次通过如下的策略（reclaim, allocate, preemption，predict） 完成资源的调度</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210328221253.png"></p>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>Cache模块封装了对API Server的节点、容器等对象的数据同步逻辑。Kubernetes的数据保存在分布式存储etcd中，所有对数据的查询和操作都通过调用API Server的接口，而非直接操作etcd。在调度时，需要集群中的节点和容器的使用资源和状态等信息。Cache模块通过调用Kubernetes的SDK，通过watch机制监听集群中的节点、容器的状态变化，将信息同步到自己的数据结构中。</p>
<p>Cache模块还封装了对API server的接口的调用。比如Cache.Bind这个接口，会去调用API Server的Bind接口，将容器绑定到指定的节点上。在kube-batch中只有cache模块需要和API Server交互，其他模块只需要调用Cache模块的接口,一方面减少api-server的压力，另一方面加快kube-batch本身的调度性能</p>
<h3 id="session"><a href="#session" class="headerlink" title="session"></a>session</h3><p>Session模块是将其他三个模块串联起来的一个模块。Kube-batch在每个调度周期开始时，都会新建一个Session对象，这个Session的初始化时，会做以下操作：</p>
<ul>
<li>调用Cache.Snapshot接口，将Cache中节点、任务和队列的信息拷贝一份副本，之后在这个调度周期中使用这份副本进行调度。因为Cache的数据会不断变化，为了保持同个调度周期中的数据一致性，在一开始就拷贝了一份副本。</li>
<li>将配置中的各个Plugin初始化，然后调用plugin的OnSessionOpen接口。Plugin在OnSessionOpen中，会初始化自己需要的数据，并将一些回调函数注册到session中。Plugin可以向Session中注册的函数是：<ol>
<li>jobOrderFns： 决定哪个训练任务优先被处理（调度、回收、抢占）</li>
<li>queueOrderFns：决定哪个训练队列优先被处理</li>
<li>taskOrderFns：决定任务中哪个容器优先被处理</li>
<li>predicateFns： 判断某个节点是否满足容器的基本调度要求。比如容器中指定的节点的标签</li>
<li>nodeOrderFns： 当多个节点满足容器的调度要求时，优先选择哪个节点</li>
<li>preemptableFns： 决定某个容器是否可以被抢占</li>
<li>reclaimableFns ：决定某个容器是否可以被回收</li>
<li>overusedFns： 决定某个队列使用的资源是否超过限额，是的话不再调度对队列中的任务</li>
<li>jobReadyFns：判断某个任务是否已经准备好，可以调用API Server的接口将任务的容器调度到节点</li>
<li>jobPipelinedFns ： 判断某个任务是否处于Pipelined状态</li>
<li>jobValidFns： 判断某个任务是否有效</li>
</ol>
</li>
</ul>
<p>注意Plugin不需要注册上面所有的函数，而是可以根据自己的需要，注册某几个函数。比如Predict plugin就只注册了predicateFns这个函数到Session中。</p>
<p>初始化成功后，Kube-batch会依次调用不同的Action的Execute方法，并将Session对象作为参数传入。在Execute中，会调用Session的各种方法。这些方法，有些最终会调用到Cache的方法， 有些是调用Plugin注册的方法。</p>
<h3 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h3><p>因为业务中只使用到了默认的action，因此就简单说下allocate跟backfill这两个，其它的如reclaim跟preempt可参考<a href="http://dockone.io/article/9021">这里</a></p>
<h4 id="allocate"><a href="#allocate" class="headerlink" title="allocate"></a>allocate</h4><p>allocate用于将有明确资源需求的pod（task）分配到某个节点，在k8s对象中，pod或者container有没有指定request及limit，对应不同的Qos，Qos表示为服务质量，在这里不展开说明，感兴趣的可参考<a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">这里</a>,对于明确指定了requests跟limit且相等的这类pod，对应的Qos为guaranteed，同等情况下会被优先保障资源。</p>
<p>如果不额外配置，allocate在kube-batch是默认的选项</p>
<p>那如果有pod没有指定资源怎么办呢？</p>
<h4 id="backfill"><a href="#backfill" class="headerlink" title="backfill"></a>backfill</h4><p>这个时候就需要靠backfill(回填)这个action来处理了,这个模块主要实现当podgroup中有pod没有被绑定成功时，其它的绑定的pod的资源会释放,在podgroup中非常关键.</p>
<h3 id="Plugins"><a href="#Plugins" class="headerlink" title="Plugins"></a>Plugins</h3><h4 id="gang"><a href="#gang" class="headerlink" title="gang"></a>gang</h4><p>gang翻译成中文是帮派的意思，电影中的帮派大多都是共进退的，因为gang插件实现的就是帮派调度，gang-scheduler是podgroup中的核心，实现All or Nothing的效果</p>
<p>那么Kube-batch如何Gang-Scheduler，逻辑如下:</p>
<blockquote>
<ul>
<li>增加一个PodGruop的CRD。调度以PodGroup为单位。同时对应还有一个PodGroupController进行PodGroup的管理</li>
<li>整个调度过程采用延迟创建Pod的过程。只有当PodGroup中的所有Pod都有合适的Node绑定时，才开始创建</li>
<li>c. 定义了一种新的Action-BackFill.当PodGroup还有Pod没绑定时，之前绑定Pod的资源会释放</li>
</ul>
</blockquote>
<p>gang注册3个function，分别是：</p>
<blockquote>
<ul>
<li><code>preemptableFn</code> 为避免gang的策略被preempt和reclaiｍ干扰，定义了preemptableFn,排除那些还未准备就绪的job，避免其被抢占。（虽然实际上这些job未真正调度到node上去，但是确实从逻辑上把资源分配给它了）</li>
<li><code>jobOrderFn</code> 为让已经就绪的job尽快被调度到节点，定义了jobOrderFn ,让已经就绪的job拥有更高的优先级</li>
<li><code>jobReadyFn</code> 用来判断一个job是否已经就绪。<code>jobReady</code>会调用所有注册了的 plugin的Ready判定函数，只有都判定为ready ,才返回true</li>
</ul>
</blockquote>
<h4 id="DRF"><a href="#DRF" class="headerlink" title="DRF"></a>DRF</h4><p>实现了Dominant Resouce Fairenss算法，这个算法能够有效对多种资源（CPU、Memory、GPU）进行调度</p>
<p>Drf目的是尽量避免集群内某一类资源使用比例偏高，而其他类型资源使用比例却很低的不良状态。在调度时，让具有最低资源占用比例的任务具有高优先级,主要关注<code>onSessionOpen</code>函数：</p>
<blockquote>
<ul>
<li>统计集群中所有node可分配资源总量</li>
<li>统计Job资源申请，计算资源占比（资源申请&#x2F;资源总量）</li>
<li>注册Job排序函数，根据资源占比进行排序，主要资源占比越低job优先级越高</li>
<li>注册事件处理函数，包括分配函数以及驱逐函数，函数实现比较简单，就是当task发生变化时，增加（分配）&#x2F;减少（驱逐）Job资源申请总量，并且更新资源占比。</li>
</ul>
</blockquote>
<p>　drf注册2个function，分别是:</p>
<blockquote>
<ul>
<li><code>jobOrderFn</code> 是job的排序函数，会让share值越小的job排在最前面，即拥有最高的优先级，这个是实现drf的关键。</li>
<li><code>preemptableFn</code>返回可抢占的job列表，job的筛选规则是：如果待选job的share值大于将被调度的job的share值，则选中该待选job</li>
</ul>
</blockquote>
<p>kub-batch的session跟cache机制是固定的，主要通过action跟plugins机制来实现多样化调度算法.</p>
<p>虽然目前业务上运行的很ok，但根据本人的实践情况来看，不太建议生产使用，原因如下:</p>
<p>本人因为历史原因，接入kube-batch是目前最好的选择，由于kube-batch项目基本已不再维护，其中的有些bug已经有人提issue，但并没有在kube-batch这个项目中fix，而是直接在volcano的fix列表中了，所以如果需要fix,还需要从volcano中cherry-pick过来，但是虽说volcano是基于kube-batch而来，但volcano的版本迭代非常快，真的很难判断cherry-pick过来之后会不会有其它问题，如果有问题再来看volcano，如没有特殊原因还不如直接从一开始就使用上volcano，多快好省.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
<li><a href="https://www.jianshu.com/p/042692685cf4">https://www.jianshu.com/p/042692685cf4</a></li>
<li><a href="http://dockone.io/article/9021">http://dockone.io/article/9021</a></li>
<li><a href="http://www.ririshihaori.com/posts/2020/1/kube-batch/">http://www.ririshihaori.com/posts/2020/1/kube-batch/</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(安装部署)</title>
    <url>/2021/03/17/Kubernetes-kubebatch-install/</url>
    <content><![CDATA[<p>上一篇中简单介绍了下kube-batch，这篇来简单说一下kube-batch的安装及配置，虽然官方github写的非常清楚，一来做个汇总，二来有些地方需要特别说明一下</p>
<span id="more"></span>

<p>因为kubernetes很早就已支持在一个集群中存在多个调度器，因此可以直接将新调度器安装在集群中</p>
<p>这里以kube-batch的release-0.5版本为准，这也是官方目前为止最新的release tag.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b release-0.5 https://github.com/kubernetes-sigs/kube-batch</span><br></pre></td></tr></table></figure>

<p>官网的文档中使用的是helm的安装方法，不过helm由于verion2跟version3有一些差别，因此不一定对所有场景都可成功，作者最开始的时候就遇到了安装不成功的情况，因为helm的安装文件中，使用了<code>&quot;helm.sh/hook&quot;: &quot;crd-install&quot;</code>,这个annotations对helm的版本有要求，所以如果安装不成功，可以直接使用yaml文件进行安装，步骤如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先安装crds</span></span><br><span class="line"><span class="built_in">cd</span> config/crds</span><br><span class="line">kubectl apply -f . -n kube-system</span><br><span class="line"><span class="comment"># 安装默认的quue</span></span><br><span class="line"><span class="built_in">cd</span> config/queue</span><br><span class="line">kubectl apply -f . -n kube-system</span><br></pre></td></tr></table></figure>

<p>这里要注意的,由于是调度器，会监听很多资源的创建情况，权限自然也需要大一点，官方也给了个权限文件，不过很简单粗暴，直接是集群admin的权限，如果怕有安全问题的话，这里就需要对权限进行收敛, 这个就因人而异，这里还是以官方的权限来说明</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-sa-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-admin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<p>这里是直接将kube-system ns中的default sa直接给了cluster-admin的权限，这样，所有在kube-system ns下的pod，如果没有额外指定sa的话，则都是集群管理员权限</p>
<p><code>kubectl apply -f . -n kube-system</code></p>
<p>然后是配置文件，官方也给了默认的配置文件，对于插件的使用，这个要看实际的情况，这里还是以官方的默认配置来说明</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">actions:</span> <span class="string">&quot;allocate, backfill&quot;</span></span><br><span class="line"><span class="attr">tiers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">priority</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gang</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">drf</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">predicates</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proportion</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nodeorder</span></span><br></pre></td></tr></table></figure>

<p>这个配置文件的信息量有点大，具体含义就不在这里展开，后篇再详说，</p>
<p>上面的配置是默认配置，如果没有指定配置文件的话，则默认使用上面的配置，当然kube-batch也支持指定配置文件</p>
<p>只需要在启动参数中指定配置文件的yaml即可,如下</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">kube-batch-conf.yaml:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    actions: &quot;allocate, backfill&quot;</span></span><br><span class="line"><span class="string">    tiers:</span></span><br><span class="line"><span class="string">    - plugins:</span></span><br><span class="line"><span class="string">      - name: gang</span></span><br><span class="line"><span class="string">      - name: priority</span></span><br><span class="line"><span class="string">    - plugins:</span></span><br><span class="line"><span class="string">      - name: drf</span></span><br><span class="line"><span class="string">      - name: predicates</span></span><br><span class="line"><span class="string">      - name: proportion</span></span><br><span class="line"><span class="string">      - name: nodeorder</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-batch-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure>

<p>最后启动deployment即可</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">progressDeadlineSeconds:</span> <span class="number">600</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxSurge:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--logtostderr</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--v</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--scheduler-conf=/etc/kube-batch/kube-batch-conf.yaml</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">kube-batch/kube-batch:v0.5</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">kube-batch</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">capabilities:</span> &#123;&#125;</span><br><span class="line">        <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">        <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/kube-batch/kube-batch-conf.yaml</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">config</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">kube-batch-conf.yaml</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">      <span class="attr">securityContext:</span> &#123;&#125;</span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">defaultMode:</span> <span class="number">256</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-batch-config</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">config</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br></pre></td></tr></table></figure>

<p>Kube-batch自带有一些prometheus的监控metrics，主要是调度时延，调度成功、调度失败数等</p>
<p><code>servicemonitor</code>如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceMonitor</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-batch</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">monitoring</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">endpoints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">honorLabels:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">interval:</span> <span class="string">30s</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">jobLabel:</span> <span class="string">app</span></span><br><span class="line">  <span class="attr">namespaceSelector:</span></span><br><span class="line">    <span class="attr">matchNames:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">kube-batch</span></span><br></pre></td></tr></table></figure>

<p>最后在kube-system ns下看到kube-batch成功运行，安装OK.</p>
<p>流行的tf也原生支持kube-batch.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(queue及podgroup)</title>
    <url>/2021/03/21/Kubernetes-kubebatch-theory/</url>
    <content><![CDATA[<p>上篇简单说了下在集群中安装kube-batch且成功运行起来之后， 现在开始对kube-batch中的一些概念进行阐明, 先从podgroup及queue说起</p>
<span id="more"></span>



<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><p>queue在kube-batch中属于集群级别的资源，配置如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.incubator.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Queue</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">weight:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>默认，如果集群中不存在queue,kube-batch会默认创建一个如上的queue，这个默认的queue主要用来存放没有绑定queue的job(这个job不是kubernetes中job资源，而是特指使用了kube-batch的对象，有可能是kubernetes job，也可能是pod等)</p>
<p>前面提到过，<strong>kube-batch对多租户的支持、多租户间的资源分配就是通过queue来实现的</strong></p>
<p>queue本身就是一种排除机制，kube-batch也是如此，对于资源的消耗，如果资源得不到满足，同样会进行排除处理.</p>
<p>这里举个最简单的例子:</p>
<p>假如一个k8s集群中存在的资源有，<code>cpu: 12c, mem: 120Gi, GPU: 4c</code></p>
<p>假如现在创建了2个queue</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.incubator.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Queue</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.incubator.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Queue</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">weight:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>其中，weight表示权重，那么集群中的所有资源会被分成3等份，所属default-1的queue占1&#x2F;3, 即<code>cpu:4c,mem:40ci,gpu:4c</code>,而所属default-2的queue占2&#x2F;3, 即<code>cpu:8c,mem:80ci,gpu:8c</code></p>
<p>当然，前面也提过，这个不是绝对的，因为kube-batch存在插件机制，租户之间可以指定优先级等插件对其它租户的资源进行抢占，使资源的资源更加偏向重要的任务</p>
<p>如果集群中没有多租户的概念，则可以不进行资源的划分，所有的任务都属于一个queue.</p>
<h3 id="PodGroup"><a href="#PodGroup" class="headerlink" title="PodGroup"></a>PodGroup</h3><p>podgroup在kube-batch是个很重要的资源对象，kube-batch会监听集群中所有podgroup，然后计算(由插件决定)所属这个group中的pod所需要的资源，如果集群中没有这些资源，则所有的pod会处理pendding状态，在queue中进行排队，如果存在，则进行调度</p>
<p>使用方法来看官方一个简单的例子:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.incubator.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodGroup</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">qj-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minMember:</span> <span class="number">6</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">qj-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">parallelism:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">scheduling.k8s.io/group-name:</span> <span class="string">qj-1</span>  <span class="comment"># 指定所属podgroup</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span>  <span class="comment"># 指明需要的资源</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">kube-batch</span>   <span class="comment"># 指定使用kube-batch做为调度器</span></span><br></pre></td></tr></table></figure>

<p>The yaml file means a Job named <code>qj-01</code> to create 6 pods(it is specified by <code>parallelism</code>), these pods will be scheduled by scheduler <code>kube-batch</code> (it is specified by <code>schedulerName</code>). <code>kube-batch</code> will watch <code>PodGroup</code>, and the annotation <code>scheduling.k8s.io/group-name</code> identify which group the pod belongs to. <code>kube-batch</code> will start <code>.spec.minMember</code> pods for a Job at the same time; otherwise, such as resources are not sufficient, <code>kube-batch</code> will not start any pods for the Job.</p>
<p>上面的yaml文件生成了个podgroup对象及一个kubernetes中的job对象</p>
<p>其中:</p>
<blockquote>
<ul>
<li>podgroup中的minMember表明至少需要满足6个pods的资源时才会进行调度，pods则是通过annotations进行绑定</li>
<li>在job中存在有annotations，通过scheduling.k8s.io&#x2F;group-name绑定的podgroup.</li>
<li>在job中指定了schedulerName为Kube-batch</li>
</ul>
</blockquote>
<p>通过这样的绑定关系就将podgroup与pod进行了关联，当然，podgroup中的minMember只是最小的pod数，如果满足这个条件也只是表明这个podgroup可以调度，但是并不一定调度成功，但是如果不满足最小pod数，则一定不会调度</p>
<p>还是以上面的例子来说明:</p>
<p>假如集群中只有4个CPU, 这时minMember设置为1，那么这个podgroup会进行调度，但是由于CPU只有4个，而job中需要同时启动6个(job的<code>parallelism</code>参数指定 )pod，也就是需要6个cpu，显然有2个无法获取到cpu，也即会有2外pod处于pendding状态，其它4个运行正常</p>
<p>而如果将minMember指定为6，则一个pod都不会调度，整个job会处理pendding状态.</p>
<p>上面这种最简单的场景在训练任务中非常常见, 往往一个训练任务由N多个pod一起进行，且每个pod都负责其中的一部分，如果不考虑增量训练的话，其中其中任一个pod的资源不能满足的话，则就不应该对其它pod进行调度,这样可以合理地利用资源</p>
<h3 id="shadowPodGroup"><a href="#shadowPodGroup" class="headerlink" title="shadowPodGroup"></a>shadowPodGroup</h3><p>从上面的例子可以看到，每一个任务都需要创建一个podgroup，对于懒人来说，有没有办法直接关联podgroup呢?</p>
<p>答案是肯定的，这就是shadowpodgroup，顾名思义就是podgroup的影子，作用就是不再需要手工地创建podgroup对象</p>
<p>只需要在annotation中指定minMemeber，kube-batch即会自动创建shadowPodgroup对象</p>
<p>因此，上面的例子则直接变成</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">qj-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">parallelism:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">scheduling.k8s.io/group-min-member:</span> <span class="number">6</span>  <span class="comment"># 指定所属podgroup</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">&quot;1&quot;</span>  <span class="comment"># 指明需要的资源</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">kube-batch</span>   <span class="comment"># 指定使用kube-batch做为调度器</span></span><br></pre></td></tr></table></figure>

<p>通过在annotation中指定<code>scheduling.k8s.io/group-min-member</code>,则会由kube-batch自动生成逻辑的podgroup，是不是很方便.</p>
<p>这里要注意的是，podgroup在kubernetes中是属于crd对象，是可以通过kubectl get 命令查询的，而shadowpodgroup则是kube-batch逻辑对象，通过kubectl get 命令无法查询 </p>
<p>以下是shawpodgroup的源码</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createShadowPodGroup</span><span class="params">(pod *v1.Pod)</span></span> *api.PodGroup &#123;</span><br><span class="line">	jobID := api.JobID(utils.GetController(pod))</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(jobID) == <span class="number">0</span> &#123;</span><br><span class="line">		jobID = api.JobID(pod.UID)</span><br><span class="line">	&#125;</span><br><span class="line">  <span class="comment">// Deriving min member for the shadow pod group from pod annotations.</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// Annotation from the newer API has priority over annotation from the old API.</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// By default, if no annotation is provided, min member is 1.</span></span><br><span class="line">	minMember := <span class="number">1</span></span><br><span class="line">	<span class="keyword">if</span> annotationValue, found := pod.Annotations[v1alpha1.GroupMinMemberAnnotationKey]; found &#123;</span><br><span class="line">		<span class="keyword">if</span> integerValue, err := strconv.Atoi(annotationValue); err == <span class="literal">nil</span> &#123;</span><br><span class="line">			minMember = integerValue</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			glog.Errorf(<span class="string">&quot;Pod %s/%s has illegal value %q for annotation %q&quot;</span>,</span><br><span class="line">				pod.Namespace, pod.Name, annotationValue, v1alpha1.GroupMinMemberAnnotationKey)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> annotationValue, found := pod.Annotations[v1alpha2.GroupMinMemberAnnotationKey]; found &#123;</span><br><span class="line">		<span class="keyword">if</span> integerValue, err := strconv.Atoi(annotationValue); err == <span class="literal">nil</span> &#123;</span><br><span class="line">			minMember = integerValue</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			glog.Errorf(<span class="string">&quot;Pod %s/%s has illegal value %q for annotation %q&quot;</span>,</span><br><span class="line">				pod.Namespace, pod.Name, annotationValue, v1alpha2.GroupMinMemberAnnotationKey)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> &amp;api.PodGroup&#123;</span><br><span class="line">		ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">			Namespace: pod.Namespace,</span><br><span class="line">			Name:      <span class="type">string</span>(jobID),</span><br><span class="line">			Annotations: <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;</span><br><span class="line">				shadowPodGroupKey: <span class="type">string</span>(jobID),</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">		Spec: api.PodGroupSpec&#123;</span><br><span class="line">			MinMember: <span class="type">int32</span>(minMember),</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从这段代码也可以看出，最后其实还是生成的podgroup对象.</p>
<p>这篇文章中介绍了queue及podgroup的使用方法，相信大家有一定的了解，但是还没有一些重要的东西未涉及，比如podgroup中的资源是如何计算的，插件又是如何工作的，这些都会在后续进行更新.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(kube-batch踩坑记)</title>
    <url>/2021/03/27/Kubernetes-kubebatch-prombles/</url>
    <content><![CDATA[<p>由于kube-batch经久未修，导致在引入kube-batch到环境中验证期间fix了很多问题，记录于此</p>
<span id="more"></span>

<p>Kube-batch版本为v1.5.0, kube-batch github上能看到的最后一个release版本.</p>
<h3 id="解决集群中如果存在pdb时出现的空指针问题"><a href="#解决集群中如果存在pdb时出现的空指针问题" class="headerlink" title="解决集群中如果存在pdb时出现的空指针问题"></a>解决集群中如果存在pdb时出现的空指针问题</h3><p>在kube-batch的稳定版本中(目前是v1.5.0),如果集群中有pdb，pdb在k8s中是一种安全机制，可<a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/">参考</a></p>
<p>如果podb的属性值有空值时，kube-batch在启动时会出现空指针异常，核心代码:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ji *JobInfo)</span></span> SetPDB(pdb *policyv1.PodDisruptionBudget) &#123;</span><br><span class="line">    ji.Name = pdb.Name</span><br><span class="line">    <span class="comment">//ji.MinAvailable = pdb.Spec.MinAvailable.IntVal  # fix之前的错误代码，以下是fix之后新增的代码</span></span><br><span class="line">  defaultMinAvailable := intstr.IntOrString&#123;Type: intstr.Int, IntVal: <span class="type">int32</span>(<span class="number">0</span>)&#125;</span><br><span class="line">    minAvailable := pdb.Spec.MinAvailable</span><br><span class="line">    ji.MinAvailable = <span class="type">int32</span>(intstr.ValueOrDefault(minAvailable, defaultMinAvailable).IntValue())</span><br><span class="line">    ji.Namespace = pdb.Namespace</span><br><span class="line"></span><br><span class="line">    ji.CreationTimestamp = pdb.GetCreationTimestamp()</span><br><span class="line">    ji.PDB = pdb</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>异常代码在于注释的那行，代码中直接获取min值，但是这个值可能为NaN值，没有做安全处理</p>
<p>fix之后的代码如注释之后的代码</p>
<p>经过本人的验证，这个<a href="https://github.com/volcano-sh/volcano/pull/523">fix</a>依然会有问题，没有考虑到pdb的所有情况，所以在volcano项目中(基于kube-batch衍生而来)，作者直接去除了pdb的支持，将pdb相关的代码直接去掉了</p>
<p><a href="https://github.com/volcano-sh/volcano/pull/523">fix</a></p>
<h3 id="failed-to-delete-pod"><a href="#failed-to-delete-pod" class="headerlink" title="failed to delete pod"></a>failed to delete pod</h3><p>在kube-batch的日志中会出现failed to delete pod之类的错误，这个错误的原因大部分是是由于cache模块在更新pod的调度状态时，首先针将pod删除，然后再添加pod， 但是在删pod时发现task此时已经不再它原来的node上，因此无法删除，也就无法再添加pod，所以fix是如果无法删除这种情况直接忽略，再添加就没问题</p>
<p><a href="https://hub.fastgit.org/kubernetes-sigs/kube-batch/pull/936">fix</a></p>
<h3 id="资源计算错误问题"><a href="#资源计算错误问题" class="headerlink" title="资源计算错误问题"></a>资源计算错误问题</h3><p>Kube-batch中对每一次调度之后都会计划queue中所有的task所占用的资源是不是超过了queue能够使用的最大资源，如果超过，则会返回一个标志位overUsed，返回这个标志位，kube-batch则不会对属于该queue中的task进行调度</p>
<p>但是kube-batch在进行资源比较时出现了计算错误,</p>
<p>核心代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">ssn.AddOverusedFn(pp.Name(), <span class="function"><span class="keyword">func</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">        queue := obj.(*api.QueueInfo)</span><br><span class="line">        attr := pp.queueOpts[queue.UID]</span><br><span class="line"></span><br><span class="line">    <span class="comment">//overused := attr.deserved.Less(attr.allocated)</span></span><br><span class="line">        overused := attr.deserved.LessEqual(attr.allocated) </span><br><span class="line">        <span class="keyword">if</span> overused &#123;</span><br><span class="line">            glog.V(<span class="number">3</span>).Infof(<span class="string">&quot;Queue &lt;%v&gt;: deserved &lt;%v&gt;, allocated &lt;%v&gt;, share &lt;%v&gt;&quot;</span>,</span><br><span class="line">                queue.Name, attr.deserved, attr.allocated, attr.share)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> overused</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p>该问题比较严重，<a href="https://hub.fastgit.org/volcano-sh/volcano/pull/1116">fix</a></p>
<h3 id="shadowpodgroup资源对象"><a href="#shadowpodgroup资源对象" class="headerlink" title="shadowpodgroup资源对象"></a>shadowpodgroup资源对象</h3><p>kube-batch支持直接在annotation中指定生成shadowpodgroup，这样就不需要每次都创建podgroup对象了</p>
<p>注意，生成的shaowpodgroup并不能通过kubectl查询得到，除了这点不一样之外，其它的跟podgroup的性质是一样的</p>
<p>使用方法:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ...</span></span><br><span class="line">  <span class="attr">templates:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">seg</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">kube-batch</span>          <span class="comment">### 这里指定使用kube-batch调度器，没有指定则会使用k8s集群中默认的调度器 ###</span></span><br><span class="line">      <span class="attr">metadata:</span></span><br><span class="line">        <span class="attr">annotations:</span></span><br><span class="line">          <span class="attr">&quot;scheduling.k8s.io/group-min-member&quot;:</span> <span class="string">&quot;2&quot;</span>  <span class="comment">### 这里指定podgroup中minMember</span></span><br><span class="line"><span class="comment"># ... </span></span><br></pre></td></tr></table></figure>

<p><a href="https://hub.fastgit.org/kubernetes-sigs/kube-batch/pull/908">merge</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
<li><a href="https://www.jianshu.com/p/042692685cf4">https://www.jianshu.com/p/042692685cf4</a></li>
<li><a href="https://argoproj.github.io/argo-workflows/fields/">https://argoproj.github.io/argo-workflows/fields/</a></li>
<li><a href="https://github.com/volcano-sh/volcano/pull/523">https://github.com/volcano-sh/volcano/pull/523</a></li>
<li><a href="https://github.com/kubernetes-sigs/kube-batch/issues/670">https://github.com/kubernetes-sigs/kube-batch/issues/670</a></li>
<li><a href="https://github.com/kubernetes-sigs/kube-batch/issues/904">https://github.com/kubernetes-sigs/kube-batch/issues/904</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/</a></li>
<li><a href="https://hub.fastgit.org/kubernetes-sigs/kube-batch/pull/908">https://hub.fastgit.org/kubernetes-sigs/kube-batch/pull/908</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(批调度器初识一)</title>
    <url>/2021/03/15/Kubernetes-kubebatch-knows/</url>
    <content><![CDATA[<p>k8s默认的调度器无法更好地实现对pod的All or Nothing调度能力， 在HPC或者分布式训练场景中，批处理能力对资源的使用率尤为重要.</p>
<span id="more"></span>

<h3 id="All-or-Nothing"><a href="#All-or-Nothing" class="headerlink" title="All or Nothing"></a>All or Nothing</h3><p>考虑以下场景:</p>
<p>有一个训练任务需要10个工作负载一起运行，每个工作负载负责其中一部分，假如有一个工作负载无法调度，则可遇见的结果是整个任务99%的概率会失败</p>
<p>则其它的工作负载在条件允许的情况下也不应该运行，运行起来只会白白浪费资源，这里要说明的是，作者目前所在的项目是负责一个分布式训练平台，还未实现增量训练功能，因此不进行调度则会更加合理，因为可以把这部分资源让给其它训练任务，因此需要引进批处理调度器，要么都进行调度，要么一个都不调度，这就是All or Nothing</p>
<p>目前对于基于kubernetes常用的开源批调度器有2个，一个是volcano，一个是kube-batch</p>
<p>CNCF大家庭中第一个批处理调度器是volcano，是由华为基于kube-batch开发而来，而Kube-batch这个项目目前也基于停止维护, 其作者已成为volcano项目的maintainer，偶尔会有fix，不过就作者目前所在的分布式训练项目而言, 经过作者的调研，引进volcano会有相当大的改造成本，短时间无法完成，反而kube-batch的接入比较快，改造较小。</p>
<h3 id="For-What"><a href="#For-What" class="headerlink" title="For What?"></a>For What?</h3><p>从kube-batch的readme.md来看，kube-batch主要聚焦在任务的<code>批调度</code>及对<code>多租户</code>的资源共享,而批调度使用的则是<code>PodGroup</code></p>
<p>将一组关联的pod当成一个group,从而在pod的概念之上又封装了一层，实现了对整体group的调度，要么都成功，要么都失败.</p>
<p>多租户则是使用的是queue,为每个租户都属于一个queue， 一个queue又与特定的资源绑定，这样可以实现同一个queue中使用的资源量，当然，kube-batch支持引入插件对queue间进行一引起操作，比如有优先级、资源抢占等功能</p>
<p>podgroup及queue不在这里展开，后续用单独的篇幅说明</p>
<h3 id="Can’t"><a href="#Can’t" class="headerlink" title="Can’t"></a>Can’t</h3><p>Kube-batch用于Kubernetes的批调度器，对于某些场景是不适合的，比如数据管理、多租户的隔离、运行时等能力。</p>
<p>很多大厂都在训练场景中引入了kube-batch，像百度、华为、vivo等，当然都基于此进行了二次开发，从公开的网站信息来看，除了华为的volcano最受欢迎之外，vivo对kube-batch的宣讲最多，很多东西也可借鉴</p>
<p>目前，作者所在的团队调度器一期项目主要是基于kube-batch，在调度器二期中将可能引入volcano，使功能更加完善</p>
<p>接下来的几篇文章都是围绕kube-batch这个工具的概念、使用以及在项目中引入kube-batch后对资源的使用率有什么影响.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch">https://github.com/kubernetes-sigs/kube-batch</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>kubectl exec 如何实现进入容器时指定用户</title>
    <url>/2022/11/27/Kubernetes-kubectl-exec-specify-username/</url>
    <content><![CDATA[<p>最近接到一个需求，用户在进入基于k8s内的容器时能否只以特定的用户进入?</p>
<p>听起来是一个很平常且合理的需求，但细想起来并不简单.</p>
<span id="more"></span>

<h3 id="背景信息"><a href="#背景信息" class="headerlink" title="背景信息"></a>背景信息</h3><p>k8s: v1.22</p>
<p>CRI: docker v20.10.18</p>
<p>CNI: cilium v1.9(并不涉及)</p>
<h3 id="问题引申"><a href="#问题引申" class="headerlink" title="问题引申"></a>问题引申</h3><p>首先，目前的kubectl exec 目前不支持使用参数指定用户进入到容器中, 即无法执行Kubectl exec –username，熟悉docker的人都知道，docker 是支持在exec 使用-u指定用户的,这时抛出第一个问题:</p>
<p><strong>为什么kubectl exec 不支持指定username进入容器?</strong></p>
<p>说实话，作者在这之前还真没有想过这个问题，但在想这个问题的时候，引出了另一个问题,因此上面的问题先放一放，先来回答另一个问题:</p>
<p><strong>当执行kubectl exec时，进入的是哪个用户?</strong></p>
<p>这个问题比较简单，当然进入的是默认的用户，那默认的用户是从何而来? 那自然是Dockerfile中指定的用户，默认情况下即为root用户，如果想进入后不为root怎么办?</p>
<p>在Dockerfile中使用<code>USER</code>关键字指定某个用户为默认用户，这样当kubectl exec进入到容器后即为该用户而非root，但毕竟dockerfile只build一次，不可能在需要更换用户的时候对镜像进行重新build只为切换用户，这样显然成本较大，而在使用docker时就比较容易，因为本身支持exec -u username,那么只要是容器中存在的用户，都可使用-u进行切换。</p>
<p>因此回到第一个问题：</p>
<p><strong>为什么kubectl exec 不支持指定username进入容器?</strong></p>
<p>作者翻阅了一些官方的issue, 很早就有人提出了相关需求，但官方给的反馈总结起来就是基于安全及整体架构考虑，<strong>这个参数涉及到多个k8s组件的修改</strong>，同时又存在user mapping问题，相关issue感兴趣的可阅读参考文章的前几个链接</p>
<p>也确实有人提了个<a href="https://github.com/kubernetes/kubernetes/pull/81883/commits">MR</a>来支持这个参数，但由于提交的代码与docker CRI绑定较为密切，而kubernetes本身是支持多种CRI的，因此并未合并进主干，直到现在也未支持该参数</p>
<p>需求注意的是， kubectl 目前有–user, –username这两个参数，但都不是实现上述目的的</p>
<blockquote>
<ul>
<li><p>–user: 指的是kubectl 使用的config文件集群相应的username</p>
</li>
<li><p>–username: 指的是kubectl 与apiServer通信的用户名</p>
</li>
</ul>
</blockquote>
<p>综上所述，在不改变现状的情况下，有没有办法让kubectl exec 支持指定username呢?</p>
<p>答案是肯定的，docker exec -u就是好的参考例子</p>
<h3 id="问题拆解"><a href="#问题拆解" class="headerlink" title="问题拆解"></a>问题拆解</h3><p>由于kubectl exec最终也是进入到容器中，那自然可以把kubectl exec命令转换成docker exec命令，那如何对应起来呢? 两者的桥梁就是DOCKER CONTAINER ID</p>
<p>简单来说就是以下流程:</p>
<ol>
<li><p>通过解析目标pod的yaml内容拿到相关信息，主要是nodeName, podnamespace, containerid, nodeName的作用是: 由于最终要转换成docker 命令，因此需要拿到pod运行在哪台什么主机上, 这样doker命令也必须运行在该主机上</p>
</li>
<li><p>新构建一个pod， pod里运行一个docker client，然后把主机上的docker.sock挂载到pod中(docker in docker)，这样再结合CONTAINER ID, 即可实现docker exec -u xxx ${CONTAINER ID}</p>
</li>
</ol>
<p>是不是很简单，更兴奋的是， 已经有人专门写了个kubectl plugin来实现这个功能，<a href="https://github.com/jordanwilson230/kubectl-plugins/blob/krew/kubectl-exec-as"><code>exec-as</code></a>,也叫<code>kubectl ssh</code></p>
<h4 id="krew"><a href="#krew" class="headerlink" title="krew"></a>krew</h4><p>kubectl有自己的插件体系，叫<a href="https://krew.sigs.k8s.io/"><code>krew</code></a>, 由于篇幅有限，不在这里展开，这里只放krew在线及离线安装方式供大家参考</p>
<p><a href="https://krew.sigs.k8s.io/docs/user-guide/setup/install">krew在线安装</a></p>
<p>安装完krew后即可使用Kubectl krew命令了</p>
<p>krew插件在线安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl krew install exec-as</span><br></pre></td></tr></table></figure>

<p><a href="https://www.jianshu.com/p/4238dd05f143">krew插件离线安装</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./krew-linux_amd64 install --manifest=krew.yaml --archive=krew-linux_amd64.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="exec-as"><a href="#exec-as" class="headerlink" title="exec-as"></a>exec-as</h4><p>在exec-as插件安装完之后即可实现以下命令实现指定用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl exec-as -u xxx podname -- bash</span><br></pre></td></tr></table></figure>

<p>安装完之后可以看出, exec-as其实就是一个shell脚本，下面的代码是他的核心逻辑</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 省略获取pod相关信息的代码</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$&#123;CONTAINER&#125;</span> != <span class="string">&quot;NONE&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">  DOCKER_CONTAINERID=$( <span class="built_in">eval</span> <span class="string">&quot;<span class="variable">$KUBECTL</span>&quot;</span> get pod <span class="string">&quot;<span class="variable">$&#123;POD&#125;</span>&quot;</span> -o go-template=<span class="string">&quot;&#x27;&#123;&#123; range .status.containerStatuses &#125;&#125;&#123;&#123; if eq .name \&quot;<span class="variable">$&#123;CONTAINER&#125;</span>\&quot; &#125;&#125;&#123;&#123; .containerID &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#x27;&quot;</span> )</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  DOCKER_CONTAINERID=$( <span class="variable">$KUBECTL</span> get pod <span class="string">&quot;<span class="variable">$&#123;POD&#125;</span>&quot;</span> -o go-template=<span class="string">&#x27;&#123;&#123; (index .status.containerStatuses 0).containerID &#125;&#125;&#x27;</span> )</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">CONTAINERID=<span class="variable">$&#123;DOCKER_CONTAINERID#*//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">read</span> -r -d <span class="string">&#x27;&#x27;</span> OVERRIDES &lt;&lt;<span class="string">EOF || :</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;apiVersion&quot;: &quot;v1&quot;,</span></span><br><span class="line"><span class="string">    &quot;metadata&quot;: &#123;</span></span><br><span class="line"><span class="string">      &quot;annotations&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;sidecar.istio.io/inject&quot; : &quot;false&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    &quot;spec&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;securityContext&quot;: &#123; &quot;runAsUser&quot;: 0 &#125;,</span></span><br><span class="line"><span class="string">        &quot;containers&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;image&quot;: &quot;docker&quot;,</span></span><br><span class="line"><span class="string">                &quot;name&quot;: &quot;&#x27;$container&#x27;&quot;,</span></span><br><span class="line"><span class="string">                &quot;stdin&quot;: true,</span></span><br><span class="line"><span class="string">                &quot;stdinOnce&quot;: true,</span></span><br><span class="line"><span class="string">                &quot;tty&quot;: true,</span></span><br><span class="line"><span class="string">                &quot;restartPolicy&quot;: &quot;Never&quot;,</span></span><br><span class="line"><span class="string">                &quot;args&quot;: [</span></span><br><span class="line"><span class="string">                  &quot;exec&quot;,</span></span><br><span class="line"><span class="string">                  &quot;-it&quot;,</span></span><br><span class="line"><span class="string">                  &quot;-u&quot;,</span></span><br><span class="line"><span class="string">                  &quot;$&#123;USERNAME&#125;&quot;,</span></span><br><span class="line"><span class="string">                  &quot;$&#123;CONTAINERID&#125;&quot;,</span></span><br><span class="line"><span class="string">                  $(to_json &quot;$&#123;COMMAND&#125;&quot;)</span></span><br><span class="line"><span class="string">                ],</span></span><br><span class="line"><span class="string">                &quot;volumeMounts&quot;: [</span></span><br><span class="line"><span class="string">                    &#123;</span></span><br><span class="line"><span class="string">                        &quot;mountPath&quot;: &quot;/var/run/docker.sock&quot;,</span></span><br><span class="line"><span class="string">                        &quot;name&quot;: &quot;docker&quot;</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ],</span></span><br><span class="line"><span class="string">        $NODESELECTOR</span></span><br><span class="line"><span class="string">        $TOLERATIONS</span></span><br><span class="line"><span class="string">        &quot;volumes&quot;: [</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                &quot;name&quot;: &quot;docker&quot;,</span></span><br><span class="line"><span class="string">                &quot;hostPath&quot;: &#123;</span></span><br><span class="line"><span class="string">                    &quot;path&quot;: &quot;/var/run/docker.sock&quot;</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"><span class="built_in">trap</span> <span class="string">&#x27;$KUBECTL delete pod $container &gt;/dev/null 2&gt;&amp;1 &amp;&#x27;</span> 0 1 2 3 15</span><br><span class="line"><span class="built_in">eval</span> <span class="string">&quot;<span class="variable">$KUBECTL</span>&quot;</span> run -it --restart=Never --image=docker --overrides=<span class="string">&quot;&#x27;<span class="variable">$&#123;OVERRIDES&#125;</span>&#x27;&quot;</span> <span class="string">&quot;<span class="variable">$container</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>其实就是上述的一二点的实现，是不是很简单，但是这远远不够，再回想一下我们的需求:</p>
<p><strong>kubectl exec命令中能否只以特定的用户进入容器</strong></p>
<p>通过<code>kubectl exec-as -u </code>是可以指定用户了，但这没办法控制用户指定什么用户，选择权是在用户侧，所以并没有达到限制<strong>只</strong>特定用户的目的</p>
<p>同时，还有如下的几点缺点:</p>
<ol>
<li><p>由于exec-as只是一个shell脚本，如果权限控制不好的话，用户随时可以改脚本的内容，并不安全</p>
</li>
<li><p>上面的代码中可以看到，新创建的pod中使用的是docker exec -u username ${containerID}进入到容器中，多数情况下要进入到容器只是用于debug场景，用完就应该退出，在这种场景下，不能直接做成一个deployment服务，因为它是long run的，会造成docker exec 一直是连着docker server的，也并不安全。</p>
</li>
<li><p>因为要使用到kubectl, 相对于给每个开发同学配置一遍环境，有没有更好、更快的办法?</p>
</li>
</ol>
<p>所以，要想达到目的，只能在exec-as上层再做一层，一是限制用户直接使用exec-as且不能修改exec-as shell脚本，二是用完即可退出docker exec</p>
<h4 id="webkubectl"><a href="#webkubectl" class="headerlink" title="webkubectl"></a>webkubectl</h4><p>webkubectl做为一个kubectl web化的开源工具，因为之前体验过，可以解决第3个缺点，先来说一下webkubectl的实现</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">_______________________________________________________________________</span><br><span class="line">|   Local Network     |          DMZ           |      VPC/Datacenter  |</span><br><span class="line">|                     |                        |                      |</span><br><span class="line">|                     |    _______________     |   ----------------   |</span><br><span class="line">|   ---------------   |    |             |  /~~~~~&gt;| Kubernetes A |   |</span><br><span class="line">|   | Your Laptop |~~~~~~~&gt;| Web Kubectl | /   |   ----------------   |</span><br><span class="line">|   ---------------   |    |             | \   |                      |</span><br><span class="line">|                     |    ---------------  \  |   ----------------   |</span><br><span class="line">|                     |                      \~~~~&gt;| Kubernetes B |   |</span><br><span class="line">|                     |                        |   ----------------   |</span><br><span class="line">-----------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>也很简单，就是用户通过上传kube config文件或者一个合法的token来连接k8s集群，</p>
<p>webkubectl是支持多用户、多session，它会为每个session开辟独立的内存空间，从而做到访问隔离.</p>
<p>截止目前，多用户这块做的并不完善，只支持用户预设定，新增用户需要重启webkubectl, 也不支持对接LDAP&#x2F;AD等账号体系.</p>
<p>多session的隔离是通过挂载temp内存文件系统(退出即销毁)，结合一些kubens、kubectx等工具实现的.</p>
<p>但也还不够，webkubectl只是一个kubectl web化的工具，需要将它与exec-as整合，</p>
<p>最后我们要实现的目的是:</p>
<p><strong>当用户登录到webkubectl后，通过kubeconfig连接session后直接进入到对应的容器中</strong>，不给用户执行exec-as的机会，从而实现文章开头的目的，其中,登录webkubectl的用户即为需要进入到容器中的用户.</p>
<p>那如何告诉kubectl exec-as接受的参数?</p>
<p>很简单，通过webkubectl登录时指定，即:</p>
<ol>
<li><p>登录webkubectl的用户即为进入到容器中的用户, 即，docker exec 中-u 的值</p>
</li>
<li><p>其它的两个参数:podname、namespace,这里<strong>为了改造最小化</strong>，就直接放在了登录webkubectl时的密码中了，通过分隔符来限定.</p>
</li>
</ol>
<p>因为最终变成:</p>
<p>登录webkubectl的用户名密码为: username:password_namespace_podname</p>
<p>在webkubectl的源码中获取到这几个参数，然后直接os.Setenv到session的环境变量(因为session是隔离的，因为不会出现错乱)中，再让执行kubectl exec-as时从session环境变量中获取即可.</p>
<p>这样，就可实现用户以指定用户进入到某个pod的容器中.</p>
<p>还有以下几优化的点:</p>
<blockquote>
<ul>
<li><p>webkubectl由于采用的内存隔离，因此在当用户退出session后(退出浏览器tab页面)，就不会保存任何数据，新创建的pod也会随之删除(看exec-as中的trap调用)</p>
</li>
<li><p>为了能够支持同一个session可以多次连接，对exec-as做了些改造，即，当新创建的pod存在且为Running状态时，就直接使用docker exec 进入，而不是每次都创建新的pod, 这样可以提高效率</p>
</li>
</ul>
</blockquote>
<p>另外，还有几个权限问题需要注意一下:</p>
<blockquote>
<ul>
<li><p>由于webkubectl中使用了unshare、mount等命令，因此当使用容器启动时需要使用privileged启动</p>
</li>
<li><p>由于exec-as中使用了kubectl delete pod，因此上传的kube config文件中对应的用户需要具备对ns pod删除的权限，注意，<strong>删除pod的ns并不是用户需要进入的pod所在的ns，而是新创建的pod(也就是由exec-as创建出来的)所在的ns的删除权限，需要进入的pod所在的ns其实只需要get&#x2F;list pod的权限即可.</strong></p>
</li>
</ul>
</blockquote>
<p>改造成本不高，目的<strong>基本</strong>达成</p>
<p>等等，为什么要说基本达成呢? 其实在<strong>易用</strong>上进行优化的空间: </p>
<blockquote>
<ul>
<li><p>改造webkubectl的用户认证模块，支持从CMDB中动态地获取用户，这样在有新用户添加时不需要重启webkubectl</p>
</li>
<li><p>username:password_namespace_podname这样糅合在一起用还是挺low的，如果有一个CMDB来记录用户与要访问pod的对应关系，用户对自己具备的pod可随时增删调整就更友好了</p>
</li>
</ul>
</blockquote>
<h3 id="成果图"><a href="#成果图" class="headerlink" title="成果图"></a>成果图</h3><p>由于时间有限, 除了易用上可优化的点外，上述提到的点作者都已改造完成，贴几张图</p>
<p>登录</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20221128103819.png"></p>
<p>创建session, 上传kube config文件</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20221128103921.png"></p>
<p>使用webkubectl登录用户连接对应的pod</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20221128103704.png"></p>
<p>进入到pod中</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20221128104430.png"></p>
<p>当然，由于时间有限, 对webkubectl、exec-as都做了些改造，目前凑合使用</p>
<p>作者深知，这么个工具要能在生产上使用，简直是简陋至极，最好的解决方案肯定是kubectl exec官方来支持</p>
<p>最后，发现一个更简单的工具, 通过改写kubectl exec – su - username来实现，不过有个限制是，容器中必须存在su命令，而且不支持自定义command，嗯，是个办法，感兴趣的可看看这个<a href="https://github.com/zhangweiqaz/po_pod.git">go_pod</a></p>
<p>请叫我工具组合侠…</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><p><a href="https://github.com/kubernetes/kubernetes/issues/30656">https://github.com/kubernetes/kubernetes/issues/30656</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/kubectl/blob/a0af655b7abaf06983c99ad5a4dc8f02ae5eb3e5/pkg/cmd/exec/exec.go#L100">https://github.com/kubernetes/kubectl/blob/a0af655b7abaf06983c99ad5a4dc8f02ae5eb3e5/pkg/cmd/exec/exec.go#L100</a></p>
</li>
<li><p><a href="https://github.com/kubernetes-client/python/issues/1223">run connect-get-namespaced-pod-exec as a specific user · Issue #1223 · kubernetes-client&#x2F;python · GitHub</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/enhancements/pull/3372">[WIP] To add username option in kubectl exec by hanamantagoudvk · Pull Request #3372 · kubernetes&#x2F;enhancements · GitHub</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/kubernetes/pull/81883">[WIP]exec: supprt exec as a specific user by max88991 · Pull Request #81883 · kubernetes&#x2F;kubernetes · GitHub</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/enhancements/pull/1224">Add exec as user KEP by max88991 · Pull Request #1224 · kubernetes&#x2F;enhancements · GitHub</a></p>
</li>
<li><p><a href="https://blog.csdn.net/whatday/article/details/104874055">linux shell trap捕捉信号 附信号表 SIGTERM SIGKILL_whatday的博客-CSDN博客_sigkill 捕捉</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/kubernetes/pull/81883#issuecomment-524544951"> WIPexec: supprt exec as a specific user by max88991 · Pull Request #81883 · kubernetes&#x2F;kubernetes · GitHub</a></p>
</li>
<li><p><a href="https://krew.sigs.k8s.io/">https://krew.sigs.k8s.io/</a></p>
</li>
<li><p><a href="https://github.com/zhangweiqaz/po_pod.git">https://github.com/zhangweiqaz/po_pod.git</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/4238dd05f143">kubectl krew 离线安装插件 - 简书</a></p>
</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes之ListOptions使用不当引发的ETCD网络风暴</title>
    <url>/2023/12/14/Kubernetes-listOption-1/</url>
    <content><![CDATA[<p>最近排查了一个因业务层使用List接口时因listOptions参数使用不当引起的etcd压力极速增长的问题,顺着listOptions把源码过了一遍, 知识又涨不少.</p>
<span id="more"></span>

<h3 id="业务背景"><a href="#业务背景" class="headerlink" title="业务背景"></a>业务背景</h3><p>kubernetes为任务型集群, 每天的任务量2k-4k, job保留时长为3天</p>
<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>在某个时刻，多个业务方反应如下情况:</p>
<ol>
<li>发布任务很慢，很久才能running起来</li>
<li>任务状态不对，有的任务实际已经执行完了，但过了很久状态才变成completion</li>
</ol>
<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><h4 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h4><p>在业务方反映的问题的同时，收到了关于apiserver的监控告警, 这个指标: <a href="https://github.com/prometheus-operator/kube-prometheus/wiki/KubeAPIErrorBudgetBurn">KubeAPIErrorBudgetBurn</a>，</p>
<p>long&#x2F;short时间内都出现了告警，给作者的第一感觉就是apiserver响应不过来了，接着查看监控dashboard, 发现apiserver的内存上涨很多,同时，Read&#x2F;Write SLI Duration相关的指标都有明显的延时，特别是对于pod的Read SLI Duration, 涨了数十倍之多，这显然是不正常</p>
<p>然后查看apiserver的日志，同样出现很多List请求10s timeout, 日志如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">List etcd3 key <span class="string">&quot;xxx&quot;</span> ,resourceVersion:, resourceVersionMatch:,<span class="built_in">limit</span>: 0, <span class="built_in">continue</span>: xxx, total: 10002ms</span><br></pre></td></tr></table></figure>

<p><strong>这段日志很重要，下文分解</strong></p>
<p>难道是etcd扛不住了???  </p>
<p>简单解释下上述提到的3个指标: </p>
<ol>
<li><p><a href="https://github.com/prometheus-operator/kube-prometheus/wiki/KubeAPIErrorBudgetBurn">KubeAPIErrorBudgetBurn</a>: 按照时间范围长短分为紧急及不紧急，不紧急的不要求立刻解决，可能apiserver自己会慢慢消化，而紧急的则建议立即处理, 链接中有一些promsql可以查看具体的metrics.</p>
</li>
<li><p><strong>Read SLI Duration</strong>: 对应prometheus中的promsql: 统计apiserver_request_total中verb为read的所有请求的SLI Duration,这是一个融合查询, 统计了在某个时间范围内所有的write操作的时延，如get、list请求</p>
</li>
<li><p><strong>Write SLI Duration</strong>: 对应prometheus中的promsql: 统计apiserver_request_total中verb为write的所有请求的SLI Duration,这是一个融合查询, 统计了在某个时间范围内所有的write操作的时延， 如update、create等请求</p>
</li>
</ol>
<h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>接着查看etcd的监控dashboard, 发现etcd实例的内存同样上涨很多, <strong>etcd_network_client_grpc_received_bytes_total</strong>倒是很正常</p>
<p>但是另一个关键指标: <strong>etcd_network_client_grpc_send_bytes_total</strong>,5分钟的增量达到了惊人的500-600Mb&#x2F;s, 1分钟的增量更是到了Gb&#x2F;s ，<strong>显然不正常</strong><br>这里也简单介绍下这2个metrics(<strong>来自ChatGPT</strong>):</p>
<ol>
<li><p><strong>etcd_network_client_grpc_send_bytes_total</strong>: 此指标衡量了通过 gRPC 客户端接收的总字节数，表示 etcd 通过客户端接收的网络流入量（以字节为单位）。</p>
</li>
<li><p><strong>etcd_network_client_grpc_received_bytes_total</strong>：相反，此指标衡量了通过 gRPC 客户端发送的总字节数，表示 etcd 通过客户端发送的网络流出量（以字节为单位）</p>
</li>
</ol>
<p><code>etcd_network_client_grpc_received_bytes_total</code>常态下很小可以理解，因为只是原始请求，最多带一些参数也不可能很大</p>
<p>但是<code>etcd_network_client_grpc_send_bytes_total</code>这个值很大，说明etcd要反馈给client端的数据量很大，第一时间想到的就是是不是有LIST接口使用不当查询了全量数据</p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>结合上面在apiserver日志中看到了相关的接口:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">List etcd3 key <span class="string">&quot;xxx&quot;</span> ,resourceVersion:, resourceVersionMatch:,<span class="built_in">limit</span>: 0, <span class="built_in">continue</span>: xxx, total: 10002ms</span><br></pre></td></tr></table></figure>

<p>可以看出有一个list接口中resourceVersion及limit都没有设置，而且时间都超过了10s, 有一些还引发了timeout.</p>
<p>所以第一时间优先排查这个LIST接口, 果然，在某个java业务(使用的是fabricio8&#x2F;kubernetes-client库)新增了一个接口，功能是想每隔3秒查询一次某个namespace下某job下的所有pod状态，但是使用LIST时因参数使用不当导致在etcd中返回了全量数据，集群中存在大量的job数据量很庞大，由于出现问题时刚好是任务高峰期且任务的时效性非常重要， <strong>对于这个接口是否合理的讨论并没有做为第一优先讨论的</strong>，采取优先解决问题的做法，使用了一个最简单的修复方式上线，即在这个接口中指定resourceVerison&#x3D;0</p>
<p>上线一段时间后观察所有的监控指标都趋于正常</p>
<p><strong>问题解决</strong></p>
<p>那么抛出另一个问题，<strong>为什么在list中指定了resourceVersion&#x3D;0就能解决问题呢?</strong></p>
<h3 id="问题复盘"><a href="#问题复盘" class="headerlink" title="问题复盘"></a>问题复盘</h3><p>问题在一开始时从apiserver的日志中就已经反应出来了, 也第一时间推动业务进行了调整,但要深入探讨起来，需要说的东西还不少</p>
<p><strong>为什么在list中指定了resourceVersion&#x3D;0就能解决问题呢?</strong></p>
<p>要回答这个问题，那么就得把数据在etcd及kube-apiserver中是如何存储及处理的说起</p>
<p>一个实际 etcd 集群存储的数据量可能很小，GB级别，甚至足够缓存到内存中。它的并发请求数量可能会高几个量级，单个 LIST 请求可能只需要返回几十 MB 到上 GB 的流量，但并发请求一多，etcd 显然也扛不住，所以最好在前面有 一层缓存，这就是 apiserver 的功能（之一）。K8s 的 LIST 请求大部分都应该被 apiserver 挡住，从它的本地缓存提供服务，但如果使用不当，就会跳过缓存直接到达 etcd，有很大的稳定性风险。</p>
<p><code>kube-apiserver</code> <code>LIST</code> 请求处理逻辑可以看到下图<a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">原图地址</a>：</p>
<p><img src="https://arthurchiao.art/assets/img/k8s-reliability-list-data/apiserver-processing-list-request.png"></p>
<p>以上可以看到，系统路径中存在<strong>两级 List&#x2F;ListWatch</strong>（<strong>但数据是同一份</strong>）：</p>
<ol>
<li>apiserver List&#x2F;ListWatch etcd</li>
<li>其它对象如controller&#x2F;operator List&#x2F;ListWatch apiserver</li>
</ol>
<p>因此，从最简形式上来说，<strong>apiserver 就是挡在 etcd 前面的一个代理</strong>（proxy），</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  +--------+              +---------------+                 +------------+</span><br><span class="line">  | Client | -----------&gt; | Proxy (cache) | --------------&gt; | Data store |</span><br><span class="line">  +--------+              +---------------+                 +------------+</span><br><span class="line"></span><br><span class="line">infra services               apiserver                         etcd</span><br></pre></td></tr></table></figure>

<ol>
<li><p>绝大部分情况下，apiserver 直接从本地缓存提供服务（因为<strong>它缓存了集群全量数据</strong>）；</p>
</li>
<li><p>某些特殊情况，例如如下比较常见的2种:</p>
<ol>
<li><strong>客户端明确要求从 etcd 读数据</strong>（追求最高的数据准确性），</li>
<li><strong>apiserver本地缓存还没建好</strong></li>
</ol>
<p>apiserver就只能将请求转发给 etcd, <strong>这里就要特别注意了客户端 LIST 参数设置不当也可能会走到这个逻辑</strong></p>
</li>
</ol>
<p>常见的List请求可归类为两种：</p>
<ol>
<li>List 全量数据：开销主要花在数据传输；</li>
<li>List指定用 label 或字段（field）过滤，只需要匹配的数据。</li>
</ol>
<p>这里需要特别说明的是第二种情况，也就是 list 请求带了过滤条件，<strong>不要以为list带了过滤条件就不会全量查询etcd</strong>,也分两种情况:</p>
<ul>
<li><p>大部分情况下，apiserver 会用自己的缓存做过滤，走缓存的操作很快，直接从apiserver的内存中就可以返回，因此<strong>耗时主要花在数据网络传输</strong></p>
</li>
<li><p>需要将请求转给 etcd 的情况</p>
</li>
</ul>
<p>注意，<strong>etcd 只是 KV 存储，并不理解 label&#x2F;field 信息，因此在etcd层面无法处理过滤请求</strong>。 实际的过程是：apiserver 从 etcd 拉全量数据，然后在内存做过滤，再返回给客户端。<strong>因此除了数据传输开销（网络带宽），这种情况下还会占用大量 apiserver CPU 和内存</strong></p>
<p>以几个常见的LIST请求为例：</p>
<ol>
<li><p><strong><code>LIST apis/cilium.io/v2/ciliumendpoints?limit=500&amp;resourceVersion=0</code></strong></p>
<p>这里同时传了两个参数，但 <code>resourceVersion=0</code> 会导致 apiserver 忽略 <code>limit=500</code>， 所以客户端拿到的是全量 ciliumendpoints 数据。</p>
<p>但由于指定了resourceVersion&#x3D;0, 所以虽然是全量数据，但是会直接从apiserver的缓存中返回。</p>
</li>
<li><p><strong><code>LIST api/v1/pods?fieldSelector=spec.nodeName%3Dnode1</code></strong></p>
<p>这个请求是获取 <code>node1</code> 上的所有 pods（<code>%3D</code> 是 <code>=</code> 的转义）。</p>
<p>根据 nodename 做过滤，给人的感觉可能是数据量不太大，但其实背后要比看上去复杂：</p>
<ul>
<li>首先，这里没有指定 resourceVersion&#x3D;0，导致 <strong>apiserver 跳过缓存，直接去 etcd 读数据</strong>；</li>
<li>其次，<strong>etcd 只是 KV 存储，没有按 label&#x2F;field 过滤功能</strong>（只处理 <code>limit/continue</code>），所以apiserver 是从 etcd 拉全量的pod数据，然后在<strong>内存做fieldselector过滤</strong>，开销也是很大的</li>
</ul>
<p>这种行为是要避免的，除非对数据准确性有极高要求，特意要绕过 apiserver 缓存。</p>
</li>
<li><p><strong><code>LIST api/v1/pods?filedSelector=spec.nodeName%3Dnode1&amp;resourceVersion=0</code></strong></p>
<p>跟 2 的区别是加上了 <code>resourceVersion=0</code>，因此 apiserver 会从缓存读数据，<strong>性能会有量级的提升</strong>。</p>
<p>但要注意，虽然实际上返回给客户端的可能只有<strong>几百 KB 到上百 MB</strong>, 但 apiserver 需要处理的数据量可能是<strong>几个 GB</strong>。</p>
</li>
</ol>
<p>以上可以看到，不同的 LIST 操作产生的影响是不一样的，而客户端看到数据还有可能只是 apiserver&#x2F;etcd 处理数据的很小一部分。如果基础服务大规模启动或重启， 就极有可能把控制平面打爆。</p>
<p>总结一下resourceVersion作用: <strong>保证客户端数据一致性和顺序性，乐观锁，实现并发控制</strong></p>
<p>设置ListOptions时，resourceVersion有三种设置方法：</p>
<ol>
<li><strong>不设置(不传递ListOptions或者不设置resourceVersion字段)，此时会直接从etcd中读取，此时数据是最新的</strong></li>
<li>设置为”0”，此时会从API Server Cache中获取数据</li>
<li>设置为指定的resourceVersion，获取resourceVersion大于指定版本的所有资源对象</li>
</ol>
<h3 id="分析源码"><a href="#分析源码" class="headerlink" title="分析源码"></a>分析源码</h3><p>从上面可以看出resourceVersion的重要性，现在来看看代码，以client-go为例list Jobs</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ListJobs lists all jobs details.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ListJobs</span><span class="params">()</span></span> <span class="type">error</span> &#123;</span><br><span class="line">    config, err := util.BuildConfig(listJobFlags.Master, listJobFlags.Kubeconfig)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> listJobFlags.allNamespace &#123;</span><br><span class="line">        listJobFlags.Namespace = <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    jobClient := versioned.NewForConfigOrDie(config)</span><br><span class="line">    jobs, err := jobClient.BatchV1alpha1().Jobs(listJobFlags.Namespace).List(context.TODO(), metav1.ListOptions&#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(jobs.Items) == <span class="number">0</span> &#123;</span><br><span class="line">        fmt.Printf(<span class="string">&quot;No resources found\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    PrintJobs(jobs, os.Stdout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最重要的是<code>metav1.ListOptions&#123;&#125;</code>这个结构体,在很多的operator的代码中，大量地使用上述这种写法，可能有人会很奇怪，按照上面的说法，这不是没有传递参数么，它不是会全量获取数据吗？</p>
<p>别着急，来看一下它的定义，由于代码过长，这里就不全贴</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ListOptions <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// A selector to restrict the list of returned objects by their labels.</span></span><br><span class="line">    <span class="comment">// Defaults to everything.</span></span><br><span class="line">    <span class="comment">// +optional</span></span><br><span class="line">    LabelSelector <span class="type">string</span> <span class="string">`json:&quot;labelSelector,omitempty&quot; protobuf:&quot;bytes,1,opt,name=labelSelector&quot;`</span></span><br><span class="line">    <span class="comment">// A selector to restrict the list of returned objects by their fields.</span></span><br><span class="line">    <span class="comment">// Defaults to everything.</span></span><br><span class="line">    <span class="comment">// +optional</span></span><br><span class="line">    FieldSelector <span class="type">string</span> <span class="string">`json:&quot;fieldSelector,omitempty&quot; protobuf:&quot;bytes,2,opt,name=fieldSelector&quot;`</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// resourceVersion sets a constraint on what resource versions a request may be served from.</span></span><br><span class="line">    <span class="comment">// See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for</span></span><br><span class="line">    <span class="comment">// details.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Defaults to unset</span></span><br><span class="line">    <span class="comment">// +optional</span></span><br><span class="line">    ResourceVersion <span class="type">string</span> <span class="string">`json:&quot;resourceVersion,omitempty&quot; protobuf:&quot;bytes,4,opt,name=resourceVersion&quot;`</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义中ResourceVersion默认为unset, 但是<strong>在client-go中 的 ListWatch&#x2F;informer 接口, 那它默认已经设置了 <code>ResourceVersion=0</code></strong>(如果客户端没传 <code>ListOption</code>，则初始化一个默认值，其中的 <code>ResourceVersion</code> 设置为空字符串)，这也就是为什么写operator时会直接传递<code>metav1.ListOptions&#123;&#125;</code>(ResourceVersion具体的设置方法将在下一篇中进行分析)</p>
<p>但是在其它语言的kubernetes client中就不一定，比如java fabric8 client中，可能就需要显式地指定resourceVersion</p>
<h3 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h3><p> 所以可以看到，某些请求看起来很简单，只是客户端一行代码的事情，但背后的数据量是惊人的，集群规模比较小的时候，这个问题可能看不出来（etcd 在 LIST 响应延迟超过某个阈值 后才开始打印 warning 日志）；规模大了之后，如果这样的请求比较多，apiserver&#x2F;etcd 肯定是扛不住的。</p>
<p>其实，<strong>业务层每3秒去list resource结果显然也是不合适的，这本就是watch要做的事情</strong>，这是后续在稳定性上要推进业务的todo之一</p>
<h3 id="更多参数"><a href="#更多参数" class="headerlink" title="更多参数"></a>更多参数</h3><p>这里官方<a href="https://kubernetes.io/zh-cn/docs/reference/using-api/api-concepts/#resourceVersion-in-metadata">resourceVersion-in-metadata</a>有个表格来说明Get&#x2F;List接口中的一些参数</p>
<h4 id="ListOptions"><a href="#ListOptions" class="headerlink" title="ListOptions{}"></a><code>ListOptions&#123;&#125;</code></h4><table>
<thead>
<tr>
<th>resourceVersionMatch param</th>
<th>paging params</th>
<th>resourceVersion not set</th>
<th>resourceVersion&#x3D;”0”</th>
<th>resourceVersion&#x3D;”{value other than 0}”</th>
</tr>
</thead>
<tbody><tr>
<td><em>unset</em></td>
<td><em>limit unset</em></td>
<td>Most Recent</td>
<td>Any</td>
<td>Not older than</td>
</tr>
<tr>
<td><em>unset</em></td>
<td>limit&#x3D;<n>, <em>continue unset</em></td>
<td>Most Recent</td>
<td>Any</td>
<td>Exact</td>
</tr>
<tr>
<td><em>unset</em></td>
<td>limit&#x3D;<n>, continue&#x3D;<token></td>
<td>Continue Token, Exact</td>
<td>Invalid, treated as Continue Token, Exact</td>
<td>Invalid, HTTP <code>400 Bad Request</code></td>
</tr>
<tr>
<td><code>resourceVersionMatch=Exact</code></td>
<td><em>limit unset</em></td>
<td>Invalid</td>
<td>Invalid</td>
<td>Exact</td>
</tr>
<tr>
<td><code>resourceVersionMatch=Exact</code></td>
<td>limit&#x3D;<n>, <em>continue unset</em></td>
<td>Invalid</td>
<td>Invalid</td>
<td>Exact</td>
</tr>
<tr>
<td><code>resourceVersionMatch=NotOlderThan</code></td>
<td><em>limit unset</em></td>
<td>Invalid</td>
<td>Any</td>
<td>Not older than</td>
</tr>
<tr>
<td><code>resourceVersionMatch=NotOlderThan</code></td>
<td>limit&#x3D;<n>, <em>continue unset</em></td>
<td>Invalid</td>
<td>Any</td>
<td>Not older than</td>
</tr>
</tbody></table>
<h4 id="GetOptions"><a href="#GetOptions" class="headerlink" title="GetOptions{}"></a><code>GetOptions&#123;&#125;</code></h4><p>基本原理与 <code>ListOption&#123;&#125;</code> 一样，不设置 <code>ResourceVersion=0</code> 会导致 apiserver 去 etcd 拿数据，应该尽量避免</p>
<table><tbody><tr><td>resourceVersion unset</td><td>resourceVersion=”0”</td><td>resourceVersion=”{value other than 0}”</td></tr><tr><td>Most Recent</td><td>Any</td><td>Not older than</td></tr></tbody></table>

<ul>
<li>Most Recent：去 etcd 拿数据；</li>
<li>Any：优先用最新的，但不保证一定是最新的；</li>
<li>Not older than：不低于某个版本号。</li>
</ul>
<h3 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h3><p>上面【分析源码】一节中只是说明了可以通完传递resourceVersion&#x3D;0给list以达到通过apiserver的缓存返回数据减少etcd的压力，但还有些其它情况apiserver的判断逻辑是如何的呢，比如确实是需要直接从etcd获取数据以保证数据是最新的，或者缓存没有建设又该如何及client-go中对resourceVersion判断赋值操作？</p>
<p>下回源码阅读分解</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a></li>
<li><a href="https://arthurchiao.art/blog/raft-paper-zh/">(译) [论文] Raft 共识算法（及 etcd&#x2F;raft 源码解析）（USENIX, 2014）</a></li>
<li><a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">K8s 集群稳定性：LIST 请求源码分析、性能评估与大规模基础服务部署调优</a></li>
<li><a href="https://www.cnblogs.com/lianngkyle/p/16272494.html">k8s client-go源码分析 informer源码分析(3)-Reflector源码分析 - 良凯尔 - 博客园</a></li>
<li><a href="https://www.cnblogs.com/lianngkyle/p/16272494.html">https://www.cnblogs.com/lianngkyle/p/16272494.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(使用kustomize实现不同环境的配置派生)</title>
    <url>/2020/07/01/Kubernetes-kustomize/</url>
    <content><![CDATA[<p>最近在思考作为一个小团队来说，在没有配置管理平台的情况下怎么样的配置管理才能更好地支持多个Kubernetes环境的差异呢, 大多数的人跟我的反应一样, 使用Helm, 这也是我一直的想法，Helm对于复杂的应用来说，其实比较实用，特别是在Helm3去除了Tiller之后，可是业务中的应用基本都属于轻应用，有更优的解决方案吗?</p>
<span id="more"></span>

<p>直到我看到了kustomize，不得不说， 现在K8s都1.18的版本了， 在1.14的时候kustomize就被直接整合在kubectl里，然而，我却从来都没关注过.</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>官方的描述:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Kubernetes native configuration management</span><br><span class="line">Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into kubectl as apply -k.</span><br></pre></td></tr></table></figure>

<p>Kustomize 允许用户以一个应用描述文件 （YAML 文件）为基础（Base YAML），然后通过 Overlay 的方式生成最终部署应用所需的描述文件，而不是像 Helm 那样只提供应用描述文件模板，然后通过字符替换（Templating）的方式来进行定制化</p>
<p>既然有了helm，为何还需要kustomize，换句话说<strong>kustomize 解决了什么问题?</strong></p>
<p>一般应用都会存在多套部署环境：开发环境、测试环境、生产环境，多套环境意味着存在多套 K8S 应用资源 YAML。而这么多套 YAML 之间只存在微小配置差异，比如镜像版本不同、Label 不同等，而这些不同环境下的YAML 经常会因为人为疏忽导致配置错误。再者，多套环境的 YAML 维护通常是通过把一个环境下的 YAML 拷贝出来然后对差异的地方进行修改。一些类似 Helm 等应用管理工具需要额外学习DSL 语法。总结以上，在 k8s 环境下存在多套环境的应用，经常遇到以下几个问题：</p>
<ul>
<li>如何管理不同环境或不同团队的应用的 Kubernetes YAML 资源</li>
<li>如何以某种方式管理不同环境的微小差异，使得资源配置可以复用，减少 copy and change 的工作量</li>
<li>如何简化维护应用的流程，<strong>不需要额外学习模板语法</strong></li>
</ul>
<p>Kustomize 通过以下几种方式解决了上述问题：</p>
<ul>
<li>kustomize 通过 Base &amp; Overlays 方式(下文会说明)方式维护不同环境的应用配置</li>
<li>kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用</li>
<li>kustomize 管理的都是 Kubernetes 原生 YAML 文件，不需要学习额外的 DSL 语法</li>
</ul>
<p>上面的表达引用自<a href="https://zhuanlan.zhihu.com/p/92153378">kustomize最佳实践</a></p>
<p>本人总结就是: <strong>不需要通过模板系统,以特定结构合并(更新)多种资源以支撑在多个环境中的部署，以消除差异.</strong></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mac</span></span><br><span class="line">brew install kustomize</span><br><span class="line"><span class="comment"># curl -s &quot;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&quot;  | bash</span></span><br></pre></td></tr></table></figure>

<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>kustomize的命令也比较简单, 最常用的就是下面两个命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 直接使用kustomize命令</span></span><br><span class="line">kustomize build ~/ldap/overlays/staging | kubectl apply -f -</span><br><span class="line"><span class="comment"># 在 kubernetes 1.14 版本， kustomize 已经集成到 kubectl 命令中，成为了其一个子命令，可使用 kubectl 来进行部署</span></span><br><span class="line">kubectl apply -k ~/ldap/overlays/staging</span><br></pre></td></tr></table></figure>

<h3 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h3><p>kustomize中有几个常用的关键字术语，这里简单介绍一下，方便后续的使用</p>
<ul>
<li><strong>base</strong></li>
</ul>
<p>base 指的是一个 kustomization , 任何的 kustomization 包括 overlay (后面提到)，都可以作为另一个 kustomization 的 base (简单理解为基础目录)。base 中描述了共享的内容，如资源和常见的资源配置</p>
<ul>
<li><strong>overlay</strong></li>
</ul>
<p>overlay 是一个 kustomization, 它修改(并因此依赖于)另外一个 kustomization. overlay 中的 kustomization指的是一些其它的 kustomization, 称为其 base. 没有 base, overlay 无法使用，并且一个 overlay 可以用作 另一个 overlay 的 base(基础)。简而言之，overlay 声明了与 base 之间的差异。通过 overlay 来维护基于 base 的不同 variants(变体)，例如开发、QA 和生产环境的不同 variants</p>
<ul>
<li><strong>variant</strong></li>
</ul>
<p>variant 是在集群中将 overlay 应用于 base 的结果。例如开发和生产环境都修改了一些共同 base 以创建不同的 variant。这些 variant 使用相同的总体资源，并与简单的方式变化，例如 deployment 的副本数、ConfigMap使用的数据源等。简而言之，variant 是含有同一组 base 的不同 kustomization</p>
<ul>
<li><strong>patch</strong></li>
</ul>
<p>修改文件的一般说明。文件路径，指向一个声明了 kubernetes API patch 的 YAML 文件</p>
<h3 id="workflows-工作流"><a href="#workflows-工作流" class="headerlink" title="workflows 工作流"></a>workflows 工作流</h3><p>kustomize 将对 Kubernetes 应用的管理转换成对 Kubernetes manifests YAML 文件的管理，而对应用的修改也通过 YAML 文件来修改。这种修改变更操作可以通过 Git 版本控制工具进行管理维护, 因此用户可以使用 Git 风格的流程来管理应用。 workflows 是使用并配置应用所使用的一系列 Git 风格流程步骤。官网提供了两种方式，一种是定制配置，另一种是现成配置</p>
<h4 id="定制场景"><a href="#定制场景" class="headerlink" title="定制场景"></a>定制场景</h4><p><a href="https://kubernetes-sigs.github.io/kustomize/guides/bespoke/">workflow</a>如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200701133434.png"></p>
<h4 id="现成配置"><a href="#现成配置" class="headerlink" title="现成配置"></a>现成配置</h4><p>在这个工作流方式中，可从别人的 repo 中 fork kustomize 配置，并根据自己的需求来配置</p>
<p><a href="https://kubernetes-sigs.github.io/kustomize/guides/offtheshelf/">workflow</a>如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200701133953.png"></p>
<p>流程非常地简洁，相信不需要过多解释， 个人觉得这两种方式除了源头上有区别外，在使用上没什么差别.</p>
<h3 id="Case-Demo"><a href="#Case-Demo" class="headerlink" title="Case Demo"></a>Case Demo</h3><p>下面会通过一个小Demo来体验kustomize是如何工作的.</p>
<h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><p>app-kustomize是一个代码git， 以golang为主，这里只列出了跟kustomize有关的文件,其它代码文档没有直接关系就先省略了.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">app-kustomize</span><br><span class="line">    deploy</span><br><span class="line">        base</span><br><span class="line">            config.yaml</span><br><span class="line">            deployment.yaml</span><br><span class="line">            service.yaml</span><br><span class="line">            kustomization.yaml </span><br><span class="line">        overlay</span><br><span class="line">            1box</span><br><span class="line">                1box-custom-env.yaml</span><br><span class="line">                kustomization.yaml</span><br><span class="line">            prod</span><br><span class="line">                deployment-patch.yaml</span><br><span class="line">                kustomization.yaml</span><br></pre></td></tr></table></figure>

<p>这里对上述的文件做个简要说明:</p>
<p><code>base</code>翻译过来就是基础的意思， 里面的资源可以理解为默认的配置或者是基础配置</p>
<p> <code>overlay</code>目录下则可以根据不同环境建立不同的文件夹, 比如我有一个测试环境,那么可以叫1box(这个名字随便), 还有一个生产环境，就叫prod, 这两个目录下都包含有<code>kustomeization.yaml</code>, 同时，两个目录下都可以通过<strong>打patch或者覆盖</strong>的方式来对base目录下对应的资源进行更新, 这也是overlay的由来.</p>
<p>使用<code>kustomize build ~/ldap/overlays/1box</code>来生成合并后的整体配置文件</p>
<p>再通过<code>kustomize build ~/ldap/overlays/1obx | kubectl apply -f -</code>进行发布到对应的环境中去</p>
<p>当然,这个流程就可以跟CI&#x2F;CD进行结合使用了. 是不是非常简单.</p>
<p>其它的配置文件不需要过多解释了.</p>
<p><code>cat base/deployment.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">p-expoter-kustom</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-TEST</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;789&quot;</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">localhost:5055/p-expoter:master-36cd406f</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">p-expoter</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/etc/config/config.yml&quot;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">config.yml</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">realty</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">alertmanager.yaml</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">config.yml</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">p-expoter-senserealty-cms</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demo-config</span></span><br></pre></td></tr></table></figure>

<p><code>cat base/service.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">p-expoter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br></pre></td></tr></table></figure>

<p><code>cat config.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">secrets.mz.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMapSecret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">p-expoter-senserealty-cms</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">p-expoter-senserealty-cms</span> </span><br><span class="line">    <span class="attr">data:</span></span><br><span class="line">      <span class="attr">alertmanager.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          global:</span></span><br><span class="line"><span class="string">            resolve_timeout: 5m</span></span><br><span class="line"><span class="string">            mongodb_password: $(MONGODB_PASSWORD)  # 在 Var中定义</span></span><br><span class="line"><span class="string">            redis_password: $(REDIS_PASSWORD)</span></span><br><span class="line"><span class="string">            # special_how: $(CONFIGMAP_HOW) </span></span><br><span class="line"><span class="string">            # special_type: $(CONFIGMAP_TYPE)</span></span><br><span class="line"><span class="string">          route:</span></span><br><span class="line"><span class="string">            receiver: default</span></span><br><span class="line"><span class="string">            group_by: [&quot;alertname&quot;, &quot;job&quot;, &quot;team&quot;]</span></span><br><span class="line"><span class="string">            group_wait: 30s</span></span><br><span class="line"><span class="string"></span>  <span class="attr">vars:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MONGODB_PASSWORD</span></span><br><span class="line">      <span class="attr">secretValue:</span> <span class="comment"># 引用secret </span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">senserealty-secret-data</span> <span class="comment"># secret 名字</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">mongodb_password</span> <span class="comment"># secret key</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REDIS_PASSWORD</span> </span><br><span class="line">      <span class="attr">secretValue:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">senserealty-secret-data</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">redis_password</span></span><br><span class="line">    <span class="comment"># - name: CONFIGMAP_HOW # 引用configmap</span></span><br><span class="line">    <span class="comment">#   configMapValue:</span></span><br><span class="line">    <span class="comment">#     name: special-config-hmmg28f4kd</span></span><br><span class="line">    <span class="comment">#     key: special.how</span></span><br><span class="line">    <span class="comment"># - name: CONFIGMAP_TYPE</span></span><br><span class="line">    <span class="comment">#   configMapValue:</span></span><br><span class="line">    <span class="comment">#     name: special-config-hmmg28f4kd</span></span><br><span class="line">    <span class="comment">#     key: special.type</span></span><br></pre></td></tr></table></figure>

<p>以上配置文件都是很常用的资源对象， 这里解释一下config.yaml并不是我们常用的configmap.yaml类型, 而是使用了CRD类型，主要是为了实现敏感信息的加密保存在git上.这块不是这篇post的重点,感兴趣的话, 大家可参考这篇<a href="https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/">post</a> ，目前可直接把这个当做为等同于configmap</p>
<h4 id="kustomization-yaml"><a href="#kustomization-yaml" class="headerlink" title="kustomization.yaml"></a>kustomization.yaml</h4><p>可以发现在base目录或者是overlay目录下都有一个kustomization.yaml文件，该文件是kustomize的核心， 包含了需要部署的资源.</p>
<p>当然它本身是有一定语法的, 详细的使用可参考<a href="https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#kustomization">这里</a>，下面会挑选一些常用的配置来说明如何使用</p>
<p><code>cat kustomization.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"></span><br><span class="line"><span class="attr">commonLabels:</span></span><br><span class="line">  <span class="attr">k8s-app:</span> <span class="string">p-expoter</span> <span class="comment"># 会在所有的资源对象上都会加上该label</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">stage</span> <span class="comment"># 会在所有的资源对象上都指定该ns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># configMapGenerator:   # secretGenerator</span></span><br><span class="line"><span class="comment"># - name: special-config</span></span><br><span class="line"><span class="comment">#   files:</span></span><br><span class="line"><span class="comment">#     - configmap.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">config.yaml</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">service.yaml</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deployment.yaml</span></span><br></pre></td></tr></table></figure>

<h5 id="resources"><a href="#resources" class="headerlink" title="resources"></a>resources</h5><p>这个字段包含需要部署的资源文件， 这个应该很容易理解</p>
<h5 id="commonLables-x2F-commonAnnotations"><a href="#commonLables-x2F-commonAnnotations" class="headerlink" title="commonLables&#x2F;commonAnnotations"></a>commonLables&#x2F;commonAnnotations</h5><p>指定了commonLables的话， 则会在所有的资源resources包含的对象上都会加上该label， 支持指定多个，这样就省去了对k8s的yaml中手工地指定lable</p>
<p>commonAnnotations同理.</p>
<h5 id="namespace"><a href="#namespace" class="headerlink" title="namespace"></a>namespace</h5><p>同上, </p>
<h5 id="configMapGenerator-x2F-secretGenerator"><a href="#configMapGenerator-x2F-secretGenerator" class="headerlink" title="configMapGenerator&#x2F;secretGenerator"></a>configMapGenerator&#x2F;secretGenerator</h5><p>这两者具有相同的功能, 可以从文件中直接生成configmap或者是secret</p>
<p>格式如上代码所示，这部分也会渲染到最终的yaml文件中， 因此，<strong>除了resources指定的资源外, 还能通过这种方式生成部署对象</strong></p>
<h5 id="patchesStrategicMerge"><a href="#patchesStrategicMerge" class="headerlink" title="patchesStrategicMerge"></a>patchesStrategicMerge</h5><p>patchesStrategicMerge则主要是用于<strong>overlay下的kustomization.yaml中</strong>, 用于使用打patch的方式将该字段指定的文件合并到base目录下对应的内容上.</p>
<p>比如, 现在需要1box环境中，在base声明的deployment.yaml的基础上，给<code>p-expoter</code>容器增加一个环境变量</p>
<p>那么就新增overlay&#x2F;1box&#x2F;1box-custom-env.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">p-expoter-kustom</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;aaabbb&quot;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">p-expoter</span> <span class="comment"># 由于env是在spec.template.containers下的, 因此需要保留这种树型的结构，同时容器的名字很重要，需要与base/deployment.yaml对应上，要不然，会出现env找不到容器，自然无法为对应容器添加成功.</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CUSTOM_ENV_VARIABLE</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">&quot;Value defined by Kustomize&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>这里需要注意的是，patch的字段都是通过Yaml的层及进行查询的, 所以要添加或者修改的东西一定要对应上层级，不然在base里是查找不到相应的字段的, 因此也无法修改, 但是不会报错</strong></p>
<p><code>cat overlay/1box/kustomization.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># namePrefix: 1box # 所有的资源名称都会加上1box-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replicas:  # 直接修改资源副本数</span></span><br><span class="line"><span class="comment"># - name: p-expoter-kustom</span></span><br><span class="line"><span class="comment">#   count: 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># images:   # 直接修改镜像</span></span><br><span class="line"><span class="comment"># - name: localhost:5055/p-expoter:master-36cd406f  # name 为base/deployment中的镜像的名字</span></span><br><span class="line"><span class="comment">#   newName: p-expoter </span></span><br><span class="line"><span class="comment">#   newTag: alpine  # 最终会替换成 images: localhost:5055/p-expoter:alpine</span></span><br><span class="line"></span><br><span class="line"><span class="attr">bases:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">../../base</span></span><br><span class="line"></span><br><span class="line"><span class="attr">patchesStrategicMerge:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">1box-custom-env.yaml</span></span><br></pre></td></tr></table></figure>

<p>以上的代码实现修改容器的启动参数, 同时为容器添加env的功能</p>
<p>使用<code>kustomize build deploy/overlay/1box</code>会发现，p-expoter容器已经被加上了env环境变量了,<strong>而其它的部分保持不变</strong>.</p>
<p>通过patchesStrategicMerge可以实现，<strong>如果指定的字段不存在,则进行添加， 如果存在，则进行覆盖的功能</strong></p>
<p><strong>注意</strong>: 如果通过在<code>kustomization.yaml</code>修改了同时又使用<code>patchesStrategicMerge</code>进行了同一字段的修改,</p>
<p>比如修改replicas,  如果在<code>kustomization.yaml中修改成了2， 在</code>patchesStrategicMerge&#96;又修改成了10</p>
<p>结果以<code>kustomization.yaml</code>中的为准，也就是最终replicas &#x3D; 2</p>
<h5 id="namePrefix-x2F-nameSuffix"><a href="#namePrefix-x2F-nameSuffix" class="headerlink" title="namePrefix&#x2F;nameSuffix"></a>namePrefix&#x2F;nameSuffix</h5><p>这两个实现给所有的资源的名字加上前缀或者后缀，以<code>-</code>进行隔离， 非常容易理解，主要用于overlay下</p>
<p>常用的也就这些了，还有一些其它的配置就不一一介绍了，官网上都非常的详细.</p>
<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>通过上面的介绍，应该对kustomize有个初步的了解， 最后的工作就是根据不同的环境将资源发布到集群中</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-k</span> <span class="string">~/ldap/overlays/1box</span></span><br></pre></td></tr></table></figure>

<p>注意:</p>
<p>如果 <code>kubectl apply -k 出现error: rawResources failed to read Resources</code>的错误</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">error: rawResources failed to <span class="built_in">read</span> Resources: Load from path ../../base/ failed: <span class="string">&#x27;../../base/&#x27;</span> must be a file (got d=<span class="string">&#x27;/Users/zhoushuke/git_uni/gitlab.bj.sensetime.com/gitlabci-golang-demo/deploy/base&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>出现这种情况是因为<code>kustomize</code>与<code>kubectl apply -k</code> 版本不匹配(kubectl的版本其实是跟着集群的版本走的, 而kustomize二进制可能不一定会对着kubectl的版本安装，有可能就直接安装了最新版本)</p>
<p>可以使用<code>kustomiz build . | kubectl apply -f -</code></p>
<p>另外一个情况下, <code>bases</code>已经在kustomize v2.1.0+ 版本中给去掉了,高版本的会直接将<code>bases</code>转换成<code>resources</code>，这个可以从渲染后的kustomization.yaml中看出</p>
<h3 id="Helm-vs-kustomize"><a href="#Helm-vs-kustomize" class="headerlink" title="Helm vs kustomize"></a>Helm vs kustomize</h3><h4 id="Differences"><a href="#Differences" class="headerlink" title="Differences"></a>Differences</h4><p>Kustomize 自称因为去掉了模板语法，更易使用，对此我保留看法，如果仅就入门使用来看，二者差异并不大。</p>
<p>Helm3也已经完全地抛弃了Tiller，在架构上更加的简洁， 因此在部署上，Kustomize 的优势也不是很大。</p>
<p>我认为他们的区别主要在工作流程上：</p>
<ul>
<li>Helm 的基础流程比较<code>瀑布</code>：定义 Chart-&gt;填充-&gt;运行，在 Chart 中没有定义的内容是无法更改的；</li>
<li>Kustomize 的用法比较<code>迭代</code>：Base 和 Overlay 都是可以独立运作的，增加新对象，或者对编写 Base 时未预料的内容进行变更，都不在话下</li>
</ul>
<h4 id="Choice"><a href="#Choice" class="headerlink" title="Choice"></a>Choice</h4><p>helm出现的时间较早, kubernetes开发&#x2F;运维大多数人或多或少都有过接触,kustomize在v1.14版本后默认集成到kubectl中大有后起之秀的能力，这两者如何选择呢?</p>
<p>对于要公开发布一个较为复杂的应用，可能涉及较多的资源类型， 例如 <code>Istio</code>，编写良好的 Chart 能给用户很大帮助，用户在缺失一点发挥空间的情况下，通过对 <code>values.yaml</code> 的阅读，就能对这种复杂的部署产生一个较为深入的认识。</p>
<p>如果是常见的业务应用，特别是在私有化部署场景中，这咱差异化并不是很大, 但是未必可以提前预料到变化，用 Kustomize 可能会是一个更好的选择，不过前提是需要有一点学习成本.</p>
<p>由于Kubernetes直接把kustomize融合进了kubectl， 可见官方也是会大力推广这种声明式配置管理方式的. </p>
<p>整体使用起来还是比较简单的, 对于小团队来说也是一个不错的选择</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes-sigs.github.io/kustomize">https://kubernetes-sigs.github.io/kustomize</a></li>
<li><a href="https://github.com/kubernetes-sigs/kustomize">https://github.com/kubernetes-sigs/kustomize</a></li>
<li><a href="https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/">https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/</a></li>
<li><a href="https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#generator">https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#generator</a></li>
<li><a href="https://kubernetes-sigs.github.io/kustomize/guides/bespoke/">https://kubernetes-sigs.github.io/kustomize/guides/bespoke/</a></li>
<li><a href="https://blog.fleeto.us/post/crud-with-kustomize/">https://blog.fleeto.us/post/crud-with-kustomize/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/92153378">https://zhuanlan.zhihu.com/p/92153378</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes之ListOption参数源码分析</title>
    <url>/2023/12/16/Kubernetes-listOption-2/</url>
    <content><![CDATA[<p>书接上一篇<a href="https://izsk.me/2023/12/14/Kubernetes-listOption-1/">Kubernetes之List参数使用不当引发的ETCD网络风暴</a>说最近排查了一个因业务层使用List接口时因参数使用不当引起的etcd压力极速增长的问题, 该篇将按图索骥来看看ListOption在源码是如何处理的</p>
<span id="more"></span>

<h3 id="处理逻辑"><a href="#处理逻辑" class="headerlink" title="处理逻辑"></a>处理逻辑</h3><p><code>kube-apiserver</code> <code>LIST</code> 请求处理逻辑可以看到下图<a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">原图地址</a>：</p>
<p><img src="https://arthurchiao.art/assets/img/k8s-reliability-list-data/apiserver-processing-list-request.png"></p>
<p>以上可以看到，系统路径中存在<strong>两级 List&#x2F;ListWatch</strong>（<strong>但数据是同一份</strong>）：</p>
<ol>
<li>apiserver List&#x2F;ListWatch etcd</li>
<li>其它对象如controller&#x2F;operator List&#x2F;ListWatch apiserver</li>
</ol>
<p>因此，从最简形式上来说，<strong>apiserver 就是挡在 etcd 前面的一个代理</strong>（proxy），</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  +--------+              +---------------+                 +------------+</span><br><span class="line">  | Client | -----------&gt; | Proxy (cache) | --------------&gt; | Data store |</span><br><span class="line">  +--------+              +---------------+                 +------------+</span><br><span class="line"></span><br><span class="line">infra services               apiserver                         etcd</span><br></pre></td></tr></table></figure>

<p>对于List请求可归类为两种：</p>
<ol>
<li>apiserver直接从自己的缓存中读数据</li>
<li>apiserver跳过缓存，直接从etcd读数据</li>
</ol>
<p>还是以使用client-go中listJob为例, 常见写法:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ListJobs lists all jobs details.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ListJobs</span><span class="params">()</span></span> <span class="type">error</span> &#123;</span><br><span class="line">    config, err := util.BuildConfig(listJobFlags.Master, listJobFlags.Kubeconfig)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> listJobFlags.allNamespace &#123;</span><br><span class="line">        listJobFlags.Namespace = <span class="string">&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    jobClient := versioned.NewForConfigOrDie(config)</span><br><span class="line">    jobs, err := jobClient.BatchV1alpha1().Jobs(listJobFlags.Namespace).List(context.TODO(), metav1.ListOptions&#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(jobs.Items) == <span class="number">0</span> &#123;</span><br><span class="line">        fmt.Printf(<span class="string">&quot;No resources found\n&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    PrintJobs(jobs, os.Stdout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>来看看ListOptions的struct, 这里因篇幅有限，所以将注释去除了</p>
<p> <strong>kubernetes based on v1.22</strong></p>
<h3 id="ListOptions"><a href="#ListOptions" class="headerlink" title="ListOptions"></a>ListOptions</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// pkg/apis/meta/v1/types.go</span></span><br><span class="line"><span class="keyword">type</span> ListOptions <span class="keyword">struct</span> &#123;</span><br><span class="line">    TypeMeta <span class="string">`json:&quot;,inline&quot;`</span></span><br><span class="line"></span><br><span class="line">    LabelSelector <span class="type">string</span> <span class="string">`json:&quot;labelSelector,omitempty&quot; protobuf:&quot;bytes,1,opt,name=labelSelector&quot;`</span></span><br><span class="line"></span><br><span class="line">    FieldSelector <span class="type">string</span> <span class="string">`json:&quot;fieldSelector,omitempty&quot; protobuf:&quot;bytes,2,opt,name=fieldSelector&quot;`</span></span><br><span class="line"></span><br><span class="line">    Watch <span class="type">bool</span> <span class="string">`json:&quot;watch,omitempty&quot; protobuf:&quot;varint,3,opt,name=watch&quot;`</span></span><br><span class="line"></span><br><span class="line">    AllowWatchBookmarks <span class="type">bool</span> <span class="string">`json:&quot;allowWatchBookmarks,omitempty&quot; protobuf:&quot;varint,9,opt,name=allowWatchBookmarks&quot;`</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ResourceVersion <span class="type">string</span> <span class="string">`json:&quot;resourceVersion,omitempty&quot; protobuf:&quot;bytes,4,opt,name=resourceVersion&quot;`</span></span><br><span class="line"></span><br><span class="line">    ResourceVersionMatch ResourceVersionMatch <span class="string">`json:&quot;resourceVersionMatch,omitempty&quot; protobuf:&quot;bytes,10,opt,name=resourceVersionMatch,casttype=ResourceVersionMatch&quot;`</span></span><br><span class="line"></span><br><span class="line">    TimeoutSeconds *<span class="type">int64</span> <span class="string">`json:&quot;timeoutSeconds,omitempty&quot; protobuf:&quot;varint,5,opt,name=timeoutSeconds&quot;`</span></span><br><span class="line"></span><br><span class="line">    Limit <span class="type">int64</span> <span class="string">`json:&quot;limit,omitempty&quot; protobuf:&quot;varint,7,opt,name=limit&quot;`</span></span><br><span class="line"></span><br><span class="line">    Continue <span class="type">string</span> <span class="string">`json:&quot;continue,omitempty&quot; protobuf:&quot;bytes,8,opt,name=continue&quot;`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>LabelSelector&#x2F;FieldSelector: 标签选择器与字段选择器, 上文提到, <strong>etcd只是KV存储，并不理解label&#x2F;field这些信息，因此在etcd层面无法处理这些过滤条件</strong>。 所以如果是没走apiserver的缓存直接到达etcd的实际的过程是：apiserver 从 etcd 拉全量数据，然后在内存做过滤，再返回给客户端</li>
<li>Watch及AllowWatchBookmarks: AllowWatchBookmarks也是个很有用的功能，但并不是本文的重点，并不展开说明</li>
<li>ResourceVersion&#x2F;ResourceVersionMatch: 资源版本</li>
<li>Limit: 分页功能, 最常见的例子是kubectl命令行工具,会自动将请求加上limit&#x3D;500这个查询参数, 可以使用kubectl –v&#x3D;8看到</li>
<li>Continue: 是个token,是否期望从server端返回最多的结果</li>
</ol>
<p>这里主要会聚焦ResourceVersion</p>
<p>从上面可以看出ResousrceVersion的默认类型为string,所以默认值为空，来看看apiserver <code>List()</code> 操作源码分析</p>
<h3 id="List-调用链"><a href="#List-调用链" class="headerlink" title="List()调用链"></a><code>List()调用链</code></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">store.List</span><br><span class="line">|-store.ListPredicate</span><br><span class="line">   |-<span class="keyword">if</span> opt == nil</span><br><span class="line">   |   opt = ListOptions&#123;ResourceVersion: <span class="string">&quot;&quot;</span>&#125;</span><br><span class="line">   |-Init SelectionPredicate.Limit/Continue fileld</span><br><span class="line">   |-list := e.NewListFunc()                               // objects will be stored <span class="keyword">in</span> this list</span><br><span class="line">   |-storageOpts := storage.ListOptions&#123;opt.ResourceVersion, opt.ResourceVersionMatch, Predicate: p&#125;</span><br><span class="line">   |</span><br><span class="line">   |-<span class="keyword">if</span> MatchesSingle ok                                   // 1. when <span class="string">&quot;metadata.name&quot;</span> is specified,  get single obj</span><br><span class="line">   |   // Get single obj from cache or etcd</span><br><span class="line">   |</span><br><span class="line">   |-<span class="built_in">return</span> e.Storage.List(KeyRootFunc(ctx), storageOpts)  // 2. get all objs and perform filtering</span><br><span class="line">      |-cacher.List()</span><br><span class="line">         | // <span class="keyword">case</span> 1: list all from etcd and filter <span class="keyword">in</span> apiserver</span><br><span class="line">         |-<span class="keyword">if</span> shouldDelegateList(opts)                     // <span class="literal">true</span> <span class="keyword">if</span> resourceVersion == <span class="string">&quot;&quot;</span></span><br><span class="line">         |    <span class="built_in">return</span> c.storage.List                        // list from etcd</span><br><span class="line">         |             |- fromRV *int64 = nil</span><br><span class="line">         |             |- <span class="keyword">if</span> len(storageOpts.ResourceVersion) &gt; 0</span><br><span class="line">         |             |     rv = ParseResourceVersion</span><br><span class="line">         |             |     fromRV = &amp;rv</span><br><span class="line">         |             |</span><br><span class="line">         |             |- <span class="keyword">for</span> hasMore &#123;</span><br><span class="line">         |             |    objs := etcdclient.KV.Get()</span><br><span class="line">         |             |    filter(objs)                   // filter by labels or filelds</span><br><span class="line">         |             | &#125;</span><br><span class="line">         |</span><br><span class="line">         | // <span class="keyword">case</span> 2: list &amp; filter from apiserver <span class="built_in">local</span> cache (memory)</span><br><span class="line">         |-<span class="keyword">if</span> cache.notready()</span><br><span class="line">         |   <span class="built_in">return</span> c.storage.List                         // get from etcd</span><br><span class="line">         |</span><br><span class="line">         | // <span class="keyword">case</span> 3: list &amp; filter from apiserver <span class="built_in">local</span> cache (memory)</span><br><span class="line">         |-obj := watchCache.WaitUntilFreshAndGet</span><br><span class="line">         |-<span class="keyword">for</span> elem <span class="keyword">in</span> obj.(*storeElement)</span><br><span class="line">         |   listVal.Set()                                 // append results to listOjb</span><br><span class="line">         |-<span class="built_in">return</span>  // results stored <span class="keyword">in</span> listObj</span><br></pre></td></tr></table></figure>

<h3 id="store"><a href="#store" class="headerlink" title="store"></a>store</h3><h4 id="List"><a href="#List" class="headerlink" title="List()"></a><code>List()</code></h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// List returns a list of items matching labels and field according to the</span></span><br><span class="line"><span class="comment">// store&#x27;s PredicateFunc.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *Store)</span></span> List(ctx context.Context, options *metainternalversion.ListOptions) (runtime.Object, <span class="type">error</span>) &#123;</span><br><span class="line">    label := labels.Everything()</span><br><span class="line">    <span class="keyword">if</span> options != <span class="literal">nil</span> &amp;&amp; options.LabelSelector != <span class="literal">nil</span> &#123;</span><br><span class="line">        label = options.LabelSelector</span><br><span class="line">    &#125;</span><br><span class="line">    field := fields.Everything()</span><br><span class="line">    <span class="keyword">if</span> options != <span class="literal">nil</span> &amp;&amp; options.FieldSelector != <span class="literal">nil</span> &#123;</span><br><span class="line">        field = options.FieldSelector</span><br><span class="line">    &#125;</span><br><span class="line">    out, err := e.ListPredicate(ctx, e.PredicateFunc(label, field), options)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> e.Decorator != <span class="literal">nil</span> &#123;</span><br><span class="line">        e.Decorator(out)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> out, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="ListPredicate"><a href="#ListPredicate" class="headerlink" title="ListPredicate()"></a><code>ListPredicate()</code></h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ListPredicate returns a list of all the items matching the given</span></span><br><span class="line"><span class="comment">// SelectionPredicate.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *Store)</span></span> ListPredicate(ctx context.Context, p storage.SelectionPredicate, options *metainternalversion.ListOptions) (runtime.Object, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> options == <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">// By default we should serve the request from etcd.</span></span><br><span class="line">        options = &amp;metainternalversion.ListOptions&#123;ResourceVersion: <span class="string">&quot;&quot;</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    p.Limit = options.Limit</span><br><span class="line">    p.Continue = options.Continue</span><br><span class="line">    list := e.NewListFunc()</span><br><span class="line">    qualifiedResource := e.qualifiedResourceFromContext(ctx)</span><br><span class="line">    storageOpts := storage.ListOptions&#123;ResourceVersion: options.ResourceVersion, ResourceVersionMatch: options.ResourceVersionMatch, Predicate: p&#125;</span><br><span class="line">    <span class="keyword">if</span> name, ok := p.MatchesSingle(); ok &#123;</span><br><span class="line">        <span class="keyword">if</span> key, err := e.KeyFunc(ctx, name); err == <span class="literal">nil</span> &#123;</span><br><span class="line">            err := e.Storage.GetToList(ctx, key, storageOpts, list)</span><br><span class="line">            <span class="keyword">return</span> list, storeerr.InterpretListError(err, qualifiedResource)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// if we cannot extract a key based on the current context, the optimization is skipped</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    err := e.Storage.List(ctx, e.KeyRootFunc(ctx), storageOpts, list)</span><br><span class="line">    <span class="keyword">return</span> list, storeerr.InterpretListError(err, qualifiedResource)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>ListPredicate()</code>中:</p>
<ol>
<li><p>如果客户端没传 **<code>ListOption</code>**，则初始化一个默认值，其中的 <code>ResourceVersion</code> 设置为空字符串;</p>
</li>
<li><p>用 listoptions 中的字段分别<strong>初始化过滤器</strong>（SelectionPredicate）的 limit&#x2F;continue 字段；</p>
</li>
<li><p>初始化返回结果，<code>list := e.NewListFunc()</code>；</p>
</li>
<li><p>将 API 侧的 ListOption 转成底层存储的 ListOption;</p>
</li>
<li><p>p.MatchesSingle()判断请求中是否指定了metadata.name,如果指定了，说明是查询单个对象，因为 <code>Name</code> 是唯一的，接下来转入查询单个 object 的逻辑；</p>
</li>
<li><p>如果p.MatchesSingle()不成立，则需要<strong>获取全量数据</strong>，然后在 apiserver 内存中根据 SelectionPredicate 中的过滤条件进行过滤，将最终结果返回给客户端；</p>
</li>
</ol>
<p>最终<code>e.Storage.GetToList()/e.Storage.List()</code>会执行到cacher。这两个funciont很相似</p>
<p>不管是获取单个 object，还是获取全量数据，都经历类似的过程：</p>
<ol>
<li>优先从 apiserver 本地缓存获取（决定因素包括 ResourceVersion 等），</li>
<li>不得已才到 etcd 去获取；</li>
</ol>
<h3 id="apiserver-cache"><a href="#apiserver-cache" class="headerlink" title="apiserver cache"></a>apiserver cache</h3><h4 id="List-1"><a href="#List-1" class="headerlink" title="List()"></a><code>List()</code></h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// List implements storage.Interface.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cacher)</span></span> List(ctx context.Context, key <span class="type">string</span>, opts storage.ListOptions, listObj runtime.Object) <span class="type">error</span> &#123;</span><br><span class="line">    resourceVersion := opts.ResourceVersion</span><br><span class="line">    pred := opts.Predicate</span><br><span class="line">    <span class="comment">// 是否必须从 etcd 读</span></span><br><span class="line">    <span class="keyword">if</span> shouldDelegateList(opts) &#123;</span><br><span class="line">        <span class="keyword">return</span> c.storage.List(ctx, key, opts, listObj)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If resourceVersion is specified, serve it from cache.</span></span><br><span class="line">    <span class="comment">// It&#x27;s guaranteed that the returned value is at least that</span></span><br><span class="line">    <span class="comment">// fresh as the given resourceVersion.</span></span><br><span class="line">    listRV, err := c.versioner.ParseResourceVersion(resourceVersion)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果缓存还没有建立好，只能从 etcd 读</span></span><br><span class="line">    <span class="keyword">if</span> listRV == <span class="number">0</span> &amp;&amp; !c.ready.check() &#123;</span><br><span class="line">        <span class="comment">// If Cacher is not yet initialized and we don&#x27;t require any specific</span></span><br><span class="line">        <span class="comment">// minimal resource version, simply forward the request to storage.</span></span><br><span class="line">        <span class="keyword">return</span> c.storage.List(ctx, key, opts, listObj)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    trace := utiltrace.New(<span class="string">&quot;cacher list&quot;</span>, utiltrace.Field&#123;<span class="string">&quot;type&quot;</span>, c.objectType.String()&#125;)</span><br><span class="line">    <span class="keyword">defer</span> trace.LogIfLong(<span class="number">500</span> * time.Millisecond)</span><br><span class="line"></span><br><span class="line">    c.ready.wait()</span><br><span class="line">    trace.Step(<span class="string">&quot;Ready&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 情况三：apiserver 缓存正常，从缓存读：保证返回的 objects 版本不低于 `listRV`</span></span><br><span class="line">    <span class="comment">// List elements with at least &#x27;listRV&#x27; from cache.</span></span><br><span class="line">    listPtr, err := meta.GetItemsPtr(listObj)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    listVal, err := conversion.EnforcePtr(listPtr)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> listVal.Kind() != reflect.Slice &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;need a pointer to slice, got %v&quot;</span>, listVal.Kind())</span><br><span class="line">    &#125;</span><br><span class="line">    filter := filterWithAttrsFunction(key, pred)</span><br><span class="line"></span><br><span class="line">    objs, readResourceVersion, err := c.watchCache.WaitUntilFreshAndList(listRV, pred.MatcherIndex(), trace)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    trace.Step(<span class="string">&quot;Listed items from cache&quot;</span>, utiltrace.Field&#123;<span class="string">&quot;count&quot;</span>, <span class="built_in">len</span>(objs)&#125;)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(objs) &gt; listVal.Cap() &amp;&amp; pred.Label.Empty() &amp;&amp; pred.Field.Empty() &#123;</span><br><span class="line">        <span class="comment">// Resize the slice appropriately, since we already know that none</span></span><br><span class="line">        <span class="comment">// of the elements will be filtered out.</span></span><br><span class="line">        listVal.Set(reflect.MakeSlice(reflect.SliceOf(c.objectType.Elem()), <span class="number">0</span>, <span class="built_in">len</span>(objs)))</span><br><span class="line">        trace.Step(<span class="string">&quot;Resized result&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> _, obj := <span class="keyword">range</span> objs &#123;</span><br><span class="line">        elem, ok := obj.(*storeElement)</span><br><span class="line">        <span class="keyword">if</span> !ok &#123;</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;non *storeElement returned from storage: %v&quot;</span>, obj)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 真正的过滤</span></span><br><span class="line">        <span class="keyword">if</span> filter(elem.Key, elem.Labels, elem.Fields) &#123;</span><br><span class="line">            listVal.Set(reflect.Append(listVal, reflect.ValueOf(elem.Object).Elem()))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    trace.Step(<span class="string">&quot;Filtered items&quot;</span>, utiltrace.Field&#123;<span class="string">&quot;count&quot;</span>, listVal.Len()&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更新最后一次读到的 ResourceVersion</span></span><br><span class="line">    <span class="keyword">if</span> c.versioner != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> err := c.versioner.UpdateList(listObj, readResourceVersion, <span class="string">&quot;&quot;</span>, <span class="literal">nil</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="shouldDelegateList"><a href="#shouldDelegateList" class="headerlink" title="shouldDelegateList()"></a><code>shouldDelegateList()</code></h4><p>判断是否必须从 etcd 读数据</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">shouldDelegateList</span><span class="params">(opts storage.ListOptions)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">    resourceVersion := opts.ResourceVersion</span><br><span class="line">    pred := opts.Predicate</span><br><span class="line">    pagingEnabled := utilfeature.DefaultFeatureGate.Enabled(features.APIListChunking)</span><br><span class="line">    hasContinuation := pagingEnabled &amp;&amp; <span class="built_in">len</span>(pred.Continue) &gt; <span class="number">0</span></span><br><span class="line">    hasLimit := pagingEnabled &amp;&amp; pred.Limit &gt; <span class="number">0</span> &amp;&amp; resourceVersion != <span class="string">&quot;0&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// If resourceVersion is not specified, serve it from underlying</span></span><br><span class="line">    <span class="comment">// storage (for backward compatibility). If a continuation is</span></span><br><span class="line">    <span class="comment">// requested, serve it from the underlying storage as well.</span></span><br><span class="line">    <span class="comment">// Limits are only sent to storage when resourceVersion is non-zero</span></span><br><span class="line">    <span class="comment">// since the watch cache isn&#x27;t able to perform continuations, and</span></span><br><span class="line">    <span class="comment">// limits are ignored when resource version is zero</span></span><br><span class="line">    <span class="keyword">return</span> resourceVersion == <span class="string">&quot;&quot;</span> || hasContinuation || hasLimit || opts.ResourceVersionMatch == metav1.ResourceVersionMatchExact</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里非常重要：</p>
<ol>
<li><p>客户端未设置 ListOption{} 中的 <code>ResourceVersion</code> 字段，对应到这里的 <code>resourceVersion == &quot;&quot;</code></p>
</li>
<li><p>客户端设置了 <code>limit=500&amp;resourceVersion=0</code> 不会导致下次 <code>hasContinuation==true</code>，因为<strong>resourceVersion&#x3D;0 将导致 limit 被忽略</strong>（<code>hasLimit</code> 那一行代码），也就是说， 虽然指定了 limit&#x3D;500，但<strong>这个请求会返回全量数据</strong>。</p>
</li>
<li><p>ResourceVersionMatch的作用是用来告诉 apiserver，该如何解读 ResourceVersion。官方的详细说明<a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter">表格</a> ，有兴趣可以看看。</p>
</li>
</ol>
<p>接下来再返回到 cacher 的 <code>GetList()</code> 逻辑，来看下具体有哪几种处理情况。</p>
<p>ListOption要求从 etcd 读数据</p>
<p>有两种情况:</p>
<ol>
<li>当客户端要求必须从etcd读取数据时，适用于数据一致性要求极其高的场景</li>
<li>当apiserver缓存还没有创建好时，比如apiserver重启到ready这阶段</li>
</ol>
<p>apiserver 会直接从 etcd 读取所有 objects 并过滤，然后返回给客户端， 适用于数据一致性要求极其高的场景。 当然，也容易误入这种场景造成 etcd 压力过大</p>
<p>这里将会从cache的<code>GetList()</code>转到etcd的<code>List()</code></p>
<h4 id="etcd-List"><a href="#etcd-List" class="headerlink" title="etcd List()"></a><code>etcd List()</code></h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//staging/src/k8s.io/apiserver/pkg/storage/etcd3/store.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// List implements storage.Interface.List.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *store)</span></span> List(ctx context.Context, key <span class="type">string</span>, opts storage.ListOptions, listObj runtime.Object) <span class="type">error</span> &#123;</span><br><span class="line">    resourceVersion := opts.ResourceVersion</span><br><span class="line">    match := opts.ResourceVersionMatch</span><br><span class="line">    pred := opts.Predicate</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// set the appropriate clientv3 options to filter the returned data set</span></span><br><span class="line">    <span class="keyword">var</span> paging <span class="type">bool</span></span><br><span class="line">    options := <span class="built_in">make</span>([]clientv3.OpOption, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="keyword">if</span> s.pagingEnabled &amp;&amp; pred.Limit &gt; <span class="number">0</span> &#123;</span><br><span class="line">        paging = <span class="literal">true</span></span><br><span class="line">        options = <span class="built_in">append</span>(options, clientv3.WithLimit(pred.Limit))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    newItemFunc := getNewItemFunc(listObj, v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> fromRV *<span class="type">uint64</span></span><br><span class="line">    <span class="comment">// 如果 RV 非空</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(resourceVersion) &gt; <span class="number">0</span> &#123;</span><br><span class="line">        parsedRV, err := s.versioner.ParseResourceVersion(resourceVersion)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> apierrors.NewBadRequest(fmt.Sprintf(<span class="string">&quot;invalid resource version: %v&quot;</span>, err))</span><br><span class="line">        &#125;</span><br><span class="line">        fromRV = &amp;parsedRV</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> returnedRV, continueRV, withRev <span class="type">int64</span></span><br><span class="line">    <span class="keyword">var</span> continueKey <span class="type">string</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ResourceVersion, ResourceVersionMatch 等处理逻辑</span></span><br><span class="line">    <span class="keyword">switch</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> withRev != <span class="number">0</span> &#123;</span><br><span class="line">        options = <span class="built_in">append</span>(options, clientv3.WithRev(withRev))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// loop until we have filled the requested limit from etcd or there are no more results</span></span><br><span class="line">    <span class="keyword">var</span> lastKey []<span class="type">byte</span></span><br><span class="line">    <span class="keyword">var</span> hasMore <span class="type">bool</span></span><br><span class="line">    <span class="keyword">var</span> getResp *clientv3.GetResponse</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        getResp = s.client.KV.Get(ctx, key, options...) <span class="comment">// 从 etcd 拉数据</span></span><br><span class="line">        numFetched += <span class="built_in">len</span>(getResp.Kvs)</span><br><span class="line">        hasMore = getResp.More</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// take items from the response until the bucket is full, filtering as we go</span></span><br><span class="line">        <span class="keyword">for</span> _, kv := <span class="keyword">range</span> getResp.Kvs &#123;</span><br><span class="line">            <span class="keyword">if</span> paging &amp;&amp; <span class="type">int64</span>(v.Len()) &gt;= pred.Limit &#123;</span><br><span class="line">                hasMore = <span class="literal">true</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            &#125;</span><br><span class="line">            lastKey = kv.Key</span><br><span class="line"></span><br><span class="line">            data, _, err := s.transformer.TransformFromStorage(kv.Value, authenticatedDataString(kv.Key))</span><br><span class="line">            <span class="comment">// ...</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// instruct the client to begin querying from immediately after the last key we returned</span></span><br><span class="line">    <span class="comment">// we never return a key that the client wouldn&#x27;t be allowed to see</span></span><br><span class="line">    <span class="keyword">if</span> hasMore &#123;</span><br><span class="line">        <span class="comment">// we want to start immediately after the last key</span></span><br><span class="line">        next, err := encodeContinue(<span class="type">string</span>(lastKey)+<span class="string">&quot;\x00&quot;</span>, keyPrefix, returnedRV)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">var</span> remainingItemCount *<span class="type">int64</span></span><br><span class="line">        <span class="comment">// getResp.Count counts in objects that do not match the pred.</span></span><br><span class="line">        <span class="comment">// Instead of returning inaccurate count for non-empty selectors, we return nil.</span></span><br><span class="line">        <span class="comment">// Only set remainingItemCount if the predicate is empty.</span></span><br><span class="line">        <span class="keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.RemainingItemCount) &#123;</span><br><span class="line">            <span class="keyword">if</span> pred.Empty() &#123;</span><br><span class="line">                c := <span class="type">int64</span>(getResp.Count - pred.Limit)</span><br><span class="line">                remainingItemCount = &amp;c</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.versioner.UpdateList(listObj, <span class="type">uint64</span>(returnedRV), next, remainingItemCount)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// no continuation</span></span><br><span class="line">    <span class="keyword">return</span> s.versioner.UpdateList(listObj, <span class="type">uint64</span>(returnedRV), <span class="string">&quot;&quot;</span>, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>client.KV.Get()</code></strong> 就进入 etcd client 库了</li>
<li><strong><code>appendListItem()</code></strong> 会<strong>对拿到的数据进行过滤</strong>，这就是我们第一节提到的 apiserver 内存过滤操作。</li>
</ul>
<h4 id="apiserver使用本地缓存"><a href="#apiserver使用本地缓存" class="headerlink" title="apiserver使用本地缓存"></a>apiserver使用本地缓存</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// GetList implements storage.Interface</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cacher)</span></span> List(ctx , key <span class="type">string</span>, opts storage.ListOptions, listObj runtime.Object) <span class="type">error</span></span><br><span class="line">    &#123;</span><br><span class="line">    <span class="comment">// 情况一：ListOption 要求必须从 etcd 读</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 情况二：apiserver 缓存未建好，只能从 etcd 读,跟情况一一</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 情况三：apiserver 缓存正常，从缓存读：保证返回的 objects 版本不低于 `listRV`</span></span><br><span class="line">    listPtr := meta.GetItemsPtr(listObj) <span class="comment">// List elements with at least &#x27;listRV&#x27; from cache.</span></span><br><span class="line">    listVal := conversion.EnforcePtr(listPtr)</span><br><span class="line">    filter  := filterWithAttrsFunction(key, pred) <span class="comment">// 最终的过滤器</span></span><br><span class="line"></span><br><span class="line">    objs, readResourceVersion, indexUsed := c.listItems(listRV, key, pred, ...) <span class="comment">// 根据 index 预筛，性能优化</span></span><br><span class="line">    <span class="keyword">for</span> _, obj := <span class="keyword">range</span> objs &#123;</span><br><span class="line">        elem := obj.(*storeElement)</span><br><span class="line">        <span class="keyword">if</span> filter(elem.Key, elem.Labels, elem.Fields)                           <span class="comment">// 真正的过滤</span></span><br><span class="line">            listVal.Set(reflect.Append(listVal, reflect.ValueOf(elem))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> c.versioner != <span class="literal">nil</span></span><br><span class="line">        c.versioner.UpdateList(listObj, readResourceVersion, <span class="string">&quot;&quot;</span>, <span class="literal">nil</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="总结建议"><a href="#总结建议" class="headerlink" title="总结建议"></a>总结建议</h3><ol>
<li>List请求默认设置 <code>ResourceVersion=0</code>以防etcd拉全量数据再过滤，导致，很慢或者扛不住</li>
<li>不要滥用List接口，如果能使用watch就使用watch</li>
<li>优先通过 label&#x2F;field selector 在服务端做过滤<br>如果需要缓存某些资源并监听变动，那需要使用 ListWatch 机制，将数据拉到本地，业务逻辑根据需要自己从 local cache 过滤。 这是 client-go 的 ListWatch&#x2F;informer 机制。</li>
<li>优先使用 namespaced API,<br>etcd 中 namespace 是前缀的一部分，因此能指定 namespace 过滤资源，速度比不是前缀的 selector 快很多,如果要 LIST 的资源在单个或少数几个 namespace，考虑使用 namespaced API：</li>
</ol>
<ul>
<li>Namespaced API: <code>/api/v1/namespaces/&lt;ns&gt;/pods?query=xxx</code></li>
<li>Un-namespaced API: <code>/api/v1/pods?query=xxx</code></li>
</ul>
<ol start="5">
<li>细化etcd&#x2F;apiserver等核心监控,比如作者之前不怎么关注的网络方面的metrics</li>
<li>经过上述的方法还是扛不住的话那可以把event等不是很重要的但很具有冲击对象的数据进行etcd分离</li>
<li>遇到性能问题是最容易出现多方的扯皮, 作者觉得最好的说服方式就是: <strong>直接用数据说话，数据不会骗人</strong></li>
</ol>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>本篇主要追了一下ResourceVersion是怎么处理的，但还有很多细节是没有展开的，比如</p>
<ol>
<li>ListOption中的其它参数都起到什么作用</li>
<li>Reflector中ListandWatch关于ResourceVersion又是怎样的</li>
<li>relist如何使用</li>
</ol>
<p>但实在是不想篇幅太长，所以给自己留个作业，下回再探讨</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2023/12/14/Kubernetes-listOption-1/">Kubernetes之List参数使用不当引发的ETCD网络风暴</a></li>
<li><a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts</a></li>
<li><a href="https://arthurchiao.art/blog/raft-paper-zh/">(译) [论文] Raft 共识算法（及 etcd&#x2F;raft 源码解析）（USENIX, 2014）</a></li>
<li><a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/">K8s 集群稳定性：LIST 请求源码分析、性能评估与大规模基础服务部署调优</a></li>
<li><a href="https://www.cnblogs.com/lianngkyle/p/16272494.html">k8s client-go源码分析 informer源码分析(3)-Reflector源码分析 - 良凯尔 - 博客园</a></li>
<li><a href="https://www.cnblogs.com/lianngkyle/p/16272494.html">https://www.cnblogs.com/lianngkyle/p/16272494.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(NFS如何实现 Volume Dynamic Provisioning)</title>
    <url>/2020/08/11/Kubernetes-nfs-provisioner/</url>
    <content><![CDATA[<p>NFS在kubernetes中做为共享存储，用的还是比较多的，之前也就是一直用着，也没有太关注是如何实现Dynamic provisioning，今天花了点时间研究了一下原理，还是挺有意思.</p>
<span id="more"></span>



<h3 id="NFS-Servers"><a href="#NFS-Servers" class="headerlink" title="NFS Servers"></a>NFS Servers</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">apt install yum -y install nfs-utils rpcbind</span><br><span class="line"><span class="comment"># 新建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /data/nfs</span><br><span class="line"><span class="comment"># 加入</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;/data/nfs/fluentd-conf    *(rw,sync,no_subtree_check,no_root_squash)&quot;</span> &gt;&gt; /etc/export</span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">exportfs -r</span><br><span class="line"><span class="comment"># 启动nfs</span></span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind</span><br><span class="line">systemctl <span class="built_in">enable</span> nfs-server</span><br><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs-server</span><br></pre></td></tr></table></figure>

<h3 id="NFS-Client"><a href="#NFS-Client" class="headerlink" title="NFS Client"></a>NFS Client</h3><p>client端只需要二步即可使用server端share出来的目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line"><span class="built_in">mkdir</span> /media/nfs_dir</span><br><span class="line"><span class="comment"># mount</span></span><br><span class="line">mount 192.168.8.150:/data/nfs /media/nfs_dir</span><br><span class="line"><span class="comment"># 查看</span></span><br></pre></td></tr></table></figure>

<p>当然，在kubernetes中主要还是使用Dynamic NFS provisioning为主, 不会直接像上面这样使用nfs</p>
<p>这里还是将3种方式都贴一下.</p>
<h3 id="使用volume"><a href="#使用volume" class="headerlink" title="使用volume"></a>使用volume</h3><p>nfs做为volume还是有好几种使用方式的:</p>
<h4 id="做为普通volume"><a href="#做为普通volume" class="headerlink" title="做为普通volume"></a>做为普通volume</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span> <span class="comment"># for versions before 1.9.0 use apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="comment"># 应用的镜像</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="comment"># 应用的内部端口</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">redis6379</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ALLOW_EMPTY_PASSWORD</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;yes&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REDIS_PASSWORD</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;redis&quot;</span>   </span><br><span class="line">        <span class="comment"># 持久化挂接位置，在docker中 </span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-persistent-storage</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="comment"># 宿主机上的目录</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-persistent-storage</span></span><br><span class="line">        <span class="attr">nfs:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/data/nfs/redis/data</span></span><br><span class="line">          <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.8</span><span class="number">.150</span></span><br></pre></td></tr></table></figure>

<h4 id="作为persistentVolume"><a href="#作为persistentVolume" class="headerlink" title="作为persistentVolume"></a>作为persistentVolume</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span> </span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pv</span> </span><br><span class="line"><span class="attr">spec:</span> </span><br><span class="line">  <span class="attr">capacity:</span> </span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span> </span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span> </span><br><span class="line">  <span class="attr">accessModes:</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span> </span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span> </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">slow</span> </span><br><span class="line">  <span class="attr">mountOptions:</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="string">hard</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="string">nfsvers=4.1</span></span><br><span class="line">  <span class="comment"># 此持久化存储卷使用nfs插件 </span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="comment"># nfs共享目录为/tmp </span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/nfs/data</span></span><br><span class="line">    <span class="comment"># nfs服务器的地址</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.5</span><span class="number">.150</span></span><br></pre></td></tr></table></figure>

<p>上面两种方式在kubernetes用nfs做为持久卷时用的还是比较少，因为需要事先在nfs中创建好相关的目录，存在一些手工操作. 重点在下.</p>
<h4 id="作为Dynamic-NFS-provisioning"><a href="#作为Dynamic-NFS-provisioning" class="headerlink" title="作为Dynamic NFS provisioning"></a>作为Dynamic NFS provisioning</h4><p>helm部署参考<a href="https://github.com/helm/charts/blob/master/stable/nfs-client-provisioner/values.yaml">nfs-client-provisioner</a></p>
<p>这里选择重要的资源对象来阐述nfs是如何实现dynamic provision功能的</p>
<p>首先来看一下storageclass,这个也没什么好说的.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-storageclass</span> <span class="comment"># IMPORTANT pvc needs to mention this name</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">nfs-test</span> <span class="comment"># name can be anything</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">archiveOnDelete:</span> <span class="string">&quot;false&quot;</span></span><br></pre></td></tr></table></figure>

<p>还会部署一个deployment资源</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pod-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-pod-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nfs-pod-provisioner-sa</span> <span class="comment"># name of service account created in rbac.yaml</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-pod-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-provisioner-v</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PROVISIONER_NAME</span> <span class="comment"># do not change</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">nfs-test</span> <span class="comment"># SAME AS PROVISONER NAME VALUE IN STORAGECLASS</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_SERVER</span> <span class="comment"># do not change</span></span><br><span class="line">              <span class="attr">value:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.5</span> <span class="comment"># Ip of the NFS SERVER</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_PATH</span> <span class="comment"># do not change</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/nfs/data</span> <span class="comment"># path to nfs directory setup</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">       <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-provisioner-v</span> <span class="comment"># same as volumemouts name</span></span><br><span class="line">         <span class="attr">nfs:</span></span><br><span class="line">           <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.5</span></span><br><span class="line">           <span class="attr">path:</span> <span class="string">/nfs/data</span></span><br></pre></td></tr></table></figure>

<p>这个deployment其实是做为一个nfs client起作用,它把整个nfs server的共享目录&#x2F;nfs&#x2F;data都挂载到了&#x2F;persistentvolumes下, 这为后面根据pvc自动生成创建pv目录有关.</p>
<p>生成一个pvc</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pvc-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">nfs-storageclass</span> <span class="comment"># SAME NAME AS THE STORAGECLASS</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span> <span class="comment">#  must be the same as PersistentVolume</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">50Mi</span></span><br></pre></td></tr></table></figure>

<p>当使用一个pod指定使用使用上面的pvc后，上面的deployment会根据pvc自动生成一个pv, pvc与该pv进行绑定, 且在nfs servers上创建相应的目录,pod将该目录挂载进了自己容器目录中，这样就实现了Dynamic  provisioning功能了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f  nfs_pvc_dynamic.yaml</span><br><span class="line">nfs-pvc-test   Bound    pvc-620ff5b1-b2df-11e9-a66a-080027db98ca   50Mi       RWX            nfs-storageclass   7s</span><br><span class="line">$ <span class="built_in">ls</span> /nfs/data/</span><br><span class="line">default-nfs-pvc-test-pvc-620ff5b1-b2df-11e9-a66a-080027db98ca</span><br></pre></td></tr></table></figure>

<p>nfs-pvc-test为pvc,  pvc-620ff5b1-b2df-11e9-a66a-080027db98ca为pv, default-nfs-pvc-test-pvc-620ff5b1-b2df-11e9-a66a-080027db98ca即为在nfs-server共享目录下创建的目录，这个目录最后被为使用该pvc的pod挂载.</p>
<p>这个deployment的源码可参考<a href="https://github.com/zdnscloud/nfs-client-provisioner/blob/master/cmd/nfs-client-provisioner/provisioner.go">nfs-client-provisioner</a>, deployment核心代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *nfsProvisioner)</span></span> Provision(options controller.VolumeOptions) (*v1.PersistentVolume, <span class="type">error</span>) &#123;</span><br><span class="line">	<span class="keyword">if</span> options.PVC.Spec.Selector != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;claim Selector is not supported&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	glog.V(<span class="number">4</span>).Infof(<span class="string">&quot;nfs provisioner: VolumeOptions %v&quot;</span>, options)</span><br><span class="line"></span><br><span class="line">	pvcNamespace := options.PVC.Namespace</span><br><span class="line">	pvcName := options.PVC.Name</span><br><span class="line"></span><br><span class="line">	pvName := strings.Join([]<span class="type">string</span>&#123;pvcNamespace, pvcName, options.PVName&#125;, <span class="string">&quot;-&quot;</span>)</span><br><span class="line"></span><br><span class="line">	fullPath := filepath.Join(mountPath, pvName) <span class="comment">// 这里指定了生成目录的格式, 其中 mountPath即为/persistentvolumes</span></span><br><span class="line">	glog.V(<span class="number">4</span>).Infof(<span class="string">&quot;creating path %s&quot;</span>, fullPath)</span><br><span class="line">	<span class="keyword">if</span> err := os.MkdirAll(fullPath, <span class="number">0777</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">&quot;unable to create directory to provision new pv: &quot;</span> + err.Error())</span><br><span class="line">	&#125;</span><br><span class="line">	os.Chmod(fullPath, <span class="number">0777</span>)</span><br><span class="line"></span><br><span class="line">	path := filepath.Join(p.path, pvName)</span><br><span class="line">	size, err := getNfsCapacity()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">&quot;unable to get nfs size to provision new pv: &quot;</span> + err.Error())</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println(<span class="string">&quot;=====&quot;</span>, size.Value())</span><br><span class="line"></span><br><span class="line">	pv := &amp;v1.PersistentVolume&#123; <span class="comment">// 根据pvc 生成pv</span></span><br><span class="line">		ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">			Name: options.PVName,</span><br><span class="line">		&#125;,</span><br><span class="line">		Spec: v1.PersistentVolumeSpec&#123;</span><br><span class="line">			PersistentVolumeReclaimPolicy: options.PersistentVolumeReclaimPolicy,</span><br><span class="line">			AccessModes:                   options.PVC.Spec.AccessModes,</span><br><span class="line">			MountOptions:                  options.MountOptions,</span><br><span class="line">			Capacity: v1.ResourceList&#123;</span><br><span class="line">				<span class="comment">//v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)],</span></span><br><span class="line">				v1.ResourceName(v1.ResourceStorage): *size,</span><br><span class="line">			&#125;,</span><br><span class="line">			PersistentVolumeSource: v1.PersistentVolumeSource&#123;</span><br><span class="line">				NFS: &amp;v1.NFSVolumeSource&#123;</span><br><span class="line">					Server:   p.server,</span><br><span class="line">					Path:     path,</span><br><span class="line">					ReadOnly: <span class="literal">false</span>,</span><br><span class="line">				&#125;,</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> pv, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>因此整个流程为:</p>
<p>pod生成使用一个pvc, 当pvc发布到集群中时, nfs-client-provisioner deployment就会根据该pvc的yaml文件获取namespace, pvc-name等信息在挂载的nfs共享目录下创建新的目录, 然后生成一个pv与该pvc绑定，pod使用该pv就相当于使用nfs server共享目录一样</p>
<p>那nfs server的共享目录是如何做到的呢?</p>
<h3 id="NFS-原理"><a href="#NFS-原理" class="headerlink" title="NFS 原理"></a>NFS 原理</h3><p>NFS 使用RPC(Remote Procedure Call)的机制进行实现，RPC使得客户端可以调用服务端的函数。同时，由于有 VFS 的存在，客户端可以像使用其它普通文件系统一样使用 NFS 文件系统。经由操作系统的内核，将 NFS 文件系统的调用请求通过 TCP&#x2F;IP 发送至服务端的 NFS 服务。NFS服务器执行相关的操作，并将操作结果返回给客户端</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200813165138.png"></p>
<p>我觉得上面那段话说的很明白, nfs实现共享存储还是通过网络将本地操作写入到nfs server端，只不过这个网络操作由RPC来完成, 同时也需要内核的支持. </p>
<p>nfs更加详细的原理，感兴趣的可以参考<a href="https://blog.51cto.com/atong/1343950">NFS原理详解</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.kubernetes.org.cn/4022.html">https://www.kubernetes.org.cn/4022.html</a></li>
<li><a href="https://blog.51cto.com/atong/1343950">https://blog.51cto.com/atong/1343950</a></li>
<li><a href="https://github.com/helm/charts/blob/master/stable/nfs-client-provisioner/values.yaml">https://github.com/helm/charts/blob/master/stable/nfs-client-provisioner/values.yaml</a></li>
<li><a href="https://github.com/zdnscloud/nfs-client-provisioner/blob/master/cmd/nfs-client-provisioner/provisioner.go">https://github.com/zdnscloud/nfs-client-provisioner/blob/master/cmd/nfs-client-provisioner/provisioner.go</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(podpreset的平替)</title>
    <url>/2023/03/30/Kubernetes-podpreset/</url>
    <content><![CDATA[<p>PodPreset是一种K8sAPI资源，用于在创建 Pod 时注入其他运行时需要的信息，这些信息包括 secrets、volume mounts、environment variables等，我们可以使用标签选择器来指定某个或某些 Pod，来将 PodPreset 预设信息应用上去。使用 PodPreset 的好处就是我们可以将一些常用 Pod 预设信息配置为模板，这样就不需要显式为每个 Pod 提供所有信息，简化 Pod 初始化配置，还能起到配置统一的效果.</p>
<span id="more"></span>

<p>但是，这么好的功能在kubernetes v1.20的版本中居然给移移了，详情<a href="https://github.com/kubernetes/kubernetes/pull/94090">issue</a>，从issue可以看出，大家对社区移除podpreset还是比较可惜的<br>不过，问题也不大，谁还没有一个平替不是, 官方支持的podpreset被移除，那么就用CRD给补回来，好在，早已有人写了一个可直接使用，这对于在集群中已经使用了podpreset功能，又想把k8s版本升级到v1.20之上的人是个福音.</p>
<p>不多说，上<a href="https://github.com/redhat-cop/podpreset-webhook">链接</a></p>
<h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><p>部署也非常简单, 将代码从github拉取下来之后</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make deploy IMG=quay.io/redhat-cop/podpreset-webhook:latest</span><br><span class="line"><span class="comment"># quay.io/redhat-cop/podpreset-webhook:latest是官方编译好的镜像</span></span><br></pre></td></tr></table></figure>
<p>成功后会将相关的资源发布到集群中.</p>
<h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><p>需要对某个namespace中的pod统一挂载多个hostpath对象</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">redhatcop.redhat.io/vlalphal</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodPreset</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pp-hostpath-dira-dirb</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">volumeMounts</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/opt/lib/dira</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">dira-volume</span></span><br><span class="line">      <span class="attr">readonly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data/dirb</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">dirb-volume</span> </span><br><span class="line">  <span class="string">volumes</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dira-volume</span></span><br><span class="line">      <span class="attr">hostPath:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/opt/lib/dira</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dirb-volume</span></span><br><span class="line">      <span class="attr">hostPath:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/data/dirb</span></span><br></pre></td></tr></table></figure>
<p>将这个yaml发布到集群中即可<br>这里要注意，该yaml是对test namespace中的所有pod都生效的，如果需要对该namespace某些pod生效，该如何操作呢?可以使用matchLabels，比如</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">redhatcop.redhat.io/vlalphal</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodPreset</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pp-hostpath-dira-dirb</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">podpreset:</span> <span class="string">enable</span> </span><br><span class="line">  <span class="string">volumeMounts</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/opt/lib/dira</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">dira-volume</span></span><br><span class="line">      <span class="attr">readonly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data/dirb</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">dirb-volume</span> </span><br><span class="line">  <span class="string">volumes</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dira-volume</span></span><br><span class="line">      <span class="attr">hostPath:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/opt/lib/dira</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dirb-volume</span></span><br><span class="line">      <span class="attr">hostPath:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/data/dirb</span></span><br></pre></td></tr></table></figure>
<p>这样的话，需要挂载hostpath的pod添加podpreset: enable label即可.</p>
<h3 id="fix"><a href="#fix" class="headerlink" title="fix"></a>fix</h3><p>不过，对于使用label selector时，podpreset-webhook有个bug会导致label不生效，我这里fix了一下，修改了lable生效逻辑<br>主要代码调整在<code>filterPodpresets</code>逻辑中:</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// filterPodPresets returns list of PodPresets which match given Pod.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">filterPodPresets</span><span class="params">(logger logr.Logger, list redhatcopv1alpha1.PodPresetList, pod *corev1.Pod)</span></span> ([]*redhatcopv1alpha1.PodPreset, <span class="type">error</span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> matchingPPs []*redhatcopv1alpha1.PodPreset</span><br><span class="line"></span><br><span class="line">  logger.Info(<span class="string">&quot;pod.GetName()=&quot;</span> + pod.GetName())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fixed this</span></span><br><span class="line">  <span class="keyword">for</span> i, pp := <span class="keyword">range</span> list.Items &#123;</span><br><span class="line"></span><br><span class="line">    selector, err := metav1.LabelSelectorAsSelector(&amp;pp.Spec.Selector)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;label selector conversion failed: %v for selector: %v&quot;</span>, pp.Spec.Selector, err)</span><br><span class="line">    &#125;</span><br><span class="line">    logger.Info(<span class="string">&quot;selector.String()=&quot;</span> + selector.String())</span><br><span class="line">    logger.Info(<span class="string">&quot;labels.Set(pod.Labels).String()=&quot;</span> + labels.Set(pod.Labels).String())</span><br><span class="line"></span><br><span class="line">    podnamerequiredvalue, found := selector.RequiresExactMatch(<span class="string">&quot;podnamerequired&quot;</span>)</span><br><span class="line">    logger.Info(<span class="string">&quot;checking if found RequiresExactMatch podnamerequired&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> found &#123;</span><br><span class="line">      logger.Info(<span class="string">&quot;podnamerequiredvalue=&quot;</span> + podnamerequiredvalue)</span><br><span class="line">      <span class="keyword">if</span> podnamerequiredvalue != pod.GetName() &#123;</span><br><span class="line">        logger.Info(<span class="string">&quot;podnamerequiredvalue not matching the pod name:&quot;</span> + pod.GetName() + <span class="string">&quot;!=&quot;</span> + podnamerequiredvalue + <span class="string">&quot;=====&gt; next loop&quot;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logger.Info(<span class="string">&quot;podnamerequiredvalue matching pod:&quot;</span> + pod.GetName() + <span class="string">&quot;==&quot;</span> + podnamerequiredvalue)</span><br><span class="line">        <span class="comment">// check if general matching</span></span><br><span class="line">        lbls := pod.Labels</span><br><span class="line">        lbls[<span class="string">&quot;podnamerequired&quot;</span>] = pod.GetName()</span><br><span class="line">        logger.Info(<span class="string">&quot;labels.Set(lbls).String()=&quot;</span> + labels.Set(lbls).String())</span><br><span class="line">        <span class="comment">// check if the pod labels match the selector</span></span><br><span class="line">        <span class="keyword">if</span> !selector.Matches(labels.Set(lbls)) &#123;</span><br><span class="line">          logger.Info(<span class="string">&quot;!selector.Matches(labels.Set(lbls)=====&gt; next loop&quot;</span>)</span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// check if the pod labels match the selector (generic case, no requirements on pod name)</span></span><br><span class="line">      <span class="keyword">if</span> !selector.Matches(labels.Set(pod.Labels)) &#123;</span><br><span class="line">        logger.Info(<span class="string">&quot;!selector.Matches(labels.Set(lbls)=====&gt; next loop&quot;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// fixed this</span></span><br><span class="line">    matchingPPs = <span class="built_in">append</span>(matchingPPs, &amp;list.Items[i])</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(matchingPPs) == <span class="number">0</span> &#123;</span><br><span class="line">    logger.Info(<span class="string">&quot;######### no final preset for pod=&quot;</span> + pod.GetName())</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> _, ppr := <span class="keyword">range</span> matchingPPs &#123;</span><br><span class="line">      logger.Info(<span class="string">&quot;##############final preset for pod=&quot;</span> + pod.GetName() + <span class="string">&quot; is name=&quot;</span> + ppr.GetName())</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> matchingPPs, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码修改后重启make deploy即可<br>最终的代码repo可见<a href="https://github.com/zhoushuke/podpreset-webhook">podpreset-webhook</a></p>
<p>完工.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/redhat-cop/podpreset-webhook">https://github.com/redhat-cop/podpreset-webhook</a></li>
<li><a href="https://github.com/zhoushuke/podpreset-webhook">https://github.com/zhoushuke/podpreset-webhook</a></li>
<li><a href="http://pwittrock.github.io/docs/tasks/inject-data-application/podpreset">http://pwittrock.github.io/docs/tasks/inject-data-application/podpreset</a></li>
<li><a href="https://blog.csdn.net/aixiaoyang168/article/details/94998390">https://blog.csdn.net/aixiaoyang168/article/details/94998390</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/pull/94090">https://github.com/kubernetes/kubernetes/pull/94090</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(kubernetes中如何实现多租户模型)</title>
    <url>/2023/07/22/Kubernetes-multi-tenancy/</url>
    <content><![CDATA[<p>某种程度上说，kubernetes是个共享的架构，旨在通过共享来降低成本,但实际中,多租户是个绕不开的话题， Kubernetes 没有终端用户或租户概念， 不过kubernetes也提供了几个特性来帮助管理不同的租户需求，社区也有一些开源实现, 作者通过实践来聊一聊kubernetes中如何实现多租户(<strong>Multi-Tenancy</strong>)模型</p>
<span id="more"></span>

<h3 id="租户类型"><a href="#租户类型" class="headerlink" title="租户类型"></a>租户类型</h3><p>在<a href="https://kubernetes.io/zh-cn/docs/concepts/security/multi-tenancy/">multi-tenantcy | Kubernetes</a>一文中提到，multi-tenantcy可以大体分为两种:  <strong>Multiple teams or Multiple customers</strong></p>
<blockquote>
<ul>
<li><p>Multiple teams: 多团队租户，一般存在于组织架构中, 有明显的层级结构，这类租户有相应的操作kubernetes集群的权限，偏开发及运维团队</p>
</li>
<li><p>Multiple customers: 指的是saas平台上的用户, 一般是终端用户，终端用户不需要能够访问kubernetes集群，kuberentes对终端用户透明。</p>
</li>
</ul>
</blockquote>
<p>但是并不一定绝对:</p>
<p>在讨论 Kubernetes 中的多租户时，“租户”没有单一的定义。 相反，租户的定义将根据讨论的是多团队还是多客户租户而有所不同。</p>
<p>在多团队使用中，租户通常是一个团队， 每个团队通常部署少量工作负载，这些工作负载会随着服务的复杂性而发生规模伸缩。 然而，“团队”的定义本身可能是模糊的， 因为团队可能被组织成更高级别的部门或细分为更小的团队。</p>
<p>相反，如果每个团队为每个新客户部署专用的工作负载，那么他们使用的是多客户租户模型。 在这种情况下，“租户”只是共享单个工作负载的一组用户。 这种租户可能大到整个公司，也可能小到该公司的一个团队。</p>
<p>在许多情况下，同一组织可能在不同的上下文中使用“租户”的两种定义。 例如，一个平台团队可能向多个内部“客户”提供安全工具和数据库等共享服务， 而 SaaS 供应商也可能让多个团队共享一个开发集群。 最后，混合架构也是可能的， 例如，某 SaaS 提供商为每个客户的敏感数据提供独立的工作负载，同时提供多租户共享的服务。</p>
<h3 id="待决问题"><a href="#待决问题" class="headerlink" title="待决问题"></a>待决问题</h3><p>总结一下多租户要解决的问题<strong>可能</strong>有:</p>
<p>这里说的是<strong>可能</strong>，原因为:<strong>不以业务需求为导向的技术落地都是空谈</strong>, 所以技术方案不存在好坏之份，只有适不适合</p>
<blockquote>
<ul>
<li><p>是否可操作集群级别资源，如 API、CRD、RBAC</p>
</li>
<li><p>配额管理是否在租户之间合理隔离</p>
</li>
</ul>
</blockquote>
<h3 id="生产需求"><a href="#生产需求" class="headerlink" title="生产需求"></a>生产需求</h3><p>这里以一个实际的综合需求展开说明:</p>
<p>需求: 公司有一个kubernetes集群,一个saas平台跑在集群中,这个saas平台提供一些工具给算法研究员，比如可以申请notebook实例(是一种独占资源), 可以发布AI任务(可以是个人任务，也可以是生产任务)，但是算力总是有限的，不可能让一个研究员无限制地占用算力，因此，<strong>需要限制某个研究员最大的资源上限</strong>, 同时又要限制每种业务所使用的算力， notebook算是一种业务类型，AI任务也算是一种业务类型</p>
<blockquote>
<ul>
<li>由于notebook是独占资源，研究员只要愿意可以启用无数个notebook实例，只要不超过设定的notebook这种业务类型的上限即可</li>
<li>对于AI任务这个业务，研究员发起的AI任务都需要丢到一个共享资源池中(会有上限，整体采用先到先得策略), 需要限制每个研究员可以发布的AI任务资源上限, 也就是说，当达到给研究员设定的上限后，研究员再发起的任务需要排队，如果整个共享资源池到达上限后,所有研究员新提交的任务都进行排队.</li>
<li>对于生产任务，资源需要尽可能倾斜</li>
</ul>
</blockquote>
<p>第2条需求听上去不太合理,先不用去质疑需求是否合理，因为<strong>需求方都是爸爸</strong></p>
<p>大多数的场景下，需求可能只到限制某个研究员最大的资源上限即可，因为给定了上限，研究员可以自己安排资源要如何分配</p>
<p>对于kubernetes来说，saas平台本身是一种业务，会有它的开发&#x2F;运维人员，这种算是团队租户，而研究员，则是saas的用户，对Kubernetes来说就是终端用户</p>
<p>这算是一个比较复杂的需求，kubernetes原生能否实现这个需求,我们一起来拆分需求</p>
<h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>kuberentes中, namespace有个resourcequota, 可以限制多种资源的使用，是一种<strong>硬隔离</strong></p>
<p>额外插一句, 这里提到多种资源，是说resourcequota并不只能够限制cpu, memory等计算资源，configmap, pvc等其实都可以限制，这里不展开,可参考<a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resourcequota</a></p>
<p>回到上面的需求, 使用namespace能否实现上面的业务需求？？</p>
<p>如果只从技术实现来讲，一定程度可以的，但是从运维友好角度来说，显然不行， <strong>OP同学会疯掉</strong></p>
<p>对于一个研究员来说，他对应多种业务，每种业务都有上限，那么就可以给这个研究员创建N个namespace， 每个namespace对应一种业务，设置上限，这样可以控制某种业务的上限使用，研究员发布的任务会落到对应的namespace中</p>
<p>但还有一个问题即共享资源池问题，研究员都在自己的namespace中,相当于将大的共享资源平摊到多个namespace中，这样的话有没有这个共享资源池影响也不大</p>
<p>这是一种很粗糙的解决方案, 一方面如果研究员基数大、业务类型多，这是个NxN的结果，且如果这个配额是个动态可变的，那么在运维友好性方面将是灾难<br>显然，只有namespace还不够，有没有更好的解决方案呢？</p>
<p>在<a href="https://kubernetes.io/zh-cn/docs/concepts/security/multi-tenancy/">kuberntes</a>中,已经有多种多租户模型,硬、软隔离都有开源实现，大体就两种思路:</p>
<ol>
<li>控制面隔离</li>
</ol>
<blockquote>
<ul>
<li><p>多集群&#x2F;多APIServer: 借助多个集群进行硬隔离, 极端地说为每个租户都创建一个集群，这样肯定所有的需求都可以得到满足，但在多集群的上层，一定会再需要一个调度层，可以将用户请求下放到对应的集群中，对规模较小的场景下不合适，这属于控制面隔离</p>
</li>
<li><p>虚拟集群:  在一个集群中，一个租户以虚拟集群的形式存在，虚拟控制面的多租户模型通过为每个租户提供专用控制面组件来扩展基于命名空间的多租户， 从而完全控制集群范围的资源和附加服务，开源实现有vcluster</p>
</li>
</ul>
</blockquote>
<p>从这里可以看出Cluster 或 Control plane 的隔离方案引入了过多的额外开销，比如每个租户需要建立独立的控制面组件，这样就降低了资源利用率；同时大量租户集群的建立，也会带来运维方面的负担</p>
<ol start="2">
<li>ns&#x2F;quota分层</li>
</ol>
<blockquote>
<ul>
<li><p>上面提到既然namespace是一种单一的资源隔离手段，如果它具备分层能力的话就会非常有用， 比如将多个命名空间组织成层次结构，并在它们之间共享某些策略和资源。 它还可以帮助你管理命名空间标签、命名空间生命周期和委托管理,并在相关命名空间之间共享资源配额，开源实现有hnc&#x2F;capsule&#x2F;volcano等</p>
</li>
<li><p>对租户的请求进行协议转换，使得每个租户看到的都是独占的 Kubernetes 集群。对于后端集群来说，多个租户实际上是利用了 Namespace 的原生隔离性机制而共享了同一个集群的资源， 这种较轻量的开源实现有kubezoo</p>
</li>
</ul>
</blockquote>
<p>另外，组织结构在公司内部普遍存在，因此这种做法可以很容易让配额随着人员或者团队的变化而变化，接下来的解决方案也是基于这种方案实现，主要调研有hnc、capsule、volcano等开源实现</p>
<p>以下开源方案能实现<strong>生产需求</strong>所述要求，主要的思路为:</p>
<p><strong>依然通过namespace来限制某个业务的资源配额，同时引入一种CRD来对研究员的总配额来限制</strong></p>
<p>或者反过来也行, 即: 通过CRD来限制某个业务的配额，再使用namespace来限制总配额</p>
<p>不管是使用哪一种方案，有一个很重要的问题需要考虑，即: 当namespace或者CRD有很多时，对于一些共性的配置，如何传播到各个namespace或者CRD资源中去？因为对于某一类业务来说，对研究员的配额可能是统一的(当然也可能有例外)，所以如果能通过继承某个要能对象来直接生成N个子对象，那运维友好性将大大提高，当然这个问题可以有多种解法，策略引擎是个很好的方式，比如kyvero、OPA等，很多对资源的传播也都是基于策略引擎开发而来</p>
<h3 id="hnc"><a href="#hnc" class="headerlink" title="hnc"></a>hnc</h3><p>hnc,<a href="https://github.com/kubernetes-sigs/hierarchical-namespaces">Hierarchical Namespace Controller</a>, 是kubernetes-sig专门为解决namespace为单一对象而出的CRD, 为namespace赋于分层的能力</p>
<p>目前的kubernetes中，ns需要集群集群员才有权限创建，且namespace为单层结构，使用hnc后，管理员可以赋于其它非管理员在一个root namespace下新建无数个sub namespace的权限</p>
<p>同时，hnc支持在root ns中定义任一对象，可将这一对象propagation(传播)到所指定的sub ns中，这样在管理上是不是就非常简单</p>
<p>所以解决上述诉求使用hnc可以这么做:</p>
<p>集群管理员以业务为管理单元，创建root ns, 将sub ns下放到业务平台上，</p>
<p>业务平台可以根据业务属性创建sub ns，比如以研究员为单位(或者以团队为单位)，限定resourcequota， 即</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root-ns-notebook <span class="comment"># define resourcequota for user in every subns</span></span><br><span class="line">  -- sub-ns-teamA-notebook <span class="comment"># define resourcequota for user in every subns</span></span><br><span class="line">      -- sub-ns-teamAnotebook-userA <span class="comment">#inherit resourcequota from tenant</span></span><br><span class="line">      -- ...</span><br></pre></td></tr></table></figure>

<p>在root ns下定义了一个teamA的二级ns， 同时在这个二级ns上定义了resourcequota，不过这个quota会被你直接传播到这个二级ns下的所有sub ns做为resourcequota，即也代表所有的user的resourcequota</p>
<p>当然，这个resourcequota的传播到哪些subns是可以通过annotation或者namesapce selector进行控制</p>
<p>同理，对于共享资源池，也可采用这种分层的ns进行控制</p>
<p>还有一个对象叫hrq(hierarchical resorucequota), 对resourcequota进行分层, 非常有用</p>
<p>由于篇幅有限，hnc的具体结节将不在本篇展开，感兴趣的可以参考作者的实践: </p>
<p><a href="https://izsk.me/2023/07/20/Kubernetes-hnc/">Kubernetes学习(hnc实现namesapce分层模型)</a></p>
<p>hnc可以很方便地解决创建namespace的权限问题，同时利用层级关系可以很容易切入到公司的组织结构，不过hnc还是偏底层了一些，没有一个以租户为单位的CRD对象，capsule由然而出.</p>
<h3 id="capsule"><a href="#capsule" class="headerlink" title="capsule"></a>capsule</h3><p>capsule基本思路与hnc基本类似，它的代码里很多特性都是基于hnc直接实现的，它有一个<strong>CRD: Tetant</strong>, 直接就可创建一个租户对象, 这个对象可以具有多种属性: 配额, 资源传播, NodePool、RBAC配置等等</p>
<p>所以解决上述诉求使用capsule可以这么做:</p>
<p>集群管理员以业务为管理单元，以notebook业务为例, 创建一个名为root-notebook-tenant对象，同时定义这个Tenant的resourcequota属性, 将创建sub namespace的权限下放到业务平台上，业务平台可以根据业务，再以研究员为单位(或者以团队为单位)来创建sub ns,  比如 namespace-nootbook-userA 属于root-notebook-tenant, 因此也将继承它的所有属性，包含resourcequota等, 即</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Root-nootbook-tenant #define resourcequota for user in every subns</span><br><span class="line">- namespace-nootbook-userA # inherit resourcequota from tenant</span><br><span class="line">- namespace-notebook-userB, # inherit resourcequota from tenant</span><br><span class="line">- ...</span><br></pre></td></tr></table></figure>

<p>tenant上的resourcequota同样可以通过annotation或者namesapce selector进行控制</p>
<p>capsule还有很多其它特性，由于篇幅有限，capsule的具体结节将不在本篇展开，感兴趣的可以参考作者的实践capsule</p>
<p><a href="https://izsk.me/2023/07/21/Kubernetes-capsule/">Kubernetes学习(capsule实现多租户模型)</a></p>
<p>总结一下，hnc很好地解决了kubernetes namespace单一层级限制问题，而capsule又基于hnc实现更直接地实现了租户对象</p>
<p>但<strong>hnc与capsule目前还不能很好地解决资源怎样分配可以更加合理问题</strong>，话说回来，这也不是它们要解决的问题，如volcano这类的调度系统，也有queue对象，都是在解决租户与资源之间关系.</p>
<p>当然，一些成熟的Kubernetes集群管理工具都会有自己的CRD来对租户进行管理, 比如rancher会有project这类的对象, kubesphere里有workspace等等</p>
<p>总之: <strong>条条大路通罗马, OP同学可以不用疯掉了</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2023/07/20/Kubernetes-hnc/">https://izsk.me/2023/07/20/Kubernetes-hnc/</a></li>
<li><a href="https://izsk.me/2023/07/21/Kubernetes-capsule/">https://izsk.me/2023/07/21/Kubernetes-capsule/</a></li>
<li><a href="https://kubernetes.io/zh-cn/docs/concepts/security/multi-tenancy/">https://kubernetes.io/zh-cn/docs/concepts/security/multi-tenancy/</a></li>
<li><a href="https://www.cnblogs.com/lianngkyle/p/15055552.html">https://www.cnblogs.com/lianngkyle/p/15055552.html</a></li>
<li><a href="https://www.kubernetes.org.cn/9127.html">https://www.kubernetes.org.cn/9127.html</a></li>
<li><a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">https://kubernetes.io/docs/concepts/policy/resource-quotas/</a></li>
<li><a href="https://github.com/kubernetes-sigs/hierarchical-namespaces">https://github.com/kubernetes-sigs/hierarchical-namespaces</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>pod的状态出现UnexpectedAdmissionError是什么鬼?</title>
    <url>/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError/</url>
    <content><![CDATA[<p>今天在排查集群一个问题时，发现相关的pod的状态为<code>UnexpectedAdmissionError</code>,在这之前从未没遇到过pod还有这种状态的，一脸好奇，在解决问题的过程中，发现越挖越深, 里面涉及到的信息也是相当的多，特此记录一下.</p>
<span id="more"></span>



<h3 id="集群信息"><a href="#集群信息" class="headerlink" title="集群信息"></a>集群信息</h3><p><code>k8s</code>版本为<code>K8s v1.15.5, 3 master + N node</code>的形式，由于业务特殊，集群中同时存在的<code>node</code>有<code>amd64</code>及<code>arm64</code>两种异构节点，但这些都不重要，重要的一点是，集群中同时存在2种<code>scheduler</code>:</p>
<ol>
<li><code>default scheduler</code>: 这个不用多说，<code>k8s</code>默认的调度器，本质上来说，是个串行的调度器</li>
<li><code>x-scheduler</code>: 自定义调度器, 用于批量去调度资源，如果有任一请求的资源不满足，其它的资源也不会调度，pod处于一直Pending状态，直到资源都满足</li>
<li>在不同的业务中会使用不现的调度器，以实现资源的合理分配，记住，<strong>集群中有两个调度器</strong>, 这个是本次问题的关键</li>
</ol>
<p>同时还要说明一点的是，这次引起问题的资源是<code>nvidia.com/gpu</code>,涉及到<strong>kubelet对于device是如何管理的</strong>，这部分是本次的重点内容</p>
<p>下述出现的资源，device，其实是一个意思，资源可能在日常中使用的较多，而官方都把资源比较是一种<code>device</code></p>
<p>慢慢道来…</p>
<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>问题现象就是工作流(这里不展开细说，简单理解就是运行完就销毁的<code>pod</code>吧)的<code>pod</code>出现了如下图的状态</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220125180525.png"></p>
<p>在作者多年丰富的工作(自吹)经验中，<code>pod</code>的状态从未出现<code>UnexpectedAdmissionError</code>这种状态，甚至作者都不知道除了常见的那几种状态外还有其它的状态，看到这作者还是有点高兴的，因为作者知道，嗯，应该会挖出个盲区</p>
<p>另外，工作流控制器返回的信息为: </p>
<p><code>Pod Update plugin resources failed due to  requested number of devices unavailable for nvidia.com/gpu. Requested: 1, Available: 0, which is unexpected</code></p>
<p>该报错在<code>kubelet</code>的日志中也有出现</p>
<p>另外，经过作者的多次尝试，发现上述问题并不总是存在，也有成功的时候</p>
<h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>看到上述的报错<code>message</code>，翻译一下:</p>
<p><strong>pod更新Plugin资源失败, 失败的原因是由于请求的nvidia.com&#x2F;gpu资源需要1个，但现在只有0个</strong></p>
<p>报错意思很明显，但作者第一时间想到的是以下3个问题:</p>
<ol>
<li>正常来讲不会出现因为资源不够而调度的情况，如果资源不够，是不应该被调度的，但从上面的报错来看，已经过了调度那个环节,因为已经配置到了<code>node</code>上</li>
<li>如果忽略问题1，就算是资源不够，也应该是<code>Pending</code>状态，而不应该是<code>UnexpectedAdmissionError</code>这种状态</li>
<li><code>pod</code>去更新plugin的什么资源，为什么需要更新？</li>
</ol>
<p>问题1跟问题2应该是同一个问题，就放一起排查吧</p>
<p>作者的猜测: 由于集群中同时存在2种调度器，对于同一<code>device</code>(比如<code>nvidia.com/gpu</code>)就可能会发生竞争关系,比如以下场景:</p>
<p>某一<code>node</code>上<code>nvidia.com/gpu</code>资源只有1个</p>
<p><code>Pod1</code>，使用了<code>default-scheduler</code>,消耗了<code>nvidia.com/gpu=1</code>，此时，<code>nvidia.com/gpu</code>为0</p>
<p><code>Pod2</code>，使用了<code>default-scheduler</code>,请求了<code>nvidia.com/gpu=1</code>，状态<code>Pending</code></p>
<p><code>Pod3</code>，使用了<code>x-scheduler</code>,请求了<code>nvidia.com/gpu=1</code>,状态<code>Pending</code></p>
<p>在特定的场景下，存在<code>pod2</code>,<code>pod3</code>都调度到这个<code>node</code>上的可能，那么在<code>pod1</code>执行完成之后，就有可能出现<code>default-scheduler</code>与<code>x-scheduler</code>同时拿到<code>nvidia.com/gpu=1</code>,对<code>Pod2</code>及<code>Pod3</code>从<code>Pending</code>状态唤醒，但是在接下来的某一时候，只会有一个<code>Pod</code>创建成功，另一个<code>Pod</code>就有可能会出现<code>UnexpectedAdmissionError</code></p>
<p><strong>如果这样的Pod越多的话,发生的可能性就越大</strong></p>
<p>因为是<code>race</code>,所以也不一定总是会发生，这也解释了【问题现】中提到的<strong>问题并不总是存在</strong>的问题</p>
<p>从后续的排查情况来看，也证实了上述猜想，开发侧对调度器引用不当，对于<code>nvidia.com/gpu</code>的资源，应该使用<code>x-scheduler</code>，但有些被调整成了<code>default-scheduler</code>也就是说，如果上面的场景，所有请求同一资源的<code>pod</code>，使用相同的<code>scheduler</code>那么就不会出现这种问题，因为相同的<code>scheduler</code>内部的加锁机制是相同的.</p>
<p>解决也很简单，<strong>对同一资源的请求，<code>schedulerName</code>设成一致即可</strong></p>
<p>问题虽然解决，但依然没有解决很多疑惑，比如:</p>
<ol>
<li><code>UnexpectedAdmissionError</code>状态怎么来的</li>
<li>pod去更新plugin的什么资源，什么时候更新等？</li>
</ol>
<p>没办法，只能去查源码了</p>
<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>首先，从<code>kubelet</code>中看到了相关的报错信息，那么就从<code>kubelet</code>开始吧, 由于环境中的<code>kubelet</code>的日志级别不高，先调整成<code>--v 4</code>,代表<code>debug</code>日志，发现以下日志:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220126203956.png"></p>
<p>结合<code>kubelet</code>侧关于<code>cm(containermanager的缩写)</code>代码,大体的调用过程: <code>scheduler(predicate.go) --&gt; kubelet(predicate.go) --&gt; manager.go </code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/v1.15.5/pkg/kubelet/lifecycle/predicate.go">predictate.go</a>中也确认存在<code>UnexpectedAdmissionError</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(w *predicateAdmitHandler)</span></span> Admit(attrs *PodAdmitAttributes) PodAdmitResult &#123;</span><br><span class="line">	node, err := w.getNodeAnyWayFunc()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		klog.Errorf(<span class="string">&quot;Cannot get Node info: %v&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span> PodAdmitResult&#123;</span><br><span class="line">			Admit:   <span class="literal">false</span>,</span><br><span class="line">			Reason:  <span class="string">&quot;InvalidNodeInfo&quot;</span>,</span><br><span class="line">			Message: <span class="string">&quot;Kubelet cannot get node info.&quot;</span>,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	admitPod := attrs.Pod</span><br><span class="line">	pods := attrs.OtherPods</span><br><span class="line">	nodeInfo := schedulernodeinfo.NewNodeInfo(pods...)</span><br><span class="line">	nodeInfo.SetNode(node)</span><br><span class="line">	<span class="comment">// ensure the node has enough plugin resources for that required in pods</span></span><br><span class="line">	<span class="keyword">if</span> err = w.pluginResourceUpdateFunc(nodeInfo, attrs); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		message := fmt.Sprintf(<span class="string">&quot;Update plugin resources failed due to %v, which is unexpected.&quot;</span>, err)</span><br><span class="line">		klog.Warningf(<span class="string">&quot;Failed to admit pod %v - %s&quot;</span>, format.Pod(admitPod), message)</span><br><span class="line">		<span class="keyword">return</span> PodAdmitResult&#123;</span><br><span class="line">			Admit:   <span class="literal">false</span>,</span><br><span class="line">			Reason:  <span class="string">&quot;UnexpectedAdmissionError&quot;</span>,</span><br><span class="line">			Message: message,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"> <span class="comment">// 省略代码 ...</span></span><br></pre></td></tr></table></figure>

<p>跟上面<code>kubelet</code>中打印出来的日志是吻合的，经过摸排发现，调用路径主要集中在<a href="https://github.com/kubernetes/kubernetes/blob/v1.15.5/pkg/kubelet/cm/devicemanager/manager.go">manager.go</a>如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Allocate --&gt; allocatePodResources --&gt; allocateContainerResources --&gt; devicesToAllocate</span><br></pre></td></tr></table></figure>

<p>最终在<code>devicesToAllocate</code>中报出<code>requested number of devices unavailable for </code>的错误一直按上述路径返向传回给<code>Allocate</code>，也就是上图中红色的部分</p>
<p>同时又可以知道，<code>predictate</code>一般属于调度相关，因此，应该是从scheduler传过来的, 从Allocate函数定义就可以看出</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span></span> Allocate(node *schedulernodeinfo.NodeInfo, attrs *lifecycle.PodAdmitAttributes)</span><br></pre></td></tr></table></figure>

<p>再根据<code>node *schedulernodeinfo.NodeInfo</code>就可一层层追到<code>scheduler</code>的代码中，由于篇幅原因，就不在这里贴了</p>
<p><code>Allocate()</code>方法作用是根据<code>scheduler</code>传来的条件为某pod分配<code>device</code>，而<code>device</code>则是根据<code>resource.limit</code>做为条件进行计算</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span></span> allocateContainerResources(pod *v1.Pod, container *v1.Container, devicesToReuse <span class="keyword">map</span>[<span class="type">string</span>]sets.String) <span class="type">error</span> &#123;</span><br><span class="line">	podUID := <span class="type">string</span>(pod.UID)</span><br><span class="line">	contName := container.Name</span><br><span class="line">	allocatedDevicesUpdated := <span class="literal">false</span></span><br><span class="line">	<span class="keyword">for</span> k, v := <span class="keyword">range</span> container.Resources.Limits &#123; <span class="comment">//根据limit进行计算</span></span><br><span class="line">		resource := <span class="type">string</span>(k)</span><br><span class="line">		needed := <span class="type">int</span>(v.Value())</span><br><span class="line">		klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;needs %d %s&quot;</span>, needed, resource) <span class="comment">//这行在kubelet代码中也出现过</span></span><br><span class="line">		<span class="keyword">if</span> !m.isDevicePluginResource(resource) &#123;</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">    <span class="comment">// 省略代码 ...</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里面有个有意思的对象:<code>deviceToReuse</code>,可重用的设备, <code>Allocate</code>调了的<code>allocatePodResources</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span></span> allocatePodResources(pod *v1.Pod) <span class="type">error</span> &#123;</span><br><span class="line">	devicesToReuse := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]sets.String)</span><br><span class="line">	<span class="keyword">for</span> _, container := <span class="keyword">range</span> pod.Spec.InitContainers &#123;</span><br><span class="line">		<span class="keyword">if</span> err := m.allocateContainerResources(pod, &amp;container, devicesToReuse); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> err</span><br><span class="line">		&#125;</span><br><span class="line">    <span class="comment">// 对于initContainer，将所分配的device不断地加入到可重用设置列表中，以便提供给container使用</span></span><br><span class="line">		m.podDevices.addContainerAllocatedResources(<span class="type">string</span>(pod.UID), container.Name, devicesToReuse)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> _, container := <span class="keyword">range</span> pod.Spec.Containers &#123;</span><br><span class="line">		<span class="keyword">if</span> err := m.allocateContainerResources(pod, &amp;container, devicesToReuse); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> err</span><br><span class="line">		&#125;</span><br><span class="line">    <span class="comment">// 而对于container，则不断地从可重用设置列表中将分配出去的设备删除</span></span><br><span class="line">		m.podDevices.removeContainerAllocatedResources(<span class="type">string</span>(pod.UID), container.Name, devicesToReuse)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>原因是<code>k8s</code>中有<code>initContainer</code>，<code>initContainer</code>可以有多个，先于<code>container</code>执行，每个<code>initContainer</code>按顺序依次执行完毕后<code>container</code>才会开始创建，而在为<code>container</code>或<code>initContainer</code>分配设备的时候会优先利用<code>deviceToReuse</code>的设备，这样可避免资源浪费</p>
<p>还有一些比较重要的功能，比如:</p>
<p><code>updateAllocatedDevices</code>函数的功能是从<code>podDevices</code>中删除所有处于终结状态的<code>pod</code>，并回收其占用的资源，所以有时会在<code>kubelet</code>的日志中看到<code>pods to be removed:xxxx</code>字样 </p>
<p><code>devicesToAllocate</code>用来生成需要向<code>plugin</code>请求的设备列表，如果可重用设备已经够用或者没有设备需求时则不向<code>plugin</code>请求分配新的设备，否则调用<code>grpc</code>向<code>plugin</code>申请分配新的设备。<br>设备分配的逻辑是首先看<code>container</code>中是否已经分配了设备，如果设备够用则返回<code>nil</code>，否则查看<code>reusableDevices</code>，取出里面的设备分配，否则根据最终缺少的设备量返回<code>healthdevice - inusedevice(m.allocatedDevices[resource])</code>，中的前<code>needed</code>个，这便是其分配设备的策略</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span></span> devicesToAllocate(podUID, contName, resource <span class="type">string</span>, required <span class="type">int</span>, reusableDevices sets.String) (sets.String, <span class="type">error</span>) &#123;</span><br><span class="line">  <span class="comment">// 省略代码...</span></span><br><span class="line"> <span class="comment">// Gets Devices in use.</span></span><br><span class="line">	devicesInUse := m.allocatedDevices[resource]</span><br><span class="line">	<span class="comment">// Gets a list of available devices.</span></span><br><span class="line">	available := m.healthyDevices[resource].Difference(devicesInUse)</span><br><span class="line">	<span class="keyword">if</span> available.Len() &lt; needed &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;requested number of devices unavailable for %s. Requested: %d, Available: %d&quot;</span>, resource, needed, available.Len())</span><br><span class="line">	&#125;</span><br><span class="line">  <span class="comment">// 省略代码... </span></span><br></pre></td></tr></table></figure>

<p>因此，<strong>报错的最终原因也是在这里，因为pod此时已经分配到了node上，但node上的可用device小于pod申请的device</strong>,导致在启动<code>container</code>时<code>predicate.go</code>报错返回</p>
<p><code>device</code>的<code>predicate</code>过程会执行二次，第一次是对<code>scheduler</code>对<code>node</code>进行筛选的时候，第二次<code>kubelet</code>在<code>container</code>启动之前会再次进行<code>device</code>的确认，而上述报错则是出现在<code>kubelet</code></p>
<p>最后除一张牛人的<code>manager.go</code>中代码调用图吧，非常清晰，<a href="https://blog.csdn.net/s812289480/article/details/84314239">原图地址</a>在这里</p>
<p><img src="https://img-blog.csdnimg.cn/20181122102312898.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3M4MTIyODk0ODA=,size_16,color_FFFFFF,t_70" alt="Allocate()"></p>
<p>到这里，其实第2个问题还是没有讲的很清楚，即<strong>kubelet怎么去管理devie-plugin资源,device-plugin注册、跟api-server同步等</strong></p>
<p>这个主要涉及到<code>kubelet</code>是如何管理<code>device</code>的，即<code>device-plugin</code>是实现原理，做为下次作业吧</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.dockone.io/article/8653">http://www.dockone.io/article/8653</a></li>
<li><a href="https://www.kubernetes.org.cn/4391.html">https://www.kubernetes.org.cn/4391.html</a></li>
<li><a href="https://www.cnblogs.com/oolo/p/11672720.html#dm-%E8%B0%83%E7%94%A8-dp-listandwatch-%E7%9A%84%E6%97%B6%E6%9C%BA">https://www.cnblogs.com/oolo/p/11672720.html#dm-%E8%B0%83%E7%94%A8-dp-listandwatch-%E7%9A%84%E6%97%B6%E6%9C%BA</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/60176">https://github.com/kubernetes/kubernetes/issues/60176</a></li>
<li><a href="https://blog.csdn.net/s812289480/article/details/84314239">https://blog.csdn.net/s812289480/article/details/84314239</a></li>
<li><a href="https://zwforrest.github.io/post/devicemanager%E5%8E%9F%E7%90%86%E5%8F%8A%E5%88%86%E6%9E%90/#allocate%E5%88%86%E9%85%8D%E8%B5%84%E6%BA%90">https://zwforrest.github.io/post/devicemanager%E5%8E%9F%E7%90%86%E5%8F%8A%E5%88%86%E6%9E%90/#allocate%E5%88%86%E9%85%8D%E8%B5%84%E6%BA%90</a></li>
<li><a href="https://github.com/kubernetes-sigs/kube-batch/issues/931">https://github.com/kubernetes-sigs/kube-batch/issues/931</a></li>
<li><a href="https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/lifecycle/predicate.go">https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/lifecycle/predicate.go</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.15.9/pkg/kubelet/cm/devicemanager/manager.go">https://github.com/kubernetes/kubernetes/blob/v1.15.9/pkg/kubelet/cm/devicemanager/manager.go</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(Kubernetes踩坑记)</title>
    <url>/2024/12/25/Kubernetes-prombles/</url>
    <content><![CDATA[<p>记录在使用Kubernetes中遇到的各种问题及解决方案, 好记性不如烂笔头</p>
<p><strong>不定期更新</strong></p>
<span id="more"></span>

<h3 id="容器启动时提示-cpuset-cpus-permission-denied"><a href="#容器启动时提示-cpuset-cpus-permission-denied" class="headerlink" title="容器启动时提示:cpuset.cpus: permission denied"></a>容器启动时提示:cpuset.cpus: permission denied</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubelet Error: unable to apply cgroup configuration: failed to write <span class="string">&quot;10-11&quot;</span>: write /sys/fs/cgroup/cpuset/kubepods/burstable/xxx/yyy/cpuset.cpus: permission denied\&quot;<span class="string">&quot;: unknown</span></span><br></pre></td></tr></table></figure>
<p>原因: 宿主机上有裸机进程占用了kubelet能够使用的cpu，由于kubelet对这些被占用的cpu不感知，导致kubelet在分配cpu时刚好又分配了这部分被占用的cpu<br>解决: 释放这些被裸机进程占用的cpu，重启Kubelet</p>
<h3 id="kubelet日志报错-unable-to-fetch-container-log-stats"><a href="#kubelet日志报错-unable-to-fetch-container-log-stats" class="headerlink" title="kubelet日志报错: unable to fetch container log stats"></a>kubelet日志报错: unable to fetch container log stats</h3><p>原因: 当将cri从docker切换为containerd后，会在kubelet pod目录下遗留cri还是docker时link文件, 会导致kubelet在运行过程中一直尝试去读取这类日志，但这些日志已经指向了不存在的路径，所以无法fetch<br>解决: 将这些目标路径不存在的link文件直接删除即可</p>
<h3 id="kubelet日志错误-Unable-to-create-endpoint-status-429"><a href="#kubelet日志错误-Unable-to-create-endpoint-status-429" class="headerlink" title="kubelet日志错误: Unable to create endpoint (status 429)"></a>kubelet日志错误: Unable to create endpoint (status 429)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to create endpoint: response status code does not match any response statuses defined <span class="keyword">for</span> this endpoint <span class="keyword">in</span> the swagger spec (status 429)</span><br></pre></td></tr></table></figure>

<p>原因: 并发创建的pod数太多触发了cilium的api-rate-limit配置上限, 更详细的说明可参考以下链接<br>解决: 参考<a href="https://izsk.me/2024/08/10/cilium-on-kubernetes-errors-apiratelimit/">cilium在kubernetes中的生产实践六(cilium排错指南)之api-rate-limit</a></p>
<h3 id="kubelet中提示-no-relationship-found-between-node-xxx-and-this-object"><a href="#kubelet中提示-no-relationship-found-between-node-xxx-and-this-object" class="headerlink" title="kubelet中提示: no relationship found between node xxx and this object"></a>kubelet中提示: no relationship found between node xxx and this object</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">User <span class="string">&quot;system:node:xxx&quot;</span> cannot list resource <span class="string">&quot;configmap&quot;</span> <span class="keyword">in</span> API group <span class="keyword">in</span> the namespace <span class="string">&quot;xxx&quot;</span>, no relationship found between node xxx and this object</span><br></pre></td></tr></table></figure>

<p>原因: 由于集群开启了Node Authorization Mode,所以节点上的kubelet能操作的权限是跟节点上运行pod相关联的，可以通过以下的方式验证</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 登录到节点kubectl</span></span><br><span class="line">kubectl --kubeconfig=/etc/kubernetes/kubelet.conf -n xxx can-i list cm/xxx</span><br><span class="line"><span class="comment"># 如果有权限则提示yes, 没有则为no</span></span><br></pre></td></tr></table></figure>

<p>解决:  如果需要访问，则可通过一个pod挂载对应的cm调度到该节点上，那么在这个node上即可通过kubelet访问到相关的cm</p>
<p>参考:  </p>
<ol>
<li><p><a href="https://enix.io/fr/blog/kubernetes-tip-and-tricks-node-authorization-mode/">Kubernetes : Le Node Authorization Mode de l&#39;API-Server</a></p>
</li>
<li><p><a href="https://blog.frognew.com/2021/05/k8s-apiserver-authorization-mode-node.html">kubernetes apiserver的node鉴权 | 青蛙小白</a></p>
</li>
</ol>
<h3 id="kube-controller-manager日志出现-unable-to-retrieve-the-complete-list-of-server-APIs-metrics-k8s-io-x2F-v1beta1"><a href="#kube-controller-manager日志出现-unable-to-retrieve-the-complete-list-of-server-APIs-metrics-k8s-io-x2F-v1beta1" class="headerlink" title="kube-controller-manager日志出现: unable to retrieve the complete list of server APIs: metrics.k8s.io&#x2F;v1beta1"></a>kube-controller-manager日志出现: unable to retrieve the complete list of server APIs: metrics.k8s.io&#x2F;v1beta1</h3><p>原因: apiservice存在Failed的service</p>
<p>解决:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get apiservice</span><br><span class="line"><span class="comment"># 查看是否存在ServiceNotFound, 删除对应的service即可</span></span><br><span class="line">kubectl delete apiservice &#123;name&#125;</span><br></pre></td></tr></table></figure>

<h3 id="kubelet启动时提示Failed-to-start-ContainerManager-failed-to-build-map-of-initial-containers-from-runtime-no-PodsandBox-found-with-Id"><a href="#kubelet启动时提示Failed-to-start-ContainerManager-failed-to-build-map-of-initial-containers-from-runtime-no-PodsandBox-found-with-Id" class="headerlink" title="kubelet启动时提示Failed to start ContainerManager failed to build map of initial containers from runtime: no PodsandBox found with Id"></a>kubelet启动时提示Failed to start ContainerManager failed to build map of initial containers from runtime: no PodsandBox found with Id</h3><p>原因: kubelet的数据目录存在脏容器数据</p>
<p>解决: 使用以下命令找到脏容器，删除后重启kubelet</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 894f35dca3eda57adef28b69acd0607efdeb34e8814e87e196bc163305576028 是上面报错中的ID</span></span><br><span class="line">docker ps -a --filter <span class="string">&quot;label=io.kubernetes.sandbox.id=894f35dca3eda57adef28b69acd0607efdeb34e8814e87e196bc163305576028&quot;</span></span><br><span class="line"><span class="comment"># 根据上述ID删除容器</span></span><br><span class="line"></span><br><span class="line">docker <span class="built_in">rm</span> ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启kubelet</span></span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<h3 id="prometheus-logs-compaction-failed-对数的性质uption-in-segment-xxxx-at-yyyy-unexpected-non-zero-byte-in-padded-page"><a href="#prometheus-logs-compaction-failed-对数的性质uption-in-segment-xxxx-at-yyyy-unexpected-non-zero-byte-in-padded-page" class="headerlink" title="prometheus logs: compaction failed, 对数的性质uption in segment xxxx at yyyy: unexpected non-zero byte in padded page"></a>prometheus logs: compaction failed, 对数的性质uption in segment xxxx at yyyy: unexpected non-zero byte in padded page</h3><p>原因: prometheus在对wal进行压缩时出现segment错误，导致创建checkout失败</p>
<p>解决: 在prometheus持久化目录下删除上述xxxx的目录，然后重启prometheus</p>
<p>重启后可能会出现unexpected gap to last checkpoint, expect: xxx, requested: yyy</p>
<p>需要将checkout目录也进行删除，然后重启prometheus实例 </p>
<h3 id="is-forbidden-User-xxx-cannot-get-resource-“services-x2F-proxy”"><a href="#is-forbidden-User-xxx-cannot-get-resource-“services-x2F-proxy”" class="headerlink" title="is forbidden: User xxx cannot get resource “services&#x2F;proxy”"></a>is forbidden: User xxx cannot get resource “services&#x2F;proxy”</h3><p>在rancher中使用非admin用户无法显示grafana的 workload metrics, 请求中提示: services http:rancher-monitoring-grafana:80 is forbidden: User xxx cannot get resource “services&#x2F;proxy” in API group “” in the namespace cattle-monitoring-system</p>
<p>原因: 需要为该用户在cattle-monitoring-system ns中授权 services&#x2F;proxy权限, 同时该用户所对应的角色需要继承 project-monitoring-view角色，这样非admin用户才能看到metrics菜单</p>
<p>参考: <a href="https://forums.rancher.com/t/cluster-member-cant-see-use-grafana-or-monitoring-stuff/15814">https://forums.rancher.com/t/cluster-member-cant-see-use-grafana-or-monitoring-stuff/15814</a></p>
<h3 id="pod状态提示UnexpectedAdmissionError"><a href="#pod状态提示UnexpectedAdmissionError" class="headerlink" title="pod状态提示UnexpectedAdmissionError"></a>pod状态提示UnexpectedAdmissionError</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220125180525.png"></p>
<p>原因: 在排查的过程中，发现这个问题涉及的东西太多，写了篇专门的文章来说明这个问题，可参考<a href="https://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError/">Kubernetes-pod-status-is-UnexpectedAdmissionError</a></p>
<h3 id="nvidia-device-plugin-提示bind-address-already-in-use"><a href="#nvidia-device-plugin-提示bind-address-already-in-use" class="headerlink" title="nvidia-device-plugin 提示bind: address already in use"></a>nvidia-device-plugin 提示bind: address already in use</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220106144313.png"></p>
<p>原因: 这个错误提示其实是有歧义的, 一般看到<code>bind: address already in use</code>都会认为是不是地址端口被占用了, 在这里其实不是，正常来讲<code>nvidia-device-plugin</code>在正常退出后会将节点上的<code>nvidia.sock</code>文件一起删除，启动时会自动创建该文件, 但如果出现退出后<code>nvidia.sock</code>文件还存在，这个时候启动<code>nvidia-device-plugin</code>就会提示上述报错</p>
<p>解决: 手动删除节点上的<code>nvidia.sock</code>,然后重启<code>nvidia-device-plugin</code>即可</p>
<h3 id="prometheus提示-x2F-metrics-x2F-resource-x2F-v1alpha1-404"><a href="#prometheus提示-x2F-metrics-x2F-resource-x2F-v1alpha1-404" class="headerlink" title="prometheus提示 &#x2F;metrics&#x2F;resource&#x2F;v1alpha1 404"></a>prometheus提示 &#x2F;metrics&#x2F;resource&#x2F;v1alpha1 404</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20211118152201.png"></p>
<p>原因: 这是因为[&#x2F;metrics&#x2F;resource&#x2F;v1alpha1]是在v1.14中才新增的特性，而当前kubelet版本为1.13</p>
<p>解决: 升级k8s的版本，这里要注意的是<strong>kubelet的版本不能为api-server的高，所以不能只升级kubelet.</strong></p>
<h3 id="Error-from-server-Forbidden-pods-“xxx”-is-forbidden-cannot-exec-into-or-attach-to-a-privileged-container"><a href="#Error-from-server-Forbidden-pods-“xxx”-is-forbidden-cannot-exec-into-or-attach-to-a-privileged-container" class="headerlink" title="Error from server (Forbidden): pods “xxx” is forbidden: cannot exec into or attach to a privileged container"></a>Error from server (Forbidden): pods “xxx” is forbidden: cannot exec into or attach to a privileged container</h3><p>原因: 排查两个方面，是否有psp，第二个是否启用了相关的admission</p>
<p>解决: 在本case中，因安全因素，开启了DenyEscalatingExec 这个admission，从api-server的配置–enable-admission-plugins中上去掉DenyEscalatingExec 即可</p>
<p>参考: <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></p>
<h3 id="kubeadm-join提示unable-to-fetch-the-kubeadm-config-ConfigMap"><a href="#kubeadm-join提示unable-to-fetch-the-kubeadm-config-ConfigMap" class="headerlink" title="kubeadm join提示unable to fetch the kubeadm-config ConfigMap"></a>kubeadm join提示unable to fetch the kubeadm-config ConfigMap</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">[<span class="string">discovery</span>] <span class="string">Successfully</span> <span class="string">established</span> <span class="string">connection</span> <span class="string">with</span> <span class="string">API</span> <span class="string">Server</span> <span class="string">&quot;xxx.xxx.xxx.xxx:16443&quot;</span></span><br><span class="line">[<span class="string">join</span>] <span class="string">Reading</span> <span class="string">configuration</span> <span class="string">from</span> <span class="string">the</span> <span class="string">cluster...</span></span><br><span class="line">[<span class="string">join</span>] <span class="attr">FYI:</span> <span class="string">You</span> <span class="string">can</span> <span class="string">look</span> <span class="string">at</span> <span class="string">this</span> <span class="string">config</span> <span class="string">file</span> <span class="string">with</span> <span class="string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span></span><br><span class="line"><span class="attr">unable to fetch the kubeadm-config ConfigMap: failed to get config map: Get https://127.0.0.1:16443/api/v1/namespaces/kube-system/configmaps/kubeadm-config: dial tcp 127.0.0.1:16443: connect:</span> <span class="string">connection</span> <span class="string">refused</span></span><br></pre></td></tr></table></figure>

<p>原因: 127.0.0.1:16443是apiserver的VIP,从报错信息来看, 对127.0.0.1:16443的访问被拒绝了, 但是在apiserver本地curl这个地址又是没问题的，还是非常诡异，可以通过以下方式解决了</p>
<p>解决: 请确认好kubeadm join时会访问的两个配置文件中的apiserver地址是否正确</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system get cm kubeadm-config -oyaml</span><br><span class="line"><span class="comment"># 其中的controlPlaneEndpoint地址</span></span><br><span class="line"></span><br><span class="line">kubectl edit cm cluster-info -oyaml -n kube-public</span><br><span class="line"><span class="comment"># 其中的server地址</span></span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://github.com/kubernetes/kubeadm/issues/1596">https://github.com/kubernetes/kubeadm/issues/1596</a></p>
<h3 id="CRD-spec-versions-Invalid-value"><a href="#CRD-spec-versions-Invalid-value" class="headerlink" title="CRD spec.versions: Invalid value"></a>CRD spec.versions: Invalid value</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210622102036.png"></p>
<p>原因: CRD yaml文件中apiVersion与versions中的版本不对应</p>
<p>参考: <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/</a></p>
<h3 id="删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象"><a href="#删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象" class="headerlink" title="删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象"></a>删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210428190009.png"></p>
<p>原因: ns处于terminating时hang住了，使用<code>--grace-period=0 -- force</code>强制删除也无效</p>
<p>解决:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存现在的ns json</span></span><br><span class="line">kubectl get ns xxxx -o json &gt; /tmp/temp.json</span><br><span class="line"><span class="comment"># 编辑temp.json，将其中的spec.finalizer字段删除保存</span></span><br><span class="line"><span class="comment"># 导出k8s访问密钥</span></span><br><span class="line"><span class="built_in">echo</span> $(kubectl config view --raw -oyaml | grep client-cert  |<span class="built_in">cut</span> -d <span class="string">&#x27; &#x27;</span> -f 6) |<span class="built_in">base64</span> -d &gt; /tmp/client.pem</span><br><span class="line"><span class="built_in">echo</span> $(kubectl config view --raw -oyaml | grep client-key-data  |<span class="built_in">cut</span> -d <span class="string">&#x27; &#x27;</span> -f 6 ) |<span class="built_in">base64</span> -d &gt; /tmp/client-key.pem</span><br><span class="line"><span class="built_in">echo</span> $(kubectl config view --raw -oyaml | grep certificate-authority-data  |<span class="built_in">cut</span> -d <span class="string">&#x27; &#x27;</span> -f 6  ) |<span class="built_in">base64</span> -d &gt; /tmp/ca.pem</span><br><span class="line"><span class="comment"># 解决namespace Terminating，根据实际情况修改&lt;namespaces&gt;</span></span><br><span class="line">curl --cert /tmp/client.pem --key /tmp/client-key.pem --cacert /tmp/ca.pem -H <span class="string">&quot;Content-Type: application/json&quot;</span> -X PUT --data-binary @/tmp/temp.json https://xxx.xxx.xxx.xxx:6443/api/v1/namespaces/&lt;namespaces&gt;/finalize</span><br></pre></td></tr></table></figure>

<h3 id="docker-启动时提示no-sockets-found-via-socket-activation"><a href="#docker-启动时提示no-sockets-found-via-socket-activation" class="headerlink" title="docker 启动时提示no sockets found via socket activation"></a>docker 启动时提示no sockets found via socket activation</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210307212429.png"></p>
<p>解决: 在start docker前先执行<code>systemctl unmask docker.socket</code>即可</p>
<h3 id="Prometheus-opening-storage-failed-invalid-block-sequence"><a href="#Prometheus-opening-storage-failed-invalid-block-sequence" class="headerlink" title="Prometheus opening storage failed: invalid block sequence"></a>Prometheus opening storage failed: invalid block sequence</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20210302200255132.png"></p>
<p>原因: 这个需要排查prometheus持久化目录中是否存在时间超出设置阈值的时间段的文件，删掉后重启即可</p>
<h3 id="kubelet提示-The-node-was-low-on-resource-ephemeral-storage"><a href="#kubelet提示-The-node-was-low-on-resource-ephemeral-storage" class="headerlink" title="kubelet提示: The node was low on resource: ephemeral-storage"></a>kubelet提示: The node was low on resource: ephemeral-storage</h3><p>原因: 节点上kubelet的配置路径超过阈值会触发驱逐，默认情况下阈值是85%</p>
<p>解决: 或者清理磁盘释放资源，或者通过可修改kubelet的配置参数<code>imagefs.available</code>来提高阈值,然后重启kubelet.</p>
<p>参考: <a href="https://cloud.tencent.com/developer/article/1456389">https://cloud.tencent.com/developer/article/1456389</a></p>
<h3 id="kubectl查看日志时提示-Error-from-server-Get-https-xxx-10250-containerLogs-spring-prod-xxx-0-xxx-dial-tcp-xxx-10250-i-x2F-o-timeout"><a href="#kubectl查看日志时提示-Error-from-server-Get-https-xxx-10250-containerLogs-spring-prod-xxx-0-xxx-dial-tcp-xxx-10250-i-x2F-o-timeout" class="headerlink" title="kubectl查看日志时提示: Error from server: Get https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx: dial tcp xxx:10250: i&#x2F;o timeout"></a>kubectl查看日志时提示: Error from server: Get <a href="https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx">https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx</a>: dial tcp xxx:10250: i&#x2F;o timeout</h3><p>原因: 目地机器的iptables对10250这个端口进行了drop，如下图</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iptables-save -L INPUT –-line-numbers</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210106185555.png"></p>
<p>解决: 删除对应的规则 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">iptables -D INPUT 10</span><br></pre></td></tr></table></figure>

<h3 id="Service解析提示-Temporary-failure-in-name-resolution"><a href="#Service解析提示-Temporary-failure-in-name-resolution" class="headerlink" title="Service解析提示 Temporary failure in name resolution"></a>Service解析提示 Temporary failure in name resolution</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201223232538.png"></p>
<p>原因: 出现这种情况很奇怪，现象显示就是域名无法解析，全格式的域名能够解析是因为在pod的&#x2F;etc&#x2F;hosts中有全域名的记录,那么问题就出在于corddns解析上，coredns从日志来看，没有任何报错，但是从pod的状态来看，虽然处于Running状态，但是0&#x2F;1可以看出coredns并未处于ready状态.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201223233150.png"></p>
<p>可以查看ep记录，会发现endpoint那一栏是空的，这也就证实了k8s把coredns的状态分为了notready状态，所以ep才没有记录，经过与其它环境比较后发现跟配置有关，最终定位在coredns的配置文件上,在插件上需要加上ready</p>
<p>解决: 在cm的配置上添加read插件，如下图</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ... 省略</span></span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        ready  <span class="comment"># 加上该行后问题解决</span></span><br><span class="line">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span><br><span class="line">          pods insecure</span><br><span class="line">          upstream /etc/resolv.conf</span><br><span class="line">          fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">       <span class="comment"># ... 省略</span></span><br></pre></td></tr></table></figure>

<p>关于coredns的ready插件的使用,可以参考<a href="https://coredns.io/plugins/ready/">这里</a></p>
<p>总结起来就是使用ready来表明当前已准备好可以接收请求，从codedns的yaml文件也可以看到有<code>livenessProbe</code></p>
<h3 id="使用Kubectl命令行时提示-Unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-Common-Name-field-use-SANs-or-temporarily-enable-Common-Name-matching-with-GODEBUG-x3D-x509ignoreCN-x3D-0"><a href="#使用Kubectl命令行时提示-Unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-Common-Name-field-use-SANs-or-temporarily-enable-Common-Name-matching-with-GODEBUG-x3D-x509ignoreCN-x3D-0" class="headerlink" title="使用Kubectl命令行时提示: Unable to connect to the server: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG&#x3D;x509ignoreCN&#x3D;0"></a>使用Kubectl命令行时提示: Unable to connect to the server: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG&#x3D;x509ignoreCN&#x3D;0</h3><p>原因: 这个跟本地的go环境有关</p>
<p>解决: 在使用kubectl前使用命令<code>export GODEBUG=x509ignoreCN=0</code>即可</p>
<h3 id="namespaces-quot-kube-system-quot-is-forbidden-this-namespace-may-not-be-deleted"><a href="#namespaces-quot-kube-system-quot-is-forbidden-this-namespace-may-not-be-deleted" class="headerlink" title="namespaces &quot;kube-system&quot; is forbidden: this namespace may not be deleted"></a>namespaces &quot;kube-system&quot; is forbidden: this namespace may not be deleted</h3><p>原因: kube-system是集群中受保护的ns, 被禁止删除，主要是防止误操作，如果需要删除的话，可以使用–force</p>
<p>参考: <a href="https://github.com/kubernetes/kubernetes/pull/62167/files">https://github.com/kubernetes/kubernetes/pull/62167/files</a></p>
<h3 id="unknown-field-volumeClaimTemplates"><a href="#unknown-field-volumeClaimTemplates" class="headerlink" title="unknown field volumeClaimTemplates"></a>unknown field volumeClaimTemplates</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201112171302.png"></p>
<p>原因: 提示这个错误的原因是资源对象是deployment, 而deployment本就是无状态的， 所以也就没有使用pv这一说法了，可以参考api</p>
<p>参考: <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps">deploymentspec-v1-apps</a></p>
<h3 id="CoreDNS提示Loop-127-0-0-1-38827-gt-53-detected-for-zone-“-”"><a href="#CoreDNS提示Loop-127-0-0-1-38827-gt-53-detected-for-zone-“-”" class="headerlink" title="CoreDNS提示Loop (127.0.0.1:38827 -&gt; :53) detected for zone “.”"></a>CoreDNS提示Loop (127.0.0.1:38827 -&gt; :53) detected for zone “.”</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201017221807.png"></p>
<p>原因: CoreDNS所在的宿主机上<code>/etc/resolv.conf</code>中存在有127.0.xx的nameserver,这样会造成解析死循环.</p>
<p>解决: 修改宿主机<code>/etc/resolv.conf</code>或者将CoreDNS的configmap中的forward修改为一个可用的地址, 如<code>8.8.8.8</code></p>
<h3 id="hostPath-volumes-are-not-allowed-to-be-used"><a href="#hostPath-volumes-are-not-allowed-to-be-used" class="headerlink" title="hostPath volumes are not allowed to be used"></a>hostPath volumes are not allowed to be used</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200909154834.png"></p>
<p>原因: 集群中存在psp禁止pod直接挂载hostpath.</p>
<p>解决: 通过添加以下的psp规则来允许或者删除存在的psp都可</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">auth-privilege-psp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">allowedHostPaths:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">pathPrefix:</span> <span class="string">/</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">      <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPorts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">9796</span></span><br><span class="line">    <span class="attr">min:</span> <span class="number">9796</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">requiredDropCapabilities:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ALL</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">      <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">RunAsAny</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">configMap</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">emptyDir</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">projected</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">secret</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">downwardAPI</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">persistentVolumeClaim</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">hostPath</span></span><br></pre></td></tr></table></figure>

<h3 id="container-has-runAsNonRoot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root"><a href="#container-has-runAsNonRoot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root" class="headerlink" title="container has runAsNonRoot and image has non-numeric user (grafana), cannot verify user is non-root"></a>container has runAsNonRoot and image has non-numeric user (grafana), cannot verify user is non-root</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200908211841.png"></p>
<p>原因: 这是由于在deploy中设置了<code>securityContext: runAsNonRoot: true</code>, 在这种情况下，当pod启动时，使用的默认用户,比如上面的grafana，k8s无法确定他是不是root用户</p>
<p>解决: 指定<code>securityContext:runAsUser: 1000</code>, 随便一个id号即可, 只要不是0(0代表root)</p>
<p>参考: <a href="https://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes">https://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes</a></p>
<h3 id="OCI-runtime-create-failed-no-such-file-or-directory"><a href="#OCI-runtime-create-failed-no-such-file-or-directory" class="headerlink" title="OCI runtime create failed: no such file or directory"></a>OCI runtime create failed: no such file or directory</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200902132758.png"></p>
<p>原因: &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pod下的数据目录已经损坏.</p>
<p>解决: 删除对应的目录即可</p>
<h3 id="镜像拉取时出现ImageInspectError"><a href="#镜像拉取时出现ImageInspectError" class="headerlink" title="镜像拉取时出现ImageInspectError"></a>镜像拉取时出现ImageInspectError</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200902123531.png"></p>
<p>原因: 这种情况下一般都是镜像损坏了</p>
<p>解决: 把相关的镜像删除后重新拉取</p>
<h3 id="kubelet日志提示-node-not-found"><a href="#kubelet日志提示-node-not-found" class="headerlink" title="kubelet日志提示: node not found"></a>kubelet日志提示: node not found</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200901183122.png"></p>
<p>原因: 这个报错只是中间过程，真正的原因在于apiserver没有启动成功，导致会一直出现这个错误</p>
<p>解决: 排查kubelet与apiserver的连通是否正常</p>
<h3 id="OCI-runtime-create-failed-executable-file-not-found-in-PATH"><a href="#OCI-runtime-create-failed-executable-file-not-found-in-PATH" class="headerlink" title="OCI runtime create failed: executable file not found in PATH"></a>OCI runtime create failed: executable file not found in PATH</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200902101139.png"></p>
<p>原因: 在path中没有nvidia-container-runtime-hook这个二进制文件，可能跟本人删除nvidia显卡驱动有关.</p>
<p>解决: nvidia-container-runtime-hook是docker nvidia的runtime文件，重新安装即可.</p>
<h3 id="Nginx-Ingress-Empty-address"><a href="#Nginx-Ingress-Empty-address" class="headerlink" title="Nginx Ingress Empty address"></a>Nginx Ingress Empty address</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get ingress</span></span><br><span class="line">NAME         HOSTS                                       ADDRESS   PORTS   AGE</span><br><span class="line">prometheus   prometheus.1box.com                                   80      31d</span><br></pre></td></tr></table></figure>

<p>会发现address中的ip是空的，而查看生产环境时却是有ip列表的.</p>
<p>原因: 这个其实不是一个错误，也不影响使用，原因在于测试环境中是不存在LoadBalance类型的svc, 如果需要address中显示ip的话需要做些额外的设置</p>
<p>解决: </p>
<ol>
<li>在nginx controller的容器中指定启动参数<code>-report-ingress-status</code></li>
<li>在nginx controller引用的configmap中添加<code>external-status-address: &quot;10.164.15.220&quot;</code></li>
</ol>
<p>这样的话,在address中变会显示<code>10.164.15.220</code>了</p>
<p>参考:</p>
<p><a href="https://github.com/nginxinc/kubernetes-ingress/issues/587">https://github.com/nginxinc/kubernetes-ingress/issues/587</a></p>
<p><a href="https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/">https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/</a></p>
<h3 id="kubelet-but-volume-paths-are-still-present-on-disk"><a href="#kubelet-but-volume-paths-are-still-present-on-disk" class="headerlink" title="kubelet: but volume paths are still present on disk"></a>kubelet: but volume paths are still present on disk</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200827183609.png"></p>
<p>原因: 这种pod已经被删除了，但是volume还存在于disk中</p>
<p>解决: 删除对应的目录<code>/var/lib/kubelet/pods/3cd73...</code></p>
<p>参考: <a href="https://github.com/longhorn/longhorn/issues/485">https://github.com/longhorn/longhorn/issues/485</a></p>
<h3 id="PLEG-is-not-healthy"><a href="#PLEG-is-not-healthy" class="headerlink" title="PLEG is not healthy"></a>PLEG is not healthy</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200827184435.png"></p>
<p>原因: 宿主机上面跑的容器太多，导致pod无法在3m钟内完成生命周期检查</p>
<p>解决:  PLEG(Pod Lifecycle Event Generator)用于kublet同步pod生命周期，本想着如果是因为时间短导致的超时，那是不是可以直接调整这个时间呢? 查看kubelet的源码发现不太行，3m时间是写在代码里的因此无法修改，当然修改再编译肯定没问题，但成本太大，所以只得优化容器的调度情况.</p>
<p>参考: <a href="https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/">https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/</a></p>
<h3 id="metrics-server-10255-connection-refused"><a href="#metrics-server-10255-connection-refused" class="headerlink" title="metrics-server: 10255 connection refused"></a>metrics-server: 10255 connection refused</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">unable to fully collect metrics:</span> [<span class="attr">unable to fully scrape metrics from source kubelet_summary:k8s-node-49:</span> <span class="string">unable</span> <span class="string">to</span> <span class="string">fetch</span> <span class="string">metrics</span> <span class="string">from</span> <span class="string">Kubelet</span> <span class="string">k8s-node-49</span> <span class="string">(xxx.xxx.xxx.49):</span> <span class="string">Get</span> <span class="string">http://xxx.xxx.xxx.49:10255/stats/summary?only_cpu_and_memory=true:</span> <span class="attr">dial tcp xxx.xxx.xxx.49:10255: connect:</span> <span class="string">connection</span> <span class="string">refused</span></span><br></pre></td></tr></table></figure>

<p>原因: 现在的k8s都默认禁用了kubelet的10255端口，出现这个错误是因此在kubelet启动命令中启用了该端口</p>
<p>解决: 将<code>- --kubelet-port=10255</code>注释</p>
<h3 id="metrics-server-no-such-host"><a href="#metrics-server-no-such-host" class="headerlink" title="metrics-server: no such host"></a>metrics-server: no such host</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">unable</span> <span class="string">to</span> <span class="string">fetch</span> <span class="string">metrics</span> <span class="string">from</span> <span class="string">Kubelet</span> <span class="string">k8s-node-234</span> <span class="string">(k8s-node-234):</span> <span class="string">Get</span> <span class="string">https://k8s-node-234:10250/stats/summary?only_cpu_and_memory=true:</span> <span class="attr">dial tcp: lookup k8s-node-234 on 10.96.0.10:53:</span> <span class="literal">no</span> <span class="string">such</span> <span class="string">host</span></span><br></pre></td></tr></table></figure>

<p>解决: 使用<code>kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</code>参数</p>
<p>参考: <a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md">https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md</a></p>
<h3 id="pod无法解析域名"><a href="#pod无法解析域名" class="headerlink" title="pod无法解析域名"></a>pod无法解析域名</h3><p>集群中新增了几台机器用于部署clickhouse用于做大数据分析，为了不让这类占用大量资源的Pod影响其它Pod，因此选择给机器打taint的形式控制该类Pod的调度, 创建Pod后发现这些Pod都会出现DNS解析异常, </p>
<p>原因； 要注意容器网络，比如这里使用的是flannel是否容忍了这些机器的taint，不然的话，flannel是无法被调度到这些机器的,因此容器间的通信会出现问题，<strong>可以将类似flannel这些的基础POD容忍所有的NoScheule与NoExecute</strong></p>
<p>解决: flannel的ds yaml中添加以下toleration，这样适用任何的场景</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoExecute</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">Exists</span></span><br></pre></td></tr></table></figure>

<h3 id="Are-you-tring-to-mount-a-directory-on-to-a-file"><a href="#Are-you-tring-to-mount-a-directory-on-to-a-file" class="headerlink" title="Are you tring to mount a directory on to a file"></a>Are you tring to mount a directory on to a file</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200430132115.png"></p>
<p>原因:  Yaml文件中使用了subPath, 但是mountPath指向了一个目录</p>
<p>解决: mountPath需要加上文件名</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200430132148.png"></p>
<h3 id="Kubernetes启动后提示slice-no-such-file-ro-directory"><a href="#Kubernetes启动后提示slice-no-such-file-ro-directory" class="headerlink" title="Kubernetes启动后提示slice: no such file ro directory"></a>Kubernetes启动后提示slice: no such file ro directory</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/41B2684F-312C-41ED-AF56-D6014C6B74E6.png"></p>
<p>原因: yum安装的kubelet默认的是cgroupfs，而docker一般默认的是systemd。但是kubernetes安装的时候建议使用systemd, kubelet跟docker的不一致, 要么修改kubelet的启动参数 , 要么修改dokcer启动参数</p>
<p>解决: </p>
<p>docker的启动参数文件为: <code>/etc/docker/daemon.json: &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd”]</code></p>
<p>kubelet的启动参数文件为: <code>/var/lib/kubelet/config.yaml:  cgroupDriver: systemd</code></p>
<h3 id="“cni0”-already-has-an-IP-address-different-from-xxx-xxxx-xxx-xxx"><a href="#“cni0”-already-has-an-IP-address-different-from-xxx-xxxx-xxx-xxx" class="headerlink" title="“cni0” already has an IP address different from xxx.xxxx.xxx.xxx"></a>“cni0” already has an IP address different from xxx.xxxx.xxx.xxx</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200430145913.png"></p>
<p>原因: 使用kubeadm reset 重复操作过, reset之后，之前flannel创建的bridge device cni0和网口设备flannel.1依然健在</p>
<p> 解决: 添加之前需要清除下网络</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">systemctl stop kubelet</span><br><span class="line">systemctl stop docker</span><br><span class="line"><span class="built_in">rm</span> -rf /var/lib/cni/</span><br><span class="line"><span class="built_in">rm</span> -rf /var/lib/kubelet/*</span><br><span class="line"><span class="built_in">rm</span> -rf /etc/cni/</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ifconfig docker0 down</span><br><span class="line">ip <span class="built_in">link</span> delete cni0</span><br><span class="line">ip <span class="built_in">link</span> delete flannel.1</span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure>

<h3 id="kubeadm初始化时提示-CPU小于2"><a href="#kubeadm初始化时提示-CPU小于2" class="headerlink" title="kubeadm初始化时提示 CPU小于2"></a>kubeadm初始化时提示 CPU小于2</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class="line">    [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br></pre></td></tr></table></figure>

<p>原因: kubeadm对资源一定的要求，如果是测试环境无所谓的话,可忽略</p>
<p>解决:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用 --ignore-preflight-errors 忽略</span><br></pre></td></tr></table></figure>

<h3 id="Unable-to-update-cni-config-no-network-found"><a href="#Unable-to-update-cni-config-no-network-found" class="headerlink" title="Unable to update cni config: no network found"></a>Unable to update cni config: no network found</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/63611F8A-F803-46F5-8792-67111E03DF91.png"></p>
<p>原因: 还未部署网络插件容器，导致在&#x2F;etc&#x2F;cni下还没有文件</p>
<p>解决: 根据实际情况部署网络插件</p>
<h3 id="while-reading-‘google-dockercfg’-metadata"><a href="#while-reading-‘google-dockercfg’-metadata" class="headerlink" title="while reading ‘google-dockercfg’ metadata"></a>while reading ‘google-dockercfg’ metadata</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/FB66ABBE-FF79-48A4-8A8B-7FDC3AED6634.png"></p>
<p>原因: 从其它机器访问上述这些url确实出现 404</p>
<p>解决: 由于是在RKE上部署k8s, 所以可能会去访问google相关的url, 不影响业务,可以忽略</p>
<h3 id="no-providers-available-to-validate-pod-request"><a href="#no-providers-available-to-validate-pod-request" class="headerlink" title="no providers available to validate pod request"></a>no providers available to validate pod request</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/ACEB6BE6-7E22-4A43-AB4A-A51E00CE9EFE.png"></p>
<p>原因: 在api-server的启动参数enable-admission中设置了PodSecrityPolicy, 但是集群中又没有任何的podsecritypolicy，因此导致整个集群都无法新建出pod</p>
<p>解决: 删除相应的podsecritypolicy即可</p>
<h3 id="unable-to-upgrade-connection-Unauthorized"><a href="#unable-to-upgrade-connection-Unauthorized" class="headerlink" title="unable to upgrade connection: Unauthorized"></a>unable to upgrade connection: Unauthorized</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/5FECFF57-F204-4ADF-A9E0-5F1D9A917194.png"></p>
<p>原因: kubelet的启动参数少了x509认证方式</p>
<p>解决: 配置证书的路径, 加上重启kubelet即可</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/447313AF-7DD6-4FB4-8F46-AE1DC468C7CA.png"></p>
<h3 id="kubectl-get-cs-提示-lt-unknown-gt"><a href="#kubectl-get-cs-提示-lt-unknown-gt" class="headerlink" title="kubectl get cs 提示&lt;unknown&gt;"></a>kubectl get cs 提示&lt;unknown&gt;</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/C73EB07F-14CD-43A7-86C0-49B4812F57A6.png"></p>
<p>原因: 这是个kubectl的bug, 跟版本相关，kubernetes有意废除get cs命令</p>
<p>解决: 目前对集群的运行无影响, 可通过加-oyaml 查看状态</p>
<h3 id="安装kubeadm时提示Depends错误"><a href="#安装kubeadm时提示Depends错误" class="headerlink" title="安装kubeadm时提示Depends错误"></a>安装kubeadm时提示Depends错误</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/2AE46262-2624-446C-909E-1FA0E76A8AD7.png"></p>
<p>原因:  跟kubeadm没多大关系, 系统安装的有问题</p>
<p>解决: 执行以下命令修复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt --fix-broken install </span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure>

<h3 id="访问service时提示Connection-refused"><a href="#访问service时提示Connection-refused" class="headerlink" title="访问service时提示Connection refused"></a>访问service时提示Connection refused</h3><p>现象: 从另一环境中把yaml文件导入到新环境后有些service访问不通</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">telnet mongodb-mst.external 27017</span><br><span class="line">Trying 10.97.135.242...</span><br><span class="line">telnet: Unable to connect to remote host: Connection refused</span><br></pre></td></tr></table></figure>

<p>首先排除了域名、端口的配置问题。</p>
<p>会发现提示连接拒绝.可以确定的是集群内的DNS是正常的.</p>
<p>那么就是通过clusterIP无法到达realserver. 查看iptables规则</p>
<p>发现提示<code>default has no endpoints --reject-with icmp-port-unreachable</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506115705.png"></p>
<p>很奇怪, 提示没有endpoints, 但是使用<code>kubectl get ep</code>又能看到ep存在且配置没有问题</p>
<p>而且这个default是怎么来的.</p>
<p>为了方便部署, 很多配置是从别的环境导出的配置, 有些service访问是没问题的, 只有少部分<code>connection refused</code></p>
<p>结比一下发现一个很有趣的问题，先来看下不正常的yaml文件:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506115805.png"></p>
<p>由于服务在集群外部署的, 因此这里使用了subset方式, 开始怀疑问题在这里, 但是后来知道这个不是重点</p>
<p>乍一看这个配置没什么问题, 部署也很正常, 但是对比正常的yaml文件，发现一个区别：</p>
<p>如果在services中的端口指定了名字, 那么在subsets中的端口也要带名字, 没有带名字的就会出现<code>connection refused</code>，这个确实之前从来没有关注过, 一个端口的情况下也不会指定名字</p>
<p>而且这面iptalbes中提示的default刚好就是这里的port name,虽然不敢相信，但是也只能试一试这个方法: 在subsets中也加了port name</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506120151.png"></p>
<p>重新部署一个，再次查看iptalbes规则 </p>
<p><code>iptables-save|grep mongodb-mst</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506120040.png"></p>
<p>OMG, 居然可行, 再看下telnet的结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Trying 10.105.116.92...</span><br><span class="line">Connected to mongodb-mst.external.svc.cluster.local.</span><br><span class="line">Escape character is <span class="string">&#x27;^]&#x27;</span>.</span><br></pre></td></tr></table></figure>

<p>访问也是没问题, 那么原因就在于:</p>
<p><strong>在service中指定了port name时, 也需要在ep中指定port name</strong></p>
<h3 id="error-converting-fieldPath-field-label-not-supported"><a href="#error-converting-fieldPath-field-label-not-supported" class="headerlink" title="error converting fieldPath: field label not supported"></a>error converting fieldPath: field label not supported</h3><p>今天遇到一个部署deployment出错的问题, yaml文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-deployment</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">4test</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">config-demo-app</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="comment"># The field we&#x27;ll use to couple our ConfigMap and Deployment</span></span><br><span class="line">        <span class="attr">configHash:</span> <span class="string">4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-demo-app</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">gcr.io/optimum-rock-145719/config-demo-app</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">envFrom:</span></span><br><span class="line">        <span class="comment"># The ConfigMap we want to use</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">configMapRef:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">        <span class="comment"># Extra-curricular: We can make the hash of our ConfigMap available at a</span></span><br><span class="line">        <span class="comment"># (e.g.) debug endpoint via a fieldRef</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CONFIG_HASH</span></span><br><span class="line">          <span class="comment">#value: &quot;4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145&quot;</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">spec.template.metadata.annotations.configHash</span></span><br></pre></td></tr></table></figure>

<p>提示以下错误:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200511180743.png"></p>
<p>会提示<code>Unsupported value:spec.template.metadata.annotations.configHash</code></p>
<p>目的很简单: container中的环境变量中引用configHash变量, 这个值是当configmap变更时比对两个不同的sha值以此达到重启pod的目的, 但fieldPath显然不支持<code>spec.template.metadata.annotations.configHash</code></p>
<p>从报错提示来看, 支持列表有<code>metadata.name, metadata.namespace, metadata.uid, spec.nodeName,spec.serviceAccountName, status.hostIp, status.PodIP, status.PodIPs</code></p>
<p>这些值用于容器中需要以下信息时可以不从k8s的apiserver中获取而是可以很方便地从这些变量直接获得</p>
<p>参考: </p>
<p><a href="https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern">https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern</a></p>
<p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2024/08/10/cilium-on-kubernetes-errors-apiratelimit/">cilium在kubernetes中的生产实践六(cilium排错指南)之api-rate-limit</a></li>
<li><a href="https://enix.io/fr/blog/kubernetes-tip-and-tricks-node-authorization-mode/">Kubernetes : Le Node Authorization Mode de l&#39;API-Server</a></li>
<li><a href="https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state">https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state</a></li>
<li><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/19317">https://github.com/kubernetes/kubernetes/issues/19317</a></li>
<li><a href="http://www.xuyasong.com/?p=1725">http://www.xuyasong.com/?p=1725</a></li>
<li><a href="https://kubernetes.io/">https://kubernetes.io/</a></li>
<li><a href="https://fuckcloudnative.io/">https://fuckcloudnative.io/</a></li>
<li><a href="https://www.cnblogs.com/breezey/p/8810039.html">https://www.cnblogs.com/breezey/p/8810039.html</a></li>
<li><a href="https://ieevee.com/tech/2018/04/25/downwardapi.html">https://ieevee.com/tech/2018/04/25/downwardapi.html</a></li>
<li><a href="https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern">https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern</a></li>
<li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps</a></li>
<li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/pull/62167/files">https://github.com/kubernetes/kubernetes/pull/62167/files</a></li>
<li><a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md">https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md</a></li>
<li><a href="https://github.com/kubernetes/kubeadm/issues/1596">https://github.com/kubernetes/kubeadm/issues/1596</a></li>
<li><a href="https://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError">https://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(为什么Pod突然就不见了?)</title>
    <url>/2021/07/26/Kubernetes-podGC/</url>
    <content><![CDATA[<p>最近发生一件很诡异的事情, 某个ns下的pods会莫名其妙地被删了, 困扰了好一阵子，排查后发现问题的起因还是挺有意思。</p>
<span id="more"></span>



<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>交代一下背景, 这些pod都是由argo-workflow发起的pod, 执行完特定的任务之后就会变成Succeeded, 如果执行时有问题，状态可能是Failed.</p>
<p>结果很直接，就是在ns下的某些状态为 Failed pod会被删掉(后来证实Succeeded状态的也会被删掉)，所以会出现尴尬的情况是想找这个pod的时候，发现这个pod却没了，之前反映过类似的问题，但一直以为是被别人删了，没有在意，但是第二次出现，感觉不是偶然</p>
<p>开发同学肯定没权限做这个事，运维侧也可以肯定没有这类操作，排查了一圈几乎可以肯定的是，<strong>不是人为的</strong>, 那不是人做的，就只能中k8s这边的某些机制触发了这个删除的操作，kubernetes可以管理千千万万的pod资源，因此gc机制是必不可少的，作者也是第一时间想到了可能是gc机制引起的.</p>
<p>在详细追踪k8s的podGC问题之前，其实还有一个嫌疑犯需要排查，那就是argo-workflow, argo-workflow做为一种任务workflow的实现方式，argo-workflow本身也可以通过<strong>CRD</strong>来检测当workflow执行到达什么状态时进行podGC, 如下图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/wecom-temp-ea679872d1ef4371071b5871e226a2c3.png"></p>
<p>但作者可以肯定的是，那些被删除的pod中并未使用argo-workflow的podGC，因此argo-workflow的嫌疑可以排除.</p>
<p>那么现在就剩k8s本身的机制了</p>
<h3 id="PodGC"><a href="#PodGC" class="headerlink" title="PodGC"></a>PodGC</h3><p>k8s中存在在各种各样的controller(感兴趣的可以看看controllermanager.go中的NewControllerInitializers中列出来的controllers对象), 每一个controller专注于解决一个方面的问题， podGC controller也是如此，专门回收pod。</p>
<p>既然pod被回收了，是不是可以从controllermanager的日志中看到什么呢?果然</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210725211611.png"></p>
<p>从上面的日志也可以证实，pod确实是controller被回收了，但是怎么个回收法呢？依据是什么,时间间隔多久等等一系列问题相继涌出</p>
<h3 id="gc-controller-go"><a href="#gc-controller-go" class="headerlink" title="gc_controller.go"></a>gc_controller.go</h3><p>源码能够得到一切答案，大多数都来自于<code>pkg/controller/podgc/gc_controller.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	<span class="comment">// gcCheckPeriod defines frequency of running main controller loop</span></span><br><span class="line">	gcCheckPeriod = <span class="number">20</span> * time.Second</span><br><span class="line">	<span class="comment">// quarantineTime defines how long Orphaned GC waits for nodes to show up</span></span><br><span class="line">	<span class="comment">// in an informer before issuing a GET call to check if they are truly gone</span></span><br><span class="line">	quarantineTime = <span class="number">40</span> * time.Second</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>首先是gc的时间间隔，很显然是20s，而且这个数值不支持从命令参数中配置</p>
<p>quarantineTime是在删除孤儿pod时等待节点ready前的时间</p>
<p>那根据什么删除的呢， 同样，在源码中给了答案</p>
<h4 id="pod-status-phase"><a href="#pod-status-phase" class="headerlink" title="pod.status.phase"></a>pod.status.phase</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gcc *PodGCController)</span></span> gcTerminated(pods []*v1.Pod) &#123;</span><br><span class="line">	terminatedPods := []*v1.Pod&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">		<span class="keyword">if</span> isPodTerminated(pod) &#123;</span><br><span class="line">			terminatedPods = <span class="built_in">append</span>(terminatedPods, pod)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	terminatedPodCount := <span class="built_in">len</span>(terminatedPods)</span><br><span class="line">	deleteCount := terminatedPodCount - gcc.terminatedPodThreshold</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> deleteCount &gt; terminatedPodCount &#123;</span><br><span class="line">		deleteCount = terminatedPodCount</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> deleteCount &lt;= <span class="number">0</span> &#123;</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	klog.Infof(<span class="string">&quot;garbage collecting %v pods&quot;</span>, deleteCount)</span><br><span class="line">	<span class="comment">// sort only when necessary</span></span><br><span class="line">	sort.Sort(byCreationTimestamp(terminatedPods))</span><br><span class="line">	<span class="keyword">var</span> wait sync.WaitGroup</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; deleteCount; i++ &#123;</span><br><span class="line">		wait.Add(<span class="number">1</span>)</span><br><span class="line">		<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(namespace <span class="type">string</span>, name <span class="type">string</span>)</span></span> &#123;</span><br><span class="line">			<span class="keyword">defer</span> wait.Done()</span><br><span class="line">			<span class="keyword">if</span> err := gcc.deletePod(namespace, name); err != <span class="literal">nil</span> &#123;</span><br><span class="line">				<span class="comment">// ignore not founds</span></span><br><span class="line">				<span class="keyword">defer</span> utilruntime.HandleError(err)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;(terminatedPods[i].Namespace, terminatedPods[i].Name)</span><br><span class="line">	&#125;</span><br><span class="line">	wait.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里的日志输出刚好也是controllermanager.go中的日志输出，主要的逻辑在如何判定一个pod是否需要被删除</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">isPodTerminated</span><span class="params">(pod *v1.Pod)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> phase := pod.Status.Phase; phase != v1.PodPending &amp;&amp; phase != v1.PodRunning &amp;&amp; phase != v1.PodUnknown &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>判断一个pod是否需要被删除，主要看一个pod的状态，在k8s，一个pod大概会有以下的状态(phases)</p>
<ul>
<li>Pending</li>
<li>Running</li>
<li>Succeeded</li>
<li>Failed</li>
<li>Unknown</li>
</ul>
<p>得到所有的pods实例，对于status.phase不等于Pending、Running、Unknown的且与terminatedPodThreshold的差值的部分的pod进行清除，会对要删除的pod的创建时间戳进行排序后删除差值个数的pod，注意这里也会把succeeded的状态pod给删除,作者对这个把succeeded状态的pod给gc了还是比较奇怪的</p>
<h4 id="gcOrphaned"><a href="#gcOrphaned" class="headerlink" title="gcOrphaned"></a>gcOrphaned</h4><p>另外，回收那些Binded的Nodes已经不存在的pods，这个没什么好说的，node都不存在了，pod也没存在的必要了</p>
<p>逻辑是调用apiserver接口，获取所有的Nodes,然后遍历所有pods，如果pod bind的NodeName不为空且不包含在刚刚获取的所有Nodes中，最后串行逐个调用gcc.deletePod删除对应的pod</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gcc *PodGCController)</span></span> gcOrphaned(pods []*v1.Pod, nodes []*v1.Node) &#123;</span><br><span class="line">	klog.V(<span class="number">4</span>).Infof(<span class="string">&quot;GC&#x27;ing orphaned&quot;</span>)</span><br><span class="line">	existingNodeNames := sets.NewString()</span><br><span class="line">	<span class="keyword">for</span> _, node := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">		existingNodeNames.Insert(node.Name)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// Add newly found unknown nodes to quarantine</span></span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">		<span class="keyword">if</span> pod.Spec.NodeName != <span class="string">&quot;&quot;</span> &amp;&amp; !existingNodeNames.Has(pod.Spec.NodeName) &#123;</span><br><span class="line">			gcc.nodeQueue.AddAfter(pod.Spec.NodeName, quarantineTime)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// Check if nodes are still missing after quarantine period</span></span><br><span class="line">	deletedNodesNames, quit := gcc.discoverDeletedNodes(existingNodeNames)</span><br><span class="line">	<span class="keyword">if</span> quit &#123;</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// Delete orphaned pods</span></span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">		<span class="keyword">if</span> !deletedNodesNames.Has(pod.Spec.NodeName) &#123;</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">		klog.V(<span class="number">2</span>).Infof(<span class="string">&quot;Found orphaned Pod %v/%v assigned to the Node %v. Deleting.&quot;</span>, pod.Namespace, pod.Name, pod.Spec.NodeName)</span><br><span class="line">		<span class="keyword">if</span> err := gcc.deletePod(pod.Namespace, pod.Name); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			utilruntime.HandleError(err)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			klog.V(<span class="number">0</span>).Infof(<span class="string">&quot;Forced deletion of orphaned Pod %v/%v succeeded&quot;</span>, pod.Namespace, pod.Name)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="gcUnscheduledTerminating"><a href="#gcUnscheduledTerminating" class="headerlink" title="gcUnscheduledTerminating"></a>gcUnscheduledTerminating</h4><p>另外，回收Unscheduled并且Terminating的pods，逻辑是遍历所有pods，过滤那些terminating(<code>pod.DeletionTimestamp != nil</code>)并且未调度成功的(pod.Spec.NodeName为空)的pods， 然后串行逐个调用gcc.deletePod删除对应的pod</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gcc *PodGCController)</span></span> gcUnscheduledTerminating(pods []*v1.Pod) &#123;</span><br><span class="line">	klog.V(<span class="number">4</span>).Infof(<span class="string">&quot;GC&#x27;ing unscheduled pods which are terminating.&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">		<span class="keyword">if</span> pod.DeletionTimestamp == <span class="literal">nil</span> || <span class="built_in">len</span>(pod.Spec.NodeName) &gt; <span class="number">0</span> &#123;</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		klog.V(<span class="number">2</span>).Infof(<span class="string">&quot;Found unscheduled terminating Pod %v/%v not assigned to any Node. Deleting.&quot;</span>, pod.Namespace, pod.Name)</span><br><span class="line">		<span class="keyword">if</span> err := gcc.deletePod(pod.Namespace, pod.Name); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			utilruntime.HandleError(err)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			klog.V(<span class="number">0</span>).Infof(<span class="string">&quot;Forced deletion of unscheduled terminating Pod %v/%v succeeded&quot;</span>, pod.Namespace, pod.Name)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="disable-podGC-controller"><a href="#disable-podGC-controller" class="headerlink" title="disable podGC controller"></a>disable podGC controller</h3><p>Podgc 是不是可以配置呢?</p>
<p>很遗憾的是，配置项不是很多，可以定义是否开启podgc controller</p>
<p>controller-manager的启动参数中有个参数:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--terminated-pod-gc-threshold int32     Default: 12500</span><br><span class="line">Number of terminated pods that can exist before the terminated pod garbage collector starts deleting terminated pods. If &lt;= 0, the terminated pod garbage collector is disabled.</span><br></pre></td></tr></table></figure>

<p>这个参数指的是在pod gc前可以保留多少个terminated pods, 默认是12500个，这个数值还是挺大的，一般集群怕是很难能到,作者由于是训练集群，存在着大量的短时间任务，因此会出现大于该值的pod，当该值小于等于0时，<strong>相当于不对terminated pods进行删除，但还是会对孤儿pod及处于terminating状态且没有绑定到node的pod进行清除</strong>.</p>
<p>参考: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></p>
<p>作者只查到这一个跟podgc相关的参数，目测好像在不修改controllermanager的情况下是没办法直接禁用podgc</p>
<p>到此，真相大白:</p>
<p>同时也给作者纠正了一个错误， 不是只有Failed状态的pod才会被gc，Successed状态的pod也会被gc掉，这个出乎作者意料之外</p>
<p>最后，想说的是，podgc跟k8s中的垃圾回收还不是一回事，虽然他们都是以controller运行，</p>
<p>podgc解决的是pod到达gc的条件后会被delete掉.</p>
<p>而garbage则解决的是<strong>对节点上的无用镜像和容器的清除</strong></p>
<p>从k8s的源码也能够看出来这两者的不同.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1097385">https://cloud.tencent.com/developer/article/1097385</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(业务程序使用serviceaccount获取kubernetes集群资源)</title>
    <url>/2019/12/11/Kubernetes-sa-handle-resources/</url>
    <content><![CDATA[<p>有时我们的业务程序也有获取kubernetes集群资源的需求, serviceaccount(简写sa)就是专门给pod用的一种鉴权机制, 在搭建Prometheus或者是fluentd时都会有创建serviceaccount的命令, 如何创建一个serviceaccount, 使其只能操作kube-system下的pods.</p>
<span id="more"></span>



<p>什么是service account? 顾名思义，相对于user account（比如：kubectl访问APIServer时用的就是user account），service account就是Pod中的Process用于访问Kubernetes API的account，它为Pod中的Process提供了一种身份标识。相比于user account的全局性权限，service account更适合一些轻量级的task，更聚焦于授权给某些特定Pod中的Process所使用.</p>
<h3 id="在kube-system下创建一个sa"><a href="#在kube-system下创建一个sa" class="headerlink" title="在kube-system下创建一个sa"></a>在kube-system下创建一个sa</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create sa pod-reader -n kube-system</span><br></pre></td></tr></table></figure>

<p>该sa会绑定一个secret, 下面需要从该secret中获取token信息</p>
<p>假如该secret 名为<code>pod-reader-token-95cwp</code></p>
<h3 id="创建role"><a href="#创建role" class="headerlink" title="创建role"></a>创建role</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>] <span class="comment"># &quot;&quot; indicates the core API group</span></span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;list&quot;</span>]</span><br></pre></td></tr></table></figure>



<h3 id="创建rolebinding"><a href="#创建rolebinding" class="headerlink" title="创建rolebinding"></a>创建rolebinding</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span> <span class="comment"># Name is case sensitive</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-reader</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>



<h3 id="创建集群信息"><a href="#创建集群信息" class="headerlink" title="创建集群信息"></a>创建集群信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl命令行下操作</span></span><br><span class="line"><span class="comment">#指定名字空间</span></span><br><span class="line">namespace=kube-system</span><br><span class="line"><span class="comment">#指定secret</span></span><br><span class="line">name=pod-reader-token-95cwp</span><br><span class="line"><span class="comment">#获取ca信息</span></span><br><span class="line">ca=$(kubectl get secret/<span class="variable">$name</span> -n <span class="variable">$namespace</span> -o jsonpath=<span class="string">&#x27;&#123;.data.ca\.crt&#125;&#x27;</span>)</span><br><span class="line"><span class="comment">#获取token</span></span><br><span class="line">token=$(kubectl get secret/<span class="variable">$name</span> -n <span class="variable">$namespace</span> -o jsonpath=<span class="string">&#x27;&#123;.data.token&#125;&#x27;</span> | <span class="built_in">base64</span> -d)</span><br><span class="line"><span class="comment">#namespace=$(kubectl get secret/$name -o jsonpath=&#x27;&#123;.data.namespace&#125;&#x27; | base64 --decode)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加集群信息，集群名为cfc</span></span><br><span class="line"><span class="comment">#kubectl config set-cluster cfc --server=https://kubernetes-cluster.local:6443 --certificate-authority-data=$&#123;ca&#125; 这种方式新版本不支持, 只能指定ca文件的路径</span></span><br><span class="line"></span><br><span class="line">kubectl config set-cluster cfc --server=https://kubernetes-cluster.local:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加上下文为cfc,对应集群为cfc,以下4步并没有先后顺序</span></span><br><span class="line">kubectl config set-context cfc --cluster=cfc --user=sa_user</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加认证信息,名为sa_user</span></span><br><span class="line">kubectl config set-credentials sa_user --token=<span class="variable">$&#123;token&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置当前的上下文为cfc, 后续所有的操作都是以这个集群的内容进行操作</span></span><br><span class="line">kubectl config use-context cfc</span><br><span class="line"></span><br><span class="line"><span class="comment">#后续可以使用以下命令切换集群</span></span><br><span class="line">kubectl config use-context kubernetes-admin</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用以下命令查看当前的集群</span></span><br><span class="line">kubectl config current-context</span><br></pre></td></tr></table></figure>

<p>我们看到service account并不复杂，只是关联了一个secret资源作为token，该token也叫service-account-<code>token</code>，该token才是真正在API Server验证(authentication)环节起作用的</p>
<h3 id="在pod-yaml中使用serviceaccount"><a href="#在pod-yaml中使用serviceaccount" class="headerlink" title="在pod yaml中使用serviceaccount"></a>在pod yaml中使用serviceaccount</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">k8s-example1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">k8s-example1</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">pod-reader</span> <span class="comment">#指定上面创建的sa</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">k8s-example1</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s/example1:latest</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>在大多数情况下，我们上线应用并没有手工地指定这个serviceAccount，那又是为何呢?</p>
<h3 id="默认的serviceaccount"><a href="#默认的serviceaccount" class="headerlink" title="默认的serviceaccount"></a>默认的serviceaccount</h3><p>在默认情况下, kubernetes会为每一个namespace创建个一个默认的sa, 而且会默认地给所有在该namespace下的pod都自动挂载到<code>/var/run/secrets/kubernetes.io/serviceaccount</code>下</p>
<p>随便查看一个运行的pod的yaml文件</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200410164958.png"></p>
<p>在pod内查看</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200410164825.png"></p>
<p>这三个文件与上面提到的service account的token中的数据是一一对应的</p>
<p>那么这个pod内的应用就能通过这3个文件与api-server通信了, 根据这个sa具有的权限获取资源.</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>service account为Pod中的Process提供了一种身份标识，在Kubernetes的身份校验(authenticating)环节，以某个service account提供身份的Pod的用户名为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">system:serviceaccount:(NAMESPACE):(SERVICEACCOUNT)</span><br></pre></td></tr></table></figure>

<p>以kube-system namespace下的“default” service account为例，使用它的Pod的username全称为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">system:serviceaccount:kube-system:default</span><br></pre></td></tr></table></figure>

<p>有了username，那么credentials呢？就是上面提到的service-account-token中的token。API Server的authenticating环节支持多种身份校验方式：client cert、bearer token、static password auth等，这些方式中有一种方式通过authenticating（Kubernetes API Server会逐个方式尝试），那么身份校验就会通过。一旦API Server发现client发起的request使用的是service account token的方式，API Server就会自动采用signed bearer token方式进行身份校验。而request就会使用携带的service account token参与验证。该token是API Server在创建service account时用API server启动参数：–service-account-key-file的值签署(sign)生成的。如果–service-account-key-file未传入任何值，那么将默认使用–tls-private-key-file的值，即API Server的私钥（server.key）。</p>
<p>通过authenticating后，API Server将根据<code>Pod username</code>所在的<code>group：system:serviceaccounts</code>和<code>system:serviceaccounts:(NAMESPACE)</code>的权限对其进行<a href="https://kubernetes.io/docs/admin/authorization/">authority</a>和<a href="https://kubernetes.io/docs/admin/admission-controllers/">admission control</a>两个环节的处理, 最终通过验证及鉴权.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/">https://kubernetes.io/docs/admin/authorization/</a></li>
<li><a href="https://kubernetes.io/docs/admin/admission-controllers/">https://kubernetes.io/docs/admin/admission-controllers/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(在云架构外运行flannel)</title>
    <url>/2020/01/07/Kubernetes-running-flannel-out-of-cloud/</url>
    <content><![CDATA[<p>该文章为翻译文档, 这里是<a href="https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e">原文地址</a>, 比较清晰地说明了flannel在VxLAN下的实现原理</p>
<span id="more"></span>



<p>在本文中，我们将详细介绍flannel及其在Kubernetes网络中的作用。我强烈建议您点击参考链接，以更好地了解flannel工作原理。</p>
<p>在<a href="https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-etcd-b332d1be474c">上一篇文章中</a>，我们介绍了<strong>etcd</strong>并对其进行了简要概述。</p>
<p>这些文章是我们的<a href="https://medium.com/@mtvallim/kubernetes-journey-up-and-running-out-of-the-cloud-introduction-f04a811c92a5">Kubernetes旅程的</a>一部分。我希望您很高兴深入了解如何从云中安装和配置Kubernetes！</p>
<h3 id="集群网络"><a href="#集群网络" class="headerlink" title="集群网络"></a>集群网络</h3><blockquote>
<p><em>“网络是Kubernetes的核心部分，但要确切地了解其预期工作方式可能会有些困难。”</em></p>
</blockquote>
<blockquote>
<p>参考：<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a></p>
</blockquote>
<p>在讨论<strong>flannel</strong>之前，了解群集的网络特性很重要。</p>
<p>在本节中，我们将主要集中于了解<strong>Pod到Pod的</strong>通信方式。</p>
<p><strong>Kubernetes网络模型</strong>遵循一些基本假设。在<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model">官方文件</a>说，<strong>Kubernetes网络模型的</strong>要求：</p>
<ul>
<li><em>Pod都可以彼此通信，而无需使用网络地址转换（NAT）。</em></li>
<li><em>节点-运行Kubernetes集群的机器。这些可以是虚拟机或物理机，或者实际上可以运行Kubernetes的任何其他机器-也可以与所有Pod通信，而无需NAT。</em></li>
<li><em>每个Pod会看到与其他Pod一样的IP</em></li>
</ul>
<h3 id="Kubernetes网络模型"><a href="#Kubernetes网络模型" class="headerlink" title="Kubernetes网络模型"></a>Kubernetes网络模型</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422150307.png"></p>
<p>每个Kubernetes节点（主节点或工作节点）都是Linux机器（VM或Bare Metal），其具有网络命名空间**-netns-<strong>以网络接口作为其主要接口。上图中的</strong>eth0**对此进行了说明。</p>
<p>仔细观察，我们可以看到每个Pod都有自己的<strong>eth</strong>接口，这意味着每个Pod都有自己的网络配置（IP，路由等）。这也意味着每个Pod都有自己的<strong>网络</strong>，因此我们有一个按网络接口分隔的网络命名空间。</p>
<p>为了说明这一点，假设您与同事一起参加在拉斯维加斯举行的会议。贵公司将赞助您参加此次会议，因此您将全部留在同一家酒店（Kubernetes节点）。这家酒店的每个房间（一个Pod）都配备了客房电话（网络接口）。</p>
<p>您可以使用客房电话与不在酒店的家人进行通讯。没有免费的午餐，因此酒店显然会向您收取使用客房电话拨打的任何外部电话的费用。为此，所有外部呼叫都要经过一个中心（<strong>eth0</strong>），该中心将对其进行注册，然后将这些呼叫重定向到外部世界。客房电话和此中央设备之间的链接类似于虚拟接口（<strong>veth</strong>）在网络中扮演的角色</p>
<p><em>“第五个设备是虚拟以太网设备。它们可以充当网络名称空间之间的隧道，以创建到另一个名称空间中的物理网络设备的桥，但也可以用作独立的网络设备。</em></p>
<p><em>veth设备总是以互连对的形式创建。”</em></p>
<blockquote>
<p>参考：<a href="http://man7.org/linux/man-pages/man4/veth.4.html">https://man7.org/linux/man-pages/man4/veth.4.html</a></p>
</blockquote>
<p>现在，您和您的同事希望例如在您离开会议或吃晚饭时让对方知道。您想节省一些钱，因此决定使用客房电话互相通话，而不是使用手机。当您在酒店办理入住手续时，礼宾部解释说，您可以通过输入<strong>＃&lt;房间号&gt;在</strong>各个房间之间互相呼叫。有一个内部中心，可以进行这种通信。这类似于<strong>cni0</strong>（如上图所示）扮演的角色，充当允许Pod到Pod通信的桥梁。</p>
<p>这家酒店可能有多栋建筑，而您的一些同事则位于第二栋建筑中。不过，礼宾部解释说根本没有问题。要与他们进行通信，您只需要在房间号之前添加建筑物号：<strong>＃ &lt;房间号&gt;<strong>。内部中央（</strong>cni</strong>）将注意到此地址不是其自己建筑物的内部地址，会将呼叫重定向到中央（<strong>eth0</strong>），这将使它们再次将呼叫重定向到另一个建筑物（另一个Kubernetes节点）中央（类似于<strong>eth0</strong>），依次将呼叫传递到它自己的内部中心，使呼叫最终在目标房间完成。有关此方案的更多技术说明，请检查<strong>在</strong>下面的“<strong>跨不同主机路由流量”</strong>部分。</p>
<p>在Kubernetes上，所有这些配置和管理都是通过<strong>CNI</strong>（容器网络接口）插件进行的</p>
<h3 id="什么是CNI"><a href="#什么是CNI" class="headerlink" title="什么是CNI"></a>什么是CNI</h3><p><strong>CNI</strong>是<strong>Container Networking Interface</strong>（<strong>容器网络接口</strong>）的缩写，它基本上是一个外部软件（模块），它实现了<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#container-network-interface-specification">规范明确定义</a>的<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#container-network-interface-specification">接口</a>，该<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#container-network-interface-specification">接口</a>允许<strong>Kubernetes</strong>执行操作以提供网络功能。</p>
<p>“<strong>每个CNI插件必须实现为由容器管理系统（例如rkt或Kubernetes）调用的可执行文件。</strong></p>
<p>CNI插件是负责将网络接口到所述容器网络命名空间（例如一个的一端VETH在主机上对），使任何必要的变更（例如，另一端附接VETH成桥）。然后，它应该通过调用适当的IPAM插件，将IP分配给该接口并设置与IP地址管理部分一致的路由。”</p>
<blockquote>
<p>参考：<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#cni-plugin">https://github.com/containernetworking/cni/blob/master/SPEC.md#cni-plugin</a></p>
</blockquote>
<h3 id="Kubernetes请求路由"><a href="#Kubernetes请求路由" class="headerlink" title="Kubernetes请求路由"></a>Kubernetes请求路由</h3><p>我们将在两种情况下更详细地说明如何在Pod之间路由流量</p>
<h4 id="在同一主机上路由流量"><a href="#在同一主机上路由流量" class="headerlink" title="在同一主机上路由流量"></a>在同一主机上路由流量</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422150719.png"></p>
<p>从<strong>Pod 4</strong>到<strong>Pod 6</strong>的逐步通信：</p>
<ol>
<li>程序包通过eth4接口离开Pod 4网络，并通过虚拟接口veth4到达根网络</li>
<li>请求到达cni0，寻找Pod 6的地址</li>
<li>请求离开cni0并重定向到veth6</li>
<li>请求通过veth6离开根网络，并通过eth6接口到达pod6网络</li>
</ol>
<h4 id="跨不同主机路由流量"><a href="#跨不同主机路由流量" class="headerlink" title="跨不同主机路由流量"></a>跨不同主机路由流量</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422151055.png"></p>
<p>从<strong>Pod 1</strong>到<strong>Pod 6</strong>的逐步通信：</p>
<ol>
<li>程序包通过eth1接口离开Pod 1网络并通过虚拟接口veth1到达根网络</li>
<li>请求离开veth1并到达cni0，寻找Pod6地址</li>
<li>请求离开cni0并重定向到eth0</li>
<li>请求从主机1离开eth0到达网关</li>
<li>请求离开网关，并通过Worker1上的eth0接口到达根网</li>
<li>请求离开eth0并到达cni0，以查找Pod6的地址；</li>
<li>请求离开cni0并重定向到veth6虚拟接口</li>
<li>请求从根netns通过veth6并到达Pod6 netns的eth6接口;</li>
</ol>
<h3 id="flannel"><a href="#flannel" class="headerlink" title="flannel"></a>flannel</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422151535.png"></p>
<p>“ Flannel是配置专为Kubernetes设计的第3层网络结构的简单方法</p>
<p><em>Flannel</em> 在每个主机上运行一个小的单一二进制代理程序，称为flanneld，并负责从更大的预配置地址空间中为每个主机分配子网租约。Flannel *直接<strong>使用Kubernetes API或etcd</strong>来存储网络配置，分配的子网以及任何辅助数据（例如主机的公共IP）。数据包使用包括VXLAN和各种云集成在内的几种后端机制之一转发。”</p>
<blockquote>
<p>参考：<a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a></p>
</blockquote>
<h4 id="backend"><a href="#backend" class="headerlink" title="backend"></a>backend</h4><blockquote>
<p><em>“</em> flannel可以与几个不同的后端进行配对。设置后，不应在运行时更改后端。</p>
<p>推荐使用<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN</a><em>。建议有经验的用户使用</em><a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw"> host-gw</a>*来提高性能并获得基础架构的支持（通常不能在云环境中使用）。建议将<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#udp">UDP</a>*仅用于调试或不支持VXLAN的非常老的内核。”</p>
</blockquote>
<blockquote>
<p>参考 <a href="%5Bgithub.com/coreos/flannel/blob/master/Documentation/backends.md">https://github.com/coreos/flannel/blob/master/Documentation/backends</a></p>
</blockquote>
<p>在本文中，我们将讨论在解决方案中将使用的<strong>VXLAN</strong>模式的内部</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200422151813.png"></p>
<h3 id="flannel-namespace"><a href="#flannel-namespace" class="headerlink" title="flannel namespace"></a>flannel namespace</h3><p>默认情况下，flannel使用CIDR <strong>10.244.0.0&#x2F;16</strong>为每个节点分配具有<strong>10.244.X.0 &#x2F; 24</strong>掩码的较小子网，而<strong>Pod</strong>将使用这些子网之一分配给给定节点的IP地址。</p>
<p>简而言之，这意味着每个节点最多可以有254个活动Pod，其中每个Pod将具有与此分配的子网不同的IP。</p>
<p>你可以看到在本说明书<strong>KUBE-flannel.yml</strong>清单，在定义<strong>网conf.json</strong>。（官方flannel repository位于下面的链接中）</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span> </span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-flannel-cfg</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">tier:</span> <span class="string">node</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">flannel</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">cni-conf.json:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;name&quot;: &quot;cbr0&quot;,</span></span><br><span class="line"><span class="string">      &quot;plugins&quot;: [</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          &quot;type&quot;: &quot;flannel&quot;,</span></span><br><span class="line"><span class="string">          &quot;delegate&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;hairpinMode&quot;: true,</span></span><br><span class="line"><span class="string">            &quot;isDefaultGateway&quot;: true</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          &quot;type&quot;: &quot;portmap&quot;,</span></span><br><span class="line"><span class="string">          &quot;capabilities&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;portMappings&quot;: true</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">      ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">net-conf.json:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span></span><br><span class="line"><span class="string">      &quot;Backend&quot;: &#123;</span></span><br><span class="line"><span class="string">        &quot;Type&quot;: &quot;vxlan&quot;</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br></pre></td></tr></table></figure>



<h4 id="Virtual-Ethernet-Devices-—-veth"><a href="#Virtual-Ethernet-Devices-—-veth" class="headerlink" title="Virtual Ethernet Devices — veth"></a>Virtual Ethernet Devices — veth</h4><blockquote>
<p>“veth设备是虚拟以太网设备。它们可以充当网络名称空间之间的隧道，以创建到另一个名称空间中的物理网络设备的桥梁，但也可以用作独立的网络设备。”</p>
</blockquote>
<blockquote>
<p>参考 <a href="http://man7.org/linux/man-pages/man4/veth.4.html">https://man7.org/linux/man-pages/man4/veth.4.html</a></p>
</blockquote>
<h4 id="Bridge-—-cni0"><a href="#Bridge-—-cni0" class="headerlink" title="Bridge — cni0"></a>Bridge — cni0</h4><p><strong>cni0</strong>是Linux网络桥接设备，所有<strong>veth</strong>设备都将连接到该桥接器，因此同一节点上的所有<strong>Pod</strong>都可以相互通信，如<strong>Kubernetes网络模型</strong>和上述酒店类比中所述	</p>
<h4 id="vxlan-flannel"><a href="#vxlan-flannel" class="headerlink" title="vxlan-flannel"></a>vxlan-flannel</h4><p><em>“</em> 虚拟可扩展局域网（VXLAN）是一种<a href="https://en.wikipedia.org/wiki/Network_virtualization">网络虚拟化</a>技术，旨在解决与大型<a href="https://en.wikipedia.org/wiki/Cloud_computing">云计算</a>部署相关的<a href="https://en.wikipedia.org/wiki/Scalability">可扩展性</a>问题。它使用类似<a href="https://en.wikipedia.org/wiki/VLAN">VLAN</a>的封装技术将<a href="https://en.wikipedia.org/wiki/OSI_model">OSI</a><a href="https://en.wikipedia.org/wiki/Layer_2">第2层</a><a href="https://en.wikipedia.org/wiki/Ethernet_frame">以太网帧</a>封装在<a href="https://en.wikipedia.org/wiki/Layer_4">第4层</a><a href="https://en.wikipedia.org/wiki/User_Datagram_Protocol">UDP</a>数据报中，并使用4789作为默认<a href="https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority">IANA</a>分配的目标UDP端口号。终止VXLAN隧道的VXLAN终结点，可以是虚拟或物理<a href="https://en.wikipedia.org/wiki/Switch_port">交换机端口</a>，称为VXLAN隧道终结点（VTEP ).”</p>
<blockquote>
<p>参考：<a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">https://en.wikipedia.org/wiki/Virtual_Extensible_LAN</a></p>
</blockquote>
<p>该接口（在我们的图中，flannel.1）的主要功能是通过覆盖来确保Pod及其网络之间的通信前提之一（记住每个节点都有一个子网，每个Pod都有该子网的IP)。</p>
<p>使用VXLAN，可以通过使两个或多个网络像在同一个网络中一样进行操作来连接两个或多个网络，也就是说，每个网络都是其自身网络的一部分，但在同一域内。</p>
<p>在以前使用的类比中，VXLAN是将一栋建筑物连接到另一栋建筑物的（物理）电话线，从而允许您所在建筑物（Node1）中一个（Pod1)与位于您的同事所在的另一座建筑物（Node2）中的Pod4通信。 </p>
<p><strong>flanneld</strong>是一个守护程序，负责使节点（及其所包含的Pod）之间的路由保持最新。</p>
<h3 id="原文地址"><a href="#原文地址" class="headerlink" title="原文地址"></a>原文地址</h3><p><a href="https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e">https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e</a></p>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(使用lxcfs实现容器资源隔离)</title>
    <url>/2023/01/09/Kubernetes-pod-xlcfs/</url>
    <content><![CDATA[<p>Linuxs利用Cgroup实现了对容器的资源限制，但在容器内部依然缺省挂载了宿主机上的procfs(内存文件系统)的&#x2F;proc目录，其包含如：meminfo, cpuinfo，stat， uptime等资源信息。一些监控工具如free&#x2F;top或遗留应用还依赖上述文件内容获取资源配置和使用情况。当它们在容器中运行时，就会把宿主机的资源状态读取出来，引起错误和不便。</p>
 <span id="more"></span>


<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>最近就出现过类似的问题:</p>
<p>在kubernetes集群中部署了一个nginx用作ingress-nginx，出现一个很奇怪的现象是，隔一段时音就会有些连接被自动断开，排查后发现问题原因在于:</p>
<p>默认情况下,nginx会根据cpu的数量来启动worker的个数, 由于nginx容器所有的节点配置非常高，cpu为256, 导致nginx中启动了256个worker， 因为集群中有很多ing对象，ingress-nginx-controller会高频率地更新配置，每次的变更都需要同步到所有worker进行加载</p>
<p>因此这种情况下，要么手工显示地指定worker的数量，当然更好的办法是让nginx能够<strong>正确地识别容器中cpu的个数</strong><br>是否可行呢？</p>
<h3 id="lxcfs"><a href="#lxcfs" class="headerlink" title="lxcfs"></a>lxcfs</h3><p>当然是可以的，<a href="https://github.com/lxc/lxcfs">lxcfs</a>作为一种CNCF推荐的方式,用于实现容器中的资源隔离, 主要有以下几种:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/proc/cpuinfo</span><br><span class="line">/proc/diskstats</span><br><span class="line">/proc/meminfo</span><br><span class="line">/proc/stat</span><br><span class="line">/proc/swaps</span><br><span class="line">/proc/uptime</span><br><span class="line">/proc/slabinfo</span><br><span class="line">/sys/devices/system/cpu/online</span><br></pre></td></tr></table></figure>
<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p>总体说起来很简单，当容器启动时，&#x2F;proc&#x2F;xxx会被挂载成host上lxcfs的目录。当请求读取&#x2F;proc&#x2F;meminfo的信息时，请求就会被导向lxcfs，而lxcfs就会通过cgroup的信息来返回正确的值给容器内的请求方</p>
<h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230109192434.png"></p>
<h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>LXCFS是基于FUSE(filesystems in user space)实现而成的一套用户态文件系统，和其他文件系统最本质的区别在于，文件系统通过用户态程序和内核FUSE模块交互完成。Linux内核从2.6.14版本开始通过FUSE模块支持在用户空间实现文件系统。通过LXCFS的源码可以看到，LXCFS主要通过调用底层fuse的lib库libfuse和内核模块fuse交互实现成一个用户态的文件系统。此外，LXCFS涉及到对cgroup文件系统的管理则是通过cgmanager用户态程序实现<br>FUSE的实现，感兴趣的可以看看<a href="https://zhuanlan.zhihu.com/p/106719192">5分钟搞懂用户空间文件系统FUSE工作原理</a></p>
<h3 id="lxcfs-on-kubernetes"><a href="#lxcfs-on-kubernetes" class="headerlink" title="lxcfs on kubernetes"></a>lxcfs on kubernetes</h3><p>在kubernetes中，已经有相应的解决方案<a href="https://github.com/cndoit18/lxcfs-on-kubernetes">lxcfs-on-kubernetes</a><br>P.S. 推荐一个fork版本,去除了对cert-manager的依赖，作者亲测有效, 项目地址<a href="https://github.com/AEGQ/lxcfs-on-kubernetes/commits/master">AEGQ lxcfs-on-kubernetes</a>,<br>lxcfs-on-kubernetes安装方式也非常简单,helm几乎不需要改动即可发布，分为manager跟agent两个程序<br>manager就是一个webhook, 当有请求到达时，为相应的容器挂载lxcfs目录<br>agent就是一个lxcfs二进制程序, 做为daemonset运行在宿主机上</p>
<p>需要通过给namespace打label来开启自动挂载的功能</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl label namespace default mount-lxcfs=enabled</span><br></pre></td></tr></table></figure>

<p>重要的是， 如果pod中没有指定limit,则看到的还是node上所有的资源，因此，<strong>必须指定limit，才能获取到正确的cgroup</strong></p>
<p>这样，发布到default里的pod都将得到正确的资源值</p>
<p><strong>集群安全无小事，不放过每处细节</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/lxc/lxcfs">https://github.com/lxc/lxcfs</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/106719192">https://zhuanlan.zhihu.com/p/106719192</a></li>
<li><a href="https://github.com/cndoit18/lxcfs-on-kubernetes">https://github.com/cndoit18/lxcfs-on-kubernetes</a></li>
<li><a href="https://www.yisu.com/zixun/9857.html">https://www.yisu.com/zixun/9857.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(rancher local path dynamic provisioner源码分析)</title>
    <url>/2020/08/17/Kubernetes-rancher-local-path-dymanic-provisioner-source-read/</url>
    <content><![CDATA[<p>之前在rancher上实战了一下local path的持久卷provisioner， 也读一读它的源码加深下映像.</p>
<span id="more"></span>



<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>还是先贴一下配置文件:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rancher.io/local-path</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">local-path-storage</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.json:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">                &quot;nodePathMap&quot;:[</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                        &quot;node&quot;:&quot;DEFAULT_PATH_FOR_NON_LISTED_NODES&quot;,</span></span><br><span class="line"><span class="string">                        &quot;paths&quot;:[&quot;/data&quot;]</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string">                ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">setup:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        mkdir -m 0777 -p $&#123;path&#125;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">teardown:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">        #!/bin/sh</span></span><br><span class="line"><span class="string">        path=$1</span></span><br><span class="line"><span class="string">        rm -rf $&#123;path&#125;</span></span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/rancher/local-path-provisioner">引用</a></p>
<p>源代码关键的就是两个go文件，一个main.go, 一个provisioner.go, 接下来分别分析</p>
<h3 id="main-go"><a href="#main-go" class="headerlink" title="main.go"></a>main.go</h3><p>main.go中大多数都是在进行一些逻辑的判断, 比如解析启动参数，解析configmap配置等.</p>
<p>比较重要的关键代码:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">startDaemon</span><span class="params">(c *cli.Context)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  provisioner, err := NewProvisioner(stopCh, kubeClient, configFile, namespace, helperImage, configMapName)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	pc := pvController.NewProvisionController(</span><br><span class="line">		kubeClient,</span><br><span class="line">		provisionerName,</span><br><span class="line">		provisioner,</span><br><span class="line">		serverVersion.GitVersion,</span><br><span class="line">	)</span><br><span class="line">	logrus.Debug(<span class="string">&quot;Provisioner started&quot;</span>)</span><br><span class="line">	pc.Run(stopCh)</span><br><span class="line">	logrus.Debug(<span class="string">&quot;Provisioner stopped&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里引用了<code>pvController.NewProvisionController</code>，这个函数是控制循环，比较复杂，本人太菜，有些看不透，可能还是对k8s的控制循环的理解不到位, 感兴趣的可以自行分析，源代码<a href="https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/controller.go">controller.go</a></p>
<p>简单理解就是这里启动了Provisioner的控制循环，这个控制循环主要检测的是provisioner对象, 这里又涉及到2个关键的接口</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Provisioner <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">// Provision creates a volume i.e. the storage asset and returns a PV object</span></span><br><span class="line">	<span class="comment">// for the volume</span></span><br><span class="line">	Provision(ProvisionOptions) (*v1.PersistentVolume, <span class="type">error</span>)</span><br><span class="line">	<span class="comment">// Delete removes the storage asset that was created by Provision backing the</span></span><br><span class="line">	<span class="comment">// given PV. Does not delete the PV object itself.</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// May return IgnoredError to indicate that the call has been ignored and no</span></span><br><span class="line">	<span class="comment">// action taken.</span></span><br><span class="line">	Delete(*v1.PersistentVolume) <span class="type">error</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>还有</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ProvisionOptions <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">// StorageClass is a reference to the storage class that is used for</span></span><br><span class="line">	<span class="comment">// provisioning for this volume</span></span><br><span class="line">	StorageClass *storageapis.StorageClass</span><br><span class="line"></span><br><span class="line">	<span class="comment">// PV.Name of the appropriate PersistentVolume. Used to generate cloud</span></span><br><span class="line">	<span class="comment">// volume name.</span></span><br><span class="line">	PVName <span class="type">string</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// PVC is reference to the claim that lead to provisioning of a new PV.</span></span><br><span class="line">	<span class="comment">// Provisioners *must* create a PV that would be matched by this PVC,</span></span><br><span class="line">	<span class="comment">// i.e. with required capacity, accessMode, labels matching PVC.Selector and</span></span><br><span class="line">	<span class="comment">// so on.</span></span><br><span class="line">	PVC *v1.PersistentVolumeClaim</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Node selected by the scheduler for the volume.</span></span><br><span class="line">	SelectedNode *v1.Node</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/volume.go">引用</a></p>
<p>这两个对象主要在provisioner.go中引用</p>
<h3 id="provisioner-go"><a href="#provisioner-go" class="headerlink" title="provisioner.go"></a>provisioner.go</h3><p><code>LocalPathProvisioner</code>实现了<code>Provisioner</code>接口</p>
<p>主要的就是<code>Provision</code>方法，关键代码:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *LocalPathProvisioner)</span></span> Provision(opts pvController.ProvisionOptions) (*v1.PersistentVolume, <span class="type">error</span>) &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  basePath, err := p.getRandomPathOnNode(node.Name) <span class="comment">// 由于支持在一个node上指定多个目录用于挂载，因此随机选择一个</span></span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	name := opts.PVName</span><br><span class="line">	folderName := strings.Join([]<span class="type">string</span>&#123;name, opts.PVC.Namespace, opts.PVC.Name&#125;, <span class="string">&quot;_&quot;</span>)</span><br><span class="line"></span><br><span class="line">	path := filepath.Join(basePath, folderName)</span><br><span class="line">	logrus.Infof(<span class="string">&quot;Creating volume %v at %v:%v&quot;</span>, name, node.Name, path)</span><br><span class="line"></span><br><span class="line">	createCmdsForPath := []<span class="type">string</span>&#123;  <span class="comment">//这里指定需要helpPod执行的命令，一个是创建，一个是删除，这个脚本在configmap中定义</span></span><br><span class="line">		<span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line">		<span class="string">&quot;/script/setup&quot;</span>,</span><br><span class="line">	&#125;</span><br><span class="line">  <span class="comment">// 这里我创建了个busybox容器根据path来创建目录</span></span><br><span class="line">	<span class="keyword">if</span> err := p.createHelperPod(ActionTypeCreate, createCmdsForPath, name, path, node.Name); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>追一下<code>createhelperPod</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *LocalPathProvisioner)</span></span> createHelperPod(action ActionType, cmdsForPath []<span class="type">string</span>, name, path, node <span class="type">string</span>) (err <span class="type">error</span>) &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  Containers: []v1.Container&#123;</span><br><span class="line">				&#123;</span><br><span class="line">					Name:    <span class="string">&quot;local-path-&quot;</span> + <span class="type">string</span>(action),</span><br><span class="line">					Image:   p.helperImage,</span><br><span class="line">					Command: <span class="built_in">append</span>(cmdsForPath, filepath.Join(<span class="string">&quot;/data/&quot;</span>, volumeDir)), <span class="comment">// 在这里定义了当容器启动后需要执行的命令，根据上面的参数，这里转化成mkdir -p /data/xxxx</span></span><br><span class="line">					VolumeMounts: []v1.VolumeMount&#123;</span><br><span class="line">						&#123;</span><br><span class="line">							Name:      <span class="string">&quot;data&quot;</span>,</span><br><span class="line">							ReadOnly:  <span class="literal">false</span>,</span><br><span class="line">							MountPath: <span class="string">&quot;/data/&quot;</span>, <span class="comment">// 容器会将Node上指定的路径挂载到自己的/data目录下然后进行创建pv目录</span></span><br><span class="line">						&#125;,</span><br><span class="line">						&#123;</span><br><span class="line">							Name:      <span class="string">&quot;script&quot;</span>,</span><br><span class="line">							ReadOnly:  <span class="literal">false</span>,</span><br><span class="line">							MountPath: <span class="string">&quot;/script&quot;</span>,</span><br><span class="line">						&#125;,</span><br><span class="line">					&#125;,</span><br><span class="line">					ImagePullPolicy: v1.PullIfNotPresent,</span><br><span class="line">				&#125;,</span><br><span class="line">			&#125;,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个方法中还有判断容器运行是否成功的语句, 通过这种方式，pv目录就创建成功了。</p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>所以整个流程变成: 集群管理员只需要将local-path-dymanic发布到集群中, 如果需要使用持久化存储，只需要声明pvc， 指定local-path-dymanic的storageClass，local-path-dymanic则会自动在调度的Node上新建出pv目录，pv pvc bound，整个流程完成.</p>
<p>具体的使用可<a href="https://izsk.me/2020/07/24/Kubernetes-Rancher-local-path-provisioner/">参考</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2020/07/24/Kubernetes-Rancher-local-path-provisioner/">https://izsk.me/2020/07/24/Kubernetes-Rancher-local-path-provisioner/</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/controller.go">https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/controller.go</a></li>
<li><a href="https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/volume.go">https://github.com/rancher/local-path-provisioner/blob/master/vendor/sigs.k8s.io/sig-storage-lib-external-provisioner/controller/volume.go</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(service机制)</title>
    <url>/2019/10/15/Kubernetes-service-theory/</url>
    <content><![CDATA[<p>service在kubernetes是一个比较核心的概念, 得益于service机制，才让业务可以don’t care后端real pod的IP 可动态变化. 可以说,<strong>service是一组具有提供相同能力pod的代理.</strong></p>
<span id="more"></span>



<p>在分析流量如何转发之前还是需要稍微带一下kubernetes的service基本概念</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a><strong>基本概念</strong></h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121182950876.png"></p>
<blockquote>
<ul>
<li>Service: 解决容器IP动态属性, 实为一个代理服务.</li>
<li>Endpoint( Endpoint Controller): 是k8s集群中的一个资源对象,存储在etcd中,用来记录一个service对应的所有pod的访问地址<ul>
<li>负责生成和维护所有endpoint对象的控制器</li>
<li>负责监听service和对应pod的变化</li>
<li>监听到service被删除，则删除和该service同名的endpoint对象</li>
<li>监听到新的service被创建，则根据新建service信息获取相关pod列表，然后创建对应endpoint对象</li>
<li>监听到service被更新，则根据更新后的service信息获取相关pod列表，然后更新对应endpoint对象</li>
<li>监听到pod事件，则更新对应的service的endpoint对象，将podIP记录到endpoint中</li>
</ul>
</li>
<li>Kube-proxy:<ul>
<li>kube-proxy负责service的实现，即实现了k8s内部从pod&lt;&#x3D;&#x3D;&gt;service和外部从node port到service的访问。</li>
<li>kube-proxy采用iptables的方式配置负载均衡，基于iptables的kube-proxy的主要职责包括两大块<ul>
<li>一块是侦听service更新事件，并更新service相关的iptables规则，</li>
<li>一块是侦听endpoint更新事件，更新endpoint相关的iptables规则（如 KUBE-SVC-链中的规则），然后将包请求转入endpoint对应的Pod。</li>
</ul>
</li>
<li>如果某个service尚没有Pod创建，那么针对此service的请求将会被drop掉</li>
</ul>
</li>
</ul>
</blockquote>
<p><code>endpoint只是维护service到pod列表的映射关系，而这种映射关系的访问链路是通过 kube-proxy实现的.</code></p>
<p>目前业务中用到了以下几种svc.</p>
<h3 id="服务类型"><a href="#服务类型" class="headerlink" title="服务类型"></a>服务类型</h3><h4 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a><strong>ClusterIP</strong></h4><p>这是最常用的一种svc, Yaml文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span>    <span class="comment">#svc所在的ns</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span>    <span class="comment">#指定svc的类型: ClusterIP, NodeIP, loadBalancer, ExternalName</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span>    <span class="comment">#svc对外暴露的端口</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span>    <span class="comment">#访问协议: TCP, UDP, HTTP...</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span>    <span class="comment">#映射到容器里的端口</span></span><br><span class="line">  <span class="attr">selector:</span>    <span class="comment">#根据以下标签选择后端pod</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure>

<p>kubectl apply -f svc.yaml后, kubernetes会为这个svc分配一个<code>clusterIP</code>的地址, 地址的范围从<code>kube-apiserver</code>的参数<code>service-cluster-ip-range</code>获取.</p>
<p><code>ClusterIP因为是个虚IP(没有实际的网卡), 所以无法ping通,也无法单独使用,必须要跟端口一起使用</code></p>
<p><code>type默认为ClusterIP, 这种service只能在集群内部访问.</code></p>
<h4 id="无label-selector"><a href="#无label-selector" class="headerlink" title="无label selector"></a><strong>无label selector</strong></h4><p>这种场景一般用在服务搭建在kubernetes集群外,但是需要在集群内部访问(<code>前提要网络可达</code>)如数据库,中间件等.</p>
<p>Yaml文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">consul</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">external</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8500</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8500</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">consul</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">external</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.214</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.211</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.213</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">8500</span></span><br></pre></td></tr></table></figure>

<p>由于consul服务是部署在集群外的, 所以这类的svc是不会自动创建endpoint对象(也就没法通过label选择后端应用),所以需要手工create endpoint对象, <code>使svc跟ep配对</code>,这样,从svc进来的请求即会路由到consul服务.</p>
<h4 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a><strong>ExternalName</strong></h4><p>这种场景一般用在服务搭建在kubernetes集群外,但是需要在集群内部访问(<code>前提要网络可达</code>).</p>
<p><code>ExternalName目的是构造一条dns记录,即通过svc访问的请求直接通过dns转发到目的服务.</code></p>
<p>Yaml文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">external</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">externalName:</span> <span class="string">r-bp123b06a718fd74.redis.rds.aliyuncs.com</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">6379</span></span><br><span class="line">  <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ExternalName</span></span><br></pre></td></tr></table></figure>



<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a><strong>服务发现</strong></h3><p>目前kubernetes支持两种服务发现: <code>环境变量, DNS</code></p>
<h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a><strong>环境变量</strong></h4><p>通过在资源对象中注入环境变量的方式,使应用能够感知对端应用. </p>
<p><code>不推荐</code></p>
<h4 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a><strong>DNS</strong></h4><p><code>LabelSelector机制解决对象的关联关系,而DNS机制则解决服务路由问题.</code></p>
<p>每个集群中都会部署dns服务,用于集群内部的<code>域名解析</code>.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/kube-dns2.png"></p>
<p>Kube-dns由3个容器组成:</p>
<blockquote>
<ul>
<li><code>kubedns</code>：kubedns进程监视 Kubernetes master 中的 Service 和 Endpoint 的变化，并维护内存查找结构来服务DNS请求 </li>
<li><code>dnsmasq</code>：dnsmasq容器添加 DNS 缓存以提高性能, 参数中指定upstream为kubedns</li>
<li><code>sidecar</code>：sidecar容器在执行双重健康检查（针对 dnsmasq 和 kubedns）时提供单个健康检查端点（监听在10054端口）</li>
</ul>
</blockquote>
<p><strong>kubernetes的新版本中,已经使用了coreDNS代替了kube-dns做为集群默认的dns, coredns在效率方面比kube-dns更高</strong></p>
<p>具体dns如何做服务发现的呢?</p>
<p><code>每个node上运行的kube-proxy是实现service的载体,而kube-proxy则是创建一系列iptables规则实现代理</code></p>
<h3 id="路由分析"><a href="#路由分析" class="headerlink" title="路由分析"></a>路由分析</h3><p><code>为何在应用中使用nginx-deployment:8008就能够访问某个应用?</code></p>
<p>首先了解一下<code>nginx-deployment</code></p>
<p><code>nginx-deployment</code>在kubernetes集群中其实是个域名, 完整的域名应该是</p>
<p><code>nginx-deployment.default.svc.cluster.local</code></p>
<p>集群中所说的服务发现一般是对于service来说的, service的域名规则如下:</p>
<p><code>service-name.namespace.svc.cluster.local</code></p>
<p>集群在部署时会要求给定一个<code>静态的ip</code>做为dns服务对外暴露的ip(也是个<code>ClusterIP, 不是真实的dns容器ip</code>)</p>
<p>同时会定义好集群默认域名后缀: <code>cluster.local</code>, 默认地对于service类型的资源来说就是<code>svc.cluster.local</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/unknown-4324463.png"></p>
<p>部署时会把这两个当成是kubelet的启动命令的参数传入, 这样每次有新pod的创建请求时都会把这两个参数传入pod中, 这样会把dns的ip写在容器&#x2F;etc&#x2F;resolv.conf</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121165045218.png"></p>
<p>除了写了nameserver之外,还有<code>search domain: 简单来讲就是通过这个列表把短域名拼接成完整域名的规则</code>.</p>
<p><code>因此,如果调用对端服务的容器跟对端服务是在一个namespace时, 是可以不指定namespace</code></p>
<p><code>而如果是在不同的namespace时,则必须使用service-xxx.namespace-xxx的形式,不然无法找到改域名.</code></p>
<p>域名解析流程如下:</p>
<blockquote>
<ul>
<li>当我们在容器内部使用<code>nginx-deployment</code>时,从&#x2F;etc&#x2F;resolv.conf文件内的规则被dns resolver拼接成了<code>nginx-deployment.default.svc.cluster.local</code></li>
<li>然后再将<code>nginx-deployment.default.svc.cluster.local</code>交由<code>10.96.0.10</code>进行解析</li>
<li><code>10.96.0.10</code>将域名解析转到kube-dns容器, 因为kube-dns内缓存了所有service与ip的映射关系,返回service的ip, 这里是ClusterIP.</li>
</ul>
</blockquote>
<p>接下来的流程就是 ClusterIP如何将流量转向real pod了.</p>
<p>假设有如下的svc, nginx:</p>
<p><code>svc name: nginx-deployment.k8s-test.cluster.local</code></p>
<p><code>clusterIP: 10.108.141.150</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121171756110.png"></p>
<p>根据ClusterIP查看iptables规则(<a href="https://www.zsythink.net/archives/1199">iptables系列讲解</a>):</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121172244270.png"></p>
<p>可以看到，目的地址为CLUSTER-IP、目的端口为80的数据包，会被转发到<code>KUBE-MARK-MASQ与KUBE-SVC-Y7T6WNKYLXPYVSY3</code>链上。</p>
<p>其中，<code>KUBE-MARK-MASQ</code>链的作用是给数据包打上特定的标记，以及后续对这些标记的数据做SNAT, <code>可以无视</code>.</p>
<p>每个KUBE-SERVICES 说明是一条service, KUBE-SVC-xxx则是具体的svc规则.</p>
<p>重点来看下<code>KUBE-SVC-Y7T6WNKYLXPYVSY3</code>链:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121181631608.png"></p>
<p>从图中可以看到KUBE-SVC-Y7T6WNKYLXPYVSY3按照等概率地(<code>--mode random --probability 0.500000000</code>)把请求转发给<code>KUBE-SEP-L665HS3BZVTLMHTG</code>及<code>KUBE-SEP-BWQQW4YACEFQQND7</code><br><code>从这里也可以看到, 第一次转发的链名称是KUBE-SVC-xxx，第二次转发给具体pod的链名称是KUBE-SEP-xxx，这里的SEP实际指的是kubernetes的endpoint对象</code></p>
<p>看下SEP链:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121182204452.png"></p>
<p>可以看到最终这两条规则转发到了CLusterIP后的某个pod的ip及端口上.</p>
<p>这样就完成了整个service name到readl pod的查找,<code>后续如何将请求转发到pod内，这部分工作是由容器网络完成的,由于容器网络跟特定使用的组件有关,不同的容器网络机制大多不同,时间关系,这里不展开讨论.</code></p>
<p>Service默认是轮询选择后端, 到达kube-proxy则是使用–mode random –probability来设置pod承载的流量</p>
<p>我们也可以在service中指定会话保持.让同一个client的请求访问同一个backend. 只需要在service YAML中指即可，路由规则与上述原理类似.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">sessionAffinity:</span> <span class="string">ClientIP</span></span><br><span class="line"><span class="attr">sessionAffinityConfig:</span></span><br><span class="line">  <span class="attr">clientIP:</span></span><br><span class="line">    <span class="attr">timeoutSeconds:</span> <span class="number">10800</span></span><br></pre></td></tr></table></figure>

<p>网上抄图总结一下: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121184027548.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/">https://kubernetes.io</a></li>
<li><a href="https://hansedong.github.io/2018/11/20/9/">https://hansedong.github.io/2018/11/20/9/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(排查访问service时提示Connection refused)</title>
    <url>/2020/05/06/Kubernetes-svc-ep-port-name/</url>
    <content><![CDATA[<p>今天排查一个在kubernetes集群中访问svc时出现的<code>Connection refused</code>的问题，比较奇怪</p>
<span id="more"></span>



<h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>背景是: 由于项目关系，需要从一个k8s环境中把yaml文件导入到一个全新的k8s环境, 导入之后发现有些service在应用间访问不通</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">telnet mongodb-mst.external 27017</span><br><span class="line">Trying 10.97.135.242...</span><br><span class="line">telnet: Unable to connect to remote host: Connection refused</span><br></pre></td></tr></table></figure>



<h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>首先排除了域名、端口的配置问题。</p>
<p>可以确定的是集群内的DNS是正常的.</p>
<p>提示连接拒绝那么就是通过clusterIP无法到达realserver. 查看iptables规则</p>
<p>发现提示<code>default has no endpoints --reject-with icmp-port-unreachable</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506115705.png"></p>
<p>很奇怪, 提示没有endpoints, 但是使用<code>kubectl get ep</code>又能看到ep存在且配置没有问题</p>
<p>而且这个default是怎么来的.</p>
<p>为了方便部署, 很多配置是从别的环境导出的配置, 有些service访问是没问题的, 只有少部分<code>connection refused</code></p>
<p>结比一下发现一个很有趣的问题，先来看下不正常的yaml文件:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506115805.png"></p>
<p>由于服务在集群外部署的, 因此这里使用了subset方式, 开始怀疑问题在这里, 但是后来知道这个不是重点</p>
<p>乍一看这个配置没什么问题, 部署也很正常, 但是对比正常的yaml文件，发现一个区别：</p>
<p>如果在services中的端口指定了名字, 那么在subsets中的端口也要带名字, 没有带名字的就会出现<code>connection refused</code>，这个确实之前从来没有关注过, 一个端口的情况下也不会指定名字</p>
<p>而且这面iptalbes中提示的default刚好就是这里的port name,虽然不敢相信，但是也只能试一试这个方法: 在subsets中也加了port name</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506120151.png"></p>
<p>重新部署一个，再次查看iptalbes规则 </p>
<p><code>iptables-save|grep mongodb-mst</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200506120040.png"></p>
<p>OMG, 居然可行, 再看下telnet的结果:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Trying 10.105.116.92...</span><br><span class="line">Connected to mongodb-mst.external.svc.cluster.local.</span><br><span class="line">Escape character is <span class="string">&#x27;^]&#x27;</span>.</span><br></pre></td></tr></table></figure>

<p>访问也是没问题, </p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>那么结果很明显了:</p>
<p><strong>在service中指定了port name时, 也需要在ep中指定同样的port name</strong></p>
<p>这是为什么呢?</p>
<p>查看<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#serviceport-v1-core">文档</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ServicePort v1 core</span></span><br><span class="line">The name of this port within the service. This must be a DNS_LABEL. All ports within a ServiceSpec must have unique names. This maps to the <span class="string">&#x27;Name&#x27;</span> field <span class="keyword">in</span> EndpointPort objects. Optional <span class="keyword">if</span> only one ServicePort is defined on this service</span><br></pre></td></tr></table></figure>

<p>提到了在不止一个端口时， svc的port name 需要与endpointport name保持一致.</p>
<p>源码里也会对比端口名字.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cache *EndpointSliceCache)</span></span> endpointInfoByServicePort(serviceNN types.NamespacedName, sliceInfoByName endpointSliceInfoByName) spToEndpointMap &#123;</span><br><span class="line">	endpointInfoBySP := spToEndpointMap&#123;&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, sliceInfo := <span class="keyword">range</span> sliceInfoByName &#123;</span><br><span class="line">		<span class="keyword">for</span> _, port := <span class="keyword">range</span> sliceInfo.Ports &#123;</span><br><span class="line">			<span class="keyword">if</span> port.Name == <span class="literal">nil</span> &#123;</span><br><span class="line">				klog.Warningf(<span class="string">&quot;ignoring port with nil name %v&quot;</span>, port)</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">// <span class="doctag">TODO:</span> handle nil ports to mean &quot;all&quot;</span></span><br><span class="line">			<span class="keyword">if</span> port.Port == <span class="literal">nil</span> || *port.Port == <span class="type">int32</span>(<span class="number">0</span>) &#123;</span><br><span class="line">				klog.Warningf(<span class="string">&quot;ignoring invalid endpoint port %s&quot;</span>, *port.Name)</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			svcPortName := ServicePortName&#123;</span><br><span class="line">				NamespacedName: serviceNN,</span><br><span class="line">				Port:           *port.Name,</span><br><span class="line">				Protocol:       *port.Protocol,</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			endpointInfoBySP[svcPortName] = cache.addEndpointsByIP(serviceNN, <span class="type">int</span>(*port.Port), endpointInfoBySP[svcPortName], sliceInfo.Endpoints)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> endpointInfoBySP</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主要是这个端口的名称平时也不怎么在意, 没想到有坑</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#serviceport-v1-core">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#serviceport-v1-core</a></li>
<li><a href="https://kubernetes.io/">https://kubernetes.io/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(Sealed Secrets如何安全地将密码存储在git中)</title>
    <url>/2020/06/24/Kubernetes-sealed-secrets/</url>
    <content><![CDATA[<p>在Devops中经常面对的一个问题就是，如何将敏感信息保存在git中, 虽然kubernetes提供了secret，但它也只是Base64转换了一下, 任何人拿到这个串都可以反转出来，不安全.</p>
<p>在GitOps的流程上, 如何解决save everything on git but secrets的问题一直是个非常有趣的话题，而今天要说的Sealed Secrets就是一个很好的工具, 可以与Kubernetes无缝接入.</p>
<span id="more"></span>



<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624125407.png"></p>
<p>Sealed Secrets 包括以下组成部分：</p>
<ol>
<li>一个部署到集群的控制器</li>
<li>一个名为 <em>kubeseal</em> 的 CLI 工具</li>
<li>一个名为 SealedSecret 的定制化资源定义 (CRD)</li>
</ol>
<p>启动后，控制器会查找集群范围的私钥&#x2F;公钥对，如果未找到，则会生成一个新的 4096 位 RSA 密钥对。私钥保存在一个 Secret 对象中，该对象位于控制器所在的命名空间中。任何想要在此集群中使用 Sealed Secrets 的人都可以公开获得此密钥的公钥部分。</p>
<p>加密时，原始 Secret 中的每个值都使用带有随机生成的会话密钥的 AES-256 方式进行对称加密。然后，将 SHA256 和原始 Secret 的命名空间&#x2F;名称作为输入参数，使用控制器的公钥对会话密钥进行非对称加密。加密过程的输出结果是一个字符串，其构造为：<strong>加密会话密钥的长度（2 个字节）</strong>+ <strong>加密会话密钥</strong> + <strong>已加密的 Secret</strong>。</p>
<p>将 SealedSecret 自定义资源部署到 Kubernetes 集群时，控制器会watch该类资源，然后使用私钥将其解封并创建一个 Secret 资源。解密时，会再次使用 SealedSecret 的命名空间&#x2F;名称作为输入参数。这样可以确保 SealedSecret 和 Secret 严格绑定到相同的命名空间和名称。</p>
<p>配套的 CLI 工具 <strong>kubeseal</strong> 用于使用公钥从 Secret 资源定义中创建 SealedSecret 定制化资源定义 (CRD)。<strong>kubeseal</strong> 可以通过 Kubernetes API 服务器与控制器进行通信，并在运行时检索加密 Secret 所需的公钥。您也可以从控制器下载公钥并保存在本地以便离线使用</p>
<h3 id="安装controller"><a href="#安装controller" class="headerlink" title="安装controller"></a>安装controller</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f controller.yaml</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624123543.png"></p>
<h3 id="密钥对"><a href="#密钥对" class="headerlink" title="密钥对"></a>密钥对</h3><p>通过上面生成的CRD之后, 查看<code>sealed-secrets-controller</code>的日志文件可以发现:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2020/06/24 16:34:47 Starting sealed-secrets controller version: v0.12.4+dirty</span><br><span class="line">controller version: v0.12.4+dirty</span><br><span class="line">2020/06/24 16:34:47 Searching <span class="keyword">for</span> existing private keys</span><br><span class="line">2020/06/24 16:34:47 New key written to senserealty/sealed-secrets-keyzslr5 <span class="comment"># 这个secret包含证书及私钥</span></span><br><span class="line">2020/06/24 16:34:47 Certificate is</span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line"><span class="comment"># ... 这里是公钥内容</span></span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line"></span><br><span class="line">2020/06/24 16:34:47 HTTP server serving on :8080</span><br></pre></td></tr></table></figure>

<p>启动时，控制器会在其命名空间中搜索带有 <strong>sealedsecrets.bitnami.com&#x2F;sealed-secrets-key</strong> 标签的 Secret。如果找不到，控制器会在其命名空间中创建新的 Secret, 查看<code>sealed-secrets-keyzslr5</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get secret -n senserealty -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">tls.crt:</span> <span class="comment"># ... 证书</span></span><br><span class="line">  <span class="attr">tls.key:</span> <span class="comment"># ... 私钥</span></span><br><span class="line"><span class="comment"># ...  </span></span><br></pre></td></tr></table></figure>

<p>该secret包含证书及私钥，私钥用于解密集群中的SealedSecret资源.</p>
<h3 id="安装kubeseal"><a href="#安装kubeseal" class="headerlink" title="安装kubeseal"></a>安装kubeseal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.12.4/kubeseal-linux-amd64 -O kubeseal</span><br><span class="line">sudo install -m 755 kubeseal /usr/local/bin/kubeseal</span><br></pre></td></tr></table></figure>

<p><strong>kubeseal</strong> 使用公钥加密 Secret，该公钥是它在运行时从在 Kubernetes 集群中运行的控制器获取的，也就是说，kubeaeal会读取~&#x2F;.kube&#x2F;config文件来获取集群权限，然后查找集群中的<code>sealed-secrets-controller</code>控制器来获取公钥.</p>
<p>如果无法获取集群权限, 可以通过以下命令直接把公钥下载到本地</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeseal --fetch-cert &gt;mycert.pem --controller-namespace senserealty</span><br></pre></td></tr></table></figure>

<p>在加密时，使用–cert来指定本地的pem公钥文件, 这样的话，就可能够保证集群权限最小化.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeseal --format=yaml --cert=public-key-cert.pem &lt; secret.yaml &gt; sealed-secret.yaml</span><br></pre></td></tr></table></figure>



<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>这里直接使用的是github上的example, 参考<a href="https://github.com/bitnami-labs/sealed-secrets/blob/master/docs/examples/config-template/deployment.yaml">这里</a></p>
<h4 id="secret"><a href="#secret" class="headerlink" title="secret"></a>secret</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">senserealty</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">server1:</span> <span class="string">foo</span></span><br><span class="line">  <span class="attr">server2:</span> <span class="string">bar</span></span><br><span class="line">  <span class="attr">database1:</span> <span class="string">baz</span></span><br></pre></td></tr></table></figure>

<p>通过kubeseal生成sealed-secret</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeseal --format=yaml &lt; secret.yaml &gt; sealed-secret.yaml  --controller-namespace senserealty</span><br><span class="line"><span class="comment"># 默认情况下 controller是安装在kube-system下的, 如果指定在其它ns,则需要使用--controller-namespace</span></span><br></pre></td></tr></table></figure>

<p>会在当前目录下生成sealed-secret.yaml，该文件内容已被加密，将该文件发布到集群中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f sealed-secret.yaml</span><br></pre></td></tr></table></figure>

<p>查看集群中的secret, 会发现secret被解密成正常的secret(通过base64加密过后的)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624140240.png"></p>
<p>同样, 可以在controller中查看到日志，显示该secret被成功解密.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624140831.png"></p>
<h4 id="configmap"><a href="#configmap" class="headerlink" title="configmap"></a>configmap</h4><p>官方的example里还包含了一个configmap, 该configmap里引用了上面的secret, 最终我们验证的时候可以看configmap中引用的内容是否正确. </p>
<p><strong>这也是生产上常用的方式, app中的配置使用configmap，但是敏感的信息也不能放在configmap中, 因此configmap中使用了模板的形式引用被kubeseal加密后的secret</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">senserealty</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myconfig.json.tmpl:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">           &quot;Server&quot;: [&#123;</span></span><br><span class="line"><span class="string">                           &quot;host&quot;: &quot;foobar&quot;,</span></span><br><span class="line"><span class="string">                           &quot;ip&quot;: &quot;10.10.10.12&quot;,</span></span><br><span class="line"><span class="string">                           &quot;port&quot;: &quot;22&quot;,</span></span><br><span class="line"><span class="string">                           &quot;env&quot;: &quot;SOME_ENV&quot;,</span></span><br><span class="line"><span class="string">                           &quot;user&quot;: &quot;myuser&quot;,</span></span><br><span class="line"><span class="string">                           &quot;password&quot;: &quot;&#123;&#123; (ds &quot;secrets&quot; &quot;server1&quot;) &#125;&#125;&quot;, # 引用变量</span></span><br><span class="line"><span class="string">                           &quot;role&quot;: &quot;foo&quot;</span></span><br><span class="line"><span class="string">                   &#125;,&#123;</span></span><br><span class="line"><span class="string">                           &quot;host&quot;: &quot;barfoo&quot;,</span></span><br><span class="line"><span class="string">                           &quot;ip&quot;: &quot;10.10.10.11&quot;,</span></span><br><span class="line"><span class="string">                           &quot;port&quot;: &quot;22&quot;,</span></span><br><span class="line"><span class="string">                           &quot;env&quot;: &quot;SOME_OTHER_STUFF&quot;,</span></span><br><span class="line"><span class="string">                           &quot;user&quot;: &quot;otheruser&quot;,</span></span><br><span class="line"><span class="string">                           &quot;password&quot;: &quot;&#123;&#123; (ds &quot;secrets&quot; &quot;server2&quot;) &#125;&#125;&quot;, # 引用变量</span></span><br><span class="line"><span class="string">                           &quot;role&quot;: &quot;foo&quot;</span></span><br><span class="line"><span class="string">                   &#125;</span></span><br><span class="line"><span class="string">           ],</span></span><br><span class="line"><span class="string">           &quot;Database&quot;: [&#123;</span></span><br><span class="line"><span class="string">                           &quot;host&quot;: &quot;somedb&quot;,</span></span><br><span class="line"><span class="string">                           &quot;ip&quot;: &quot;10.10.10.10&quot;,</span></span><br><span class="line"><span class="string">                           &quot;port&quot;: &quot;1521&quot;,</span></span><br><span class="line"><span class="string">                           &quot;sid&quot;: &quot;FOO&quot;,</span></span><br><span class="line"><span class="string">                           &quot;env&quot;: &quot;BAZ&quot;,</span></span><br><span class="line"><span class="string">                           &quot;user&quot;: &quot;abcdefg123&quot;,</span></span><br><span class="line"><span class="string">                           &quot;password&quot;: &quot;&#123;&#123; (ds &quot;secrets&quot; &quot;database1&quot;) &#125;&#125;&quot;, # 引用变量</span></span><br><span class="line"><span class="string">                           &quot;role&quot;: &quot;foo&quot;</span></span><br><span class="line"><span class="string">                   &#125;</span></span><br><span class="line"><span class="string">           ]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br></pre></td></tr></table></figure>

<p>发布到集群中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f configmap.yaml</span><br></pre></td></tr></table></figure>



<h3 id="deployment"><a href="#deployment" class="headerlink" title="deployment"></a>deployment</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">example</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">example</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">initContainers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">inject-secrets</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">hairyhenderson/gomplate:alpine</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="comment"># The /secrets directory will contain one file per secret item.</span></span><br><span class="line">        <span class="comment"># The secret item&#x27;s key will become the file name, while its value goes in the file contents.</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-secrets-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/secrets</span></span><br><span class="line">        <span class="comment"># The config map containing the config file template will be available here.</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-template-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/input</span></span><br><span class="line">        <span class="comment"># While the expanded template will be put in a volume shared with the application which will run</span></span><br><span class="line">        <span class="comment"># in the main container when this init container is done.</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/output</span></span><br><span class="line">        <span class="comment"># Now we can actually run the template expander and inject secrets into the template.</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>]</span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">gomplate</span> <span class="string">-d</span> <span class="string">secrets=/secrets/</span> <span class="string">&lt;/input/myconfig.json.tmpl</span> <span class="string">&gt;/output/myconfig.json</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="comment"># This is an example application that assumes a complex configuration file in /config/myconfig.json.</span></span><br><span class="line">      <span class="comment"># The JSON format here is just an example; any textual file format would work.</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">app</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">bitnami/minideb:buster</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/config</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>]</span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">         echo Your app will get this config file:</span></span><br><span class="line"><span class="string">         cat /config/myconfig.json</span></span><br><span class="line"><span class="string">         # dummy application</span></span><br><span class="line"><span class="string">         sleep 100000h</span></span><br><span class="line"><span class="string"></span>      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">emptyDir:</span> &#123;&#125;</span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-template-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-secrets-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">example</span></span><br></pre></td></tr></table></figure>

<p> 该deployment有一个initcontainer, 用<a href="https://github.com/hairyhenderson/gomplate">goemplate</a>将secret填充configmap中的变量, 然后containers中的容器直接打印最终的config配置</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624142108.png"></p>
<p>从上面可以看出, configmap中的变量跟原始的secret中的值完全一样. 达到了对secret加解密的作用.</p>
<h3 id="更新secret"><a href="#更新secret" class="headerlink" title="更新secret"></a>更新secret</h3><p>SealedSecret支持对存在的secret进行更新.</p>
<p>可使用 <code>--merge-into</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一个secret, 文件名为foo,内容为bar</span></span><br><span class="line"><span class="built_in">echo</span> -n bar | kubectl create secret generic mysecret --dry-run --from-file=foo=/dev/stdin -o json \</span><br><span class="line">  | kubeseal &gt; mysealedsecret.json</span><br><span class="line"><span class="comment"># 更新secret, 新增一个文件为bar, 内容为baz</span></span><br><span class="line"><span class="built_in">echo</span> -n baz | kubectl create secret generic mysecret --dry-run --from-file=bar=/dev/stdin -o json \</span><br><span class="line">  | kubeseal --merge-into mysealedsecret.json</span><br></pre></td></tr></table></figure>

<p>当然也可以按照正常的更新secret的方式，先更新secret ，然后使用SealedSecret加密，然后发布到集群中，然后更新app.</p>
<p>最后可以看到，app中的config也更新了.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200624190637.png"></p>
<h3 id="使用范围"><a href="#使用范围" class="headerlink" title="使用范围"></a>使用范围</h3><p>SealedSecret提供了3种对ns的安全限制， 我大概看了下说明, 觉得虽然会更安全, 但是会带来使用上的复杂性, 大部分情况下，secret还是会跟app在同一ns中,因此这块内容本人没有实验， 可参考<a href="https://github.com/bitnami-labs/sealed-secrets/blob/master/README.md#scopes">scope</a></p>
<ul>
<li><code>strict</code> (default): the secret must be sealed with exactly the same <em>name</em> and <em>namespace</em>. These attributes become <em>part of the encrypted data</em> and thus changing name and&#x2F;or namespace would lead to “decryption error”.</li>
<li><code>namespace-wide</code>: you can freely <em>rename</em> the sealed secret within a given namespace.</li>
<li><code>cluster-wide</code>: the secret can be unsealed in <em>any</em> namespace and can be given <em>any</em> name.</li>
</ul>
<h3 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h3><p>如果没有由控制器管理的私钥，就无法在 SealedSecret 中对已加密的数据进行解密。如果SealedSecret被集群广泛使用, 那么就需要小心集群发生灾难性异常，在恢复之后或者迁移到一个新集群之后，新集群中部署的控制器必须使用相同的私钥才能解封 SealedSecrets。如果没有此私钥，则必须使用新的私钥&#x2F;公钥对重新生成所有 SealedSecrets，这对于包含大量 Secret 资源的部署可能会是一项繁重的任务。因此备份私钥非常有必要， 可以使用以下命令从控制器中获取证书及私钥写到文件中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get secret -n senserealty -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml &gt; master.yaml</span><br></pre></td></tr></table></figure>

<p>这样，即使整个集群崩溃或者迁移到新集群之后，使用相同的私钥创建的SealedSecret CRD同样可以解密由这个私钥加密的secret.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">master.yaml</span></span><br><span class="line"><span class="comment"># 当集群中已经存在sealedsecrets.bitnami.com/sealed-secrets-key时，则不会新建</span></span><br><span class="line"><span class="attr">2020/06/24 23:26:07 Starting sealed-secrets controller version:</span> <span class="string">v0.12.1+dirty</span></span><br><span class="line"><span class="attr">controller version:</span> <span class="string">v0.12.1</span></span><br><span class="line"><span class="number">2020</span><span class="string">/06/24</span> <span class="number">23</span><span class="string">:26:07</span> <span class="string">Searching</span> <span class="string">for</span> <span class="string">existing</span> <span class="string">private</span> <span class="string">keys</span></span><br><span class="line"><span class="number">2020</span><span class="string">/06/24</span> <span class="number">23</span><span class="string">:26:07</span> <span class="string">-----</span> <span class="string">sealed-secrets-keyhvdtf</span></span><br><span class="line"><span class="number">2020</span><span class="string">/06/24</span> <span class="number">23</span><span class="string">:26:07</span> <span class="string">HTTP</span> <span class="string">server</span> <span class="string">serving</span> <span class="string">on</span> <span class="string">:8080</span></span><br></pre></td></tr></table></figure>



<p>通过这种方式, 敏感信息可以只被少数人掌握, 开发同学则直接引用加密后的信息即可</p>
<p>当然，上面的例子只是一个demo, SealedSecret提供了一种思路，生产中SealedSecret如何更好地对接kubernetes app, 还需要进一步地研究.</p>
<p>就跟SealedSecret官方说的那样:</p>
<p><strong>Problem:</strong> “I can manage all my K8s config in git, except Secrets.”</p>
<p><strong>Solution:</strong> Encrypt your Secret into a SealedSecret, which <em>is</em> safe to store - even to a public repository. The SealedSecret can be decrypted only by the controller running in the target cluster and nobody else (not even the original author) is able to obtain the original Secret from the SealedSecret</p>
<p><strong>SealedSecret在GitOps的流程上又推进了一步.</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://aws.amazon.com/cn/blogs/china/managing-secrets-deployment-in-kubernetes-using-sealed-secrets/">https://aws.amazon.com/cn/blogs/china/managing-secrets-deployment-in-kubernetes-using-sealed-secrets/</a></li>
<li><a href="https://github.com/bitnami-labs/sealed-secrets">https://github.com/bitnami-labs/sealed-secrets</a></li>
<li><a href="https://github.com/bitnami-labs/sealed-secrets/blob/master/README.md#scopes">https://github.com/bitnami-labs/sealed-secrets/blob/master/README.md#scopes</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(当在kubernetes集群内的某个pod中访问google.com时到底发生了什么)</title>
    <url>/2020/03/02/Kubernetes-what-happend-when-curl-internet-inside-pod/</url>
    <content><![CDATA[<p>今天同事接到一个需求, 需求是这样的: 需要对接友商的系统(域名, 公网能够解析),但是友商系统有个白名单, 只有在白名单中的ip才能有权限访问, 这个ip应该是什么? </p>
<p>补充一下架构背景： 应用都是部署在kubernetes集群中的， kubernetes部署在阿里云ECS上. CNI使用的flannel</p>
<p>这么长时间跟kubernetes打交道, 还真没想过这个问题, 刚好可以测试一翻, 探探究竟</p>
<p>期间会涉及到CNI(container network interface)相关的话题,在这里不会做深入研究, 一笔带过.详细的学习记录在这里可以找到<a href="https://izsk.me/2020/01/05/Kubernetes-flannel-details/">Kubernetes学习(flannel深入学习)</a></p>
<span id="more"></span>



<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>那么把这个问题改变下, ?<code>在Pod中请求baidu.com时，到底发生了什么</code></p>
<ol>
<li>kubernetes集群中的应用是否能够访问公网, 如果都出不去,那都不用谈对接友商的系统了</li>
<li>如果能够访问公网, 那出现在友商时的remote-ip一定不会是pod的ip,也不会是clusterIP, 如果是这两个ip的话在response回来的路由上是无法找到这两个地址的, 因为这两个都是集群内部才能够访问</li>
<li>那么就有可能是nodeIP或者是公司公网出口IP, 后者可能性大一点.</li>
<li>请求会不会经过flannel.</li>
</ol>
<p>关于第3点, 可以打办公网的例子, 在办公网内的所有机器的ip都是内网ip, 内网的ip都是在同一局域网有效的, 那么在访问外网的时候都会通过一个公网IP(可能多个)出去, 所以在办公网通过<code>myips.com</code>查看自己ip的时候看到的ip跟使用<code>ip addr</code>看到的不一样</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413150855.png"> </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200413151032.png"></p>
<p>那么在办公网访问公网的过程中一定是发生了某个转换, 这样请求response返回时能够区分是我这台机器请求的.</p>
<h3 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h3><p>nat分为SNAT、DNAT</p>
<p>SNAT Source Network Address Translation 源网络地址转换，其作用是将ip数据包的<a href="https://www.baidu.com/s?wd=%E6%BA%90%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">源地址转换</a>成另外一个地址，可能有人觉得奇怪，好好的为什么要进行<a href="https://www.baidu.com/s?wd=ip%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">ip地址转换</a>啊，为了弄懂这个问题，我们假设局域网内网主机A（192.168.2.8）要和外网主机B（61.132.62.131）通信，A向B发出IP数据包，如果没有SNAT对A主机进行<a href="https://www.baidu.com/s?wd=%E6%BA%90%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">源地址转换</a>，A与B主机的通讯会不正常中断，因为当路由器将内网的数据包发到<a href="https://www.baidu.com/s?wd=%E5%85%AC%E7%BD%91IP&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">公网IP</a>后，<a href="https://www.baidu.com/s?wd=%E5%85%AC%E7%BD%91IP&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">公网IP</a>会给你的私网IP回数据包，这时，<a href="https://www.baidu.com/s?wd=%E5%85%AC%E7%BD%91IP&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">公网IP</a>根本就无法知道你的私网IP应该如何走了。所以问它上一级路由器，当然这是肯定的，因为从公网上根本就无法看到私网IP，因此你无法给他通信。为了实现数据包的正确发送及返回，网关必须将A的址转换为一个合法的公网地址，同时为了以后B主机能将数据包发送给A，这个合法的公网地址必须是网关的外网地址，如果是其它公网地址的话，B会把数据包发送到其它网关，而不是A主机所在的网关，A将收不到B发过来的数据包，所以内网主机要上公网就必须要有合法的公网地址，而得到这个地址的方法就是让网关进行SNAT(<a href="https://www.baidu.com/s?wd=%E6%BA%90%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4uHTYnywbPH6srjcYuWR40ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3En1ndPjm1rHcs">源地址转换</a>），将内网地址转换成公网址(一般是网关的外部地址)</p>
<p>上例145机器可以访问外网。其他局域网机器可以将请求发送给145机器，145机器发出去后，再原路将响应数据返还给局域网机器。这个过程就是 NAT 转换。</p>
<p>NAT 映射表记录了转换关系。</p>
<table>
<thead>
<tr>
<th>源IP</th>
<th>源端口</th>
<th>NAT 网关机器IP</th>
<th>NAT 网关机器端口</th>
<th>目标IP</th>
<th>目标端口</th>
</tr>
</thead>
<tbody><tr>
<td>192.168.128.140</td>
<td>8080</td>
<td>192.168.128.145</td>
<td>18080</td>
<td>172.217.27.142</td>
<td>80</td>
</tr>
</tbody></table>
<ol>
<li>192.168.128.140:8080 [request google.com] –&gt; 192.168.128.145:18080</li>
<li>192.168.128.145:18080 [request google.com] –&gt; google.com (172.217.27.14:80)</li>
<li>google.com [response] –&gt; 192.168.128.145:18080</li>
<li>查找映射表得知，18080获得的数据返还给 192.168.128.140:8080</li>
<li>google.com [response] –&gt; 192.168.128.140:8080</li>
</ol>
<p>这就是办公网访问公网的原理</p>
<p>同时还有一个DNAT Destination Network Address Translation 目的网络地址转换, 原理差不多, 可自行百度</p>
<p>为什么要说到SNAT呢, 其实在kube-proxy操作iptables时也大量地存在SNAT的情况, 大家可能使用命令查看下</p>
<p><code>iptables -v -t nat -nL</code></p>
<p>那么问题又变成: <code>这个IP是nodeIP还是也有个类似公网IP的ip</code>?</p>
<p>测试如下: </p>
<p>通过在一个Kubernetes集群中的Pod(A)中访问另一个Kubernetes的Pod(B), 查看Pod B,日志如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200411221053.png"></p>
<p><code>x-forward-for</code>的这个ip并不是Pod A所有的NodeIP, 也不是我知道的任何一个机器的ip，那这个ip会不会是做SNAT操作所在机器的IP呢?</p>
<h3 id="为何找不到请求是如何转发到这个IP的"><a href="#为何找不到请求是如何转发到这个IP的" class="headerlink" title="为何找不到请求是如何转发到这个IP的"></a>为何找不到请求是如何转发到这个IP的</h3><p>为了查看这个118.xxx 到底是什么IP, 使用<code>tcpdump</code>进行抓包, 发现并没有这个ip相关的记录, 按道理说, 想要做SNAT转换，在源机器的iptables中会有所发现, 或者网关指向SNAT机器, 但是一番操作后, 并没有任何跟改IP相关的请求流,而且我也没权限登录改机器</p>
<p>没办法, 最后只能问it机房的同事, 得到的反馈的信息是, 这个IP确认是SNAT网关的公网IP</p>
<p>那么毫无疑问, 这个IP是SNAT后的IP, 但是<strong>为何找不到请求是如何转发到这个IP的呢?</strong></p>
<p>原来,<strong>在阿里的vpc环境下, 除了使用指定网关或者是使用iptables劫持流量的方式外, console可以直接指定使用SNAT, 只需要添加一个NAT网关, NAT网关上绑定一个公网IP, 同时指定在该vpc下的哪些ECS机器需要使用nat</strong>, 那么这些机器的所有访问公网的流量都会通过NAT网关到公网.详情见<a href="https://help.aliyun.com/document_detail/65183.html?spm=a2c4g.11186623.6.563.1ee91f5b9NZiJr">阿里云NAT网关</a></p>
<p>这个操作因为是直接在VPC网络进行的操作, 是不需要对ECS机器做操作的, 因此在ECS机器上是没有相关配置的</p>
<p>那么问题就很清晰了, 从pod中curl google.com会经过SNAT, 那又有一个问题, 这样的请求会不会经过flannel？</p>
<h3 id="是否会经过flannel？"><a href="#是否会经过flannel？" class="headerlink" title="是否会经过flannel？"></a>是否会经过flannel？</h3><p>由于篇幅这里不会细究flannel的网络模块, 只说实验结论.</p>
<p>可以再做个测试, 由于访问公网无法在对端进行tcpdump, 因此测试kubernetes集群间访问</p>
<p>在ClusterA 的podA访问ClusterB的PodB, podB中是一个nginx, 绑定了一个域名, 在内网可以解析 </p>
<p> <code>curl xxx.com</code></p>
<p>在podA所在的宿主机上进行tcpdump</p>
<p><code>tcpdump -i any host 172.16.104.207 -vvv -nn</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200411223024.png"></p>
<p>在对端(212)主机上进行tcpdump</p>
<p><code>tcpdump -i any host 172.16.104.207 -vvv -nn</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200411222622.png"></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>会发现: <strong>当两台机器都处于不同的kubernetes集群,如果机器间跨内网能够通信的话,那么就不需要SNAT的地址转换了, 而且不会经过flannel, 直接从pod–&gt; docker –&gt; eth0 –&gt; 到达对端</strong>, 这点可以通过tcpdump在两边抓包查看</p>
<p>再通过上面对公网的实验可以得到, 在kubernetes的pod中访问公网的资源,也是不会经过flannel的, 因为flannel解决的集群间的跨主机访问, 在对路由匹配的时候不会匹配到flannel的路由, 因此直接通过eth0就转向公网.</p>
<h3 id="科普"><a href="#科普" class="headerlink" title="科普"></a>科普</h3><p>这里要简单提一下, 为何会存在SNAT的情况？</p>
<p>当然, 如果没有SNAT的话,少一层转换理论上肯定是更快的, 但是对于办公网或者多云管理场景下, 不可能为所有的机器都配置公网IP(弹性IP), 在IPV4已经枯竭的情况下, ip资源显得更加可贵, 因此目前基本所有的云厂商的弹性IP都是收费的, 通过SNAT这种方式, 只需要一个公网IP就能够解决成百上千在内网的机器(或者说Pod)访问公网的需求,还是很必要的.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/">https://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/</a></li>
<li><a href="https://github.com/jpetazzo/nsenter">https://github.com/jpetazzo/nsenter</a></li>
<li><a href="https://help.aliyun.com/document_detail/65183.html?spm=a2c4g.11186623.6.563.1ee91f5b9NZiJr">https://help.aliyun.com/document_detail/65183.html?spm=a2c4g.11186623.6.563.1ee91f5b9NZiJr</a></li>
<li><a href="https://izsk.me/2020/01/05/Kubernetes-flannel-details/">https://izsk.me/2020/01/05/Kubernetes-flannel-details/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(解决x509 certificate is valid for xxx, not yyy)</title>
    <url>/2021/01/20/Kubernetes-x509-not-ip/</url>
    <content><![CDATA[<p>最近因机房网段调整，影响到一个kubernetes集群中的一个master节点，在更换该master节点ip的时候，出现了有关证书的问题，特此记录一下</p>
<span id="more"></span>



<p>错误如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to register node <span class="string">&quot;xxxxx&quot;</span> with API server: Post https://xxxx:6443/api/v1/nodes: x509: certificate is valid <span class="keyword">for</span> 10.96.0.1, yyyy, not xxxx</span><br></pre></td></tr></table></figure>

<p>xxxx表示的本地地址</p>
<p>这个错误的意思是, 当kubelet在使用xxxx与api-server通信时，api-server证书返回的地址列表中不包含有xxxx这个地址，导致tls证书校验不通过，可以使用以下命令来查看api-server的证书中包含的所有地址列表</p>
<p>如果使用的是kubeadm搭建的集群，那api-server的证书在<code>/etc/kubernetes/pki</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -noout -text -<span class="keyword">in</span> apiserver.crt</span><br></pre></td></tr></table></figure>

<p>通过上述命令可以列出所有被写在证书的地址列表,比如:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">X509v3 Subject Alternative Name:</span><br><span class="line">                DNS:shaolin, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:10.27.53.32</span><br></pre></td></tr></table></figure>

<p>证书中包含的可以是ip地址，也可以是域名，会发现确实不包含xxxx这个地址</p>
<p>那既然没有包含这个地址，是不是可以修改这个证书的地址列表呢，答案是肯定的:</p>
<p>由于需要生成新的证书，最好备份整个证书目录，整个过程如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成密钥对</span></span><br><span class="line">openssl genrsa -out apiserver.key 2048	</span><br><span class="line"><span class="comment"># 生成</span></span><br><span class="line">openssl req -new -key apiserver.key -subj <span class="string">&quot;/CN=kube-apiserver,&quot;</span> -out apiserver.csr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 apiserver.ext文件，包含所有的地址列表，ip:xxxx即为要包含的新节点的ip， 内容如下：</span></span><br><span class="line">subjectAltName = DNS:wudang,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP:10.96.0.1, IP:10.24.138.208, IP:xxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用ca根证书生成新crt</span></span><br><span class="line">openssl x509 -req -<span class="keyword">in</span> apiserver.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out apiserver.crt -days 3650 -extfile apiserver.ext</span><br><span class="line"><span class="comment"># apiserver.key跟apiserver.crt是一组密钥对</span></span><br></pre></td></tr></table></figure>

<p>再次查看新的apiserver.crt文件会发现已经包含新增加的ip地址</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">openssl x509 -noout -text -<span class="keyword">in</span> apiserver.crt</span><br></pre></td></tr></table></figure>

<p>重启api-server容器后，会发现x509证书错误消失</p>
<p>如果更改的是worker node的ip的话就更简单，只是ip变更一般主机名不会变，只需要直接修改kubelet的–node-ip，然后重启kubelet即可</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/">https://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part2/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(PVC为什么需要延迟绑定)</title>
    <url>/2020/05/12/Kubernetes-why-local-PV-need-bound-delay/</url>
    <content><![CDATA[<p>日常中经常使用pv、pvc来保障有状态服务的数据持久化, 自从kubernertes v1.14对local PV GA之后, 直接使用目录或者是块设备来做为持久化pv变得更为方便，毕竟使用分布式存储还是有些成本的.借这个机会说一说 pv、pvc延迟绑定问题.</p>
<span id="more"></span>



<h3 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h3><p>如果没有使用动态pv provisioner, 则只能事先由集群管理员手工进行pv的准备.</p>
<p>一般分成挂载 –&gt; 格式 –&gt; 目录，最安全的是直接把整个块设备当成local PV，这样好保证IO隔离.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ck-pv-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span>  </span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/bigdata</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">clickhouse</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">enable</span></span><br></pre></td></tr></table></figure>



<h3 id="PVC"><a href="#PVC" class="headerlink" title="PVC"></a>PVC</h3><p>在对象中使用pvc来绑定pv</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volume-claim</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">accessModes:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">          <span class="attr">storageClassName:</span> <span class="string">bigdata-storage</span></span><br></pre></td></tr></table></figure>



<h3 id="default-storageclass"><a href="#default-storageclass" class="headerlink" title="default storageclass"></a>default storageclass</h3><p>如果pv， pvc中使用了一个不存在的storageclass， 也不会发生报错，这是因为集群中存在一个默认的storageclass， 虽然指定的storagelcass可以不存在, 但是在做绑定决策的时候,还是会考虑pv与pvc的storageclass定义, 因此如果没有使用storageclass, 直接置空即可，集群自动使用default storageclass.</p>
<h3 id="storageclass"><a href="#storageclass" class="headerlink" title="storageclass"></a>storageclass</h3><p>当然，也可以自定义，storageclass, 生成一个storageclass比较简单</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bigdata-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/no-provisioner</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是，在它的 <code>provisioner</code> 字段，我们指定的是 <code>no-provisioner</code>。这是因为我们这里是手动创建的 PV，所以不需要动态来生成 PV</p>
<p>对于local PV，另一个比较重要的的参数<code>volumeBindingMode: WaitForFirstConsumer</code>这个参数到底什么意思呢?为什么需要它?</p>
<h3 id="WaitForFirstConsumer"><a href="#WaitForFirstConsumer" class="headerlink" title="WaitForFirstConsumer"></a>WaitForFirstConsumer</h3><p>考虑这么一种情况:</p>
<p>有一个pod, 使用的pvc叫pvc-1， 我们希望它只运行在node-2上，在当前的集群中存在两台主机符合pod的pvc的要求, 假如node-1上是pv-1， node-2上是pv-2，这两个完全一样.</p>
<p>这时如果创建pod, pv控制器看到pv-1与pvc-1是匹配的，因此将它们绑定在一起, 如果没有其它限制条件, 在调度阶段pod将会被调度到node-1上, 这显然与我们的期望不同，我们是希望它调度到node-2上，<strong>pv与pvc的绑定关系是发生在调度之前的,就会造成pv与pvc的绑定成功, 但是pod的调度却不能成功的局面</strong>. 因此，需要一种机制来将这种绑定延迟到调度阶段,这就是<strong>WaitForFirstConsumer</strong>的作用</p>
<p>WaitForFirstConsumer其实就是告诉volume控制器，虽然找到了适合的pv， 但请不要现在就执行绑定操作,而要等到第一个声明使用了该pvc的pod出现在调度器之后，调度器综合考虑所有的调度规则后, 最终决定跟哪个pv进行绑定</p>
<p>通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度</p>
<p>通过<strong>延迟绑定</strong>机制，原本<strong>实时</strong>发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度</p>
<h3 id="static-local-PV-provisioner"><a href="#static-local-PV-provisioner" class="headerlink" title="static local PV provisioner"></a><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">static local PV provisioner</a></h3><p>local PV不支持动态绑定, k8s-sigs也提供了一种可以自动管理local PV的插件, 详情在<a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">github</a></p>
<p>这个插件主要做的事情就是通过监控某一目录下是否有新的挂载点或者是块设备来自动地根据模块生成pv, 这样就需要手工地创建，同时也支持在删除pvc的时候自动清除pv.</p>
<p>当static provisioner启动后, 它就会通过daemonset自动地检查每个宿主机的指定目录, 然后调用kubernetes api自动创建pv</p>
<p>主要组件实现以下功能:</p>
<ul>
<li>Discovery: The discovery routine periodically reads the configured discovery directories and looks for new mount points that don’t have a PV, and creates a PV for it.</li>
<li>Deleter: The deleter routine is invoked by the Informer when a PV phase changes. If the phase is Released, then it cleans up the volume and deletes the PV API object.</li>
<li>Cache: A central cache stores all the Local PersistentVolumes that the provisioner has created. It is populated by a PV informer that filters out the PVs that belong to this node and have been created by this provisioner. It is used by the Discovery and Deleter routines to get the existing PVs.</li>
<li>Controller: The controller runs a sync loop that coordinates the other components. The discovery and deleter run serially to simplify synchronization with the cache and create&#x2F;delete operations.</li>
</ul>
<p>本人目前没有在生产环境下使用这个，只是在测试环境下使用, 感兴趣的话可以参考github.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local">https://kubernetes.io/docs/concepts/storage/storage-classes/#local</a></li>
<li><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li>
<li><a href="https://www.qikqiak.com/k8strain/storage/local/">https://www.qikqiak.com/k8strain/storage/local/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(Operator)</title>
    <url>/2019/04/03/Kubernetes%E5%AD%A6%E4%B9%A0(Operator)/</url>
    <content><![CDATA[<h3 id="官方定义"><a href="#官方定义" class="headerlink" title="官方定义"></a><strong>官方定义</strong></h3><p>An Operator is a method of packaging, deploying and managing a Kubernetes application.</p>
<p>旨在简化复杂有状态应用管理的框架，它是一个感知应用状态的控制器，通过扩展Kubernetes API来自动创建、管理和配置应用实例.</p>
<span id="more"></span>

<h3 id="车库故事"><a href="#车库故事" class="headerlink" title="车库故事"></a><strong>车库故事</strong></h3><p>诞生于车库, <a href="https://yq.aliyun.com/articles/685522">Kubernetes API 与 Operator，不为人知的开发者战争</a></p>
<p><code>operator的初衷是为开发者解决运维工作</code></p>
<h3 id="先从kubernetes说起"><a href="#先从kubernetes说起" class="headerlink" title="先从kubernetes说起"></a><strong>先从kubernetes说起</strong></h3><p><code>两大重要特性:</code></p>
<blockquote>
<ul>
<li>声明式API: 我们提交一个定义好的API对象来”声明”我所期望的状态是什么样子</li>
<li>控制器模式: 无条件的, 无限循环的watch每个api对象,确保每个集群的状态与声明的状态一致</li>
<li>标签选择器: 通过标签确认资源之间的联系</li>
</ul>
</blockquote>
<p>流程可概括如下:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-1.png" alt="operator-c1-1"></p>
<p>为什么kubernetes会有ReplicSet, Deployment, statefulSet, Operator等多种api资源?</p>
<h4 id="应用情景"><a href="#应用情景" class="headerlink" title="应用情景"></a><strong>应用情景</strong></h4><blockquote>
<ul>
<li>无状态: </li>
<li>有状态:  mysq, redis…</li>
<li>分布式: etcd, zookeeper…</li>
</ul>
</blockquote>
<h4 id="StatefulSet与Operator"><a href="#StatefulSet与Operator" class="headerlink" title="StatefulSet与Operator"></a><strong>StatefulSet与Operator</strong></h4><p>statefulSet为kubernetes运行有状态应用的资源类型, 一般都搭配共享存储来持久化</p>
<p>StatefulSet 的核心原理,其实是对分布式应用的两种状态进行了保持:</p>
<blockquote>
<ul>
<li>分布式应用的拓扑状态, 或者说,节点之间的启动顺序;</li>
<li>分布式应用的存储状态, 或者说,每个节点依赖的持久化数据(比如容器 re-scheduler到另一node上,重新加载他的网络存储以及其中的数据)</li>
</ul>
</blockquote>
<p>打个比方:  如果想部署一个3节点的etcd集群</p>
<blockquote>
<ul>
<li>直接使用deployment方式很难维护各节点之间的启动顺序</li>
<li>使用statefulSet部署一个etcd集群, statefulSet可以保证etcd节点的启动顺序按照yaml期望的方式部署, 但是还是需要指定各节点之间的连接关系</li>
</ul>
</blockquote>
<p>如果使用operator的话, 则只需要告诉operator, 这是一个3节点的etcd集群, etcd的版本是xxxx即可, 其它的工作都交由operator控制器去完成. operator声明的api对象<code>不再是单体应用的描述,而是整个分布式应用集群的动态逻辑</code></p>
<p>operator是在分布式应用领域发力,使用户自己定义kuberenetes的控制器来完成业务逻辑.</p>
<p><a href="https://github.com/operator-framework/awesome-operators?spm=a2c4e.11153940.blogcont685522.11.1e4394133MN7B9">openrator项目列表</a></p>
<p>那openrator是如何做到对分布式应用如此清晰快捷?</p>
<h3 id="TPR-x2F-CRD"><a href="#TPR-x2F-CRD" class="headerlink" title="TPR&#x2F;CRD"></a><strong>TPR&#x2F;CRD</strong></h3><p>TPR: Thrid Part Resousrce 第三方资源</p>
<p>CRD:Custom Resource Definition 资源自定义</p>
<p>在kubernetes 1.7之后，TPR升级为CRD, 总结来说,<code>用户可自己编写任何的资源类型,同时实现对该资源类型的控制器</code></p>
<p>本质上,CRD做为kubernetes的特殊资源,它的工作方式如下:</p>
<h3 id="Operator的工作方式"><a href="#Operator的工作方式" class="headerlink" title="Operator的工作方式"></a><strong>Operator的工作方式</strong></h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-2.png" alt="operator-c1-2"></p>
<p>一个operator案例应该实现以下特性:</p>
<blockquote>
<ul>
<li>Operator自身以deployment的方式部署</li>
<li>Operator自动创建一个CRD资源类型,用户可以用该类型创建CR</li>
<li>Operator利用Kubernetes内置的Serivce&#x2F;ReplicaSet等API管理应用</li>
<li>Operator应该向后兼容,并且在Operator自身退出或删除时不影响应用的状态</li>
<li>Operator应该支持应用版本更新</li>
</ul>
</blockquote>
<h3 id="使用Operator部署etcd集群"><a href="#使用Operator部署etcd集群" class="headerlink" title="使用Operator部署etcd集群"></a><strong>使用Operator部署etcd集群</strong></h3><blockquote>
<ul>
<li>搭建一个kubernetes集群,这里使用的是kubeadm搭建</li>
<li>下载官方维护的etcd-operator-git</li>
<li>创建RBAC规则,主要声明对哪些资源有操作权限</li>
<li>kubectl apply -f etcd-operator-deployment.yaml</li>
<li>指定etcd的节点数及版本</li>
<li>kubectl apply -f example-etcd-cluster.yaml</li>
</ul>
</blockquote>
<p>这样一个 etcd集群就部署完成,这些操作具体发生了什么?</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">当执行完etcd-operator-deployment.yaml后,启动一个容器,容器的entrypoing为etcd-operator</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">这个容器的作用是声明了一个CRD资源类型,在kubernetes中注册这个CRD,同时该CRD也是控制器,在集群中所有这个的CRD下的对象都将被watch</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">该类型名字为etcdclusters.etcd.database.coreos.com,这个名字在源代码的etcd-operator-master\pkg\apis\etcd\v1beta2\register.go中被定义</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">再比如,etcd增删改查都写在etcd-operator中etcd-operator-master\pkg\util\etcdutil</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">而在example-etcd-cluster.yaml则生成了etcdclusters.etcd.database.coreos.com的实例</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">该实例包含size跟version两个属性</span></span><br></pre></td></tr></table></figure>

<p>查看etcd的pod数是不是与预期相符</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-3.PNG" alt="operator-c1-3"></p>
<p>进容器查看etcd启动状态</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-4.PNG" alt="ceph-c1-4"></p>
<p>大家一定很好奇,etcd启动命令里为什么不是etcd节点ip, 而且一大串的像域名一样的字符?</p>
<p>这是因为etcd的启动命令是在pod启动之前就已经生成好的,这个时候pod还没启动,也是没有分配ip,所以使用域名的形式</p>
<p>这个域名又是如何产生的,通过这个域名又是如何找到pod的呢?</p>
<p>其实在生成etcd集群的同时etcd-operator会自动生成一个与example-etcd-cluster同名的headless service资源</p>
<p>service类型的资源会分配一个vip(可指定),访问service时由该vip转到后端某个pod上</p>
<p>headless service(spec.clusterIP为None)类型资源不会分配vip, 访问该service直接返回所有后端pod列表</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-5.jpg" alt="operator-c1-5"></p>
<p>查看生成的headless service与endpoints</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-6.PNG" alt="operator-c1-6"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-7.PNG" alt="operator-c1-7"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-8.PNG" alt="operator-c1-8"></p>
<p><code>当创建了一个headless service的资源时,它所代理的所有pod的ip地址都会绑定DNS记录,格式如下</code></p>
<p>比如: example-etcd-cluster-6n7hb78tkm.example-etcd-cluster.default.svc</p>
<p>尊崇以下规则 : pod名.service名.namespaces.svc</p>
<p><code>域名解析由kube-dns负责, endpoints由kube-proxy管理,kube-proxy会订阅所有service的变更从而更新iptalbes规则,endpointscontroller会订阅pod跟service对象的变更,并根据当前集群中的对象生成endpoint对象将service跟pod关联</code></p>
<p>关于kube-proxy与kube-dns,篇幅有限,可参考这两篇文章:</p>
<p><a href="https://blog.csdn.net/iiiiher/article/details/77099059">kube-dns架构图解</a></p>
<p><a href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/">service 和 kube-proxy 原理</a></p>
<p>整个服务发现流程:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-9.jpg" alt="operator-c1-9"></p>
<h3 id="etcd-operator常用命令"><a href="#etcd-operator常用命令" class="headerlink" title="etcd-operator常用命令"></a><strong>etcd-operator常用命令</strong></h3><h4 id="扩缩容"><a href="#扩缩容" class="headerlink" title="扩缩容"></a>扩缩容</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">当前的节点数为3, 版本为3.2.13</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3--&gt; 5</span></span><br><span class="line">vim example/example-etcd-cluster.yaml</span><br><span class="line">kubectl apply -f example/example-etcd-cluster.yaml</span><br></pre></td></tr></table></figure>

<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-10.PNG" alt="operator-c1-10"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-11.PNG" alt="operator-c1-11"></p>
<h4 id="版本升级"><a href="#版本升级" class="headerlink" title="版本升级"></a><strong>版本升级</strong></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">版本为3.2.13</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3.2.13 --&gt; 3.1.10</span></span><br><span class="line">vim example/example-etcd-cluster.yaml</span><br><span class="line">kubectl apply -f example/example-etcd-cluster.yaml</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kubectl describe pod时会发现docker 的启动命令没有改变</span></span><br></pre></td></tr></table></figure>

<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-12.PNG" alt="operator-c1-12"></p>
<h4 id="节点宕机"><a href="#节点宕机" class="headerlink" title="节点宕机"></a><strong>节点宕机</strong></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除一个etcd pod, 这里删除种子节点</span></span><br><span class="line">kubectl delete pod example-etcd-cluster-k6vklz2frv --now</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">etcd-operator会创建一个新节点出来</span></span><br></pre></td></tr></table></figure>

<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-13.PNG" alt="operator-c1-13"></p>
<p>etcd-operator会创建一个新节点出来</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-14.PNG" alt="operator-c1-14"></p>
<p>在原etcd容器中已无法ping通被删除pod</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-15.PNG" alt="operator-c1-15"></p>
<p>而新生成的etcd节点的启动命令则不会包含被删除的节点</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-16.PNG" alt="operator-c1-16"></p>
<p>查看etcd member list, 旧节点已删除,新节点加入</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/operator-c1-17.PNG" alt="operator-c1-17"></p>
<h4 id="备份还原"><a href="#备份还原" class="headerlink" title="备份还原"></a><strong>备份还原</strong></h4><p>目前官方维护了etcd-backup-operator,支持远程备份到AWS S3&#x2F; Azure Blob Service (ABS)&#x2F;Google Cloud Storage (GCS)</p>
<p>或者从上述几个远程存储上还原etcd数据,etcd-restore-operator</p>
<p>同时官方维护了一个<a href="https://coreos.com/operators/etcd/docs/latest/user/backup_cronjob/README.html">定时备份任务</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.hi-linux.com/posts/40915.html">etcd入门</a></li>
<li><a href="https://coreos.com/operators/etcd/docs/latest/user/walkthrough/backup-operator.html">etcd-backup-operator</a></li>
<li><a href="https://github.com/coreos/etcd-operator">etcd-operator-git</a></li>
<li><a href="https://yq.aliyun.com/articles/685522">Kubernetes API 与 Operator，不为人知的开发者战争</a></li>
<li><a href="https://blog.csdn.net/iiiiher/article/details/77099059">kube-dns架构图解</a></li>
<li><a href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/">service 和 kube-proxy 原理</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(etcd)</title>
    <url>/2018/08/16/Kubernetes%E5%AD%A6%E4%B9%A0(etcd)/</url>
    <content><![CDATA[<h3 id="WHAT-etcd"><a href="#WHAT-etcd" class="headerlink" title="WHAT etcd"></a><strong>WHAT etcd</strong></h3><blockquote>
<ul>
<li>go language, open-source</li>
<li>distributed,highly-available key-value store</li>
<li>shared configuration</li>
<li>service discovery</li>
</ul>
</blockquote>
<span id="more"></span>

<h3 id="WHY-etcd"><a href="#WHY-etcd" class="headerlink" title="WHY etcd"></a><strong>WHY etcd</strong></h3><blockquote>
<ul>
<li>API based on HTTP+JSON</li>
<li>support SSL&#x2F;TLS Auth</li>
<li>1000 request&#x2F;s&#x2F;n</li>
<li>high performance,highly-available high-consistency</li>
</ul>
</blockquote>
<h3 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a><strong>CAP理论</strong></h3><p>谈到分布式系统,不得不提CAP理论,先从两张经典的照片说起:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/cap1.jpg" alt="cap1"></p>
<p><code>名词解析:</code></p>
<blockquote>
<ul>
<li>Partition Tolerance:分区容错性,在分布式系统中,可能会出现分区之间无法通信</li>
<li>Consistency: 一致性:同一个数据在集群中的所有节点,同一时刻是否都是同样的值</li>
<li>Availability: 可用性:集群中一部分节点故障后,集群整体是否还能处理客户端的请求</li>
</ul>
</blockquote>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/cap2.png" alt="cap2"></p>
<p><strong>CAP理论:对于分布式系统,C,A,P无法同时满足</strong></p>
<blockquote>
<ul>
<li>分区容错(P)是基本要求,C跟A不能同时满足,CAP理论的C指的是强一致性</li>
<li>强一致性: 当更新操作完成之后,任何多个后续进程的访问都会返回最新的更新过的值.这种是对用户最友好的,就是用户上一次写什么,下一次就保证能读到什么.根据 CAP 理论,这种实现需要牺牲可用性</li>
<li>弱一致性:   系统并不保证后续进程访问都会返回最新的更新过的值.系统在数据写入成功之后,不承诺立即可以读到最新写入的值,也不会具体的承诺多久之后可以读到.</li>
<li>最终一致性: 弱一致性的特定形式,即系统保证在没有后续更新的前提下,经过一个时间窗口后能访问到更新后的数据</li>
</ul>
</blockquote>
<p><strong>一致性问题是由并发问题产生的</strong></p>
<p><strong>CAP无法同时满足是由网络分区产生的</strong></p>
<p><code>etcd是如何保证强一致性同时可用性也很高的呢?</code></p>
<h3 id="etcd架构"><a href="#etcd架构" class="headerlink" title="etcd架构"></a><strong>etcd架构</strong></h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/etcd2.png" alt="etcd2"></p>
<blockquote>
<ul>
<li>HTTP Server: 用于接收用户发送的API请求及心跳请求转发到具体模块.</li>
<li>Raft: etcd数据强一致性算法的具体实现.</li>
<li>WAL: etcd的数据存储方式.</li>
<li>Store: 用于处理etcd支持的各类功能的事务,包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等,是etcd 对用户提供的大多数API功能的具体实现.</li>
</ul>
</blockquote>
<h3 id="etcd角色"><a href="#etcd角色" class="headerlink" title="etcd角色"></a><strong>etcd角色</strong></h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/etcd3.jpg" alt="etcd3"></p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a><strong>工作原理</strong></h3><p>etcd通常做为一个分布式系统,由多个节点相互通信构成整体对外服务,任意时刻至多存在一个有效的Leader节点,同时每个etcd节点都维护了一个状态机,每个节点都存储了完整的数据.<br>主节点处理所有来自客户端写操作,每次写请求都需要集群中大多数据节点投票能过后Leader节点才能将修改内部状态机,并返回将结果返回给客户端<br>follower节点只响应来自领导人和候选人的请求.领导人来处理所有来自客户端的请求(默认设置下,如果一个客户端与追随者进行通信,追随者会将信息发送给领导人)<br><strong>通过Raft协议保证集群所有节点的数据是一致的</strong></p>
<h3 id="etcd选举"><a href="#etcd选举" class="headerlink" title="etcd选举"></a><strong>etcd选举</strong></h3><p>当集群初始化时候,每个节点都是Follower角色,集群中存在至多1个有效的主节点,通过心跳与其他节点同步数据,心跳周期默认为100ms.<br>当Follower在一定时间内没有收到来自主节点的心跳,会将自己角色改变为Candidate,并发起一次选主投票；当收到包括自己在内超过半数节点赞成后,选举成功(选举为Leader的节点一定包含所有已提交的日志),当收到票数不足半数选举失败,或者选举超时,(默认选举时间为1000ms).若本轮未选出主节点,将进行下一轮选举(出现这种情况,是由于多个节点同时选举,所有节点均为获得过半选票,为了避免陷入选主失败循环,每个节点未收到心跳发起选举的时间是一定范围内的随机值,这样能够避免2个节点同时发起选主).<br>Candidate节点收到来自主节点的信息后,会立即终止选举过程,进入Follower角色.<br><strong>通过RAFT协议保证在符合条件的下一定能够选主成功</strong></p>
<h3 id="RAFT协议"><a href="#RAFT协议" class="headerlink" title="RAFT协议"></a><strong>RAFT协议</strong></h3><p>关于Raft的几个重要的<code>结论</code>:</p>
<blockquote>
<ul>
<li>RAFT充分利用了go语言的并发模型,使得etcd性能强悍.</li>
<li>节点数据一致性通过日志复制(wal, Write Ahead Log,预写日志)实现,每次有数据更新的时候产生二阶段提交(two-phase commit),第一阶段先提交日志,通过heartbeat把该日志广播给其它follow,如果超过半数的follower都成功写了log,那么leader开始第二阶段的提交:正式写入数据,然后同样广播给follower,follower也根据自身情况选择写入或者不写入并返回结果给leader。leader先写自己的数据,然后告诉follower也开始持久化数据.这两阶段中如果任意一个都有超过半数的follower返回false或者根本没有返回,那么这个分布式事务是不成功的.</li>
<li>领导人完全原则(Leader Completeness Property):被选举为领导人的节点一定拥有所有已经被提交的日志条目,读写请求只向主节点发送(v2版本),以此保证一致性,(v3版本中,支持在客户端指定读请求方式,默认为etcdctl –consistency&#x3D;’l’,说明使用线性读,&#x3D;’s’则使用串行读,这种方式读可以落到集群任一节点上,该节点直接从本地读取结束返回给客户端,不再需要把请求转给leader, 但可能会返回脏数据).</li>
<li>默认配置下,领导人在处理只读的请求之前必须检查自己是否还是Leader(如果一个更新的领导人被选举出来,它自己的信息就已经过期了).</li>
</ul>
</blockquote>
<h3 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a><strong>日志复制</strong></h3><p>etcd的数据一致性是通过raft的日志复制实现的,在etcd&#x2F;member&#x2F;目录中有两个子目录,snap和wal.那么为什么Etcd会有snap目录呢？<br>主要有两个原因:</p>
<blockquote>
<ul>
<li>snapshot是wal快照,wal一直在追加日志占用资源大,为了节约磁盘空间,当wal文件达到一定数据,就会对之前的数据进行压缩,形成快照,但对于etcd来说,snapshot是一种昂贵的操作,默认情况下,每10000次变更会生成一次,可通过api修改,etcd –snapshot-count&#x3D;5000</li>
<li>当新的节点加入到集群中,为了同步数据,就会把snapshot发送到新节点,这样能够节约传输数据(生成的快照文件比wal文件要小很多,5倍左右),使之尽快加入到集群中</li>
</ul>
</blockquote>
<h3 id="etcd在云翼中的使用"><a href="#etcd在云翼中的使用" class="headerlink" title="etcd在云翼中的使用"></a><strong>etcd在云翼中的使用</strong></h3><p>现阶段云翼中使用到的etcd有hubor(hubor正逐渐实现用mysql替换etcd),captain.balloon中借用k8s向etcd写入deployment数据.captain中使用etcd存储网段信息,最后容器在node上启动之后通过cni调用captain获取容器ip.</p>
<p>每个机房部署一个k8s-master,3个etcd组成一个集群,各机房集群独立,etcd服务相对稳定,加上日常的定时备份机制,同时出现两个节点都契机的可能性很小,万一出现整个机房故障的话,master都挂了,etcd也无法服务.</p>
<p>同时,etcd的raft机制不太合适跨机房部署, <code>网络延迟高的话,很容易重新选举</code>,两个机房部署一套集群,如果一个机房整体故障,别一个机房还是无法正常工作.而如果采用两个机房部署两套集群,再通过其它工具在这两个集群之间同步数据,则受网络因素影响较大.</p>
<p>etcd的接口分为v2,v3接口,默认使用v2接口,v3接口中存留了k8s的包括deployment,servcice等元信息,而在v2保留有pod的业务数据</p>
<p><code>从上面也可看出,etcd做为配置共享的k-v系统,key跟value的值不能太大,官方建议value大小为1m.</code></p>
<h3 id="etcd常用命令"><a href="#etcd常用命令" class="headerlink" title="etcd常用命令"></a><strong>etcd常用命令</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">etcd集群启动</span></span><br><span class="line">/export/servers/etcd/etcd --data-dir=/export/Data/etcd --listen-peer-urls http://10.226.1.168:2380 --initial-advertise-peer-urls http://10.226.1.168:2380 --listen-client-urls http://10.226.1.168:2379,http://10.226.1.13:2379 --advertise-client-urls http://10.226.1.168:2379 --name 10.226.1.168  --initial-cluster 10.226.1.13=http://10.226.1.13:2380,10.226.1.45=http://10.226.1.45:2380,10.226.1.168=http://10.226.1.168:2380 --initial-cluster-state new</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动命令参数说明:</span> </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">data-dir: 指定节点的数据存储目录,这些数据包括节点ID,集群ID,集群初始化配置,Snapshot文件,若未指定—wal-dir,还会存储WAL文件,集群中所有节点的数据存储目录都必须保持一致</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">wal-dir: 指定节点的was文件的存储目录,若指定了该参数,wal文件会和其他数据文件分开存储。</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">name: 节点名称</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-advertise-peer-urls: 告知集群其它节点通信url</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">listen-peer-urls: 告知客户端监控url</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">advertise-client-urls: 告知客户端url, 也就是服务的url</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-cluster-token: 集群的ID,为了防止同一台机器上有多个etcd集群混淆</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-cluster: 集群中所有节点</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-cluster-state new 一般用于事先已知节点信息</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-cluster-state existing 往已存在的集群中增加或删除节点</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">initial-cluster-state --force-new 强制启动一个新集群，会重置集群<span class="built_in">id</span>和集群所有成员信息</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">默认情况下通过etcd的2379/4001接收客户端请求,2380/7001 端口在各个节点中同步 raft 状态及数据</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看集群状态:</span></span><br><span class="line">/export/server/etcd/etcdctl --endpoint http://10.226.1.13:2379 cluster-health</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看集群成员</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoints http://10.226.1.13:2379 member list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">导出集群所有数据到文件中</span></span><br><span class="line">/export/server/etcd/etcdctl --endpoint http://10.226.1.13:2379 ls --recursive -p | grep -v &#x27;/$&#x27; | xargs -n 1 -I% sh -c &#x27;echo -n %:; ./etcdctl --endpoint http://10.226.1.13:2379 get %;&#x27; &gt; etcd_keys.txt</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">备份</span></span><br><span class="line">/export/server/etcd/etcdctl backup --data-dir /export/Data/etcd --backup-dir /tmp/etcd_bkup</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">恢复</span></span><br><span class="line">/export/servers/etcd/etcd -data-dir=/tmp/etcd_backup/ --force-new-cluster</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清空集群指定目录下的所有数据</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 rm --recursive /captain</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">增加key</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 put /captain/pod/10.226.96.2 xxx</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除key</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 del /captain/pod/10.226.96.2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看key</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 get /captain/pod/10.226.96.2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改key</span></span><br><span class="line">/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 set /captain/pod/10.226.97.23 &quot;$(/export/servers/etcd/etcdctl --endpoint http://10.226.1.13:2379 get /captain/pod/10.226.97.23 |sed &#x27;s/jstack-31-41-3164228140-jbkgv/jstack-31-41-3819379906-vm2sj/g&#x27;)&quot;</span><br></pre></td></tr></table></figure>

<h3 id="etcd性能"><a href="#etcd性能" class="headerlink" title="etcd性能"></a><strong>etcd性能</strong></h3><p><a href="">etcd官网</a>上的性能测试数据</p>
<p>google cloud computer engine&#x2F;3 machine of 8 CPUs+16GB mem+50GB ssd&#x2F;ubuntu 17.04&#x2F;etd3.2.0,go1.8.3</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/etcd4.jpg" alt="etcd4"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/etcd5.jpg" alt="etcd5"></p>
<h3 id="etcd-VS-zookeeper"><a href="#etcd-VS-zookeeper" class="headerlink" title="etcd VS zookeeper"></a><strong>etcd VS zookeeper</strong></h3><p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/etcd_vs_zookeeper.png" alt="etcd_vs_zookeeper"></p>
<p>关于zookeeper,下次单独做个分享.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.hi-linux.com/posts/40915.html">etcd入门</a></li>
<li><a href="https://coreos.com/etcd/docs/latest/demo.html">etcd Demo</a></li>
<li><a href="https://coreos.com/etcd/docs/latest/">etcd Doc</a></li>
<li><a href="http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle">etcd:从应用场景到实现原理的全方位解读</a></li>
<li><a href="https://raft.github.io/">RAFT</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(亲和性及反亲和性)</title>
    <url>/2018/09/23/Kubernetes%E5%AD%A6%E4%B9%A0(%E4%BA%B2%E5%92%8C%E6%80%A7%E5%8F%8A%E5%8F%8D%E4%BA%B2%E5%92%8C%E6%80%A7)/</url>
    <content><![CDATA[<p>经常听到K8S里亲和性与反亲和性, 这其实是更复杂的调度策略, 用在需要更细力度的控制节点调度的场景.<code>比如不希望对外的一些服务和内部的服务跑在同一个节点上了,害怕内部服务对外部的服务产生影响;但是有的时候我们的服务之间交流比较频繁,又希望能够将这两个服务的 Pod 调度到同一个的节点上.</code></p>
<p>这就需要用到 Kubernetes 里面的一个概念: <code>亲和性和反亲和性</code></p>
<span id="more"></span>

<p>还是要提出非常重要的一点,在K8S的调度流程:</p>
<blockquote>
<ul>
<li>(预选)先排除完全不符合pod运行要求的节点</li>
<li>(优先)根据一系列算法,算出node的得分,最高没有相同的,就直接选择</li>
<li>如果有相同的话,就随机选一个</li>
</ul>
</blockquote>
<p>亲和性与反亲和性也遵循以上规律.</p>
<p>在说亲和性特点之前,有必要再说一说nodeselect.</p>
<h3 id="nodeSelect"><a href="#nodeSelect" class="headerlink" title="nodeSelect"></a><strong>nodeSelect</strong></h3><p>大多数情况下, 如果应用没有特殊要求, 也不会去设定Nodeselect, 因为大体上容器所运行的机器都具有相同的属性, 同时, nodeselect也是一种非常常用的调度方式, 比如某些应用对磁盘IO要高要求, 切好有一批机器是SSD硬盘, 那么这个时候就可以给这些机器打上tag, 在k8s里称之为label, 这样的话,我在调度POD的时候通过这个label来进行机器的选择, 我们可以使用–show-labels也查看node节点所具有的标签.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes --show-labels</span></span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS </span><br><span class="line">master    Ready     master    147d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    67d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    127d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure>

<p>我们再来看看一个标准的deployment的yaml文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">busybox-pod</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-busybox</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;3600&quot;</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-busybox</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">com:</span> <span class="string">youdianzhishi</span></span><br></pre></td></tr></table></figure>

<p>这里提定了nodeselect, 即这个deployment会调度到带有com的label上, 如果所有node里没有该属性的node, 则我们的 Pod 就会一直处于 Pending 状态, 直到有为此.</p>
<p><code>所以, nodeSelect是一个硬限制</code></p>
<h3 id="亲和性-Affinity"><a href="#亲和性-Affinity" class="headerlink" title="亲和性(Affinity)"></a><strong>亲和性(<strong>Affinity</strong>)</strong></h3><p> 那如果我想做到某些密切调用的应用部署到同一台机器上,这样可以减少网络传输等消耗过程, 有好办法吗?</p>
<p>当时, 使用nodeselect肯定是可以的, 只要在deployment中使用相同的label即可. 但是如果我想更精确的控制POD的调度行为呢？</p>
<p><code>亲和性/反亲和性特性就是一种更细粒度的控制策略</code></p>
<p>亲和性又可以分为:</p>
<blockquote>
<ul>
<li>节点亲和性(nodeAffinity): 节点亲和性主要是用来控制 pod 要部署在哪些主机上,以及不能部署在哪些主机上的</li>
<li>pod亲和性(podAffinity): pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题</li>
</ul>
</blockquote>
<p>同时, 亲和性在执行程度上,又可分为:</p>
<blockquote>
<ul>
<li><code>软策略(preferredDuringSchedulingIgnoredDuringExecution)</code>:  如果你没有满足调度要求的节点的话,pod 就会忽略这条规则,继续完成调度过程,说白了就是<strong>满足条件最好了,没有的话也无所谓了</strong>的策略</li>
<li><code>硬策略(requiredDuringSchedulingIgnoredDuringExecution)</code>: 比较强硬了,如果没有满足条件的节点的话,就不断重试直到满足条件为止,简单说就是<strong>你必须满足我的要求,不然我就不干</strong>的策略</li>
<li><code>RequiredDuringSchedulingRequiredDuringExecution</code>: 在调度期间要求满足亲和性或者反亲和性规则,如果不能满足规则,则POD不能被调度到对应的主机上. 在之后的运行过程中,如果因为某些原因（比如修改label）导致规则不能满足,系统会尝试把POD从主机上删除</li>
</ul>
</blockquote>
<h4 id="节点亲和性"><a href="#节点亲和性" class="headerlink" title="节点亲和性"></a><strong>节点亲和性</strong></h4><p>如上所说, 节点亲和性限定的是pod需要调度到什么资源上, <code>是node与pod的调度关系</code>,同时它支持一些逻辑运算, 比如in,not in之类的.</p>
<p>比如说,现在我们用一个 Deployment 来管理3个 pod 副本,现在我们来控制下这些 pod 的调度</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">affinity</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">affinity</span></span><br><span class="line">        <span class="attr">role:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginxweb</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">NotIn</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">node03</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 软策略</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">preference:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">com</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">youdianzhishi</span></span><br></pre></td></tr></table></figure>

<p>上面的deployment里同时指定了硬策略与软策略,满足的需求为:该节点的label不能是(硬性条件)kubernetes.io&#x2F;hostname&#x3D;node03, 除此之外, 如果存在com&#x3D;youdianzhishi的节点,则尽可能调度(也不一定能成功).</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes --show-labels</span></span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure>

<p>调度情况如下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f node-affinity-demo.yaml</span></span><br><span class="line">deployment.apps &quot;affinity&quot; created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -l app=affinity -o wide</span></span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-7b4c946854-5gfln   1/1       Running   0          47s       10.244.4.214   node02</span><br><span class="line">affinity-7b4c946854-l8b47   1/1       Running   0          47s       10.244.4.215   node02</span><br><span class="line">affinity-7b4c946854-r86p5   1/1       Running   0          47s       10.244.4.213   node02</span><br></pre></td></tr></table></figure>

<p>从结果可以看出 pod 都被部署到了 node02,其他节点上没有部署 pod,这里的匹配逻辑是 label 的值在某个列表中,现在Kubernetes提供的操作符有下面的几种:</p>
<blockquote>
<ul>
<li>In: label 的值在某个列表中</li>
<li>NotIn: label 的值不在某个列表中</li>
<li>Gt: label 的值大于某个值</li>
<li>Lt: label 的值小于某个值</li>
<li>Exists: 某个 label 存在</li>
<li>DoesNotExist: 某个 label 不存在</li>
</ul>
</blockquote>
<p>同时对于nodeSelectorTerms与matchExpressions</p>
<blockquote>
<ul>
<li>如果<code>nodeSelectorTerms</code>下面有多个选项的话,满足任何一个条件就可以了</li>
<li>如果<code>matchExpressions</code>有多个选项的话,则必须同时满足这些条件才能正常调度 POD</li>
</ul>
</blockquote>
<p>node的反亲和性则只要使用上面列出的逻辑运算就可以很好的先排除一些不符合条件的节点.</p>
<h4 id="POD亲和性"><a href="#POD亲和性" class="headerlink" title="POD亲和性"></a><strong>POD亲和性</strong></h4><p>pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现,可以是单个主机,也可以是多个主机组成的 cluster、zone 等等）,而 pod 反亲和性主要是解决 pod 不能和哪些 pod 部署在同一个拓扑域中的问题,它们都是处理的 <code>pod 与 pod 之间的调度</code>,比如一个 pod 在一个节点上了,那么我这个也得在这个节点,或者你这个 pod 在节点上了,那么我就不想和你待在同一个节点上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get nodes --show-labels</span></span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">master    Ready     master    154d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master,node-role.kubernetes.io/master=</span><br><span class="line">node02    Ready     &lt;none&gt;    74d       v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,course=k8s,kubernetes.io/hostname=node02</span><br><span class="line">node03    Ready     &lt;none&gt;    134d      v1.10.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,jnlp=haimaxy,kubernetes.io/hostname=node03</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">affinity</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">15</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">affinity</span></span><br><span class="line">        <span class="attr">role:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginxweb</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">busybox-pod</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br></pre></td></tr></table></figure>

<p>上面的deployment里指定了硬策略,topologykey表示某个拓扑域, 节点满足的需求为必须包含app&#x3D;busybox-pod.</p>
<p>目前只有node02上面有app&#x3D;busybox-pod的label,所以理想情况是3个pod都 应该在node02上才符合预期</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide -l app=busybox-pod</span></span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">test-busybox   1/1       Running   164        7d        10.244.4.205   node02</span><br></pre></td></tr></table></figure>

<p>查看pod调度情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide -l app=affinity</span></span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-564f9d7db9-lzzvq   1/1       Running   0          3m        10.244.4.216   node02</span><br><span class="line">affinity-564f9d7db9-p79cq   1/1       Running   0          3m        10.244.4.217   node02</span><br><span class="line">affinity-564f9d7db9-spfzs   1/1       Running   0          3m        10.244.4.218   node02</span><br></pre></td></tr></table></figure>

<p>我们尝试把node02上的app&#x3D;busybox-pod这个pod删掉,然后再进行调度</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f node-selector-demo.yaml</span></span><br><span class="line">pod &quot;test-busybox&quot; deleted</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f pod-affinity-demo.yaml</span></span><br><span class="line">deployment.apps &quot;affinity&quot; deleted</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f pod-affinity-demo.yaml</span></span><br><span class="line">deployment.apps &quot;affinity&quot; created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide -l app=affinity</span></span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE       IP        NODE</span><br><span class="line">affinity-564f9d7db9-fbc8w   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-n8gcf   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br><span class="line">affinity-564f9d7db9-qc7x6   0/1       Pending   0          2m        &lt;none&gt;    &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>会发现3个pod都是Pending状态, 原因就是没有任何一个node符合app&#x3D;busybox-pod这个条件, pod则会无限期pending.</p>
<h3 id="反亲和性-antiAffinity"><a href="#反亲和性-antiAffinity" class="headerlink" title="反亲和性(antiAffinity)"></a><strong>反亲和性(antiAffinity)</strong></h3><p>当时node是没有nodeaAntiAffinity的, 对于node来说, 反亲和就是使用not 取反就行.</p>
<p>podAntiAffinity则是说如果一个节点上运行了某个 pod,那么我们的 pod 则希望被调度到其他节点上去.</p>
<p>比如还是上面的例子, 我修改一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: affinity</span><br><span class="line">  labels:</span><br><span class="line">    app: affinity</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  revisionHistoryLimit: 15</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: affinity</span><br><span class="line">        role: test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: nginxweb</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略</span><br><span class="line">          - labelSelector:</span><br><span class="line">              matchExpressions:</span><br><span class="line">              - key: app</span><br><span class="line">                operator: In</span><br><span class="line">                values:</span><br><span class="line">                - busybox-pod</span><br><span class="line">            topologyKey: kubernetes.io/hostname</span><br></pre></td></tr></table></figure>

<p>只是把podAffinity变成 了podAntiAffinity,则达到的效果为如果某个节点上有app&#x3D;busybox-pod,则不会调度到该node上.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f pod-antiaffinity-demo.yaml</span></span><br><span class="line">deployment.apps &quot;affinity&quot; created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide</span></span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE       IP             NODE</span><br><span class="line">affinity-bcbd8854f-br8z8                  1/1       Running     0          5s        10.244.4.222   node02</span><br><span class="line">affinity-bcbd8854f-cdffh                  1/1       Running     0          5s        10.244.4.223   node02</span><br><span class="line">affinity-bcbd8854f-htb52                  1/1       Running     0          5s        10.244.4.224   node02</span><br><span class="line">test-busybox                              1/1       Running     0          23m       10.244.2.10    node03</span><br></pre></td></tr></table></figure>

<p>当然这里也可以In换成NotIn.</p>
<p>上边的例子中可以通过修改topologyKey来限制拓扑域的范围,实现把相关服务部署在不同的容灾域等其它功能</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>最后总结一下三种亲和性和反亲和性策略的比较如下表所示:</p>
<p>支持的操作符:</p>
<table>
<thead>
<tr>
<th>策略名称</th>
<th>匹配目标</th>
<th>支持的操作符</th>
<th align="center">支持拓扑域 设计</th>
<th>目标</th>
</tr>
</thead>
<tbody><tr>
<td>nodeAffinity</td>
<td>主机标签</td>
<td>In,NotIn,Exists,DoesNotExist,Gt,Lt</td>
<td align="center">不支持</td>
<td>决定Pod可以部署在哪些主机上</td>
</tr>
<tr>
<td>podAffinity</td>
<td>Pod标签</td>
<td>In,NotIn,Exists,DoesNotExist</td>
<td align="center">支持</td>
<td>决定Pod可以和哪些Pod部署在同一拓扑域</td>
</tr>
<tr>
<td>PodAntiAffinity</td>
<td>Pod标签</td>
<td>In,NotIn,Exists,DoesNotExist</td>
<td align="center">支持</td>
<td>决定Pod不可以和哪些Pod部署在同一拓扑域</td>
</tr>
</tbody></table>
<p>场景:</p>
<p>**nodeAffinity使用场景 **:</p>
<blockquote>
<ul>
<li>将S1服务的所有Pod部署到指定的符合标签规则的主机上</li>
<li>将S1服务的所有Pod部署到除部分主机外的其他主机上</li>
</ul>
</blockquote>
<p>**podAffinity使用场景 **:</p>
<blockquote>
<ul>
<li>将某一特定服务的pod部署在同一拓扑域中,不用指定具体的拓扑域</li>
<li>如果S1服务使用S2服务,为了减少它们之间的网络延迟（或其它原因）,把S1服务的POD和S2服务的pod部署在同一拓扑域中</li>
</ul>
</blockquote>
<p><strong>podAntiAffinity使用场景:</strong></p>
<blockquote>
<ul>
<li>将一个服务的POD分散在不同的主机或者拓扑域中,提高服务本身的稳定性</li>
<li>给POD对于一个节点的独占访问权限来保证资源隔离,保证不会有其它pod来分享节点资源</li>
<li>把可能会相互影响的服务的POD分散在不同的主机上</li>
</ul>
</blockquote>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(污点及容忍)</title>
    <url>/2018/11/16/Kubernetes%E5%AD%A6%E4%B9%A0(%E6%B1%A1%E7%82%B9%E5%8F%8A%E5%AE%B9%E5%BF%8D)/</url>
    <content><![CDATA[<p> 上次说了kubernetes里的<code>亲和性和反亲和性</code>,今天说一说污点以及容忍特性.</p>
<span id="more"></span>

<h3 id="污点-Taints"><a href="#污点-Taints" class="headerlink" title="污点(Taints)"></a><strong>污点(Taints)</strong></h3><p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式,都是调度 pod 到预期节点上,而<code>Taints</code>恰好与之相反,如果一个节点标记为 Taints ,除非 pod 也被标识为可以容忍污点节点,否则该 Taints 节点不会被调度 pod</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe node master</span></span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=master</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">......</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>我们可以使用上面的命令查看 master 节点的信息,其中有一条关于 Taints 的信息:node-role.kubernetes.io&#x2F;master:NoSchedule,就表示给 master 节点打了一个污点的标记,其中影响的参数是<code>NoSchedule</code>,表示 pod 不会被调度到标记为 taints 的节点,除了 NoSchedule 外,还有另外两个选项:</p>
<blockquote>
<ul>
<li><code>PreferNoSchedule</code>:NoSchedule 的软策略版本,表示尽量不调度到污点节点上去</li>
<li><code>NoExecute</code>:该选项意味着一旦 Taint 生效,如该节点内正在运行的 pod 没有对应 Tolerate 设置,会直接被逐出</li>
</ul>
</blockquote>
<p>污点 taint 标记节点的命令如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">taint</span> <span class="string">nodes</span> <span class="string">node02</span> <span class="string">test=node02:NoSchedule</span></span><br><span class="line"><span class="string">node</span> <span class="string">&quot;node02&quot;</span> <span class="string">tainted</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果要取消污点节点</span></span><br><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">taint</span> <span class="string">nodes</span> <span class="string">node02</span> <span class="string">test-</span></span><br><span class="line"><span class="string">node</span> <span class="string">&quot;node02&quot;</span> <span class="string">untainted</span></span><br></pre></td></tr></table></figure>

<p>上面的命名将 node02 节点标记为了污点,影响策略是 NoSchedule,只会影响新的 pod 调度,如果仍然希望某个 pod 调度到 taint 节点上,则必须在 Spec 中做出<code>Toleration</code>定义,才能调度到该节点,比如现在我们想要将一个 pod 调度到 master 节点</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">taint</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">taint</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">taint</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure>

<p>由于 master 节点被标记为了污点节点,所以我们这里要想 pod 能够调度到 master 节点去,就需要增加容忍的声明</p>
<h3 id="容忍-Tolerations"><a href="#容忍-Tolerations" class="headerlink" title="容忍(Tolerations)"></a><strong>容忍(Tolerations)</strong></h3><p> 上面的例子中有如下声明</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">   <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">   <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们来看一看结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f taint-demo.yaml</span></span><br><span class="line">deployment.apps &quot;taint&quot; created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -o wide</span></span><br><span class="line">NAME                                      READY     STATUS             RESTARTS   AGE       IP             NODE</span><br><span class="line">......</span><br><span class="line">taint-845d8bb4fb-57mhm                    1/1       Running            0          1m        10.244.4.247   node02</span><br><span class="line">taint-845d8bb4fb-bbvmp                    1/1       Running            0          1m        10.244.0.33    master</span><br><span class="line">taint-845d8bb4fb-zb78x                    1/1       Running            0          1m        10.244.4.246   node02</span><br></pre></td></tr></table></figure>

<p>我们可以看到有一个 pod 副本被调度到了 master 节点,这就是容忍的使用方法.<br>对于 tolerations 属性的写法,其中的 key、value、effect 与 Node 的 Taint 设置需保持一致, 还有以下几点说明:</p>
<blockquote>
<ul>
<li>如果 operator 的值是 Exists,则 value 属性可省略</li>
<li>如果 operator 的值是 Equal,则表示其 key 与 value 之间的关系是 equal(等于)</li>
<li>如果不指定 operator 属性,则默认值为 Equal</li>
<li>空的 key 如果再配合 Exists 就能匹配所有的 key 与 value,也是是能容忍所有 node 的所有 Taints</li>
<li>空的 effect 匹配所有的 effect</li>
</ul>
</blockquote>
<p>其实我们在使用kubeadm创建kubernetes集群的时候, 默认情况下master是不会被调度pod的, 其实就是给master节点打了一个污点.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(当输入kubectl run时到底发生了什么)</title>
    <url>/2020/04/27/Kubernetes-what-happend-when-type-kubectl-run/</url>
    <content><![CDATA[<p>该文非常详细地说明了当输入kubectl run 时，整个流程到底发生了什么, 强烈推荐一读.</p>
<p>该文为转发文章, 详细查看<a href="https://github.com/jamiehannaford/what-happens-when-k8s/tree/master/zh-cn">github</a></p>
<span id="more"></span>

<blockquote>
<p>为了确保整体的简单性和易上手，Kubernetes 通过一些简单的抽象隐去操作背后的复杂逻辑，但作为一名有梦想的工程师，掌握其背后的真正思路是十分有必要的。本文以 Kubectl 创建 Pod 为例，向你揭露从客户端到 Kubelet 的请求的完整生命周期。</p>
</blockquote>
<p>想象一下，当你想在 Kubernetes 集群部署 Nginx 时，你会执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl run nginx --image=nginx --replicas=3</span><br></pre></td></tr></table></figure>

<p>几秒后，你将看到三个 Nginx Pod 分布在集群工作节点上。这相当神奇，但它背后究竟发生了什么？</p>
<p>Kubernetes 是一个神奇的框架，它通过用户友好（user-friendly）的 API 处理跨基础架构的 Workload 部署。通过简单的抽象隐藏了背后的复杂性。但是，为了充分理解它为我们提供的价值，我们需要理解它的原理。</p>
<p>本指南将带领你充分了解从 Kubectl 客户端到 Kubelet 请求的完整生命周期，并在必要时通过源代码解释它到底是什么。</p>
<p><strong>注</strong>：本文所有内容基于 <code>Kubernetes v1.14.0</code>。</p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li><a href="#what-happens-when-i-type-kubectl-run">What happens when I type kubectl run?</a><ul>
<li><a href="#kubectl">Kubectl</a><ul>
<li><a href="#validation-and-generators">Validation and generators</a></li>
<li><a href="#api-groups-and-version-negotiation">API groups and version negotiation</a></li>
<li><a href="#client-auth">Client auth</a></li>
</ul>
</li>
<li><a href="#kube-apiserver">kube-apiserver</a><ul>
<li><a href="#authentication">Authentication</a></li>
<li><a href="#authorization">Authorization</a></li>
<li><a href="#admission-controller">Admission Controller</a></li>
</ul>
</li>
<li><a href="#etcd">etcd</a></li>
<li><a href="#control-loops">Control loops</a><ul>
<li><a href="#deployment-controller">Deployment Controller</a></li>
<li><a href="#replicaset-controller">ReplicaSet Controller</a></li>
<li><a href="#informers">Informers</a></li>
<li><a href="#scheduler">Scheduler</a></li>
</ul>
</li>
<li><a href="#kubelet">Kubelet</a><ul>
<li><a href="#pod-sync">Pod Sync</a></li>
<li><a href="#cri-and-pause-container">CRI and pause container</a></li>
<li><a href="#cni-and-pod-networking">CNI and pod networking</a></li>
<li><a href="#inter-host-networking">Inter-host networking</a></li>
<li><a href="#container-startup">Container startup</a></li>
</ul>
</li>
<li><a href="#wrap-up">Wrap-up</a></li>
</ul>
</li>
</ul>
<p>Created by <a href="https://github.com/ekalinin/github-markdown-toc">gh-md-toc</a></p>
<h2 id="Kubectl"><a href="#Kubectl" class="headerlink" title="Kubectl"></a>Kubectl</h2><h3 id="Validation-and-generators"><a href="#Validation-and-generators" class="headerlink" title="Validation and generators"></a>Validation and generators</h3><p>首先，当我们敲下回车键执行命令后， Kubectl 会执行客户端验证，以确保非法的请求（例如，创建不支持的资源或使用<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L264">格式错误的镜像名称</a>）快速失败，并不会发送给 kube-apiserver，即通过减少不必要的负载来提高系统性能。</p>
<p>验证通过后， Kubectl 开始构造它将发送给 kube-apiserver 的 HTTP 请求。在 Kubernetes 中，访问或更改状态的所有尝试都通过 kube-apiserver 进行，​​后者又与 etcd 进行通信。 Kubectl 客户端也不例外。为了构造 HTTP 请求， Kubectl 使用称为 <a href="https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators">generators</a> 的东西，这是一个负责序列化的抽象概念。</p>
<p>你可能没有注意到，通过 <code>kubectl run</code> 不仅可以运行 <code>deployment</code>，还可以通过指定参数 <code>--generator</code> 来部署其它 workload。</p>
<p>如果没有指定 <code>--generator</code> 参数的值， Kubectl 将会自动<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L319-L339">推断</a>资源的类型，具体如下：</p>
<ul>
<li>具有 <code>--restart-policy=Always</code> 的资源被视为 Deployment；</li>
<li>具有 <code>--restart-policy=OnFailure</code> 的资源被视为 Job；</li>
<li>具有 <code>--restart-policy=Never</code> 的资源被视为 Pod。</li>
</ul>
<p>Kubectl 还将确定是否需要触发其他操作，例如记录命令（用于部署或审计），或者此命令是否是 dry run。</p>
<blockquote>
<p>From wikipedia</p>
<p>空运行（dry run）也称为试运行（practice run），是刻意为了减轻可能失效的影响而有的测试流程。例如飞机公司会先在飞机停在陆地上时进行其弹射座椅的测试，之后才在飞机升空后主进行类似测试。陆地上的测试即为空运行。</p>
<p>在验收程序（也称为工厂验收测试）的领域中，空运行是指分包商需在产品交给客户，进行真正的验收测试之前，先进行的完整测试。</p>
</blockquote>
<p>当 Kubectl 判断出要创建一个 Deployment 后，它将使用 <code>DeploymentV1Beta1 generator</code> 配合我们提供的参数，生成一个<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/generate/versioned/run.go#L237">运行时对象（Runtime Object）</a>。</p>
<h3 id="API-groups-and-version-negotiation"><a href="#API-groups-and-version-negotiation" class="headerlink" title="API groups and version negotiation"></a>API groups and version negotiation</h3><p>这里值得指出的是， Kubernetes 使用的是一个分类为 API Group 的版本化 API。它旨在对资源进行分类，以便于推理。</p>
<p>同时，它还为单个 API 提供了更好的版本化方案。 Deployment 的 API Group 为 <code>apps</code>，其最新版本为 <code>v1</code>。这就是为什么需要在 Deployment manifests 顶部指定 <code>apiVersion: apps/v1</code> 的原因。</p>
<p>回归正文， Kubectl 生成运行时对象之后，它开始为它<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L674-L686">查找合适的 API Group 和版本</a>，然后组装一个知道该资源的各种 REST 语义的<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L705-L708">版本化客户端</a>。</p>
<p>这个发现阶段称为版本协商 (version negotiation)，涉及 Kubectl 扫描 remote API 上的 <code>/apis</code> 路径以检索所有可能的 API Group。</p>
<p>由于 kube-apiserver 在 <code>/apis</code> 路径中暴露其 OpenAPI 格式的 scheme 文档，因此客户端可以轻松的找到匹配的 API。</p>
<p>为了提高性能， Kubectl 还将 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/cli-runtime/pkg/genericclioptions/config_flags.go#L234">OpenAPI scheme 缓存到 <code>~/.kube/cache/discovery</code> 目录</a>。如果要了解 API 发现的完整过程，你可以尝试删除该目录并在运行 Kubectl 命令时将 <code>-v</code> 参数的值设为最大，然后你将会在日志中看到所有试图找到这些 API 版本的 HTTP 请求。</p>
<p>最后一步才是真正地<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L709">发送 HTTP 请求</a>。一旦请求获得成功的响应， Kubectl 将会根据所需的<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubectl/cmd/run/run.go#L459">输出格式</a>打印 success message。</p>
<h3 id="Client-auth"><a href="#Client-auth" class="headerlink" title="Client auth"></a>Client auth</h3><p>我们在上文中没有提到的一件事是客户端身份验证（这是在发送 HTTP 请求之前处理的），现在让我们来看看。</p>
<p>为了成功发送请求， Kubectl 需要先进行身份验证。用户凭据一般存储在 <code>kubeconfig</code> 文件中，但该文件可以存储在不同的位置。为了定位到它， Kubectl 执行以下操作：</p>
<ul>
<li>如果指定参数 <code>--kubeconfig</code>，那么采用该值；</li>
<li>如果指定环境变量 <code>$KUBECONFIG</code>，那么采用该值；</li>
<li>否则<a href="https://github.com/kubernetes/client-go/blob/kubernetes-1.14.0/tools/clientcmd/loader.go#L52">查看默认的目录</a>，如 <code>~/.kube</code>，并使用找到的第一个文件。</li>
</ul>
<p>解析文件后，它会确定当前要使用的上下文，当前指向的集群以及与当前用户关联的所有身份验证信息。如果用户提供了额外的参数（例如 <code>--username</code>），则这些值优先，并将覆盖 kubeconfig 中指定的值。</p>
<p>一旦有了上述信息， Kubectl 就会填充客户端的配置，以便它能够适当地修饰 HTTP 请求：</p>
<ul>
<li>x509 证书使用 <a href="https://github.com/kubernetes/client-go/blob/kubernetes-1.14.0/rest/transport.go#L80-L89"><code>tls.TLSConfig</code></a> 发送（包括 CA 证书）；</li>
<li>bearer tokens 在 HTTP 请求头 Authorization 中<a href="https://github.com/kubernetes/client-go/blob/kubernetes-1.14.0/transport/round_trippers.go#L316">发送</a>；</li>
<li>用户名和密码通过 HTTP 基础认证<a href="https://github.com/kubernetes/client-go/blob/kubernetes-1.14.0/transport/round_trippers.go#L197">发送</a>；</li>
<li>OpenID 认证过程是由用户事先手动处理的，产生一个像 bearer token 一样被发送的 token。</li>
</ul>
<h2 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h2><h3 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h3><p>我们的请求已经发送成功，接下来呢？kube-apiserver！</p>
<p>kube-apiserver 是客户端和系统组件用来持久化和检索集群状态的主要接口。为了执行其功能，它需要能够验证请求是否合法。此过程称为认证 （Authentication）。</p>
<p>为了验证请求，当服务器首次启动时， kube-apiserver 会查看用户提供的所有 <a href="https://v1-14.docs.kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">CLI 参数</a>，并组装合适的 authenticator 列表。</p>
<p>举个例子：</p>
<ul>
<li>如果指定参数 <code>--client-ca-file</code>，它会附加 x509 authenticator 到列表中；</li>
<li>如果指定参数 <code>--token-auth-file</code>，它会附加 token authenticator 到列表中。</li>
</ul>
<p>每次收到请求时，都会<a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/authentication/request/union/union.go#L53">遍历身份验证器列表</a>，直到成功为止：</p>
<ul>
<li><a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/authentication/request/x509/x509.go#L89">x509 handler</a> 会验证 HTTP 请求是否是通过 CA 根证书签名的 TLS 密钥编码的；</li>
<li><a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/authentication/request/bearertoken/bearertoken.go#L37">bearer token handler</a> 会验证 HTTP Authorization header 指定的 token 是否存在于 <code>--token-auth-file</code> 参数提供的 token 文件中；</li>
<li><a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/plugin/pkg/authenticator/request/basicauth/basicauth.go#L39">basicauth handler</a> 会简单验证 HTTP 请求的基本身份凭据。</li>
</ul>
<p>如果所有 authenticator 都认证失败，则请求失败并返回汇总的错误信息。</p>
<p>如果认证成功，则会从请求中删除 <code>Authorization</code> 标头，并<a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/filters/authentication.go#L74-L77">将用户信息添加到其上下文中</a>。为之后的操作（例如授权和准入控制器）提供访问先前建立的用户身份的能力。</p>
<h3 id="Authorization"><a href="#Authorization" class="headerlink" title="Authorization"></a>Authorization</h3><p>好的，请求已发送，kube-apiserver 已成功验证我们是谁。终于解脱了？！</p>
<p>想太多！</p>
<p>虽然我们证明了自己是谁，但还没证明有权执行此操作。毕竟，身份 (identity) 和许可 (permission) 并不是一回事。因此 kube-apiserver 需要授权。</p>
<p>kube-apiserver 处理授权的方式与身份验证非常相似：基于 <a href="https://v1-14.docs.kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">CLI 参数</a> 输入，汇集一系列 authorizer， 这些 authorizer 将针对每个传入请求运行。如果所有 authorizer 都拒绝该请求，则该请求将导致 <code>Forbidden</code> 响应并且<a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/filters/authorization.go#L76">不再继续</a>。如果单个 authorizer 批准，则请求继续。</p>
<p>Kubernetes v1.14 的 authorizer 实例：</p>
<ul>
<li><a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/plugin/pkg/authorizer/webhook/webhook.go#L152">webhook</a>：与集群外的 HTTP(S) 服务交互；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/auth/authorizer/abac/abac.go#L224">ABAC</a>：执行静态文件中定义的策略；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/plugin/pkg/auth/authorizer/rbac/rbac.go#L74">RBAC</a>：执行由集群管理员添加为 k8s 资源的 RBAC 规则；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/plugin/pkg/auth/authorizer/node/node_authorizer.go#L80">Node</a>：确保 kubelet 只能访问自己节点上的资源。</li>
</ul>
<h3 id="Admission-Controller"><a href="#Admission-Controller" class="headerlink" title="Admission Controller"></a>Admission Controller</h3><p>好的，到目前为止，我们已经过认证并获得了 kube-apiserver 的授权。那接下来呢？</p>
<p>从 kube-apiserver 的角度来看，它相信我们是谁并允许我们继续，但是对于 Kubernetes， 系统的其他组件对应该和不应该允许发生的内容有异议。所以 <a href="https://v1-14.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Admission Controller</a> 该闪亮登场了。</p>
<p>虽然 Authorization 的重点是回答用户是否具有权限，但是 Admission Controllers 仍会拦截该请求，以确保其符合集群的更广泛期望和规则。它们是对象持久化到 etcd 之前的最后一个堡垒，因此它们封装了剩余的系统检查以确保操作不会产生意外或负面结果。</p>
<p>Admission Controller 的工作方式类似于 Authentication 和 Authorization 的工作方式，但有一个区别：如果单个 Admission Controller 失败，整个链断开，请求将失败。</p>
<p>Admission Controller 设计的真正优势在于它致力于提升<em>可扩展性</em>。每个控制器都作为插件存储在 <a href="https://github.com/kubernetes/kubernetes/tree/v1.14.0/plugin/pkg/admission">plugin&#x2F;pkg&#x2F;admission</a> 目录中，最后编译进 kube-apiserver 二进制文件。</p>
<p>Kubernetes 目前提供十多种 Admission Controller，此处建议阅读文档 <a href="https://v1-14.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Kubernetes Admission Controller</a>。</p>
<h2 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h2><p>到目前为止， Kubernetes 已经完全审查了传入的请求，并允许它往下走。在下一步中，kube-apiserver 将反序列化 HTTP 请求，构造运行时对象（runtime object）（有点像 kubectl generator 的逆过程），并将它们持久化到 etcd。</p>
<p>这里插入一下，kube-apiserver 是怎么知道在接受我们的请求时该怎么做呢？</p>
<p>在提供任何请求之前，kube-apiserver 会发生一系列非常复杂的步骤。让我们从第一次运行 kube-apiserver 二进制文件开始：</p>
<ol>
<li>当运行 kube-apiserver 二进制文件时，它会创建一个<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/cmd/kube-apiserver/app/server.go#L157">服务链</a>，允许 apiserver 聚合。这是一种支持多 apiserver 的方式；</li>
<li>之后，它会创建一个用作默认实现的 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/cmd/kube-apiserver/app/server.go#L179">generic apiserver</a>；</li>
<li>使用生成的 OpenAPI scheme 填充 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/server/config.go#L147">apiserver 配置</a>；</li>
<li>然后，kube-apiserver 遍历 scheme 中指定的所有 API Group， 并为其构造 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/master/master.go#L420">storage provider</a>。当你访问或变更资源状态时， kube-apiserver 就会调用这些 API Group；</li>
<li>对于每个 API Group， 它还会迭代每个组版本，并为每个 HTTP 路由<a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/groupversion.go#L99">安装 REST 映射</a>。这允许 kube-apiserver 映射请求，并且一旦找到匹配就能够委托给正确的代码逻辑；</li>
<li>对于本文的特定用例，将注册一个 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/installer.go#L737">POST handler</a>，该处理程序将委托给 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/handlers/create.go#L46">create resource handler</a>。</li>
</ol>
<p>到目前为止， kube-apiserver 完全知道存在哪些路由及内部映射，当请求匹配时，可以知道调用哪些处理程序和存储程序。这是非常完美的设计模式。这里我们假设 HTTP 请求已经被 kube-apiserver 收到了：</p>
<ol>
<li>如果程序处理链可以将请求与注册的路由匹配，它会将该请求交给注册到该路由的 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/server/handler.go#L146">dedicated handler</a>。否则它会回退到 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/server/mux/pathrecorder.go#L248">path-based handler</a>（这是调用 <code>/apis</code> 时会发生的情况）。如果没有为该路由注册处理程序，则会调用 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/server/mux/pathrecorder.go#L254">not found handler</a>，最终返回 <code>404</code>；</li>
<li>幸运的是，我们有一个处理器名为 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/handlers/create.go#L46">createHandler</a>! 它有什么作用？它将首先解码 HTTP 请求并执行基础验证，例如确保请求提供的 JSON 与我们的版本化 API 资源匹配；</li>
<li><a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/handlers/create.go#L126-L138">审计和准入控制阶段</a>；</li>
<li>然后，资源会通过 <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/registry/generic/registry/store.go#L359">storage provider</a> <a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/handlers/create.go#L156-L161">存储到 etcd 中</a>。默认情况下，保持到 etcd 的键的格式为 <code>&lt;namespace&gt;/&lt;name&gt;</code>，当然，它也支持自定义；</li>
<li>资源创建过程中出现的任何错误都会被捕获，最后 storage provider 会执行 get 调用来确认该资源是否被成功创建。如果需要额外的清理工作 (finalization)，就会调用后期创建的处理器和装饰器；</li>
<li>最后，<a href="https://github.com/kubernetes/apiserver/blob/kubernetes-1.14.0/pkg/endpoints/handlers/create.go#L170-L177">构造 HTTP 响应</a>并返回给客户端。</li>
</ol>
<p>这么多步骤！能够坚持走到这里是非常了不起的，并且我们意识到了 kube-apiserver 实际上做了很多工作。总结一下：我们部署的 Deployment 现在存在于 etcd 中，但仍没有看到它真正的 work…</p>
<p><strong>注</strong>：在 Kubernetes v1.14 之前，这往后还有 Initializer 的步骤，该步骤在 v1.14 <a href="https://github.com/kubernetes/kubernetes/issues/67113">被 webhook admission 取代</a>。</p>
<h2 id="Control-loops"><a href="#Control-loops" class="headerlink" title="Control loops"></a>Control loops</h2><h3 id="Deployment-Controller"><a href="#Deployment-Controller" class="headerlink" title="Deployment Controller"></a>Deployment Controller</h3><p>截至目前，我们的 Deployment 已经存储于 etcd 中，并且所有的初始化逻辑都已完成。接下来的阶段将涉及 Deployment 所依赖的资源拓扑结构。</p>
<p>在 Kubernetes， Deployment 实际上只是 ReplicaSet 的集合，而 ReplicaSet 是 Pod 的集合。那么 Kubernetes 如何从一个 HTTP 请求创建这个层次结构呢？这就不得不提 Kubernetes 的内置控制器 （Controller）。</p>
<p>Kubernetes 系统中使用了大量的 Controller， Controller 是一个用于将系统状态从<code>当前状态</code>调谐到<code>期望状态</code>的异步脚本。所有内置的 Controller 都通过组件 kube-controller-manager 并行运行，每种 Controller 都负责一种具体的控制流程。</p>
<p>首先，我们介绍一下 Deployment Controller：</p>
<p>将 Deployment 存储到 etcd 后，我们通过 kube-apiserver 可以看到它。当这个新资源可用时， Deployment Controller 会检测到它，它的工作是监听 Deployment 的更改。在我们的例子中， Controller 通过<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0//pkg/controller/deployment/deployment_controller.go#L122">注册创建事件的回调函数</a>（更多相关信息，参见下文）。</p>
<p>当我们的 Deployment 首次可用时，将执行此回调函数，并<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/deployment/deployment_controller.go#L166-L170">将该对象添加到内部工作队列（internal work queue）</a>。</p>
<p>当它处理我们的 Deployment 对象时，控制器将<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/deployment/deployment_controller.go#L571">检查我们的 Deployment</a> 并意识到没有与之关联的 ReplicaSet 或 Pod。</p>
<p>它通过使用标签选择器 (label selectors) 查询 kube-apiserver 来实现此功能。有趣的是，这个同步过程是状态不可知的。另外，它以相同的方式调谐新对象和已存在的对象。</p>
<p>在意识到没有与其关联的 ReplicaSet 或 Pod 后，Deployment Controller 就会开始执行<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/deployment/sync.go#L378">弹性伸缩流程 (scaling process)</a>。它通过推出（例如，创建）一个 ReplicaSet， 为其分配 label selector 并将其版本号设置为 1。</p>
<p>ReplicaSet 的 PodSpec 字段是从 Deployment 的 manifest 以及其他相关元数据中复制而来。有时 Deployment 在此之后也需要更新（例如，如果设置了 process deadline）。</p>
<p>当完成以上步骤之后，该 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/deployment/sync.go#L67">Deployment 的 status 就会被更新</a>，然后重新进入与之前相同的循环，等待 Deployment 与期望的状态相匹配。由于 Deployment Controller 只关心 ReplicaSet， 因此调谐过程将由 ReplicaSet Controller 继续。</p>
<h3 id="ReplicaSet-Controller"><a href="#ReplicaSet-Controller" class="headerlink" title="ReplicaSet Controller"></a>ReplicaSet Controller</h3><p>在上一步中，Deployment Controller 创建了属于该 Deployment 的第一个 ReplicaSet， 但仍然没有创建 Pod。 所以这里我们需要引入 ReplicaSet Controller！</p>
<p>ReplicaSet Controller 的工作是监视 ReplicaSet 及其相关资源 Pod 的生命周期。与大多数其它控制器一样，它通过触发某些事件的处理程序来实现。</p>
<p>当创建 ReplicaSet 时（由 Deployment Controller 创建），ReplicaSet Controller 会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/replicaset/replica_set.go#L583">检查新 ReplicaSet 的状态</a>，并意识到现有状态与期望状态之间存在偏差。然后，它试图通过<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/replicaset/replica_set.go#L460">调整 pod 的副本数</a>来调谐这种状态。</p>
<p>Pod 的创建也是<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/controller/replicaset/replica_set.go#L478-L499">批量</a>)进行的，从数量 <code>SlowStartInitialBatchSize</code> 开始，然后在每次成功的迭代中以一种 <code>slow start</code> 操作加倍。这样做的目的是在大量 Pod 启动失败时（例如，由于资源配额），可以减轻 kube-apiserver 由于大量不必要的 HTTP 请求导致崩溃的风险。</p>
<p>Kubernetes 通过 Owner References （子资源的某个字段中引用其父资源的 ID） 来执行严格的资源对象层级结构。这确保了一旦 Controller 管理的资源被删除（级联删除），子资源就会被垃圾收集器删除，同时还为父资源提供了一种有效的方式来避免他们竞争同一个子资源（想象两对父母认为他们拥有同一个孩子的场景）。</p>
<p>Owner References 的另一个好处是，它是有状态的。如果重启任何的 Controller，那么由于资源对象的拓扑关系与 Controller 无关，该重启时间不会影响到系统的稳定运行。这种对资源隔离的重视也体现在 Controller 本身的设计中： Controller 不能对自己没有明确拥有的资源进行操作，它们之间互不干涉，互不共享。</p>
<p>有时系统中也会出现孤儿 （orphaned） 资源，通常由以下两种途径产生：</p>
<ul>
<li>父资源被删除，但子资源没有被删除</li>
<li>垃圾收集策略禁止删除子资源</li>
</ul>
<p>当发生这种情况时， Controller 将会确保孤儿资源拥有新的 Owner。 多个父资源可以相互竞争同一个孤儿资源，但只有一个会成功（其他父资源会收到一个验证错误）。</p>
<h3 id="Informers"><a href="#Informers" class="headerlink" title="Informers"></a>Informers</h3><p>你可能已经注意到，有些 Controller（例如 RBAC 授权器或 Deployment Controller）需要检索集群状态然后才能正常运行。</p>
<p>以 RBAC authorizer 举例，当请求进入时， authorizer 会将用户的初始状态缓存下来供以后使用，然后用它来检索与 etcd 中的用户关联的所有<code>角色（Role）</code>和<code>角色绑定（RoleBinding）</code>。</p>
<p>那么 Controller 是如何访问和修改这些资源对象的呢？答案是引入 Informer。</p>
<p>Infomer 是一种模式，它允许 Controller 订阅存储事件并列出它们感兴趣的资源。除了提供一个很好的工作抽象，它还需要处理很多细节，如缓存（缓存很重要，因为它减少了不必要的 kube-apiserver 连接，并减少了服务器端和控制端的重复序列化成本）。通过使用这种设计，它还允许 Controller 以线程安全 （thread safe） 的方式进行交互，而不必担心线程冲突。</p>
<p>有关 Informer 的更多信息，可深入阅读 <a href="http://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores">《Kubernetes: Controllers, Informers, Reflectors and Stores》</a></p>
<h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>当所有的 Controller 正常运行后，etcd 中就会保存一个 Deployment、一个 ReplicaSet 和 三个 Pod， 并且可以通过 kube-apiserver 查看到。然而，这些 Pod 还处于 <code>Pending</code> 状态，因为它们还没有被调度到集群中合适的 Node 上。最终解决这个问题的 Controller 是 Scheduler。</p>
<p>Scheduler 作为一个独立的组件运行在集群控制平面上，工作方式与其他 Controller 相同：监听事件并调谐状态。</p>
<p>具体来说， Scheduler 的作用是过滤 PodSpec 中 <code>NodeName</code> 字段为空的 Pod 并尝试将其调度到合适的节点。</p>
<p>为了找到合适的节点， Scheduler 会使用特定的算法，默认调度算法工作流程如下：</p>
<ol>
<li>当 Scheduler 启动时，会注册<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/scheduler/algorithmprovider/defaults/defaults.go#L37">一系列默认的预选策略</a>，这些预选策略会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/scheduler/core/generic_scheduler.go#L184">对候选节点进行评估</a>，判断候选节点是否满足候选 Pod 的需求。例如，如果 PodSpec 显式地限制了 CPU 和内存资源，并且节点的资源容量不满足候选 Pod 的需求时，Pod 就不会被调度到该节点上（资源容量 &#x3D; 节点资源总量 - 节点中已运行的容器需求资源 （CPU 和内存）总和）；</li>
<li>一旦选择了适当的节点，就会对剩余的节点运行一系列<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/scheduler/core/generic_scheduler.go#L639-L645">优先级函数</a>，以对候选节点进行打分。例如，为了在整个系统中分散工作负载，它将偏好于资源请求较少的节点（因为这表明运行的工作负载较少）。当它运行这些函数时，它为每个节点分配一个成绩。然后选择分数最高的节点进行调度。</li>
</ol>
<p>一旦算法找到了合适的节点， Scheduler 就会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/scheduler/scheduler.go#L559-L565">创建一个 Binding 对象</a>，该对象的 Name 和 Uid 与 Pod 相匹配，并且其 <code>ObjectReference</code> 字段包含所选节点的名称，然后通过<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/scheduler/factory/factory.go#L734">发送 POST 请求</a>给 kube-apiserver。</p>
<p>当 kube-apiserver 接收到此 Binding 对象时，注册表会将该对象反序列化 （registry deserializes） 并更新 Pod 资源中的以下字段：</p>
<ol>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/registry/core/pod/storage/storage.go#L176">将 NodeName 的值设置为 Binding 对象 ObjectReference 中的 NodeName</a>；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/registry/core/pod/storage/storage.go#L180-L182">添加相关的注释 (annotations)</a>；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/registry/core/pod/storage/storage.go#L183-L186">将 PodScheduled 的 status 设置为 True</a>。</li>
</ol>
<p>一旦 Scheduler 将 Pod 调度到某个节点上，该节点的 Kubelet 就会接管该 Pod 并开始部署。</p>
<p>附注：自定义调度器：有趣的是预测和优先级函数 （predicates and priority functions） 都是可扩展的，可以使用参数 <code>--policy-config-file</code> 来定义。这引入了一定程度的灵活性。管理员还可以在独立部署中运行自定义调度器（具有自定义处理逻辑的调度器）。如果 PodSpec 中包含 <code>schedulerName</code>，Kubernetes 会将该 pod 的调度移交给使用该名称注册的调度器。</p>
<h2 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h2><h3 id="Pod-Sync"><a href="#Pod-Sync" class="headerlink" title="Pod Sync"></a>Pod Sync</h3><p>截至目前，所有的 Controller 都完成了工作，让我们来总结一下：</p>
<ol>
<li>HTTP 请求通过了认证、授权和准入控制阶段；</li>
<li>一个 Deployment、ReplicaSet 和三个 Pod 被持久化到 etcd；</li>
<li>最后每个 Pod 都被调度到合适的节点。</li>
</ol>
<p>然而，到目前为止，所有的状态变化仅仅只是针对保存在 etcd 中的资源对象，接下来的步骤涉及到在工作节点之间运行具体的容器，这是分布式系统 Kubernetes 的关键因素。这些事情都是由 Kubelet 完成的。</p>
<p>在 Kubernetes 集群中，每个 Node 节点上都会启动一个 Kubelet 服务进程，该进程用于处理 Scheduler 下发到本节点的 Pod 并管理其生命周期。这意味着它将处理 Pod 与 Container Runtime 之间所有的转换逻辑，包括挂载卷、容器日志、垃圾回收等操作。</p>
<p>一个有用的方法，你可以把 Kubelet 当成一种特殊的 Controller，它每隔 20 秒（可以自定义）向 kube-apiserver 查询 Pod，过滤 NodeName 与自身所在节点匹配的 Pod 列表。</p>
<p>一旦获取到了这个列表，它就会通过与自己的内部缓存进行比较来检测差异，如果有差异，就开始同步 Pod 列表。我们来看看同步过程是什么样的：</p>
<ol>
<li>如果 Pod 正在创建， Kubelet 就会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet.go#L1504">暴露一些指标</a>，可以用于在 Prometheus 中追踪 Pod 启动延时；</li>
<li>然后，<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet_pods.go#L1333">生成一个 PodStatus 对象</a>，表示 Pod 当前阶段的状态。Pod 的 Phase 状态是 Pod 在其生命周期中的高度概括，包括 <code>Pending</code>，<code>Running</code>，<code>Succeeded</code>，<code>Failed</code> 和 <code>Unkown</code> 这几个值。状态的产生过程非常复杂，因此很有必要深入深挖一下：<ul>
<li>首先，串行执行一系列 <code>PodSyncHandlers</code>，每个处理器检查 Pod 是否应该运行在该节点上。当其中之一的处理器认为该 Pod 不应该运行在该节点上，则 Pod 的 Phase 值就会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet_pods.go#L1340-L1345">变成 <code>PodFailed</code></a> 并将从该节点被驱逐。例如，以 Job 为例，当一个 Pod 失败重试的时间超过了 <code>activeDeadlineSeconds</code> 设置的值，就会将该 Pod 从该节点驱逐出去；</li>
<li>接下来，Pod 的 Phase 值由 init 容器和主容器状态共同决定。由于主容器尚未启动，容器被视为处于<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet_pods.go#L1284">等待阶段</a>，如果 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet_pods.go#L1298-L1301">Pod 中至少有一个容器处于等待阶段，则其 Phase 值为 <code>Pending</code></a>。、；</li>
<li>最后，Pod 的 Condition 字段由 Pod 内所有容器状态决定。现在我们的容器还没有被容器运行时 (Container Runtime) 创建，所以，Kubelet <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/status/generate.go#L72-L83">将 <code>PodReady</code> 的状态设置为 False</a>。</li>
</ul>
</li>
<li>生成 PodStatus 之后，Kubelet 就会将它发送到 Pod 的 status 管理器，该管理器的任务是通过 kube-apiserver 异步更新 etcd 中的记录；</li>
<li>接下来运行一系列 admit handlers 以确保该 Pod 具有正确的权限（包括强制执行 <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet.go#L864-L865">AppArmor profiles 和 NO_NEW_PRIVS</a>），在该阶段被拒绝的 Pod 将永久处于 <code>Pending</code> 状态；</li>
<li>如果 Kubelet 启动时指定了 <code>--cgroups-per-qos</code> 参数，Kubelet 就会为该 Pod 创建 cgroup 并设置对应的资源限制。这是为了更好的 Pod 服务质量（QoS）；</li>
<li>为 Pod <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kubelet_pods.go#L826-L839">创建相应的数据目录</a>，包括：<ul>
<li>Pod 目录 (通常是 <code>/var/run/kubelet/pods/&lt;podID&gt;</code>)；</li>
<li>Pod 的挂载卷目录 (<code>&lt;podDir&gt;/volumes</code>)；</li>
<li>Pod 的插件目录 (<code>&lt;podDir&gt;/plugins</code>)。</li>
</ul>
</li>
<li>卷管理器会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/volumemanager/volume_manager.go#L339">挂载 <code>Spec.Volumes</code> 中定义的相关数据卷</a>，然后等待挂载成功；</li>
<li>从 kube-apiserver 中检索 <code>Spec.ImagePullSecrets</code>，然后将对应的 Secret 注入到容器中；</li>
<li>最后，通过容器运行时 （Container Runtime） 启动容器（下面会详细描述）。</li>
</ol>
<h3 id="CRI-and-pause-container"><a href="#CRI-and-pause-container" class="headerlink" title="CRI and pause container"></a>CRI and pause container</h3><p>到了这个阶段，大量的初始化工作都已经完成，容器已经准备好开始启动了，而容器是由容器运行时（例如 Docker）启动的。</p>
<p>为了更具可扩展性， Kubelet 使用 CRI （Container Runtime Interface） 来与具体的容器运行时进行交互。简而言之， CRI 提供了 Kubelet 和特定容器运行时实现之间的抽象。通过 <a href="https://github.com/google/protobuf">protocol buffers</a>（一种更快的 JSON） 和 <a href="https://grpc.io/">gRPC API</a>（一种非常适合执行 Kubernetes 操作的API）进行通信。</p>
<p>这是一个非常酷的想法，因为通过在 Kubelet 和容器运行时之间使用已定义的接口约定，容器编排的实际实现细节变得无关紧要。重要的是接口约定。这允许以最小的开销添加新的容器运行时，因为没有核心 Kubernetes 代码需要更改！</p>
<p>回到部署我们的容器，当一个 Pod 首次启动时， Kubelet <a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_sandbox.go#L65">调用 RunPodSandbox 远程过程命令 （remote procedure command RPC）</a>。沙箱 （sandbox） 是描述一组容器的 CRI 术语，在 Kubernetes 中对应的是 Pod。这个术语是故意模糊的，因此其他不使用容器的运行时，不会失去其意义（想象一个基于 hypervisor 的运行时，沙箱可能指的是 VM）。</p>
<p>在我们的例子中，我们使用的是 Docker。 在 Docker 中，创建沙箱涉及创建 <code>pause</code> 容器。</p>
<p><code>pause</code> 容器像 Pod 中的所有其他容器的父级一样，因为它承载了工作负载容器最终将使用的许多 Pod 级资源。这些“资源”是 Linux Namespaces (IPC，Network，PID)。</p>
<blockquote>
<p>如果你不熟悉容器在 Linux 中的工作方式，那么我们快速回顾一下。 Linux 内核具有 Namespace 的概念，允许主机操作系统分割出一组专用资源（例如 CPU 或内存）并将其提供给一个进程，就好像它是世界上唯一使用它们的东西一样。 Cgroup 在这里也很重要，因为它们是 Linux 管理资源隔离的方式。 Docker 使用这两个内核功能来托管一个保证资源强制隔离的进程。更多信息，可深入阅读 <a href="https://jvns.ca/blog/2016/10/10/what-even-is-a-container/">What even is a Container?</a></p>
</blockquote>
<p><code>pause</code> 容器提供了一种托管所有这些 Namespaces 的方法，并允许子容器共享它们。通过成为同一 Network Namespace 的一部分，一个好处是同一个 Pod 中的容器可以使用 localhost 相互访问。</p>
<p><code>pause</code>  容器的第二个好处与 PID Namespace 有关。在这些 Namespace 中，进程形成一个分层树（hierarchical tree），顶部的“init” 进程负责“收获”僵尸进程。更多信息，请深入阅读 <a href="https://www.ianlewis.org/en/almighty-pause-container">great blog post</a>。</p>
<p>创建 <code>pause</code> 容器后，将开始检查磁盘状态然后启动主容器。</p>
<h3 id="CNI-and-pod-networking"><a href="#CNI-and-pod-networking" class="headerlink" title="CNI and pod networking"></a>CNI and pod networking</h3><p>现在，我们的 Pod 有了基本的骨架：一个 <code>pause</code> 容器，它托管所有 Namespaces 以允许 Pod 间通信。但容器网络如何运作以及建立的？</p>
<p>当 Kubelet 为 Pod 设置网络时，它将任务委托给 <code>CNI (Container Network Interface)</code> 插件。其运行方式与 Container Runtime Interface 类似。简而言之， CNI 是一种抽象，允许不同的网络提供商对容器使用不同的网络实现。</p>
<p>Kubelet 通过 stdin 将 JSON 数据（配置文件位于 <code>/etc/cni/net.d</code> 中）传输到相关的 CNI 二进制文件（位于 <code>/opt/cni/bin</code>） 中与之交互。下面是一个简单的示例 JSON 配置文件：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;cniVersion&quot;:</span> <span class="string">&quot;0.3.1&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;bridge&quot;:</span> <span class="string">&quot;cnio0&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;isGateway&quot;:</span> <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;ipMasq&quot;:</span> <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;ipam&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;:</span> <span class="string">&quot;host-local&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;ranges&quot;:</span> [</span><br><span class="line">          [&#123;<span class="attr">&quot;subnet&quot;:</span> <span class="string">&quot;$&#123;POD_CIDR&#125;&quot;</span>&#125;]</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;routes&quot;:</span> [&#123;<span class="attr">&quot;dst&quot;:</span> <span class="string">&quot;0.0.0.0/0&quot;</span>&#125;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>CNI 插件还可以通过 <code>CNI_ARGS</code> 环境变量为 Pod 指定其他的元数据，包括 Pod Name 和 Namespace。</p>
<p>接下来会发生什么取决于 CNI 插件，这里，我们以 <code>bridge</code> CNI 插件为例：</p>
<ol>
<li>该插件首先会在 Root Network Namespace（也就是宿主机的 Network Namespace） 中设置本地 Linux 网桥，以便为该主机上的所有容器提供网络服务；</li>
<li>然后它会将一个网络接口 （veth 设备对的一端）插入到 <code>pause</code> 容器的 Network Namespace 中，并将另一端连接到网桥上。你可以这样来理解 veth 设备对：它就像一根很长的管道，一端连接到容器，一端连接到 Root Network Namespace 中，允许数据包在中间传输；</li>
<li>然后它会为 <code>pause</code> 容器的网络接口分配一个 IP 并设置相应的路由，于是 Pod 就有了自己的 IP。IP 的分配是由 JSON 配置文件中指定的 IPAM Plugin 实现的；<ul>
<li>IPAM Plugin 的工作方式和 CNI 插件类似：通过二进制文件调用并具有标准化的接口，每一个 IPAM Plugin 都必须要确定容器网络接口的 IP、子网以及网关和路由，并将信息返回给 CNI 插件。最常见的 IPAM Plugin 称为 host-local，它从预定义的一组地址池为容器分配 IP 地址。它将相关信息保存在主机的文件系统中，从而确保了单个主机上每个容器 IP 地址的唯一性。</li>
</ul>
</li>
<li>对于 DNS， Kubelet 将为 CNI 插件指定 Kubernetes 集群内部 DNS 服务器 IP 地址，确保正确设置容器的 <code>resolv.conf</code> 文件。</li>
</ol>
<h3 id="Inter-host-networking"><a href="#Inter-host-networking" class="headerlink" title="Inter-host networking"></a>Inter-host networking</h3><p>到目前为止，我们已经描述了容器如何与宿主机进行通信，但跨主机之间的容器如何通信呢？</p>
<p>通常情况下， Kubernetes 使用 Overlay 网络来进行跨主机容器通信，这是一种动态同步多个主机间路由的方法。一个较常用的 Overlay 网络插件是 <code>flannel</code>，它提供了跨节点的三层网络。</p>
<p>flannel 不会管容器与宿主机之间的通信（这是 CNI 插件的职责），但它对主机间的流量传输负责。为此，它为主机选择一个子网并将其注册到 etcd。然后，它保留集群路由的本地表示，并将传出的数据包封装在 UDP 数据报中，确保它到达正确的主机。</p>
<p>更多信息，请深入阅读 <a href="https://github.com/coreos/flannel">CoreOS’s documentation</a>。</p>
<h3 id="Container-startup"><a href="#Container-startup" class="headerlink" title="Container startup"></a>Container startup</h3><p>所有的网络配置都已完成。还剩什么？真正地启动工作负载容器！</p>
<p>一旦 <code>sanbox</code> 完成初始化并处于 <code>active</code> 状态， Kubelet 将开始为其创建容器。首先<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L736">启动 PodSpec 中定义的 Init Container</a>，然后再启动主容器。具体过程如下：</p>
<ol>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L95">拉取容器的镜像</a>。如果是私有仓库的镜像，就会使用 PodSpec 中指定的 imagePullSecrets 来拉取该镜像；</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L124">通过 CRI 创建容器</a>。 Kubelet 使用 PodSpec 中的信息填充了一个 <code>ContainerConfig</code> 数据结构（在其中定义了 command， image， labels， mounts， devices， environment variables 等），然后通过 protobufs 发送给 CRI。 对于 Docker 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 Dockerd 守护进程。在这个过程中，它会将一些元数据（例如容器类型，日志路径，sandbox ID 等）添加到容器中；</li>
<li>然后 Kubelet 将容器注册到 CPU 管理器，它通过使用 <code>UpdateContainerResources</code> CRI 方法给容器分配给本地节点上的 CPU 资源；</li>
<li>最后<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L144">容器真正地启动</a>；</li>
<li>如果 Pod 中包含 <a href="https://v1-14.docs.kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/">Container Lifecycle Hooks</a>，容器启动之后就会<a href="https://github.com/kubernetes/kubernetes/blob/v1.14.0/pkg/kubelet/kuberuntime/kuberuntime_container.go#L170-L185">运行这些 Hooks</a>。 Hook 的类型包括两种：Exec（执行一段命令） 和 HTTP（发送HTTP请求）。如果 PostStart Hook 启动的时间过长、挂起或者失败，容器将永远不会变成 Running 状态。</li>
</ol>
<h2 id="Wrap-up"><a href="#Wrap-up" class="headerlink" title="Wrap-up"></a>Wrap-up</h2><p>最后的最后，现在我们的集群上应该会运行三个容器，分布在一个或多个工作节点上。所有的网络，数据卷和秘钥都由 Kubelet 填充，并通过 CRI 接口添加到容器中并配置成功！</p>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kube-batch学习(nodeorder插件使用)</title>
    <url>/2021/07/07/Kubernetes-kubebatch-plugin-nodeorder/</url>
    <content><![CDATA[<p>AI场景跟大多数的业务不太一样的是: 网络端需要尽可能地靠近，对于大多数业务来说，为了保证其可用性，一般副本都会分散地部署在不同node，而AI业务通常伴随着海量的数据交换，一个job中的多个pod需要协同处理，如果分散在多个node上，task间的任务交换的快慢就得依赖于网络的传输的快慢，而如果是在一台node上的话，那就没有这部分的消耗，一个job中的pods如何做到尽可能地调度在同一台机器上呢, kube-batch除了能够支持poggroup外，也是能够支持的podaffinit的.</p>
<span id="more"></span>

<p>一个job中的pods如何做到尽可能地调度在同一台机器上呢， 最容易让人想到的是podAffinit，podAffinit是针对于在集群中已经存在的pod，其它的pod可以通过podAffinit来让他们部署在一起，这里有个很大的问题在于， 如果同时使用了podgroup，也就是说在podgroup中的pod在绑定节点之前在集群中是不存在的，也就是办法通过labelsector找到这些pod，那要怎么办呢? </p>
<p>为解决这个问题，kube-batch进行了详细的<a href="https://github.com/kubernetes-sigs/kube-batch/pull/587">issue讨论</a>，为此，kube-batch引入了一个全新的plugins, 最开始叫Prioritize，后改名为nodeorder.</p>
<p>假使kube-batch使用以下的配置:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">actions:</span> <span class="string">&quot;allocate, backfill&quot;</span></span><br><span class="line"><span class="attr">tiers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gang</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nodeorder</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">predicates</span></span><br></pre></td></tr></table></figure>

<p>之前提到过，actions指定了kube-batch在调度时需要执行的操作，同时，这些操作会关联一些plugins(简单来说就是一些算法)来实现相关功能，比如说，actions是allocate，allocate意为分配资源，但是在分配资源时有时也会有一些要求，比如优先级高的先分配，或者把某个任务当成一个整体进行分配(gang)等等，同时，不同的actions可能关联同一个plugins，比如对于资源回收时，也可能存在先回收优先级低的pod的资源，这就是actions及plugins之间的关系</p>
<p>这里将nodeorder放在靠前的位置，同时启用gang插件.</p>
<p>nodeorder的流程如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210707221640.png"></p>
<p>从图中可看出Prioritize只发生在Allocate及Preempt actions中， 上面的配置中只启用了Allocate,当kube-batch从所有机器中筛选出所有符合条件的node列表后，在Allocate中开始执行Prioritize的function，详细流程:</p>
<blockquote>
<ul>
<li>并行地在筛选出来的node列表中执行所有的priority functions</li>
<li>根据优先级规则是否满足工作负载调度标准对节点进行评分</li>
<li>一旦从所有优先级返回分数，则聚合分数并确定得分最高的节点</li>
<li>将最后一步中选定的节点委托给AllocateFn，以将工作负载绑定到该节点</li>
</ul>
</blockquote>
<p>从上图中可以看出priority functions包含interpodAffinityFn，从这个命名来看是跟podAffinity有关的，假如我的job定义了这样的podAffinity</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">affinity:</span></span><br><span class="line">      <span class="attr">podAffinity:</span></span><br><span class="line">        <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span></span><br><span class="line">          <span class="attr">podAffinityTerm:</span></span><br><span class="line">            <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">argo-workflow/mpi-task</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">&#x27;<span class="template-variable">&#123;&#123;workflow.parameters.CURRENT_OP_RUN_ID&#125;&#125;</span>&#x27;</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br></pre></td></tr></table></figure>

<p>使用了preferredDuringSchedulingIgnoredDuringExecution，尽可能地调度到一台节点上，确实是interpodAffinityFn起了作用</p>
<p><code>Talk is cheap, show me the code</code></p>
<p>主要的逻辑代码位于<code>kube-batch/vendor/k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/interpod_affinity.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">processNode := <span class="function"><span class="keyword">func</span><span class="params">(i <span class="type">int</span>)</span></span> &#123;</span><br><span class="line">        nodeInfo := nodeNameToInfo[allNodeNames[i]]</span><br><span class="line">        <span class="keyword">if</span> nodeInfo.Node() != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> hasAffinityConstraints || hasAntiAffinityConstraints &#123;</span><br><span class="line">                    <span class="comment">// We need to process all the nodes.</span></span><br><span class="line">                    <span class="keyword">for</span> _, existingPod := <span class="keyword">range</span> nodeInfo.Pods() &#123;</span><br><span class="line">                        <span class="keyword">if</span> err := processPod(existingPod); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                          pm.setError(err)</span><br><span class="line">                      &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// The pod doesn&#x27;t have any constraints - we need to check only existing</span></span><br><span class="line">                <span class="comment">// ones that have some.</span></span><br><span class="line">                <span class="keyword">for</span> _, existingPod := <span class="keyword">range</span> nodeInfo.PodsWithAffinity() &#123;</span><br><span class="line">                    <span class="keyword">if</span> err := processPod(existingPod); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                        pm.setError(err)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>其中</p>
<p>pod                                                         一个<strong>需被调度的Pod</strong><br> hasAffinityConstraints                        “被调度的pod”是否有定义亲和配置<br> hasAntiAffinityConstraints                “被调度的pod”是否有定义亲和配置<br> existingPod                                            一个待处理的<strong>亲和目标pod</strong><br> existingPodNode                                  运行此“亲和目标pod”的节点–“<strong>目标Node</strong><br> existingHasAffinityConstraints          “亲和目标pod”是否存在亲和约束<br> existingHasAntiAffinityConstraints    “亲和目标pod”是否存在反亲和约束</p>
<p>上面调用的processPod，传入的是一个existingPod，</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">processPod := <span class="function"><span class="keyword">func</span><span class="params">(existingPod *v1.Pod)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">        existingPodNode, err := ipa.info.GetNodeInfo(existingPod.Spec.NodeName)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> apierrors.IsNotFound(err) &#123;</span><br><span class="line">                klog.Errorf(<span class="string">&quot;Node not found, %v&quot;</span>, existingPod.Spec.NodeName)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">        existingPodAffinity := existingPod.Spec.Affinity</span><br><span class="line">        existingHasAffinityConstraints := existingPodAffinity != <span class="literal">nil</span> &amp;&amp; existingPodAffinity.PodAffinity != <span class="literal">nil</span></span><br><span class="line">        existingHasAntiAffinityConstraints := existingPodAffinity != <span class="literal">nil</span> &amp;&amp; existingPodAffinity.PodAntiAffinity != <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> hasAffinityConstraints &#123;</span><br><span class="line">            <span class="comment">// For every soft pod affinity term of &lt;pod&gt;, if &lt;existingPod&gt; matches the term,</span></span><br><span class="line">            <span class="comment">// increment &lt;pm.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span></span><br><span class="line">            <span class="comment">// value as that of &lt;existingPods&gt;`s node by the term`s weight.</span></span><br><span class="line">            terms := affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution</span><br><span class="line">            pm.processTerms(terms, pod, existingPod, existingPodNode, <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>第一次循环的时候通过亲和性规则显然是找不到pod的</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *podAffinityPriorityMap)</span></span> processTerm(term *v1.PodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, weight <span class="type">float64</span>) &#123;</span><br><span class="line">    namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(podDefiningAffinityTerm, term)</span><br><span class="line">    selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        p.setError(err)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    match := priorityutil.PodMatchesTermsNamespaceAndSelector(podToCheck, namespaces, selector)</span><br><span class="line">    <span class="keyword">if</span> match &#123; <span class="comment">// 第一次无法匹配</span></span><br><span class="line">        <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">            p.Lock()</span><br><span class="line">            <span class="keyword">defer</span> p.Unlock()</span><br><span class="line">            <span class="keyword">for</span> _, node := <span class="keyword">range</span> p.nodes &#123;</span><br><span class="line">                <span class="keyword">if</span> priorityutil.NodesHaveSameTopologyKey(node, fixedNode, term.TopologyKey) &#123;</span><br><span class="line">                    p.counts[node.Name] += weight</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *podAffinityPriorityMap)</span></span> processTerms(terms []v1.WeightedPodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, multiplier <span class="type">int</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> i := <span class="keyword">range</span> terms &#123;</span><br><span class="line">        term := &amp;terms[i]</span><br><span class="line">        p.processTerm(&amp;term.PodAffinityTerm, podDefiningAffinityTerm, podToCheck, fixedNode, <span class="type">float64</span>(term.Weight*<span class="type">int32</span>(multiplier)))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是第二次循环的时候就能发现第1个pod了,在整个循环期间需要计算weight值，最后得分最高的node为最终选中的node</p>
<p>更加详细的代码详解可参考: <a href="https://www.jianshu.com/p/a931ad4f0242">https://www.jianshu.com/p/a931ad4f0242</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kube-batch/pull/587">https://github.com/kubernetes-sigs/kube-batch/pull/587</a></li>
<li><a href="https://docs.openshift.com/container-platform/3.6/admin_guide/scheduling/pod_affinity.html#admin-guide-sched-affinity-examples3-pods">https://docs.openshift.com/container-platform/3.6/admin_guide/scheduling/pod_affinity.html#admin-guide-sched-affinity-examples3-pods</a></li>
<li><a href="https://www.jianshu.com/p/a931ad4f0242">https://www.jianshu.com/p/a931ad4f0242</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>LVS-DR工作原理图文详解</title>
    <url>/2016/04/13/LVS-DR%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="VS-x2F-DR的体系结构"><a href="#VS-x2F-DR的体系结构" class="headerlink" title="VS&#x2F;DR的体系结构:"></a><strong>VS&#x2F;DR的体系结构:</strong></h3><p>为了阐述方便，我根据官方原理图另外制作了一幅图，如下图所示：</p>
<span id="more"></span>

<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR1.jpg" alt="LVS-DR1"></p>
<p>LVS-DR:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR2.jpg" alt="LVS-DR2"></p>
<p>DR 值 Direct Routing，直接路由，DR 模型中，Director 和 Realserver 处在同一网络中，对于 Director，VIP 用于接受客户端请求，DIP 用于和 Realserver 通信。对于 Realserver，每个 Realserver 都配有和 Director 相同的 VIP（此 VIP 隐藏，关闭对 ARP 请求的响应），仅用户响应客户端的请求，RIP 用于和 Director 通信。<br>当客户端请求集群服务时，请求报文发送至 Director 的 VIP（Realserver的 VIP 不会响应 ARP 请求），Director 将客户端报文的源和目标 MAC 地址进行重新封装，将报文转发至 Realserver，Realserver 接收转发的报文。此时报文的源 IP 和目标 IP 都没有被修改，因此 Realserver 接受到的请求报文的目标 IP 地址为本机配置的 VIP，它将使用自己的 VIP 直接响应客户端。<br>LVS-DR 模型中，客户端的响应报文不会经过 Director，因此 Director 的并发能力有很大提升。</p>
<h3 id="LVS-DR-模型特性"><a href="#LVS-DR-模型特性" class="headerlink" title="LVS-DR 模型特性:"></a><strong>LVS-DR 模型特性:</strong></h3><blockquote>
<ul>
<li>保证前端路由器将目标地址为 VIP 的报文通过 ARP 解析后送往 Director。</li>
<li>静态绑定：在前端路由将 VIP 对应的目标 MAC 地址静态配置为Director VIP 接口的 MAC 地址。</li>
<li>arptables：在各 Realserver 上，通过 arptables 规则拒绝Realserver响应来自客户端对 VIP 的 ARP 广播请求</li>
<li>修改内核参数：在 Realserver 上修改内核参数，并结合地址的配置方式实现拒绝响应对 VIP 的 ARP 广播请求</li>
<li>各RIP 必须与 DIP 在同一个物理网络中</li>
<li>RS 的 RIP 可以使用私有地址，也可以使用公网地址，以方便配置</li>
<li>Director 仅负责处理入站请求，响应报文由 Realserver 直接发往客户端</li>
<li>Realserver 不能将网关指向 DIP，而直接使用前端网关</li>
<li>不支持端口映射</li>
</ul>
</blockquote>
<p>我将结合这幅原理图及具体的实例来讲解一下LVS-DR的原理，包括数据包、数据帧的走向和转换过程。<br>官方的原理说明：Director接收用户的请求，然后根据负载均衡算法选取一台realserver，将包转发过去，最后由realserver直接回复给用户。</p>
<p>实例场景设备清单：</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR3.jpg" alt="LVS-DR3"></p>
<p>说明：我这里为了方便，client是与vip同一网段的机器。如果是外部的用户访问，将client替换成gateway即可，因为IP包头是不变的，变的只是源mac地址。</p>
<blockquote>
<p>① client向目标vip发出请求，Director接收。此时IP包头及数据帧头信息如下：<br><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR4.jpg" alt="LVS-DR4"><br>② VS根据负载均衡算法选择一台active的realserver(假设是192.168.57.122)，将此RIP所在网卡的mac地址作为目标mac地址，发送到局域网里。此时IP包头及数据帧头信息如下：<br><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR5.jpg" alt="LVS-DR5"><br>③ realserver(192.168.57.122)在局域网中收到这个帧，拆开后发现目标IP(VIP)与本地匹配，于是处理这个报文。随后重新封装报文，发送到局域网。此时IP包头及数据帧头信息如下：<br><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/LVS-DR6.jpg" alt="LVS-DR6"><br>④ 如果client与VS同一网段，那么client(192.168.57.135)将收到这个回复报文。如果跨了网段，那么报文通过gateway&#x2F;路由器经由Internet返回给用户。</p>
</blockquote>
<h3 id="LVS-x2F-DR模式工作原理常见问题"><a href="#LVS-x2F-DR模式工作原理常见问题" class="headerlink" title="LVS&#x2F;DR模式工作原理常见问题:"></a><strong>LVS&#x2F;DR模式工作原理常见问题:</strong></h3><ol>
<li><p>LVS&#x2F;DR如何处理请求报文的，会修改IP包内容吗？<br>1.1 vs&#x2F;dr本身不会关心IP层以上的信息，即使是端口号也是tcp&#x2F;ip协议栈去判断是否正确，vs&#x2F;dr本身主要做这么几个事：<br>1）接收client的请求，根据你设定的负载均衡算法选取一台realserver的ip；<br>2）以选取的这个ip对应的mac地址作为目标mac，然后重新将IP包封装成帧转发给这台RS；<br>3）在hash table中记录连接信息。<br>vs&#x2F;dr做的事情很少，也很简单，所以它的效率很高，不比硬件负载均衡设备差多少。<br>数据包、数据帧的大致流向是这样的：client –&gt; VS –&gt; RS –&gt; client<br>1.2 前面已作了回答，vs&#x2F;dr不会修改IP包的内容.</p>
</li>
<li><p>RealServer为什么要在lo接口上配置VIP？在出口网卡上配置VIP可以吗？<br>2.1 既然要让RS能够处理目标地址为vip的IP包，首先必须要让RS能接收到这个包。<br>在lo上配置vip能够完成接收包并将结果返回client。<br>2.2 答案是不可以将VIP设置在出口网卡上,否则会响应客户端的arp request,造成client&#x2F;gateway arp table紊乱，以至于整个load balance都不能正常工作。</p>
</li>
<li><p>RealServer为什么要抑制arp帧？<br>这个问题在上一问题中已经作了说明，这里结合实施命令进一步阐述。我们在具体实施部署的时候都会作如下调整：<br> echo “1” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_ignore<br> echo “2” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_announce<br> echo “1” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_ignore<br> echo “2” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_announce<br>我相信很多人都不会弄懂它们的作用是什么，只知道一定得有。我这里也不打算拿出来详细讨论，只是作几点说明，就当是补充吧。<br> echo “1” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_ignore<br> echo “2” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_announce<br>这两条是可以不用的，因为arp对逻辑接口没有意义。</p>
</li>
<li><p>如果你的RS的外部网络接口是eth0，那么<br> echo “1” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_ignore<br> echo “2” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_announce<br>其实真正要执行的是：<br> echo “1” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;eth0&#x2F;arp_ignore<br> echo “2” &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;eth0&#x2F;arp_announce<br>所以我个人建议把上面两条也加到你的脚本里去，因为万一系统里上面两条默认的值不是0，那有可能是会出问题滴。</p>
<blockquote>
<ul>
<li>arp_ignore:<br>定义对目标地址为本地IP的ARP询问不同的应答模式0<br>0 - (默认值): 回应任何网络接口上对任何本地IP地址的arp查询请求<br>1 - 只回答目标IP地址是来访网络接口本地地址的ARP查询请求<br>2 -只回答目标IP地址是来访网络接口本地地址的ARP查询请求,且来访IP必须在该网络接口的子网段内<br>3 - 不回应该网络界面的arp请求，而只对设置的唯一和连接地址做出回应<br>4-7 - 保留未使用<br>8 -不回应所有（本地地址）的arp查询<br>确定了向外发送ARP请求的发出地址 也即使VIP 地址</li>
<li>arp_announce - INTEGER<br>对网络接口上，本地IP地址的发出的，ARP回应，作出相应级别的限制:<br>确定不同程度的限制,宣布对来自本地源IP地址发出Arp请求的接口<br>0 - (默认) 在任意网络接口（eth0,eth1，lo）上的任何本地地址<br>1 - 尽量避免不在该网络接口子网段的本地地址做出arp回应. 当发起ARP请求的源IP地址是被设置应该经由路由达到此网络接口的时候很有用.此时会检查来访IP是否为所有接口上的子网段内ip之一.如果改来访IP不属于各个网络接口上的子网段内,那么将采用级别2的方式来进行处理.<br>2 - 对查询目标使用最适当的本地地址.在此模式下将忽略这个IP数据包的源地址并尝试选择与能与该地址通信的本地地址.首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址. 如果没有合适的地址被发现,将选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接口来进行发送.限制了使用本地的vip地址作为优先的网络接口</li>
</ul>
</blockquote>
</li>
<li><p>LVS&#x2F;DR load balancer（director）与RS为什么要在同一网段中？<br>从第一个问题中大家应该明白vs&#x2F;dr是如何将请求转发给RS的了吧？它是在数据链路层来实现的，所以director必须和RS在同一网段里面。</p>
</li>
<li><p>为什么director上eth0接口除了VIP另外还要配一个ip（即DIP）？</p>
<blockquote>
<ul>
<li>如果是用了keepalived等工具做HA或者Load Balance,则在健康检查时需要用到DIP。</li>
<li>没有健康检查机制的HA或者Load Balance则没有存在的实际意义。</li>
</ul>
</blockquote>
</li>
<li><p>LVS&#x2F;DR ip_forward需要开启吗？<br>不需要。因为director跟realserver是同一个网段，无需开启转发。</p>
</li>
<li><p>director的vip的netmask一定要是255.255.255.255吗？<br>lvs&#x2F;dr里，director的vip的netmask 没必要设置为255.255.255.255，也不需要再去<br>route add -host $VIP dev eth0:0<br>director的vip本来就是要像正常的ip地址一样对外通告的,不要搞得这么特殊.</p>
</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.cnblogs.com/czh-liyu/archive/2011/11/29/2267963.html">http://www.cnblogs.com/czh-liyu/archive/2011/11/29/2267963.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(Loki踩坑记)</title>
    <url>/2022/05/15/Loki-Prombles/</url>
    <content><![CDATA[<p>grafana出品的loki日志框架完美地与kubernetes的label理念结合，相对于EFK来说更加轻量级，非常适合不需要日志聚合的场景.目前新上集群考虑都彩loki做为基础工具, 直接在grafana中展示</p>
<p><strong>在这里记录下使用Loki踩过的坑, 不定期更新</strong></p>
<span id="more"></span>



<h3 id="LOKI启动时提示-panic-invalid-page-type-11-10"><a href="#LOKI启动时提示-panic-invalid-page-type-11-10" class="headerlink" title="LOKI启动时提示 panic: invalid page type: 11: 10"></a>LOKI启动时提示 panic: invalid page type: 11: 10</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220607165538.png"></p>
<p>原因: 对应的index table文件已经损坏</p>
<p>解决: 删除相应的index文件即可解决</p>
<h3 id="日志的label不对"><a href="#日志的label不对" class="headerlink" title="日志的label不对"></a>日志的label不对</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220513132435.png"></p>
<p>原因: promtail中的scrape_config存在问题.</p>
<p>参考: <a href="https://izsk.me/2022/05/15/Loki-log-with-wrong-labels/">https://izsk.me/2022/05/15/Loki-log-with-wrong-labels/</a></p>
<h3 id="grafana中开启实时日志时提示Query-error"><a href="#grafana中开启实时日志时提示Query-error" class="headerlink" title="grafana中开启实时日志时提示Query error"></a>grafana中开启实时日志时提示Query error</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20211102143833.png"></p>
<p>原因: 官方的解释是Note that live tailing relies on two websocket connections: one between the browser and the Grafana server, and another between the Grafana server and the Loki server. If you run any reverse proxies, please configure them accordingly.</p>
<p>也就是说，如果在web与grafana,grafana与loki之间存在如nginx类的proxy,则需要开启websocket特性，洽洽作者的grafana是在nginx后的</p>
<p>解决: nginx添加websocket配置,<a href="https://www.nginx.com/blog/websocket-nginx/">详见</a></p>
<p>参考: <a href="https://github.com/grafana/grafana/blob/b5d8cb25e18fc73f37b3546246363464c9298684/docs/sources/features/datasources/loki.md">https://github.com/grafana/grafana/blob/b5d8cb25e18fc73f37b3546246363464c9298684/docs/sources/features/datasources/loki.md</a></p>
<h3 id="Loki-file-size-too-small-nerror-creating-index-client"><a href="#Loki-file-size-too-small-nerror-creating-index-client" class="headerlink" title="Loki: file size too small\nerror creating index client"></a>Loki: file size too small\nerror creating index client</h3><p>解决: 删除loki的持久化目录下的boltdb-shipper-active&#x2F;index_18xxx目录 </p>
<p>参考: <a href="https://github.com/grafana/loki/issues/3219">https://github.com/grafana/loki/issues/3219</a></p>
<h3 id="protail-context-deadline-exceeded"><a href="#protail-context-deadline-exceeded" class="headerlink" title="protail: context deadline exceeded"></a>protail: context deadline exceeded</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210607142742.png"></p>
<p>原因: promtail无法连接loki所致</p>
<h3 id="promtail-cpu使用过高"><a href="#promtail-cpu使用过高" class="headerlink" title="promtail cpu使用过高"></a>promtail cpu使用过高</h3><p>原因: 由于集群中存在大量的job类pod，这会对loki的服务发现会有很大的压力，需要调整promtail的配置，查看官方的issue，后续可能会将ds由promtail转到服务端来做，promtail需要调整的配置主要为</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">target_config:</span></span><br><span class="line">  <span class="attr">sync_period:</span> <span class="string">30s</span></span><br><span class="line"><span class="attr">positions:</span></span><br><span class="line">  <span class="attr">filename:</span> <span class="string">/run/promtail/positions.yaml</span></span><br><span class="line">  <span class="attr">sync_period:</span> <span class="string">30s</span></span><br></pre></td></tr></table></figure>

<p>将 sync_period由默认的10s换成30s</p>
<p>可以使用以下的命令获取到pprof文件分析性能</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl localhost:3100/debug/pprof/profile\?seconds\=20</span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://github.com/grafana/loki/issues/1315">https://github.com/grafana/loki/issues/1315</a></p>
<h3 id="Maximum-active-stream-limit-exceeded"><a href="#Maximum-active-stream-limit-exceeded" class="headerlink" title="Maximum active stream limit exceeded"></a>Maximum active stream limit exceeded</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210312182643.png"></p>
<p>原因： 同下，需要调整limit config中的max_streams_per_user， 设置为0即可</p>
<h3 id="server-returned-HTTP-status-429-Too-Many-Requests"><a href="#server-returned-HTTP-status-429-Too-Many-Requests" class="headerlink" title="server returned HTTP status 429 Too Many Requests"></a>server returned HTTP status 429 Too Many Requests</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210219121747.png"></p>
<p>原因: limit config中的参数: <code>ingestion_burst_size </code>默认值太小，调整即可</p>
<p>参考: <a href="https://github.com/grafana/loki/issues/1923">https://github.com/grafana/loki/issues/1923</a></p>
<h3 id="Please-verify-permissions"><a href="#Please-verify-permissions" class="headerlink" title="Please verify permissions"></a>Please verify permissions</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201110195022.png"></p>
<p>原因: 这条其实是warn,不影响promtail的正常工作，如果调整过日志的路径的话要确认promtail挂载的路径是否正常</p>
<h3 id="loki-invalid-schema-config"><a href="#loki-invalid-schema-config" class="headerlink" title="loki: invalid schema config"></a>loki: invalid schema config</h3><p>原因: loki的配置文件格式错误.</p>
<h3 id="promtail-too-many-open-files"><a href="#promtail-too-many-open-files" class="headerlink" title="promtail: too many open files"></a>promtail: too many open files</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200909190248.png"></p>
<p>原因: &#x2F;var&#x2F;log&#x2F;pods下面的文件数量太多，导致超过内核参数(fs.inotify.max_user_instances)设置配置.</p>
<p>解决：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先查看当前机器设置的配置</span></span><br><span class="line"><span class="built_in">cat</span> /proc/sys/fs/inotify/max_user_instances</span><br><span class="line"><span class="comment"># 再查看promtail启动时watch的文件数</span></span><br><span class="line"><span class="built_in">cat</span> /run/promtail/positions.yaml | <span class="built_in">wc</span> -l</span><br><span class="line"><span class="comment"># 如果这个值比max_user_instances要大，则会出现上面的错误，可以通过修改内核参数进行调整</span></span><br><span class="line">sysctl -w fs.inotify.max_user_instances=1024</span><br><span class="line"><span class="comment"># 生效</span></span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<p>参考: <a href="https://github.com/grafana/loki/issues/1153">https://github.com/grafana/loki/issues/1153</a></p>
<h3 id="promtail-no-such-file-ro-directory"><a href="#promtail-no-such-file-ro-directory" class="headerlink" title="promtail: no such file ro directory"></a>promtail: no such file ro directory</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200909200457.png"></p>
<p>原因： promtail daemonset启动时会自动挂载好几个hostpath,如果docker containers的配置调整过，则需要volume跟volumemount都需要对应上.</p>
<h3 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h3><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/grafana/loki/issues/429">https://github.com/grafana/loki/issues/429</a></li>
<li><a href="https://github.com/grafana/loki/issues/3219">https://github.com/grafana/loki/issues/3219</a></li>
<li><a href="https://github.com/grafana/loki/issues/1153">https://github.com/grafana/loki/issues/1153</a></li>
<li><a href="https://github.com/grafana/loki/issues/1923">https://github.com/grafana/loki/issues/1923</a></li>
<li><a href="https://blog.csdn.net/weixin_44997607/article/details/108419144">https://blog.csdn.net/weixin_44997607/article/details/108419144</a></li>
<li><a href="https://github.com/grafana/loki/issues/1315">https://github.com/grafana/loki/issues/1315</a></li>
<li><a href="https://www.nginx.com/blog/websocket-nginx/">https://www.nginx.com/blog/websocket-nginx/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(Loki日志系统初试)</title>
    <url>/2020/07/15/Loki/</url>
    <content><![CDATA[<p>基于kubernetes为容器平台的业种中， 对于日志的收集，使用最多的是FEK， 不过有时候，FEK在架构上会略显重， ES的查询及全文检索功能其实使用的不是很多.LoKi做为日志架构的新面孔, 由grafana开源， 使用了与Prometheus同样的label理念, 同时摒弃了全文检索的能力, 因此比较轻便, 非常具有潜力</p>
<span id="more"></span>



<h3 id="like-Prometheus-but-for-logs"><a href="#like-Prometheus-but-for-logs" class="headerlink" title="like Prometheus, but for logs"></a>like Prometheus, but for logs</h3><p><code>Loki</code>是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签。项目受 Prometheus 启发，官方的介绍就是：<code>Like Prometheus, but for logs</code>，类似于 Prometheus 的日志系统</p>
<p>与其他日志聚合系统相比，<code>Loki</code>具有下面的一些特性：</p>
<ul>
<li>不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。</li>
<li>通过使用与 Prometheus 相同的标签记录流对日志进行索引和分组，这使得日志的扩展和操作效率更高。</li>
<li>特别适合储存 Kubernetes Pod 日志; 诸如 Pod 标签之类的元数据会被自动删除和编入索引。</li>
<li>受 Grafana 原生支持。</li>
</ul>
<p>Loki 由以下3个部分组成：</p>
<ul>
<li><code>loki</code>是主服务器，负责存储日志和处理查询。</li>
<li><code>promtail</code>是代理，负责收集日志并将其发送给 loki，当然也支持其它的收集端如fluentd等</li>
<li><code>Grafana</code>用于 UI 展示</li>
</ul>
<p>同时Loki也提示了command line工具，通过这个工具可以使用http的方式与loki进行交互，这个后面说</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200718162524.png"></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>官方提供了多种的部署方式, 这里选择使用<code>helm</code>, 如果只是想试用的话则非常简单, 直接参考<a href="https://github.com/grafana/loki/blob/v1.5.0/docs/installation/helm.md">helm</a>即可run起来，这里就不做搬运工了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm repo add loki https://grafana.github.io/loki/charts</span><br><span class="line">helm repo update</span><br><span class="line">helm upgrade --install loki loki/loki-stack</span><br></pre></td></tr></table></figure>



<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>对于loki来说，loki端的配置相对来说会比较简单, 采集端由于可以使用pipeline， 因此会稍复杂点，但是如果接触过prometheus，那其它会觉得还是挺情切.</p>
<p>先来看服务端loki的配置</p>
<h4 id="loki-yaml"><a href="#loki-yaml" class="headerlink" title="loki.yaml"></a>loki.yaml</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">auth_enabled:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">chunk_store_config:</span></span><br><span class="line">  <span class="attr">max_look_back_period:</span> <span class="string">0s</span></span><br><span class="line"><span class="attr">ingester:</span></span><br><span class="line">  <span class="attr">chunk_block_size:</span> <span class="number">262144</span></span><br><span class="line">  <span class="attr">chunk_idle_period:</span> <span class="string">3m</span></span><br><span class="line">  <span class="attr">chunk_retain_period:</span> <span class="string">1m</span></span><br><span class="line">  <span class="attr">lifecycler:</span></span><br><span class="line">    <span class="attr">ring:</span></span><br><span class="line">      <span class="attr">kvstore:</span></span><br><span class="line">        <span class="attr">store:</span> <span class="string">inmemory</span></span><br><span class="line">      <span class="attr">replication_factor:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">max_transfer_retries:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">limits_config:</span></span><br><span class="line">  <span class="attr">enforce_metric_name:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">reject_old_samples:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">reject_old_samples_max_age:</span> <span class="string">168h</span></span><br><span class="line"><span class="attr">schema_config:</span></span><br><span class="line">  <span class="attr">configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">&quot;2018-04-15&quot;</span></span><br><span class="line">    <span class="attr">index:</span></span><br><span class="line">      <span class="attr">period:</span> <span class="string">168h</span></span><br><span class="line">      <span class="attr">prefix:</span> <span class="string">index_</span></span><br><span class="line">    <span class="attr">object_store:</span> <span class="string">filesystem</span></span><br><span class="line">    <span class="attr">schema:</span> <span class="string">v9</span></span><br><span class="line">    <span class="attr">store:</span> <span class="string">boltdb</span></span><br><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">http_listen_port:</span> <span class="number">3100</span></span><br><span class="line"><span class="attr">storage_config:</span></span><br><span class="line">  <span class="attr">boltdb:</span></span><br><span class="line">    <span class="attr">directory:</span> <span class="string">/data/loki/index</span></span><br><span class="line">  <span class="attr">filesystem:</span></span><br><span class="line">    <span class="attr">directory:</span> <span class="string">/data/loki/chunks</span></span><br><span class="line"><span class="attr">table_manager:</span></span><br><span class="line">  <span class="attr">retention_deletes_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">retention_period:</span> <span class="string">0s</span></span><br></pre></td></tr></table></figure>

<p>loki服务端主要定义了数据到达控制面之后的一些策略, 比如ingester size大小， 存储路径等的信息，最好使用SSD.</p>
<p>再看看采集端配置</p>
<h4 id="promtail"><a href="#promtail" class="headerlink" title="promtail"></a>promtail</h4><p>prometail同时充当服务端及客户端</p>
<h5 id="server"><a href="#server" class="headerlink" title="server"></a>server</h5><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">http_listen_port:</span> <span class="number">3101</span></span><br></pre></td></tr></table></figure>

<p>promtail做为服务端, 可同时提供http及grpc能力,而需要进行数据发送到loki时，promtail则为客户端</p>
<h5 id="client"><a href="#client" class="headerlink" title="client"></a>client</h5><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">client:</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">http://loki:3100/loki/api/v1/push</span></span><br><span class="line">      <span class="attr">backoff_config:</span></span><br><span class="line">        <span class="attr">max_period:</span> <span class="string">5s</span></span><br><span class="line">        <span class="attr">max_retries:</span> <span class="number">20</span></span><br><span class="line">        <span class="attr">min_period:</span> <span class="string">100ms</span></span><br><span class="line">      <span class="attr">batchsize:</span> <span class="number">102400</span></span><br><span class="line">      <span class="attr">batchwait:</span> <span class="string">1s</span></span><br><span class="line">      <span class="attr">external_labels:</span> &#123;&#125;</span><br><span class="line">      <span class="attr">timeout:</span> <span class="string">10s</span></span><br></pre></td></tr></table></figure>

<p>当做为client时, promtail做为日志的收集端，然后发送到loki实例</p>
<p>backoff_config 参数指定了当发送日志到loki实例失败时采取的重试策略</p>
<p>external_labels则可以在所有的日志发送到loki实例前加入k-v的labels.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># WARNING: If one of the remote Loki servers fails to respond or responds</span></span><br><span class="line"><span class="comment"># with any error which is retryable, this will impact sending logs to any</span></span><br><span class="line"><span class="comment"># other configured remote Loki servers.  Sending is done on a single thread!</span></span><br><span class="line"><span class="comment"># It is generally recommended to run multiple promtail clients in parallel</span></span><br><span class="line"><span class="comment"># if you want to send to multiple remote Loki instances.</span></span><br></pre></td></tr></table></figure>

<h4 id="target-config"><a href="#target-config" class="headerlink" title="target_config"></a>target_config</h4><p>控制从发现的日志文件中读取行为,比如多久读取一次</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">target_config:</span></span><br><span class="line">  <span class="attr">sync_period:</span> <span class="string">10s</span></span><br></pre></td></tr></table></figure>

<h4 id="scrape-configs"><a href="#scrape-configs" class="headerlink" title="scrape_configs"></a>scrape_configs</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-pods-name</span></span><br><span class="line">  <span class="attr">pipeline_stages:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">docker:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br><span class="line">  <span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_label_name</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__service__</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_node_name</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__host__</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">drop</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__service__</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">__meta_kubernetes_pod_label_(.+)</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">separator:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_namespace</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__service__</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">job</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_namespace</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">namespace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_name</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">pod</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_container_name</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">container</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">replacement:</span> <span class="string">/var/log/pods/*$1/*.log</span></span><br><span class="line">    <span class="attr">separator:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">source_labels:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_uid</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_container_name</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__path__</span></span><br></pre></td></tr></table></figure>

<p>上面是最重要的一个配置, 会发现跟prometheus的配置文件很像， 这里不对上述配置语法进行细说， 感兴趣的可以参考prometheus文档， 这里使用了<strong>跟prometheus相同的服务发现机制, 同时也是基于lable进行一系列的增删改的操作， 最终实现一条日志上存在多个label，我们可以基于这些label来查询日志</strong></p>
<p>这个是直接使用官方提供的example, 可以根据实际情况进行修改.</p>
<h5 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h5><p>可以看到上面的配置中出现一个pipeline的选项，这个是用于指定采集到日志后可以进行的操作, 以stage为单位, 目前支持4种stage.</p>
<ol>
<li>解析阶段：解析当前日志行并从中提取数据。然后，提取的数据可供其他阶段使用。 </li>
<li>转换阶段：转换从先前阶段提取的数据。 </li>
<li>行动阶段：从先前阶段中提取数据并对其进行处理。在日志行中添加或修改现有标签 更改日志行的时间戳 更改日志行的内容 根据提取的数据创建指标等</li>
<li>筛选阶段：可以根据某些条件选择应用阶段的子集或丢弃条目</li>
</ol>
<p>同时每个stage又有一些预设的可直接供使用的操作.</p>
<p>比如上面指定的docker,就属于解析阶段, 它的作用是<code>通过使用标准Docker格式解析日志行来提取数据</code></p>
<p>这对于直接将应用日志输出到console,然后通过docker的格式打印出来的情况下非常方便. 这也是最常见场景</p>
<p>官方的<a href="https://github.com/grafana/loki/blob/v1.5.0/docs/clients/promtail/pipelines.md">说明</a></p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ol>
<li>选择数据源</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200718160645.png"></p>
<ol start="2">
<li>填写URL</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200718160706.png"></p>
<ol start="3">
<li>切换到<code>Explore --&gt; 选择Loki</code></li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200708194140.png"></p>
<p>Log labels下拉框中可以下拉选择通过上述服务发现生成的的日志labels, 中间则可以使用label语法进行筛选</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;app=<span class="string">&quot;ingress-nginx&quot;</span>, pod=<span class="string">&quot;nginx-ingress-controller-fm24n&quot;</span>&#125; |=<span class="string">&quot;error&quot;</span></span><br></pre></td></tr></table></figure>

<p>前半部分很好理解, 后面的<code>|=&quot;error&quot;</code>则是在前半部分筛选出来的结果中再次grep出存在error的日志</p>
<p>这里使用的是LogQL语法, 见下文</p>
<ol start="4">
<li>添加dashboard</li>
</ol>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200708194343.png"></p>
<h3 id="LogQL"><a href="#LogQL" class="headerlink" title="LogQL"></a>LogQL</h3><p>Loki 使用一种称为 <a href="https://github.com/grafana/loki/blob/master/docs/logql.md#filter-expression">LogQL</a> 的语法来进行日志检索，语法类似 PromQL</p>
<h4 id="Log-Stream-Selector"><a href="#Log-Stream-Selector" class="headerlink" title="Log Stream Selector"></a>Log Stream Selector</h4><p>也是使用大括号进行条件的筛选, 同时支持正则</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;app=<span class="string">&quot;mysql&quot;</span>,name=~<span class="string">&quot;mysql-backup.+&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>=</code>: exactly equal.</li>
<li><code>!=</code>: not equal.</li>
<li><code>=~</code>: regex matches.</li>
<li><code>!~</code>: regex does not match.</li>
</ul>
<h4 id="Filter-Expression"><a href="#Filter-Expression" class="headerlink" title="Filter Expression"></a>Filter Expression</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;instance=~<span class="string">&quot;kafka-[23]&quot;</span>,name=<span class="string">&quot;kafka&quot;</span>&#125; != <span class="string">&quot;kafka.server:type=ReplicaManager&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>|=</code>: Log line contains string.</li>
<li><code>!=</code>: Log line does not contain string.</li>
<li><code>|~</code>: Log line matches regular expression.</li>
<li><code>!~</code>: Log line does not match regular expression.</li>
</ul>
<h4 id="Metric-Queries"><a href="#Metric-Queries" class="headerlink" title="Metric Queries"></a>Metric Queries</h4><p>这个其实就跟prometheus中的很想像了.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rate(&#123;job=<span class="string">&quot;mysql&quot;</span>&#125; |= <span class="string">&quot;error&quot;</span> != <span class="string">&quot;timeout&quot;</span> [5m])</span><br></pre></td></tr></table></figure>

<ul>
<li><code>rate</code>: calculates the number of entries per second</li>
<li><code>count_over_time</code>: counts the entries for each log stream within the given range.</li>
<li><code>bytes_rate</code>: calculates the number of bytes per second for each stream.</li>
<li><code>bytes_over_time</code>: counts the amount of bytes used by each log stream for a given range.</li>
</ul>
<h4 id="Aggregation-operators"><a href="#Aggregation-operators" class="headerlink" title="Aggregation operators"></a>Aggregation operators</h4><p>当然还支持一些聚合操作，比如</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">avg(rate((&#123;job=<span class="string">&quot;nginx&quot;</span>&#125; |= <span class="string">&quot;GET&quot;</span>)[10s])) by (region)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>sum</code>: Calculate sum over labels</li>
<li><code>min</code>: Select minimum over labels</li>
<li><code>max</code>: Select maximum over labels</li>
<li><code>avg</code>: Calculate the average over labels</li>
<li><code>stddev</code>: Calculate the population standard deviation over labels</li>
<li><code>stdvar</code>: Calculate the population standard variance over labels</li>
<li><code>count</code>: Count number of elements in the vector</li>
<li><code>bottomk</code>: Select smallest k elements by sample value</li>
<li><code>topk</code>: Select largest k elements by sample value</li>
</ul>
<p>还有很多比如’and, or’的操作都是支持, 就不一一搬运了</p>
<h4 id="logcli"><a href="#logcli" class="headerlink" title="logcli"></a><a href="https://github.com/grafana/loki/blob/master/docs/getting-started/logcli.md">logcli</a></h4><p>logcli做为一个命令行工具, 可以直接与loki进行交互, 目前grafana是不支持对loki进行告警, 可以通过logcli写脚本的方式进行，也是种办法</p>
<h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>要知道es产生的索引可能跟日志本身一样大, loki最大的特点就是同一类型的日志带有相同的label，这样的话产生的索引就会压缩几个数量级，这就不需要像es查询一样将索引放入内存中， 从而可以使内存加载更大的日志量, 另一方面, loki实现了类型于分布式的grep,会将查询分解成较小的shard并行处理, 因为不需要全文检索，因此速度上会更快.</p>
<p>关于性能方面也局限于官方的说法, 还没有做过性能测试，目前还没有在生产上使用loki, 但是在QA&#x2F;DEV环境中使用,体验还是不错的, 不用在grafana&#x2F;kibana来回切换了.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://dockone.io/article/10443">http://dockone.io/article/10443</a></li>
<li><a href="https://blog.csdn.net/Linkthaha/article/details/100575651">https://blog.csdn.net/Linkthaha/article/details/100575651</a></li>
<li><a href="https://github.com/grafana/loki/blob/c3d3f2ba77/docs/operations/loki-canary.md">https://github.com/grafana/loki/blob/c3d3f2ba77/docs/operations/loki-canary.md</a></li>
<li><a href="https://github.com/grafana/loki/blob/v1.5.0/docs/clients/promtail/stages/docker.md">https://github.com/grafana/loki/blob/v1.5.0/docs/clients/promtail/stages/docker.md</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>MPI框架学习一(架构组件)</title>
    <url>/2022/04/20/MPI-Structure/</url>
    <content><![CDATA[<p>Mpi: Message Passing Interface，消息传递接口</p>
<p>由于AI业务中使用到了MPI, 网上看MPI教程写的也是云里雾里，很难理解清楚，作者这里结合自身负责的业务来理解MPI的实现方式.</p>
<p>如果有理解存在有遗留或者错误的地方，请各位过客不吝赐教.</p>
<span id="more"></span>

<p>在AI多机多卡训练通信过程中，各节点之间需要进行数据交换，集群中大量的数据交换是很耗费时间的，因此需要一种在跨节点的情况下能快速进行数据交流的通道，MPI就是一种很常用的通信框架,在<strong>HPC</strong>中使用也较广.</p>
<p>MPI是一组用于多节点数据通信的标准，而非一种语言或者接口。MPI虽然很庞大 但是它的<strong>最终目的是服务于进程间通信这一目标的</strong>, 不同的实现，使用方法有所不同，如mpich or openmpi等。</p>
<p>mpirun vs mpiexec</p>
<p>mpirun与mpiexec在MPI中都可以叫做为launcher(启动器), 是比较通用的起名或者是别名的符号链接(symbolic link)，实现可能会所不同，比如OpenMPI ，它自己的进程启动器称为 <code>orterun</code> .为了兼容性，<code>orterun</code>也被符号链接(symbolic link)为 <code>mpirun</code>和 <code>mpiexec</code></p>
<p>hydra vs mpd</p>
<p>hydra与mpd同为进程管理器，mpd在mpich2-1.3+版本中被废弃，取而代之的是hydra，hydra是一种比较轻量的PM(相对于mpd)，利用ssh、rsh、pbs、slurm和sge等调度工具部署运行并行程序</p>
<p>MPICH2是由Argonne国家实验室维护的开源MPI库，是高性能计算领域使用最为广泛的MPI库之一，作者所有的小组内也是以mpich2为公共组件，因此以下的case都以mpich2为例.</p>
<p>MPI的编程方式，是“一处代码，多处执行”。编写过多线程的人应该能够理解，就是同样的代码，不同的进程执行不同的路径</p>
<p>以下是<code>Hydra Process Management Framework</code>官方的架构图: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220420093355.png"></p>
<p><a href="https://wiki.mpich.org/mpich/index.php/Hydra_Process_Management_Framework">Hydra framework</a>由以下几个基本部分组成：</p>
<ol>
<li>User Interface，用户接口UI</li>
<li>Resource Managerment Kernel，资源管理核心RMK</li>
<li>Process manager，进程管理PM</li>
<li>Bootstrap，启动服务的方式，如ssh，fork，pbs，slurm，sge</li>
<li>Process Binding，进程监听，如plpa</li>
<li>Commnication Subsystem，例如IB，MX</li>
<li>Process Managerment proxy，进程管理代理</li>
<li>I&#x2F;O demux engine ，IO复用引擎</li>
</ol>
<p>从这个图也很容易看出:</p>
<p>JLE一般运行在称之为管理节点上(当然在计算节点也是ok的)，用于接收指令然后将任务下发</p>
<p>计算节点则一般是跑具体任务的,而ManagermentTools则是在管理节点与计算节点间的桥梁</p>
<p>下面详细来解释一下上面的组件, 机器翻译自[wiki][<a href="https://wiki.mpich.org/mpich/index.php/Hydra_Process_Management_Framework]:wiki%E7%9A%84%E6%96%87%E5%AD%97%E6%97%B6%E9%97%B4%E6%9C%89%E7%82%B9%E6%97%A9%E4%BA%86%EF%BC%8C%E4%BD%86%E6%95%B4%E4%BD%93%E4%B8%8D%E5%8F%97%E5%BD%B1%E5%93%8D%EF%BC%8C%E6%9C%BA%E7%BF%BB%E5%87%BA%E6%9D%A5%E8%BF%98%E8%A1%8C">https://wiki.mpich.org/mpich/index.php/Hydra_Process_Management_Framework]:wiki的文字时间有点早了，但整体不受影响，机翻出来还行</a>.</p>
<ul>
<li><p><strong>User interface</strong><br>此层的主要职责是从用户那里收集有关应用程序的信息： 在何处启动进程；将进程映射到内核；读取 标准输入(stdin) 并将其转发到适当的进程 (es)； 从不同的程序中读取标准输出&#x2F;标准错误并合适地指导。</p>
<p><strong>mpich2中的mpiexec.hydra就是个userinterface程序</strong></p>
</li>
<li><p><strong>Resource Mangerment Kernel(RMK)</strong><br>RMK 提供了可与资源管理器进行交互的插件功能，比如Torque，Moab或者Cobalt。例如, 如果应用程序在启动作业之前需要在系统上分配节点, RMK 将完成这部分。同样, RMK 还可以允许在单个系统预留用于多个作业的情况下解耦作业启动。</p>
<p><strong>在当前的实现中, RMK 非常简单, 不提供任何这些功能</strong></p>
</li>
<li><p><strong>Process Manager(PM)</strong><br>进程管理器提供了必要的环境设置以及主要的进程管理功能，例如进程管理器pmiserv进程管理器提供了MPICH PMI（PMI）功能。但目前仅支持PMI1，但我们也计划添加 PMI-2（目前正在起草中)。其他进程管理器也可以通过其他接口支持。</p>
</li>
<li><p><strong>Process Managerment Proxy(PMP)</strong><br>进程管理代理基本上是一个帮助器代理, 它是在系统的每个节点上生成的, 以帮助进程管理器进行进程生成、进程清理、信号转发、I&#x2F;O转发以及任何进程管理器特定的功能。<br>它基本上可以执行进程管理器可以做的任何任务, 因此, 甚至可以创建进程管理代理的层次结构, 其中每个代理都充当其子树的进程管理器。</p>
<p><strong>mpich2的hydra_pmi_proxy就是个PMP</strong></p>
</li>
<li><p><strong>Bootstrap</strong><br>引导服务器主要充当预配置的守护进程系统, 允许上一层服务在整个系统中启动进程,目前运行多种方式。<br>例如, ssh 引导服务器fork进程, 每一个都通过执行一个 ssh 到另外一台机器，启动一个进程。</p>
</li>
<li><p><del><strong>Processing Binding</strong></del><br><del>进程监听组件主要处理提取系统体系结构信息 (例如, 处理器的数量、可用的内核和 SMT 线程、它们的拓扑、共享缓存等), 以及将进程绑定到可移植的不同内核中用一种简便的方式。PLPA 是一个这样的体系结构, 已经在Hydra中使用, 但它只提供有限的信息。</del></p>
</li>
<li><p><strong>Communication Subsystem</strong><br>通信子系统是不同代理之间以可伸缩方式进行通信的一种方式。这仅与下面描述的预启动和预连接代理有关。<br>此组件提供可伸缩的通信机制, 而与系统的规模无关 (例如, 基于IB或MX)。</p>
</li>
<li><p><strong>I&#x2F;O Demux Engine</strong></p>
<p>这基本上是一个便利组件，不同的组件可以“注册”它们的文件描述符，并且 demux 引擎可以等待这些描述符中的任何一个上的事件。这提供了一个集中的事件管理机制；所以我们不需要让不同的线程阻塞不同的事件。I&#x2F;O 解复用引擎使用同步回调机制。也就是说，对于每个文件描述符，调用进程都会提供一个函数指针，指向在描述符上发生事件时必须调用的函数。demux 引擎阻塞所有已注册文件描述符上的事件，并在有事件时调用适当的回调。该组件在实现中非常有用，但在架构本身中并不起关键作用</p>
</li>
</ul>
<p>这里RMK、PB、I&#x2F;O Demux Engine都比较复杂，可以不用关心是什么.</p>
<p>PM跟PMP从名字上来看也比较容易理解，PMP可以理解为是PM在每个计算节点上负责管理的代理人角色</p>
<p>另外要重点说一下Bootstrap，确切来说是Bootstrap Server </p>
<p>大多数场景下远程启动进程都是使用默认的ssh launcher,意思是mpiexec.hydra通过ssh的方式登录计算节点来启动mpi_proxy程序,但是在ssh的过程中是不能有输入密码操作的，因此需要事先配置好ssh免密</p>
<p>比如最简单的程度</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># user interface端</span></span><br><span class="line">mpiexec.hydra -n 4 -hosts <span class="string">&#x27;node1,node2&#x27;</span> <span class="string">&#x27;run-AI-training-long-time-jobs&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应在计算节点上的proxy启动命令</span></span><br><span class="line">hydra_pmi_proxy --control-port prm-510815</span><br><span class="line">f0-1dec-481c-b5ef-0073528b860b-0j59fe-1919734637:11000 --rmk user --demux poll --pgid 0 --retries 10 --usize -2 --proxy-id 0</span><br></pre></td></tr></table></figure>

<p>如果没有指定-launcher(有些实现可能是-bootstrap)，则使用默认值ssh</p>
<p>新版本(好像是在2.1之后)的mpiexec.hydra支持<code>launcher manual</code>, 即mpi_proxy的启动可通过人工的方式，这在docker场景下比较有用，在用docker做为计算节点时可以不用配置ssh免密，在启动docker时通过entrypoint直接启动mpi_proxy,这个时候就需要指定launcher为manual了.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># user interface端</span></span><br><span class="line">mpiexec.hydra -launcher manual -n 4 -hosts <span class="string">&#x27;node1,node2&#x27;</span> <span class="string">&#x27;run-AI-training-long-time-jobs&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应在计算节点上的proxy启动命令</span></span><br><span class="line">hydra_pmi_proxy --launcher manual --control-port prm-510815</span><br><span class="line">f0-1dec-481c-b5ef-0073528b860b-0j59fe-1919734637:11000 --rmk user --demux poll --pgid 0 --retries 10 --usize -2 --proxy-id 0</span><br></pre></td></tr></table></figure>

<p>这样就需要两端自行启动相关进程了.</p>
<p>关于mpi的架构这块，能理清楚那几个组件的作用就可以了.</p>
<p>上面的命令行支持非常多的参数，本身也没有什么说明性，至于<code>对应在计算节点上的proxy启动命令</code>为什么是那样的、节点与节点间是如何通信的等问题。</p>
<p>都值得再更.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/69497154">https://zhuanlan.zhihu.com/p/69497154</a></li>
<li><a href="https://www.cxyzjd.com/article/wgbljl/24038539">https://www.cxyzjd.com/article/wgbljl/24038539</a></li>
<li><a href="https://blog.csdn.net/wgbljl/article/details/24038539">https://blog.csdn.net/wgbljl/article/details/24038539</a></li>
<li><a href="http://cali2.unilim.fr/intel-mpi/doc/Reference_Manual.pdf">http://cali2.unilim.fr/intel-mpi/doc/Reference_Manual.pdf</a></li>
<li><a href="https://wiki.mpich.org/mpich/index.php/Developer_Documentations">https://wiki.mpich.org/mpich/index.php/Developer_Documentations</a></li>
<li><a href="https://wiki.mpich.org/mpich/index.php/Hydra_Process_Management_Framework">https://wiki.mpich.org/mpich/index.php/Hydra_Process_Management_Framework</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>工程侧如何加速模型训练</title>
    <url>/2022/05/29/Model-Traning-High-Speed-Tech/</url>
    <content><![CDATA[<p>模型训练除了算法侧需要优化之外，工程师也可以做很多落地来加速这个过程，今天就结合作者在落地训练平台的经验来聊一聊其中常用技术.</p>
<span id="more"></span>

<p>训练常常提到的单机单卡，单机多卡，多机多卡以及数据并行及模型并行的概念(<strong>作者并不是算法研究工作者,也没有能力深入了解其代码细节，所以接下来只会广义上引出一些概念</strong>)</p>
<h3 id="GPU使用"><a href="#GPU使用" class="headerlink" title="GPU使用"></a>GPU使用</h3><h4 id="单机单卡"><a href="#单机单卡" class="headerlink" title="单机单卡"></a>单机单卡</h4><p>这是最简单的一种方式, 使用一台机器上的一张卡运行任务, 常常用于本地调试目的，这个没什么好说的</p>
<h4 id="单机多卡"><a href="#单机多卡" class="headerlink" title="单机多卡"></a>单机多卡</h4><p>有时只使用一张卡效率太慢，如果刚好一台机器上又有好几块GPU,那么自然就会想到是不是可以从单卡扩展到多卡上，同时在多张卡上运行</p>
<h4 id="多机多卡"><a href="#多机多卡" class="headerlink" title="多机多卡"></a>多机多卡</h4><p>一般情况下，一台服务器可安装8块GPU,如果性能还是无法满足的情况下，就需要使用多机多卡方式了，从单机扩展到多机实现分布式</p>
<p><strong>单机多卡</strong>和<strong>多机多卡（分布式）</strong>的区别：</p>
<ul>
<li><strong>单机多卡</strong>：只需运行一份代码，由该代码分配该台机器上GPU资源的使用</li>
<li><strong>多机多卡</strong>：每台机器上都需要运行一份代码，机器之间需要互相通信传递梯度，并且模型参数的更新也存在<strong>同步</strong>训练模式和<strong>异步</strong>训练模式的区别</li>
</ul>
<h3 id="GPU并行"><a href="#GPU并行" class="headerlink" title="GPU并行"></a>GPU并行</h3><p>那么有以上几种对GPU的使用, 存在多GPU的情况下，又涉及到另一个问题就是: 多GPU之间如何协调数据、模型的关系呢?</p>
<p>显然要么把数据切分，要么把模型切分，业界大体以数据并行、模型并行的方式实现,而又以数据并行使用较多</p>
<h4 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220529112651.png"></p>
<p><strong>数据并行时，每个GPU设备上保持了同样的模型数据</strong>，且一次完整的训练过程包括以下3步：</p>
<ul>
<li>1.CPU负责将不同的训练数据（mini-batch）分别喂给GPU0和GPU1设备；</li>
<li>2.不同的显卡设备上存储了完全一致的模型，通过mini-batch数据进行了前向和反向传播；</li>
<li>3.位于不同GPU设备上的模型进行权重同步和更新</li>
</ul>
<p>其中第一点关心的是训练数据如何平均地分配给GPU,也就是dataloader, 这个大多数的训练框架都会实现</p>
<p>数据并行又分为同步、异步, 这里就不展开了</p>
<h4 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h4><p>相对于数据并行而言，模型并行就是将模型切成若干份放到若干GPU中(<strong>模型就是网络,分为很多层，我们可以将网络层切割分布到不同的GPU上</strong>)</p>
<p>关于模型并行作者所在的业务中几乎没有用到，而且网络上使用的也不是太多，作者了解不多，这里就不误人子弟了，有空的时候再详细研究研究</p>
<p>OK, 回到正题, 那么多卡的情况下，他们的通信如何加速呢？</p>
<h3 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h3><p>还是这张图</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220529112651.png"></p>
<p>从这个图我们可以知道，不管是在单机多卡还是多机多卡训练场景下， 如果有parameter server参与，则ps很容易成为瓶颈, 如果想让速度更快，就成本而言，有那么下面几种方式:</p>
<p>成本小:</p>
<blockquote>
<ul>
<li>把训练时的单卡 batchsize 加大，将计算量增大而传输量不变</li>
<li>让研究员尽量训更大的网络，让他们多放点 Conv 而不是 FC，这让计算量增加的比参数量（即传输的数据量）更多一些</li>
<li>说服研究员不要坚持等价的 SGD，换用 ASGD，计算与通讯同时进行</li>
</ul>
</blockquote>
<p>成本小的方案一般需要通过调参得到最优解</p>
<p>成本大:</p>
<blockquote>
<ul>
<li>把 PS 变成多机版本，每台服务器管一部分 weight，从而带宽压力可以均摊到更多服务器上，降低传输量</li>
<li>依靠更好的硬件<ul>
<li>机器间通讯：万兆网络是底线，Infiniband 和 RoCE 的带宽从 56Gbps 一直上升到 100Gbps 甚至 200Gbps，让 <a href="https://link.zhihu.com/?target=https://www.mellanox.com/">Mellanox</a> 这家公司生意越来越好（现在被 NVIDIA 直接收购了）</li>
<li>机器内通讯：AMD 比牙膏厂更早支持 PCIe 4.0，让带宽翻了个倍；NVIDIA 搞出 NVLink，愣是把通讯带宽搞上了不可思议的 600Gb&#x2F;s……</li>
</ul>
</li>
<li>Mellanox 表示：我们的<strong>智能 Switch</strong> 可以直接当 PS，我没带宽瓶颈！</li>
</ul>
</blockquote>
<p>直接换性能更好的硬件效果是立竿见影，就是太费钱, 所以要千方百计的<strong>降低传输</strong>,又引出了若干niubility的技术</p>
<h4 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h4><p>DMA全称为Direct Memory Access，即直接内存访问。意思是外设对内存的读写过程可以不用CPU参与而直接进行</p>
<p>如果说DMA解决的是本机外设直接操作内存的话，而RDMA则是Remote Direct Memory Access, 解决的对远程节点的内存直接访问</p>
<p>关于DMA，这篇博文写的不错，可以参考: <a href="https://mp.weixin.qq.com/s/UqSydz8hJCFcX5CF30gXSw">海思专家如何看待RDMA技术？</a></p>
<p>RDMA本身指的是一种技术,不是一类特定的产品, 具体协议层面，包含Infiniband（IB），RDMA over Converged Ethernet（RoCE）和internet Wide Area RDMA Protocol（iWARP）。三种协议都符合RDMA标准。</p>
<p>DMA需要硬件的支持，所以成本比较贵</p>
<h4 id="IB"><a href="#IB" class="headerlink" title="IB"></a>IB</h4><p>需要硬件、协议、软件的全套支持，现在IB已经达到了全双工惊人的400Gb&#x2F;s</p>
<p>这玩意就更贵了,除了网关外，交换机也需要配套</p>
<p><a href="https://max.book118.com/html/2019/0609/8101047056002027.shtm">高性能计算知识：高性算InfiniBand网络详解.pdf</a></p>
<h4 id="RoCE"><a href="#RoCE" class="headerlink" title="RoCE"></a>RoCE</h4><p>RoCE从英文全称就可以看出它是基于以太网链路层的协议，v1版本网络层仍然使用了IB规范，而v2使用了UDP+IP作为网络层，使得数据包也可以被路由</p>
<p>某种程度上可以做为IB的低成本替代方案，不过性能肯定是不及IB的</p>
<p><a href="https://cloud.tencent.com/developer/article/1771431">浅析RoCE网络技术</a></p>
<h4 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a>MPI</h4><p>mpi做为老牌的消息通信接口，多用于HPC或者超算的并发编程领域, 作者所在的平台中也使用到了基于mpi实现的mpich2库.</p>
<p><a href="https://izsk.me/2022/04/20/MPI-Structure/">MPI框架学习一架构组件</a></p>
<p>MPI工作大多在CPU端，如果想作用于GPU,则会有些损耗(MPI传递GPU中的数据是有一个步骤是从Device拷贝数据到Host的内存), 所以对GPU来说(切确来说是nvidia gpu),则不得不说nccl</p>
<p>####NCCL</p>
<p>Nvidia于2015年公开发布NCCL,一个开源的、基于自身硬件的开源的集合通信库实现。其算法基本实现原理，和mpi的实现是基本类似的，由于其完全基于自家硬件，可以进行充分的优化，所以基于nvidia-gpu时，使用nccl性能是很强的，目前nccl是v2版本</p>
<p>NCCL 最初只支持单机多 GPU 通信，从 NCCL2 开始支持多机多 GPU 通信</p>
<p>以nvidia v100 gpu为例，来看看v100的Hardware Architecture</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220529172604.png"></p>
<p>图中不同的连线代表不同的通信方式，有3种通信方式: nvlink、PCIE、QPI</p>
<h5 id="nvlink"><a href="#nvlink" class="headerlink" title="nvlink"></a>nvlink</h5><p>nvlink是nvidia出品的高速 GPU 互连技术, 目前已经更新至nvlink v3(A100上就是)，v100是 nvlink v2</p>
<p>由于<strong>nvlink是全双工的通道</strong>，单向通道带宽为25Gb&#x2F;s，双向则为50Gb&#x2F;s，6个的话最大300Gb&#x2F;s</p>
<p>这里要注意, 一块GPU上的nvlink通道是有限的，v100上最大是6通道，所以从图上可以看到，GPU与GPU之间的通道是nvlink， 但是只有6通道，所以没办法实现一块GPU跟其它的GPU都建立nvlink通道，所以没办法建立nvlink的两块gpu只得通过PCIE来进行通信了</p>
<h5 id="PCIE"><a href="#PCIE" class="headerlink" title="PCIE"></a>PCIE</h5><p>PCIE(PCI-Express)，实质上是一种高速串行计算机扩展总线标准, 最新为PCIE v5版本，GPU上用的最多的还是PCIE v4</p>
<p>PCIE v4版本的速度大概在16Gb&#x2F;s，而且还得看PCIE的插槽数量</p>
<p>所以可看出，如果GPU与GPU之间通过PCIE通道，则会比nvlink v2差很多.</p>
<p>同样，要注意<strong>PCIE v4直连CPU最大只能有16条</strong></p>
<h5 id="QPI"><a href="#QPI" class="headerlink" title="QPI"></a>QPI</h5><p>Intel的QuickPath Interconnect技术缩写为QPI，译为快速通道互联,也是全双工的</p>
<p>每个<a href="https://product.pconline.com.cn/itbk/diy/cpu/1111/2572054.html">QPI总线</a>总带宽&#x3D;每秒传输次数(即QPI频率)×每次传输的有效数据(即16bit&#x2F;8&#x3D;2Byte)×双向</p>
<p>即QPI频率为4.8GT&#x2F;s的总带宽&#x3D;4.8GT&#x2F;s×2Byte×2&#x3D;19.2GB&#x2F;s，QPI频率为6.4GT&#x2F;s的总带宽&#x3D;6.4GT&#x2F;s×2Byte×2&#x3D;25.6GB&#x2F;s</p>
<p>另外说一下，nvidia v100的GPU与cpu也可以通过nvlink实现通信</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220529180635.png"></p>
<p>所以，这就是为什么很多大厂在GPU调度时要做GPU的拓扑感知的原因, 也是为了让<strong>GPU之间的路径更短, 不同的通道不同速度</strong></p>
<p>最后，来说说在nvidia gpu的场景下，怎么知道走的是哪个方案呢, 使用以下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nvidia-smi topo -m</span><br><span class="line">        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  CPU Affinity</span><br><span class="line">GPU0     X      PIX     SYS     SYS     SYS     SYS     0-11,24-35</span><br><span class="line">GPU1    PIX      X      SYS     SYS     SYS     SYS     0-11,24-35</span><br><span class="line">GPU2    SYS     SYS      X      PIX     PHB     PHB     12-23,36-47</span><br><span class="line">GPU3    SYS     SYS     PIX      X      PHB     PHB     12-23,36-47</span><br><span class="line">mlx5_0  SYS     SYS     PHB     PHB      X      PIX</span><br><span class="line">mlx5_1  SYS     SYS     PHB     PHB     PIX      X </span><br><span class="line"></span><br><span class="line">Legend:</span><br><span class="line"></span><br><span class="line">  X    = Self</span><br><span class="line">  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)</span><br><span class="line">  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node</span><br><span class="line">  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)</span><br><span class="line">  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)</span><br><span class="line">  PIX  = Connection traversing a single PCIe switch</span><br><span class="line">  NV<span class="comment">#  = Connection traversing a bonded set of # NVLinks</span></span><br></pre></td></tr></table></figure>

<p>可以看到，上述服务器上有4块GPU卡，48个CPU核。</p>
<p>GPU0和GPU1之间是通过PCIe switch通信的（不经过cpu）；</p>
<p>GPU2和GPU3之间也是通过PCIe switch通信的；</p>
<p>其它的GPU卡两两之间是通过QPI通信的（PCIe + QPI总线）；</p>
<p>而mlx5_0 和mlx5_1是Mellanox ConnectX-4 PCIe网卡设备(10&#x2F;25&#x2F;40&#x2F;50千兆以太网适配器，另外该公司是IBA芯片的主要厂商)。</p>
<p>当然，就算只说通信方向也还有很多的方法可以加速训练过程. 就CPU来说，外设也有远近之分，NUMA技术就是解决这个问题之一</p>
<p>百度飞桨的深度学习分布式训练专题非常不错，值得看看.</p>
<p><a href="https://zhuanlan.zhihu.com/p/453295832">数据并行：提升训练吞吐的高效方法 |深度学习分布式训练专题</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2022/04/20/MPI-Structure/">https://izsk.me/2022/04/20/MPI-Structure/</a></li>
<li><a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/">https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/</a></li>
<li><a href="https://developer.aliyun.com/article/11181">https://developer.aliyun.com/article/11181</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33714726">https://zhuanlan.zhihu.com/p/33714726</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/453295832">https://zhuanlan.zhihu.com/p/453295832</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/367327698">https://zhuanlan.zhihu.com/p/367327698</a></li>
<li><a href="https://blog.51cto.com/u_15642578/5316839">https://blog.51cto.com/u_15642578/5316839</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/367327698">https://zhuanlan.zhihu.com/p/367327698</a></li>
<li><a href="https://link.zhihu.com/?target=https://www.mellanox.com/">https://link.zhihu.com/?target=https%3A//www.mellanox.com/</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1771431">https://cloud.tencent.com/developer/article/1771431</a></li>
<li><a href="https://mp.weixin.qq.com/s/UqSydz8hJCFcX5CF30gXSw">https://mp.weixin.qq.com/s/UqSydz8hJCFcX5CF30gXSw</a></li>
<li><a href="https://product.pconline.com.cn/itbk/diy/cpu/1111/2572054.html">https://product.pconline.com.cn/itbk/diy/cpu/1111/2572054.html</a></li>
<li><a href="https://max.book118.com/html/2019/0609/8101047056002027.shtm">https://max.book118.com/html/2019/0609/8101047056002027.shtm</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(loki日志存在错误的label)</title>
    <url>/2022/05/15/Loki-log-with-wrong-labels/</url>
    <content><![CDATA[<p>业务中使用的日志架构是基于Loki来搭建的，最近发现了一个很诡异的问题，有些log的label与label之间对应不上，经过一番尝试后虽然解决了问题，但不知道原由。</p>
<span id="more"></span>

<h3 id="集群信息"><a href="#集群信息" class="headerlink" title="集群信息"></a>集群信息</h3><table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes stack</td>
<td>v1.15.9</td>
</tr>
<tr>
<td>Loki</td>
<td>v2.3.0</td>
</tr>
<tr>
<td>Promtail</td>
<td>v2.3.0</td>
</tr>
<tr>
<td>Grafana</td>
<td>V8.1.6</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>日志路径: log location –&gt; promtail –&gt; loki –&gt; grafana</p>
<h3 id="k8s日志结构"><a href="#k8s日志结构" class="headerlink" title="k8s日志结构"></a>k8s日志结构</h3><p>先简单了解下k8s集群的日志结构，这样以便引出发现的问题</p>
<p>在v1.14之前及之后会有所区别，表现如下: </p>
<blockquote>
<ul>
<li><strong>before 1.14</strong>: <code>/var/log/pods/&lt;pod_uid&gt;/&lt;container_name&gt;/&lt;num&gt;.log</code></li>
<li><strong>1.14 or later</strong>: <code>/var/log/pods/&lt;namespace&gt;_&lt;pod_name&gt;_&lt;pod_uid&gt;/&lt;container_name&gt;/&lt;num&gt;.log</code></li>
</ul>
</blockquote>
<p>详细的解释参考这个<a href="https://github.com/kubernetes/kubernetes/pull/74441">PR</a></p>
<p>另外，<code>&lt;num&gt;.log</code>中的num代表pod重启的次数,参考这个[pull][<a href="https://github.com/kubernetes/kubernetes/pull/74441/files]%EF%BC%8C">https://github.com/kubernetes/kubernetes/pull/74441/files]，</a> 不过对于本文要解决的问题，num不重要.</p>
<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>通过loki收集到的日志，通过grafana UI查询之后，出现如下问题:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220513132435.png"></p>
<p>从这个图可以看到, <strong>pod跟filename是对应不上的</strong>, 但从<a href="#k8s%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84">k8s日志结构</a>来看, pod名跟filename中的部分是需要对应上的.</p>
<p>所以问题是: <strong>从这个filename采集到的日志是属于这个pod的吗?</strong></p>
<p>显然, log的label没有对应上最开始需要check一下是否是配置的问题，由于log的label的都是由promtail进行处理的，因此如果有问题，也只能是promtail配置文件的问题</p>
<h3 id="Promtail配置"><a href="#Promtail配置" class="headerlink" title="Promtail配置"></a>Promtail配置</h3><p>由于篇幅有限，这里只贴与pod相关的配置进行说明.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># Pods with a label &#x27;app&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-pods-app</span></span><br><span class="line">    <span class="attr">pipeline_stages:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">docker:</span> &#123;&#125;</span><br><span class="line">    <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br><span class="line">    <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="comment"># 将pod中带有label为app的label重写为app</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_label_app</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">app</span></span><br><span class="line">      <span class="comment"># 将app为空的值丢弃</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">drop</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">app</span></span><br><span class="line">      <span class="comment"># 将pod中带有label为component的label重写为component</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_label_component</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">component</span></span><br><span class="line">      <span class="comment"># prometheus中预定义的label</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_node_name</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">node_name</span></span><br><span class="line">      <span class="comment"># prometheus中预定义的label</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_namespace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">namespace</span></span><br><span class="line">      <span class="comment"># prometheus中预定义的label</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_name</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">pod</span></span><br><span class="line">      <span class="comment"># prometheus中预定义的label</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_container_name</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">container</span></span><br><span class="line">      <span class="comment"># 重点是以下部分</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">/var/log/pods/*$1/*.log</span></span><br><span class="line">        <span class="attr">separator:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">source_labels:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_uid</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_container_name</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__path__</span></span><br></pre></td></tr></table></figure>

<p>promtail的配置是使用了kubernetes的serviceDiscovery及prometheus的relabel机制, 这两部分不是本文的重点，感兴趣的可以参考官网说明,这里只说明两点:</p>
<ol>
<li>以__开头的变量会在relabel后从label set中删除</li>
<li><code>__path__</code> 是一个特殊标签,最终<code>__path__</code>会变成filename,参考<a href="https://grafana.com/docs/loki/latest/clients/promtail/scraping">loki File Target Discovery</a></li>
</ol>
<p>由于是pod跟filename对应不上,那就只能是这两部分的relabel有问题，由于<code>__meta_kubernetes_pod_container_name</code>为prometheus中预定义的值，出错的可能不大，因此问题概率出在filename, 也就是配置中最下的部分</p>
<p>诈一看也没发现在任何问题，loki stack的helm charts也是这么使用的, 翻阅<a href="https://github.com/grafana/loki/issues/3443">loki github issue</a>也只发现一个相关问题, 但现象不太一样, 被<a href="https://github.com/grafana/helm-charts/pull/279">merge 279</a> fix了，但是个人对这个issue 的close表示怀疑.</p>
<p>不过作者参照这个mege对应的配置也做过尝试，发现确实无法解决作者的问题.</p>
<p>经过作者的不斷调整promtail的配置，发现有一种情况居然给解决了</p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">  <span class="attr">replacement:</span> <span class="string">/var/log/pods/$1/**/*.log</span></span><br><span class="line">  <span class="attr">separator:</span> <span class="string">_</span></span><br><span class="line">  <span class="attr">source_labels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pod</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">__meta_kubernetes_pod_uid</span></span><br><span class="line">  <span class="attr">target_label:</span> <span class="string">__path__</span></span><br></pre></td></tr></table></figure>

<p>由于promtail中使用的对文件的查询是基于一个开源库<a href="https://github.com/bmatcuk/doublestar">dobulestar</a></p>
<p>pod跟filename能正常work后， 作者还是没想明白为何这种配置是work的(作者认为【Promtail配置】中的配置也是没问题)</p>
<p>知其然不知其所以然，是真的很痛苦，作者写记录这篇软文也是想给可能有同样困惑的同学提供一种解决方法.</p>
<p>作者也在loki github中提了 issue，目前未有反馈，如果有知道原因的同学可以联系作者告知，万分感谢</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/grafana/loki/issues/3402">https://github.com/grafana/loki/issues/3402</a></li>
<li><a href="https://github.com/grafana/helm-charts/commit/4c20b59b79d6599ecf7c7caaff169d2901bab80f">https://github.com/grafana/helm-charts/commit/4c20b59b79d6599ecf7c7caaff169d2901bab80f</a></li>
<li><a href="https://github.com/grafana/loki/issues/3443">https://github.com/grafana/loki/issues/3443</a></li>
<li><a href="https://github.com/grafana/helm-charts/pull/202">https://github.com/grafana/helm-charts/pull/202</a></li>
<li><a href="https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#regex">https://grafana.com/blog/2022/03/21/how-relabeling-in-prometheus-works/#regex</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/pull/74441/files">https://github.com/kubernetes/kubernetes/pull/74441/files</a></li>
<li><a href="https://github.com/bmatcuk/doublestar">https://github.com/bmatcuk/doublestar</a></li>
<li><a href="https://github.com/grafana/loki/issues/6140">https://github.com/grafana/loki/issues/6140</a></li>
<li><a href="https://grafana.com/docs/loki/latest/clients/promtail/scraping">https://grafana.com/docs/loki/latest/clients/promtail/scraping</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>从工程角度说AI平台建设</title>
    <url>/2023/05/27/Necessary-for-distibuted-training-platform/</url>
    <content><![CDATA[<p>作者参与AI模型生产平台的建设已经有很多一段时间了,对于这类平台来说，因为牵扯到算法，会比一般的业务相对复杂，同时业界也是缺少AI Platform事实标准，要想做到如同使用SASS般丝滑，这其中还是有很多的坑要趟。</p>
<p>这次作者站在<strong>工程的角度</strong>同时结合作者亲身的经验来聊一聊<code>一个好的AI训练平台需要解决哪些问题</code>，算是做为阶段性的回顾及思考.</p>
<span id="more"></span>

<h3 id="什么是AI平台"><a href="#什么是AI平台" class="headerlink" title="什么是AI平台"></a>什么是AI平台</h3><p>AI平台是个比较笼统的说法，可以理解为跟AI相关的一类平台，但是AI又可以分为好些类, 这个要看具体的业务了,作者所在的AI平台叫做模型生产平台, 其它的还有AI数据平台、AI推理平台等等，这里就不具体区分了，但大多数的AI平台都面临相同的几类问题, 所以下文就用AI平台表达吧.</p>
<p>首先，AI要阐述下什么算是AI平台?</p>
<p>这个问题对于不了解AI业务的同学可能不太好回答，因为会让人感觉知道答案，但像是又少了些什么，作者也是如此，没接触AI之前，只觉得不就是倒腾一些算法，接触之后才发现，<strong>水太深</strong>, 这里放上在网上收藏的一篇博文，作者觉得写的非常好，很适合回答<code>什么是AI平台</code>这个问题，这是地址<a href="https://zhuanlan.zhihu.com/p/102581335">一文介绍AI商品模型训练平台（深度学习平台）</a></p>
<p><strong>划重点</strong></p>
<p>AI平台提供业务到产品、数据到模型、端到端，线上化的人工智能应用解决方案。用户能在AI平台能够使用不同的深度学习框架进行大规模的训练，对数据集和模型进行管理和迭代，同时通过API和本地部署等方式接入到具体业务场景中使用</p>
<p><strong>哦，原来，算法解决的是得到业务结果，而工程解决的是如何便捷地使用这些算法能力</strong></p>
<h3 id="为什么需要它"><a href="#为什么需要它" class="headerlink" title="为什么需要它?"></a>为什么需要它?</h3><p>所以，为什么需要平台呢？我直接在机器上不也能跑起来么？</p>
<p>当然,站在个人的角度来讲没有问题，但如果是多个人共用这台机器呢? 进一步多人用的还不是同一个框架？如果要很方便地把整个环境迁移到另外的机器上? 如果每个人用的版本还有差异?</p>
<p>是不是就有点不好处理</p>
<p><strong>所谓的平台，其它提供的是一种能力</strong>,像Saas一样，给你这样的一个平台，可以屏蔽掉很多用户不需要关注的方面，用户只需要使用就好，从而让用户更专注业务层面.</p>
<p>同样，放上在网上收藏的一篇博文，作者觉得写的非常好，很适合回答<code>为什么需要它</code>这个问题，感兴趣的可以稳步<a href="https://insights.thoughtworks.cn/why-machine-learning-platform/?hmsr=toutiao.io&utm_campaign=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io">为什么需要机器学习平台</a></p>
<p>从上面的链接中比较全面的提到了要做好一个AI平台会面临的痛点,下面是作者之前画的现在负责的AI模型生成平台训练流程</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210819224928.png"></p>
<p>显然，上面所述的痛点作者也都面临了，同时，由于所有的模块都是基于kubernetes之上建设, 需要考虑的问题会更多.</p>
<p>一个系统怎么也绕不开从计算、网络、存储三板斧说起.</p>
<h3 id="计算资源"><a href="#计算资源" class="headerlink" title="计算资源"></a>计算资源</h3><p>硬件好是硬道理，再怎么优化1C也没2C香，计算除了硬件本身的优劣之外，一个好的调度系统也能起到事半功倍的效果.</p>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>AI跟其它业务不一样的是，AI需要大量消耗GPU来进行图形加速, 这是GPU擅长的领域，同时，也需要CPU进行大量的计算,更特别的是，<strong>一个AI任务一般都是由多个job共同完成的</strong>，多个job中的任一个job资源得不到满足都不应该发起这个任务，因为其它的job就算发起了，也没办法完成这个任务(有些大厂云厂商已经实现了从失败的checkpoint点再次执行任务)，整个任务对于资源分配来讲，要么都分配，要么都不分配，这就是组调度的特性: <strong>All or Nothing</strong>，这样可以将资源分配给其它满足条件的任务,提高资源使用率.</p>
<p>面对更复杂的资源请求，如果还存在异构资源的情况下，就会对平台的调度能力提出更高、更智能的要求</p>
<p>云原生的解决方案比较常见的有kube-batch、volcano等开源调度器,kube-batch是不建议使用了，volcano加入了CNCF，内置了多种调度算法，非常使用用在AI任务中</p>
<h4 id="vGPU"><a href="#vGPU" class="headerlink" title="vGPU"></a>vGPU</h4><p>vGPU是一种对GPU进行虚拟化的技术，出发点在于AI中也存在很多的任务无法<strong>用满</strong>一块GPU，通过虚拟化的方式可以让GPU得到更加充分的利用，另一方面是云厂商的<strong>超卖</strong>,作者所在的AI平台上，未使用vGPU，因为整卡的资源都不够用，就谈不上再切分了.</p>
<p>那vGPU需不需要呢?当然要看具体业务而定，作者也收藏了一篇好文比较详细地阐述了这个问题，值得一看<a href="https://blog.csdn.net/oneflow_official/article/details/118866237">浅谈GPU虚拟化和分布式深度学习框架的异同</a></p>
<p>在kuberntes中，各大GPU厂商也出了自己的vGPU plugin，如果有需要再次开发的地方，kubernetes本身的插件化也完全可以支持.</p>
<h4 id="关注CPU"><a href="#关注CPU" class="headerlink" title="关注CPU"></a>关注CPU</h4><p>大多数的AI项目都是使用Python写的，python社区存在着大量很成熟的AI框架，但不得不考虑python语言的性能问题，当然作者不是算法侧的，在算法方面的研究也不是很深，但是不可否认的是，python确实<strong>不够快</strong>,在海量计算的情况下，如果内存、CPU资源设置不合理的话，就会得到截然不同的结果</p>
<p>作者就在某次的性能验证中发现如果CPU设置的不合理，会直接让性能降低N倍</p>
<p>内存还好观察，如果是超过了给定的limit，会有OOM的现象，比较容易发现，而CPU则不存在OOM,由于<strong>CPU是切片运行的</strong>如果CPU一直处于100%的状态运行，会频繁的进行CPU的切换导致性能下降，而AI任务中动不动就可能成千上万次的迭代，整体影响就非常大了，这个从总体时间上就有很非常直观的体现。</p>
<h4 id="GPU拓扑"><a href="#GPU拓扑" class="headerlink" title="GPU拓扑"></a>GPU拓扑</h4><p>GPU的拓扑结构也是非常关键的，比如在一台机器上，有8张GPU，同一个AI任务需要分配2张GPU,则分配在GPU1与GPU2上会比分配在GPU1与GPU7上效率更高，原理很简单, <strong>地理位置更接近，信息交换更快</strong></p>
<p>关于GPU拓扑结构的感知，现在也有一些方案支持，kuberentes中要想做到GPU的拓扑感知，得在调度器上实现，不过遗憾的是， 现在还是不特别成熟，用在生产上的好像也比较少</p>
<h4 id="Node拓扑"><a href="#Node拓扑" class="headerlink" title="Node拓扑"></a>Node拓扑</h4><p>GPU拓扑相对于同一台机器上GPU与GPU之间来说影响相对比较小，但跨Node的GPU之间的通信，影响就比较大了，因此对于同一个AI任务上，如果一台GPU能够满足要求，要<strong>尽可能</strong>地都调度在同一台机器上，原则上就是<strong>减少跨主机通信,这个成本对性能非常重要</strong></p>
<h4 id="Multi-Tenant"><a href="#Multi-Tenant" class="headerlink" title="Multi-Tenant"></a>Multi-Tenant</h4><p>计算资源很重要, 如何有效地提高资源使用率，<strong>多租户的资源隔离的软硬如何权衡，队列如何设计？</strong></p>
<p>业务需求千奇百怪，多租户模型会是个取舍问题</p>
<p>kubernetes中如何实现多租户模型, 本人也有一些实践，感兴趣的可以移步<a href="https://izsk.me/2023/06/26/Kubernetes-multi-tenant/">kubernetes中如何实现多租户模型</a></p>
<h3 id="高性能网络"><a href="#高性能网络" class="headerlink" title="高性能网络"></a>高性能网络</h3><p>同理，原则上做到<strong>减少跨主机通信</strong>只能说是尽量，但一定存在跨主机通信的情况， 这个情景，网络上的差别又有天壤之别，Infinitband network就会比Ethernet快很多，高性能网络这个对AI过程也是非常重要.</p>
<p>Infinitband network这个是硬件网络,本身协议就支持RDMA, 但付出的代价就是成本会随之直线升高，土豪专用.</p>
<p>在Ethernet上， 也有一些成熟的方案可以很提升网络侧的性能，比如RDMA, RoCE等，在跨主机通信时可以直接绕过操作系统直接进行内存数据交换，不用进行用户、内核态copy数据，相当高效.</p>
<p>可以通过sriov技术实现在Infinitband网卡上虚拟出多块网卡，由于Infinitband网卡本身的性能就很强劲，加上sriov，虚拟出来的网卡跟Infinitband物理网卡的性能差别不大, 作者所负责的平台就是采用的sriov+infinitband进行的组网,感兴趣的可以移步<a href="https://izsk.me/2021/07/02/Kubernetes-Infinitband-SRIOV-network-1-backgroud/">Kubernetes学习(k8s基于InfiniBand实现HPC高性能容器网络组网方案实践一)</a>.</p>
<h4 id="CNI"><a href="#CNI" class="headerlink" title="CNI"></a>CNI</h4><p>当然上面说的是硬件层面的网络，在基于kubernetes架构上的系统，集群间网络的性能也是个需要认真选型的点, CNI 使用cilium(基于ebpf)就会比kube-proxy快很多</p>
<p>cilium的生产实践，感兴趣的可以移步<a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">cilium在kubernetes中的生产实践一(cilium介绍)</a>, 还在更新</p>
<h3 id="高性能存储"><a href="#高性能存储" class="headerlink" title="高性能存储"></a>高性能存储</h3><p>AI任务中会涉及到大量的数据处理，因此计算与存储之间的数据交换性能也显得格外重要,除了存储本身需要高性能IO之外，网络的吞吐量也很关键, 特别是在使用分布式存储系统时.</p>
<p>业界常用的如Gluster, Ceph-hook等提供了较高的性能，Ceph-hook对可运维能力要求比较高.</p>
<h4 id="缓存加速"><a href="#缓存加速" class="headerlink" title="缓存加速"></a>缓存加速</h4><p>在AI任务中，也是存在<strong>热点数据</strong>的，比如一些常用的数据集，可能在多个AI任务中都会使用到，那提前将这些热点数据先缓存在内存中，这样可以避免每次都去存在系统中获取，也将降低时间.</p>
<p>Alluiox就是一款不错的开源组件，更重要的是它统一了多种常用对象存储的接口，对上层应用屏蔽了底层的存储介质.</p>
<h3 id="弹性架构"><a href="#弹性架构" class="headerlink" title="弹性架构"></a>弹性架构</h3><p>好的业务架构需要好的基础架构，而丝滑般的可伸缩性必将为好的架构添上一双翅膀</p>
<p>首先是infra层，<strong>弹性资源池</strong>就是一个很好的解决方案, 将算力、存储、网络进行打包</p>
<p>这需要可观测性做到极致，系统能够清楚地知道什么时机需要扩缩容，体量需要多大</p>
<p>但如果要充分考虑多租户、硬隔离，调度，而且还要替人做决定等因素，要想实现一个功能完善的弹性架构，还是需要多部门通力合作。</p>
<h3 id="可观测性"><a href="#可观测性" class="headerlink" title="可观测性"></a>可观测性</h3><p>可观测性是作者面临的最大难题，不仅要解决的是业务层的可观测性，更难的在于AI任务的可观测性:</p>
<blockquote>
<ul>
<li>引入的AI框架如何监控</li>
<li>AI任务测试覆盖率如何保障</li>
<li>整体链路如何打通</li>
<li>AI可视化如何实现</li>
<li>…</li>
</ul>
</blockquote>
<p>等等，一系列问题中的每个问题都值得深思，可观测性对于平台稳定性来说不言而喻，每一个环节都需要联合各方诸侯推进</p>
<p>任重道远…</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/</a></li>
<li><a href="https://izsk.me/2023/06/26/Kubernetes-multi-tenant/">https://izsk.me/2023/06/26/Kubernetes-multi-tenant/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/102581335">https://zhuanlan.zhihu.com/p/102581335</a></li>
<li><a href="https://izsk.me/2021/07/02/Kubernetes-Infinitband-SRIOV-network-1-backgroud/">https://izsk.me/2021/07/02/Kubernetes-Infinitband-SRIOV-network-1-backgroud/</a></li>
<li><a href="https://insights.thoughtworks.cn/why-machine-learning-platform/?hmsr=toutiao.io&amp;utm_campaign=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io">https://insights.thoughtworks.cn/why-machine-learning-platform/?hmsr=toutiao.io&amp;utm_campaign=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx-LVS-HAProxy常见负载均衡软件的优缺点总结</title>
    <url>/2016/02/13/Nginx-LVS-HAProxy%E5%B8%B8%E8%A7%81%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%BD%AF%E4%BB%B6%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>PS:Nginx&#x2F;LVS&#x2F;HAProxy是目前使用最广泛的三种负载均衡软件，本人都在多个项目中实施过，参考了一些资料，结合自己的一些使用经验，总结一下:</p>
<span id="more"></span>

<ol>
<li>一般对负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术。具体的应用需求还得具体分析，如果是中小型的Web应用，比如日PV小于1000万，用Nginx就完全可以了；如果机器不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或重要的服务，且服务器比较多时，可以考虑用LVS。</li>
<li>一种是通过硬件来进行进行，常见的硬件有比较昂贵的F5和Array等商用的负载均衡器，它的优点就是有专业的维护团队来对这些服务进行维护、缺点就是花销太大，所以0对于规模较小的网络服务来说暂时还没有需要使用；另外一种就是类似于Nginx&#x2F;LVS&#x2F;HAProxy的基于Linux的开源免费的负载均衡软件，这些都是通过软件级别来实现，所以费用非常低廉。</li>
<li>目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx&#x2F;HAProxy+Keepalived作负载均衡器；后端采用MySQL数据库一主多从和读写分离，采用LVS+Keepalived的架构。当然要根据项目具体需求制定方案。</li>
</ol>
<p>下面说说各自的特点和适用场合:</p>
<h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx:"></a><strong>Nginx:</strong></h3><p>Nginx优点是：</p>
<blockquote>
<ul>
<li>工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。</li>
<li>Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；相反LVS对网络稳定性依赖比较大，这点本人深有体会；</li>
<li>Nginx安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来。LVS的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。</li>
<li>可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。</li>
<li>Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。</li>
<li>Nginx不仅仅是一款优秀的负载均衡器&#x2F;反向代理软件，它同时也是功能强大的Web应用服务器。LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。</li>
<li>Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可以考虑用其作为反向代理加速器。</li>
<li>Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有lighttpd了，不过lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃。</li>
<li>Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多。</li>
</ul>
</blockquote>
<p>Nginx的缺点是:</p>
<blockquote>
<ul>
<li>Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点。</li>
<li>对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测。不支持Session的直接保持，但能通过ip_hash来解决。</li>
</ul>
</blockquote>
<h3 id="LVS"><a href="#LVS" class="headerlink" title="LVS:"></a><strong>LVS:</strong></h3><p>使用Linux内核集群实现一个高性能、高可用的负载均衡服务器，它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)。</p>
<p>LVS的优点是：</p>
<blockquote>
<ul>
<li>抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。</li>
<li>配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。</li>
<li>工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived，不过我们在项目实施中用得最多的还是LVS&#x2F;DR+Keepalived。</li>
<li>无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会收到大流量的影响。</li>
<li>应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等。</li>
</ul>
</blockquote>
<p>LVS的缺点是：</p>
<blockquote>
<ul>
<li>软件本身不支持正则表达式处理，不能做动静分离；而现在许多网站在这方面都有较强的需求，这个是Nginx&#x2F;HAProxy+Keepalived的优势所在。</li>
<li>如果是网站应用比较庞大的话，LVS&#x2F;DR+Keepalived实施起来就比较复杂了，特别后面有Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了，相对而言，Nginx&#x2F;HAProxy+Keepalived就简单多了。</li>
</ul>
</blockquote>
<h3 id="HAProxy"><a href="#HAProxy" class="headerlink" title="HAProxy:"></a><strong>HAProxy:</strong></h3><p>HAProxy的特点是：</p>
<blockquote>
<ul>
<li>HAProxy也是支持虚拟主机的。</li>
<li>HAProxy的优点能够补充Nginx的一些缺点，比如支持Session的保持，Cookie的引导；同时支持通过获取指定的url来检测后端服务器的状态。</li>
<li>HAProxy跟LVS类似，本身就只是一款负载均衡软件；单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的。</li>
<li>HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，大家可以用LVS+Keepalived对MySQL主从做负载均衡。</li>
<li>HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：<blockquote>
<ul>
<li>① roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；</li>
<li>② static-rr，表示根据权重，建议关注；</li>
<li>③ leastconn，表示最少连接者先处理，建议关注；</li>
<li>④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；</li>
<li>⑤ ri，表示根据请求的URI；</li>
<li>⑥ rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；</li>
<li>⑦ hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；</li>
<li>⑧ rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="Nginx和LVS对比的总结"><a href="#Nginx和LVS对比的总结" class="headerlink" title="Nginx和LVS对比的总结:"></a><strong>Nginx和LVS对比的总结:</strong></h3><blockquote>
<ul>
<li>Nginx工作在网络的7层，所以它可以针对http应用本身来做分流策略，比如针对域名、目录结构等，相比之下LVS并不具备这样的功能，所以Nginx单凭这点可利用的场合就远多于LVS了；但Nginx有用的这些功能使其可调整度要高于LVS，所以经常要去触碰触碰，触碰多了，人为出问题的几率也就会大。</li>
<li>Nginx对网络稳定性的依赖较小，理论上只要ping得通，网页访问正常，Nginx就能连得通，这是Nginx的一大优势！Nginx同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；LVS就比较依赖于网络环境，目前来看服务器在同一网段内并且LVS使用direct方式分流，效果较能得到保证。另外注意，LVS需要向托管商至少申请多一个ip来做Visual IP，貌似是不能用本身的IP来做VIP的。要做好LVS管理员，确实得跟进学习很多有关网络通信方面的知识，就不再是一个HTTP那么简单了。</li>
<li>Nginx安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。LVS的安装和配置、测试就要花比较长的时间了；LVS对网络依赖比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦得多。</li>
<li>Nginx也同样能承受很高负载且稳定，但负载度和稳定度差LVS还有几个等级：Nginx处理所有流量所以受限于机器IO和配置；本身的bug也还是难以避免的。</li>
<li>Nginx可以检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前LVS中 ldirectd也能支持针对服务器内部的情况来监控，但LVS的原理使其不能重发请求。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而恼火。</li>
<li>Nginx对请求的异步处理可以帮助节点服务器减轻负载，假如使用apache直接对外服务，那么出现很多的窄带链接时apache服务器将会占用大 量内存而不能释放，使用多一个Nginx做apache代理的话，这些窄带链接会被Nginx挡住，apache上就不会堆积过多的请求，这样就减少了相当多的资源占用。这点使用squid也有相同的作用，即使squid本身配置为不缓存，对apache还是有很大帮助的。</li>
<li>Nginx能支持http、https和email（email的功能比较少用），LVS所支持的应用在这点上会比Nginx更多。在使用上，一般最前端所采取的策略应是LVS，也就是DNS的指向应为LVS均衡器，LVS的优点令它非常适合做这个任务。重要的ip地址，最好交由LVS托管，比如数据库的 ip、webservice服务器的ip等等，这些ip地址随着时间推移，使用面会越来越大，如果更换ip则故障会接踵而至。所以将这些重要ip交给 LVS托管是最为稳妥的，这样做的唯一缺点是需要的VIP数量会比较多。Nginx可作为LVS节点机器使用，一是可以利用Nginx的功能，二是可以利用Nginx的性能。当然这一层面也可以直接使用squid，squid的功能方面就比Nginx弱不少了，性能上也有所逊色于Nginx。Nginx也可作为中层代理使用，这一层面Nginx基本上无对手，唯一可以撼动Nginx的就只有lighttpd了，不过lighttpd目前还没有能做到 Nginx完全的功能，配置也不那么清晰易读。另外，中层代理的IP也是重要的，所以中层代理也拥有一个VIP和LVS是最完美的方案了。具体的应用还得具体分析，如果是比较小的网站（日PV小于1000万），用Nginx就完全可以了，如果机器也不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或者重要的服务，机器不发愁的时候，要多多考虑利用LVS。</li>
</ul>
</blockquote>
<p>现在对网络负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术：</p>
<blockquote>
<ul>
<li>第一阶段：利用Nginx或HAProxy进行单点的负载均衡，这一阶段服务器规模刚脱离开单服务器、单数据库的模式，需要一定的负载均衡，但是仍然规模较小没有专业的维护团队来进行维护，也没有需要进行大规模的网站部署。这样利用Nginx或HAproxy就是第一选择，此时这些东西上手快， 配置容易，在七层之上利用HTTP协议就可以。这时是第一选择。</li>
<li>第二阶段：随着网络服务进一步扩大，这时单点的Nginx已经不能满足，这时使用LVS或者商用Array就是首要选择，Nginx此时就作为LVS或者Array的节点来使用，具体LVS或Array的是选择是根据公司规模和预算来选择，Array的应用交付功能非常强大，本人在某项目中使用过，性价比也远高于F5，商用首选！但是一般来说这阶段相关人才跟不上业务的提升，所以购买商业负载均衡已经成为了必经之路。</li>
<li>第三阶段：这时网络服务已经成为主流产品，此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升，这时无论从开发适合自身产品的定制，以及降低成本来讲开源的LVS，已经成为首选，这时LVS会成为主流。<br>最终形成比较理想的基本架构为：Array&#x2F;LVS —- Nginx&#x2F;Haproxy —- Squid&#x2F;Varnish —- AppServer。</li>
</ul>
</blockquote>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.ha97.com/5646.html">http://www.ha97.com/5646.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx Ingress Authentication</title>
    <url>/2020/01/10/Nginx-Ingress-Authentication/</url>
    <content><![CDATA[<p>Kubernetes中经常使用ingress做为从外界访问kubernetes集群内服务的入口, 而nginx ingress则做为最常用的方式, 有时候会遇到暴露出来的页面是需要受限访问的, 在nginx ingress中同样提供了nginx原生的认证方式，项目中用到了<code>白名单</code>跟<code>用户密码认证</code>,一定程度上加强了安全性.</p>
<span id="more"></span>

<h3 id="Nginx-Ingress"><a href="#Nginx-Ingress" class="headerlink" title="Nginx Ingress"></a><strong>Nginx Ingress</strong></h3><p>定义一个最常用的ingress</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ingress-withno-auth</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">foo.bar.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">http-svc</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>上述是个最简单的ingress, 当访问foo.bar.com时就会转到http-svc所指向的服务</p>
<p><strong>Nginx ingress中支持使用annotations来使用各种功能</strong>,可参考<a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md">这里</a></p>
<p>如果我需要限定IP段才能访问这个网页, 那就需要使用到<code>whitelist-source-range</code></p>
<h3 id="whitelist-source-range"><a href="#whitelist-source-range" class="headerlink" title="whitelist-source-range"></a><strong>whitelist-source-range</strong></h3><p>在metadata.annotations中使用whitelist-source-range来对访问的ip做限定，只有指定的ip才能够访问，多个ip之间用逗号分隔</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/whitelist-source-range:</span> <span class="number">125.200</span><span class="number">.2</span><span class="number">.2</span><span class="string">,183.60.236.90</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ingress-with-whitelist</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">foo.bar.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">http-svc</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>这样，<strong>不在指定ip列表中的访问会提示403, access forbiden</strong></p>
<p>如果需要用户名密码访问呢</p>
<h3 id="Basic-Authentication"><a href="#Basic-Authentication" class="headerlink" title="Basic Authentication"></a><strong>Basic Authentication</strong></h3><p>可使用baisc authentication来进行难，这也是nginx本身提供的一种难方式</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="comment"># type of authentication</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/auth-type:</span> <span class="string">basic</span></span><br><span class="line">    <span class="comment"># name of the secret that contains the user/password definitions</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/auth-secret:</span> <span class="string">basic-auth</span></span><br><span class="line">    <span class="comment"># message to display with an appropriate context why the authentication is required</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/auth-realm:</span> <span class="string">&#x27;Authentication Required - foo&#x27;</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ingress-with-whitelist</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">foo.bar.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">http-svc</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>其中<code>basic-auth</code>是以<code>secret</code>的形式存在</p>
<p>先安装httpd工具</p>
<p><code>yum install -y httpd</code></p>
<p>使用htpasswd生成密码</p>
<p><code>htpasswd -c pass.wd foo</code></p>
<p>根据提示输入用户foo的密码</p>
<p>生成的密码文件会存在于pass.wd文件中，使用该文件生成secret</p>
<p><code>kubectl create secret generic basic-auth --from-file=pass.wd</code></p>
<p>或者, 也可以直接使用openssl来生成密码文件:<br><code>printf &quot;admin:$(openssl passwd -crypt 12345678)\n&quot; &gt;/tmp/auth</code><br><strong>注意: 使用openssl, 密码只能最长8位, 超过8位的会被截断</strong><br><strong>如果想使用kubernetes的secret来保存密码, 则生成的密码文件名一定要是auth, 否则的话访问会提示503错误.</strong><br><code>kubectl create secret generic basic-auth --from-file=/tmp/auth</code></p>
<p>这样, 在访问时就会弹出需要输入用户密码的界面了, <strong>如果密码错误，则会提示401 Unauthorized</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/examples/auth/basic/">https://kubernetes.github.io/ingress-nginx/examples/auth/basic/</a></li>
<li><a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Opentelemetry调研实践二(opentelemetry架构及名词介绍)</title>
    <url>/2021/10/27/OpenTelemetry-Introduct/</url>
    <content><![CDATA[<p>历史文章:</p>
<p><a href="https://izsk.me/2021/10/19/OpenTelemetry-what-is-observability/">可观测性到底在说什么</a></p>
<p><a href="https://izsk.me/2021/10/19/OpenTelemetry-what-is-observability/">上文</a>简单说了下可观测性，然后引出了<strong>主角: opentelemetry</strong></p>
<p>可观测性一个很重要的领域<code>Trace</code>有两个业界标杆: 一个是<a href="https://opentracing.io/">OpenTracing</a>，另一个<a href="https://opencensus.io/">OpenCensus</a>，OpenTracing其实是一个规范，jeager就是基于opentracing实现的开源工具，而OpenCensus则是由google开源的度量工具，简单来说，这两者在可观测性领域功能高度重合，因此，在CNCF主导下进行了合并形成opentelemetry项目，OpenTracing跟penCensus共同推进opentelemetry，两者的官网也赫赫表达基本不再维护，同时opentelemetry也致力于trace、logging、metrics间的关联性.</p>
<span id="more"></span>



<h3 id="OpenTelemetry核心工作"><a href="#OpenTelemetry核心工作" class="headerlink" title="OpenTelemetry核心工作"></a>OpenTelemetry核心工作</h3><blockquote>
<ul>
<li>1.规范的制定和协议的统一，规范包含数据传输、API的规范，协议的统一包含：HTTP W3C的标准支持及GRPC等框架的协议标准</li>
<li>2.多语言SDK的实现和集成，用户可以使用SDK进行代码自动注入和手动埋点，同时对其他三方库（Log4j、LogBack等）进行集成支持</li>
<li>3.数据收集系统的实现，当前是基于OpenCensus Service的收集系统，包括Agent和Collector。</li>
</ul>
</blockquote>
<p>由此可见，OpenTelemetry的自身定位很明确：数据采集和标准规范的统一，对于数据如何去使用、存储、展示、告警，官方是不涉及的</p>
<h3 id="OpenTelemetry状态"><a href="#OpenTelemetry状态" class="headerlink" title="OpenTelemetry状态"></a>OpenTelemetry状态</h3><p>OpenTelemetry要解决的<strong>是对可观测性的大一统</strong>，它提供了一组API和SDK来标准化遥测数据的采集和传输，opentelemetry并不想对所有的组件都进行重写，而是最大程度复用目前业界在各大领域常用工具，通过提供了一个安全，厂商中立的能用协议、组件，这样就可以按照需要形成pipeline将数据发往不同的后端。</p>
<p>由于opentelemetry提出时间并不长，大家可以从<a href="https://opentelemetry.io/status/">status</a>中可以看到最新的状态，可以看到，目前只有Tracing实现了1.0的release，而Metrics处于<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/document-status.md#feature-freeze">feature-freeze</a>，这个状态说的是metrics目前暂不接受在metrics中添加新的功能，以便技术委员会优先实现其它功能，还未达到stable状态，而在Logging则还处于草案阶段，opentelemetry官方的解释是日志相对存在体量大，延时高的问题，相对棘手且优先级对比另两者不高。</p>
<p>tracing已经在2021上半年实现了stable,目前官方给出的时间线是在2021下半年实现metrics的stable，在2022实现logging的stable.</p>
<p>为什么tracing是首先stable的，作者个人感觉可能是tracing目前在业界普及度还不够吧，而像metrics方面，prometheus早已成了业务标准</p>
<h3 id="OpenTelemetry架构"><a href="#OpenTelemetry架构" class="headerlink" title="OpenTelemetry架构"></a>OpenTelemetry架构</h3><p>这里作者还要重点说明的是，就算是Service Mesh中对于链路追踪也普遍有一个问题: 就是无论你在数据平面如何做流量劫持，如何透传信息，以及如何生成或者继承Span，<strong>入口流量和出口流量之间的链路都存在无法串联的问题</strong>， 这个问题要解决还是需要服务来埋点透传，将链路信息透传到下一次请求当中去，这个问题是无法避免的，而OpenTelemetry的后续推行，可以解决这方面的标准化问题。</p>
<p>来看一张图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210930101301.png"></p>
<p>注: 由于logging还处于草案状态，这里没有描述出来。</p>
<p>opentelemetry也是个插件式的架构，针对不同的开发语言会有相应的Client组件，叫<strong>Instrumenttation</strong>，也就是在代码中埋点调用的api&#x2F;sdk采集telemetry数据</p>
<p>将获取到的数据pull&#x2F;push到上图中定义好的<strong>pipeline</strong>中,这部分是可高度定制的, opentelemetry提供统一的<a href="https://github.com/open-telemetry/opentelemetry-specification">Specification</a></p>
<p><strong>pipeline</strong>就是<strong>Collection</strong>的实现，大概分为3部分，这里简单介绍一下，后期在详解collection的配置文件时再进行展开:</p>
<blockquote>
<ul>
<li>receivers(接收者): 定义从client端的数据要以何种数据模型进行接收, 支持很多种数据模型.</li>
<li>processors: 将receivers的数据进行某些处理，比如批量、性能分析等.</li>
<li>exporters: 将processors后的数据导出到特定的后端，比如metrics数据存储到prometheus中.</li>
</ul>
</blockquote>
<p>整个流程是pipeline式的,可以根据实际情况自定义pipeline,所以说collector定制性很高.</p>
<p>这里要说明的是receivers、processors、exporters有官方的定义<a href="https://github.com/open-telemetry/opentelemetry-collector">opentelemetry-collector</a></p>
<p>但是也有很多开源产品基于opentelemetry-collector提供相应的实现,详细列表参考<a href="https://github.com/open-telemetry/opentelemetry-collector-contrib">opentelemetry-collector-contrib</a></p>
<p>总结一下，opentelemetry按功能分层来说,可以分为：采集器（Instrument）、发送器（TransPort）、收集器（Collector）、存储（Srotage）、展示（API&amp;UI）,这跟其它APM系统是类似的，只是opentelemetry在很多组件上选择了利用而不是重新造轮子.</p>
<p>在opentelemetry里面的名词也是非常多的，有必要把一些常用的名词给大家解释一下.</p>
<p>建议大家多多阅读官方文档，也可参考<a href="https://jckling.github.io/2021/04/02/Jaeger/OpenTelemetry%20%E8%A7%84%E8%8C%83%E9%98%85%E8%AF%BB/">OpenTelemetry 规范阅读</a>，里面详细介绍了各个名词的含义，对大家理解有帮助</p>
<h3 id="OpenTelemetry核心概念"><a href="#OpenTelemetry核心概念" class="headerlink" title="OpenTelemetry核心概念"></a>OpenTelemetry核心概念</h3><p>以下名词引用官方文档<a href="https://opentelemetry.io/docs/concepts/components/">components</a>的翻译</p>
<h4 id="Proto"><a href="#Proto" class="headerlink" title="Proto"></a>Proto</h4><p>语言无关的接口类型。描述很多下文组件的定义。</p>
<p>####Client</p>
<p>其实就是client, 应用调用OpenTelemetry API的client, 语言相关的代码封装库，引入trace、metrics、log等</p>
<h4 id="API-x2F-SDK"><a href="#API-x2F-SDK" class="headerlink" title="API&#x2F;SDK"></a>API&#x2F;SDK</h4><p>API 包由用于检测的横切公共接口组成，导入到第三方库和应用程序代码的 OpenTelemetry 客户端的任何部分都被认为是 API 的一部分,</p>
<p>而SDK是API的具体实现，这部分是语言相关的.</p>
<h4 id="Specification"><a href="#Specification" class="headerlink" title="Specification"></a>Specification</h4><p>Specification则是定义了API&#x2F;SDK及数据模型，所有包含第三方实现的都需要遵循spec定义的规范</p>
<h4 id="Semantic-Conventions"><a href="#Semantic-Conventions" class="headerlink" title="Semantic Conventions"></a>Semantic Conventions</h4><p>OpenTelemetry 项目保证所有的instrumentation(不论任何语言)都包含相同的语义信息</p>
<h4 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h4><p>resource附加于某个process产生的所有trace的键值对，在初始化阶段指定并传递到collector中</p>
<h4 id="Baggage"><a href="#Baggage" class="headerlink" title="Baggage"></a>Baggage</h4><p>为添加在metrics、log、traces中的注解信息，键值对需要唯一，无法更改</p>
<h4 id="Propagators"><a href="#Propagators" class="headerlink" title="Propagators"></a>Propagators</h4><p>传播器，比如在多进程的调用中，开启传播器用于跨服务传播spanContext</p>
<h4 id="Attributes"><a href="#Attributes" class="headerlink" title="Attributes"></a>Attributes</h4><p>其实就是tag, 可以给span添加metadata数据,<code>SetAttributes</code>属性可以多次添加，不存在就添加，存在就覆盖</p>
<h4 id="Events"><a href="#Events" class="headerlink" title="Events"></a>Events</h4><p>类似于日志, 比如在trace中嵌入请求体跟响应体</p>
<h4 id="Collector"><a href="#Collector" class="headerlink" title="Collector"></a>Collector</h4><p>虽然Collector翻译过来叫接收，但它负责遥测数据源的接收、处理和导出三部分，提供了与供应商无关的实现</p>
<p>collector是个实体组件，有两个部署方案，可以ds进行部署，各个host负责该host上的遥测数据</p>
<p>或者以deployment的方式部署，接收所有遥测数据</p>
<p>其中，Metrics、Logging、Trace、Baggage叫<strong>Signal</strong>,以上的对象中除了<strong>Collector</strong>,其它几个由于没有具体部署对象，名词一多理解起来还是比较费劲，需要多翻官方文档</p>
<p>这里重点介绍Collector，在介绍Collector前，先来介绍一个协议。</p>
<h3 id="OpenTelemetry协议-OTLP"><a href="#OpenTelemetry协议-OTLP" class="headerlink" title="OpenTelemetry协议-OTLP"></a>OpenTelemetry协议-OTLP</h3><p><a href="https://github.com/open-telemetry/opentelemetry-specification/tree/main/specification/protocol">OTLP(OpenTelemetry Protocol)</a>是一种协议，该此协议用于将应用产生的遥感数据发送到 OpenTelemetry Collector.</p>
<p><img src="https://raw.github.com/open-telemetry/opentelemetry.io/main/iconography/Otel_Collector.svg"></p>
<p>otlp是opentelemetry中比较核心的存在，在遥测数据及Collector之间制定了包括编码(encoding)、传输(transport)、传递(delivery)等协议</p>
<p>目前OTLP协议在数据源的状态也是不一样的.</p>
<ul>
<li>Tracing: <strong>Stable</strong></li>
<li>Metrics: <strong>Stable</strong></li>
<li>Logs: <strong>Beta</strong></li>
</ul>
<p>otlp也对http、grpc进行了较好的开箱即用的封装.</p>
<p>另外，OTEP(OpenTelemetry Enhancement Proposal)是加强版的otlp, 目前处于不稳定状态，这里不展开.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h3><blockquote>
<ul>
<li><a href="https://codingnote.cc/p/246692">https://codingnote.cc/p/246692</a></li>
<li><a href="https://opentelemetry.io/registry">https://opentelemetry.io/registry</a></li>
<li><a href="https://opentelemetry.io/docs/concepts/components">https://opentelemetry.io/docs/concepts/components</a></li>
<li><a href="https://signoz.io/blog/opentelemetry-kubernetes">https://signoz.io/blog/opentelemetry-kubernetes</a></li>
<li><a href="https://www.infoq.cn/article/pky4szfhbvifxrcriqcr">https://www.infoq.cn/article/pky4szfhbvifxrcriqcr</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-collector">https://github.com/open-telemetry/opentelemetry-collector</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib">https://github.com/open-telemetry/opentelemetry-collector-contrib</a></li>
<li><a href="https://tech.ipalfish.com/blog/2020/12/29/design-dimensions-of-tracing-systems">https://tech.ipalfish.com/blog/2020/12/29/design-dimensions-of-tracing-systems</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/tree/main/specification/protocol">https://github.com/open-telemetry/opentelemetry-specification/tree/main/specification/protocol</a></li>
<li><a href="https://jckling.github.io/2021/04/02/Jaeger/OpenTelemetry%20%E8%A7%84%E8%8C%83%E9%98%85%E8%AF%BB">https://jckling.github.io/2021/04/02/Jaeger/OpenTelemetry%20%E8%A7%84%E8%8C%83%E9%98%85%E8%AF%BB</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Opentelemetry调研实践三(全链路追踪的TraceID与SpanID)</title>
    <url>/2021/10/31/OpenTelemetry-Trace/</url>
    <content><![CDATA[<p>历史文章:</p>
<p><a href="https://izsk.me/2021/10/19/OpenTelemetry-what-is-observability/">可观测性到底在说什么</a></p>
<p><a href="https://izsk.me/2021/10/31/OpenTelemetry-Introduct/">opentelemetry架构及名词介绍</a></p>
<p>在<a href="https://izsk.me/2021/10/31/OpenTelemetry-Introduct/">opentelemetry架构及名词介绍</a> 中就引出了一个问题: <strong>无论在数据平面如何做流量劫持，如何透传信息，以及如何生成或者继承Span，入口流量和出口流量之间的链路都存在无法串联的问题， 这个问题要解决还是需服务来埋点透传，将链路信息透传到下一次请求当中去</strong></p>
<span id="more"></span>



<p>一个最简单的golang的例子</p>
<p>Main of ServiceA – &gt; FuncionA of ServiceA  – &gt; FunctionB of ServiceA  – &gt; Main of ServiceB</p>
<p>调用从主函数Main of ServiceA(<strong>入口流量</strong>)到调用Main of ServiceB(<strong>出口流量</strong>）的中间的这段调用，对于大多数的APM都无法捕捉得到，原因是APM不能理解<strong>业务逻辑</strong>，现在大热的Istio也做不到，它本质上还是通过劫持入口及出口流量，对于方法级的调用也无法实现追踪(这里不考虑java使用字节码技术实现)，如果想知道这层调用关系，则需要:</p>
<blockquote>
<ul>
<li>生成一个ID1: 使用该ID可以将所有经过的节点串连起来，按时间排序就是<strong>timeline</strong></li>
<li>生成一个ID2:可以通过该ID体现父子关系，串起来就是<strong>调用栈</strong></li>
</ul>
</blockquote>
<p>这在分布式链路跟踪中刚好对应两个重要的概念：跟踪（trace）和 跨度（ span）</p>
<p>trace 是请求在分布式系统中的整个链路视图，span 则代表整个链路中不同服务内部的视图(有向无环图，DAG)</p>
<p>一个span代表系统中具有开始时间和执行时长的逻辑运行单元, 所有span 组合在一起就是整个 trace 的视图</p>
<p>这就是Trace要解决的问题， 对于ID1,一般称之为TraceID, 对于ID2，一般称之为Span</p>
<h3 id="TraceID"><a href="#TraceID" class="headerlink" title="TraceID"></a>TraceID</h3><p>这个很好理解，如果要标记一次请求经过的所有路径，那么给这条请求经过的所有节点都使用同一个标记即可，那么反过来，通过这个标志即可得到这条请求经过的所有节点。</p>
<p>所以一般情况下，都会在网关处给每次请求都生成一个全局唯一的ID做为TraceID,将该TraceID放在Header中向后传递下去，后面的服务都使用该ID</p>
<h3 id="SpanID"><a href="#SpanID" class="headerlink" title="SpanID"></a>SpanID</h3><p>很多人不理解为什么需要SpanID？</p>
<p>既然有了TraceID，如果再加上节点的被调用的时间，是不是也可以还原出整个请求的调用链路视图呢?</p>
<p>答案是可行的，但是使用调用时间远不如SpanID方便。</p>
<p>当请求到达每个服务后，服务都会为请求生成spanid，第一个spanid称之为root span，而随请求一起从上游传过来的上游服务的 spanid 会被记录成parent-spanid或者叫 pspanid。当前服务生成的 spanid 随着请求一起再传到下游服务时，这个spanid 又会被下游服务当做 pspanid 记录</p>
<p>所以，SpanID本身就已经形成了父子关系，而使用调用时间的话，还需要进行时间戳的比对，这在一定量级的场景下对性能是个考验。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Span <span class="keyword">struct</span> &#123;</span><br><span class="line">    TraceID    <span class="type">int64</span> <span class="comment">// 用于标示一次完整的请求id</span></span><br><span class="line">    Name       <span class="type">string</span></span><br><span class="line">    ID         <span class="type">int64</span> <span class="comment">// 当前这次调用span_id</span></span><br><span class="line">    ParentID   <span class="type">int64</span> <span class="comment">// 上层服务的调用span_id  最上层服务parent_id为null</span></span><br><span class="line">    Annotation []Annotation <span class="comment">// 用于标记的时间戳</span></span><br><span class="line">    Debug      <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="调用链"><a href="#调用链" class="headerlink" title="调用链"></a>调用链</h3><p>先来看一张经典图:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20211031225945.png"></p>
<p>当用户发起一个请求时，首先到达前端A服务，然后分别对B服务和C服务进行RPC调用；B服务处理完给A做出响应，但是C服务还需要和后端的D服务和E服务交互之后再返还给A服务，最后由A服务来响应用户的请求</p>
<p>整个调用过程追踪</p>
<ul>
<li><p>请求到来生成一个全局TraceID，通过TraceID可以串联起整个调用链，一个TraceID代表一次请求。</p>
</li>
<li><p>除了TraceID外，还需要SpanID用于记录调用父子关系。每个服务会记录下parent id和span id，通过他们可以组织一次完整调用链的父子关系。</p>
</li>
<li><p>一个没有parent id的span成为root span，可以看成调用链入口。</p>
</li>
<li><p>所有这些ID可用全局唯一的64位整数表示；</p>
</li>
<li><p>整个调用过程中每个请求都要透传TraceID和SpanID。</p>
</li>
<li><p>每个服务将该次请求附带的TraceID和附带的SpanID作为parent id记录下，并且将自己生成的SpanID也记录下。</p>
</li>
<li><p>要查看某次完整的调用则 只要根据TraceID查出所有调用记录，然后通过parent id和span id组织起整个调用父子关系。</p>
</li>
<li><p>调用链核心工作</p>
</li>
<li><ul>
<li>调用链数据生成，对整个调用过程的所有应用进行埋点并输出日志。</li>
<li>调用链数据采集，对各个应用中的日志数据进行采集。</li>
<li>调用链数据存储及查询，对采集到的数据进行存储，由于日志数据量一般都很大，不仅要能对其存储，还需要能提供快速查询。</li>
<li>指标运算、存储及查询，对采集到的日志数据进行各种指标运算，将运算结果保存起来。</li>
<li>告警功能，提供各种阀值警告功能。</li>
</ul>
</li>
</ul>
<p>目前大部分的全链路追踪实现都是基于Google的Dapper实现</p>
<p>下面介绍几种可做为ID的方案.</p>
<h3 id="nginx生成TraceID"><a href="#nginx生成TraceID" class="headerlink" title="nginx生成TraceID"></a>nginx生成TraceID</h3><p>nginx为每一条请求都生成一个唯一的ID，这个ID由nginx本身保证了唯一性，天然是个做为TraceID的好方案，只需要在nginx的配置中开启以下参数即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">upstream app_server &#123;</span><br><span class="line">    server 10.0.0.1:80;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    add_header X-Request-ID <span class="variable">$request_id</span>; <span class="comment"># Return to client</span></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://app_server;</span><br><span class="line">        proxy_set_header X-Request-ID <span class="variable">$request_id</span>; <span class="comment"># Pass to app server</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参考<a href="https://www.nginx.com/blog/application-tracing-nginx-plus/">application-tracing-nginx-plus</a></p>
<p>后端代码可直接从http header中获取相应的X-Request-ID的值做为ctx传递即可</p>
<h3 id="kong生成TraceID"><a href="#kong生成TraceID" class="headerlink" title="kong生成TraceID"></a>kong生成TraceID</h3><p>kong做为ingress controller的场景下，可通过<strong>correlation-id</strong>插件来生成uuid做为traceID</p>
<p>可以定义全局使用(全局的plugins不需要手工在ingress中进行绑定)</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">configuration.konghq.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KongClusterPlugin</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kong-global-correlation-id</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">kong</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">global:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">config:</span> </span><br><span class="line">  <span class="attr">header_name:</span> <span class="string">X-Request-Id</span></span><br><span class="line">  <span class="attr">generator:</span> <span class="string">uuid</span></span><br><span class="line">  <span class="attr">echo_downstream:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">plugin:</span> <span class="string">correlation-id</span></span><br></pre></td></tr></table></figure>

<p>header_name指定header名字</p>
<p>generator指定使用uuid生产ID</p>
<p>echo_downstream指定将header返回给调用方</p>
<p>参考<a href="https://docs.konghq.com/hub/kong-inc/correlation-id/">correlation-id</a></p>
<p>如果不是全局的，可在ingress中进行绑定</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">konghq.com/override:</span> <span class="string">kong-global-correlation-id</span></span><br></pre></td></tr></table></figure>

<p>对于已经存在相同名字的header，correlation-id会进行忽略，不进行任何操作</p>
<h3 id="opentelemetry生成TraceID"><a href="#opentelemetry生成TraceID" class="headerlink" title="opentelemetry生成TraceID"></a>opentelemetry生成TraceID</h3><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// IDGenerator allows custom generators for TraceID and SpanID.</span></span><br><span class="line"><span class="keyword">type</span> IDGenerator <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">// DO NOT CHANGE: any modification will not be backwards compatible and</span></span><br><span class="line">	<span class="comment">// must never be done outside of a new major release.</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// NewIDs returns a new trace and span ID.</span></span><br><span class="line">	NewIDs(ctx context.Context) (trace.TraceID, trace.SpanID)</span><br><span class="line">	<span class="comment">// DO NOT CHANGE: any modification will not be backwards compatible and</span></span><br><span class="line">	<span class="comment">// must never be done outside of a new major release.</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// NewSpanID returns a ID for a new span in the trace with traceID.</span></span><br><span class="line">	NewSpanID(ctx context.Context, traceID trace.TraceID) trace.SpanID</span><br><span class="line">	<span class="comment">// DO NOT CHANGE: any modification will not be backwards compatible and</span></span><br><span class="line">	<span class="comment">// must never be done outside of a new major release.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> randomIDGenerator <span class="keyword">struct</span> &#123;</span><br><span class="line">	sync.Mutex</span><br><span class="line">	randSource *rand.Rand</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> _ IDGenerator = &amp;randomIDGenerator&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewSpanID returns a non-zero span ID from a randomly-chosen sequence.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gen *randomIDGenerator)</span></span> NewSpanID(ctx context.Context, traceID trace.TraceID) trace.SpanID &#123;</span><br><span class="line">	gen.Lock()</span><br><span class="line">	<span class="keyword">defer</span> gen.Unlock()</span><br><span class="line">	sid := trace.SpanID&#123;&#125;</span><br><span class="line">	gen.randSource.Read(sid[:])</span><br><span class="line">	<span class="keyword">return</span> sid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewIDs returns a non-zero trace ID and a non-zero span ID from a</span></span><br><span class="line"><span class="comment">// randomly-chosen sequence.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gen *randomIDGenerator)</span></span> NewIDs(ctx context.Context) (trace.TraceID, trace.SpanID) &#123;</span><br><span class="line">	gen.Lock()</span><br><span class="line">	<span class="keyword">defer</span> gen.Unlock()</span><br><span class="line">	tid := trace.TraceID&#123;&#125;</span><br><span class="line">	gen.randSource.Read(tid[:])</span><br><span class="line">	sid := trace.SpanID&#123;&#125;</span><br><span class="line">	gen.randSource.Read(sid[:])</span><br><span class="line">	<span class="keyword">return</span> tid, sid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">defaultIDGenerator</span><span class="params">()</span></span> IDGenerator &#123;</span><br><span class="line">	gen := &amp;randomIDGenerator&#123;&#125;</span><br><span class="line">	<span class="keyword">var</span> rngSeed <span class="type">int64</span></span><br><span class="line">	_ = binary.Read(crand.Reader, binary.LittleEndian, &amp;rngSeed)</span><br><span class="line">	gen.randSource = rand.New(rand.NewSource(rngSeed))</span><br><span class="line">	<span class="keyword">return</span> gen</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述是opentelemetry默认生成TraceID跟SpanID的方法，同时，opentelemetry支持使用自定义算法去生成，只需要生写上述的NewIDs及NewSpanID即可，比如:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义ID生成规则</span></span><br><span class="line"><span class="keyword">type</span> Generator <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *Generator)</span></span> NewIDs(ctx context.Context) (trace.TraceID, trace.SpanID) &#123;</span><br><span class="line">  val := ctx.Value(<span class="string">&quot;request_id&quot;</span>).(uuid.UUID)</span><br><span class="line">  tid := trace.TraceID&#123;&#125;</span><br><span class="line">  req, _ := val.MarshalText()</span><br><span class="line">  <span class="built_in">copy</span>(tid[:], req)</span><br><span class="line"></span><br><span class="line">  sid := trace.SpanID&#123;&#125;</span><br><span class="line">  rand.Read(sid[:])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tid, sid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义NewTracerProvider时使用WithIDGenerator</span></span><br><span class="line">tp := sdktrace.NewTracerProvider(</span><br><span class="line">  sdktrace.WithSpanProcessor(processor),</span><br><span class="line">  sdktrace.WithResource(resource.NewWithAttributes(</span><br><span class="line">    semconv.ServiceNameKey.String(<span class="string">&quot;vault-observe&quot;</span>),</span><br><span class="line">  )),</span><br><span class="line">  sdktrace.WithIDGenerator(&amp;Generator&#123;&#125;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">otel.SetTracerProvider(tp)</span><br></pre></td></tr></table></figure>





<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h3><blockquote>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1832719">https://cloud.tencent.com/developer/article/1832719</a></li>
<li><a href="https://www.nginx.com/blog/application-tracing-nginx-plus/">https://www.nginx.com/blog/application-tracing-nginx-plus/</a></li>
<li><a href="https://docs.konghq.com/hub/kong-inc/correlation-id/">https://docs.konghq.com/hub/kong-inc/correlation-id/</a></li>
<li><a href="https://andydote.co.uk/2021/05/27/vault-observe/">https://andydote.co.uk/2021/05/27/vault-observe/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Oracle之DBMS_SCHEDULER包管理计划任务</title>
    <url>/2017/08/03/Oracle%E4%B9%8BDBMS_SCHEDULER%E5%8C%85%E7%AE%A1%E7%90%86%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p>最近在分析oracle9i的statspack报告,statspack默认是不安装的,需要自己安装且要添加snapshot的定时生成计划,刚好有用到的job,schedule,还是比较强大,稍微记录下DBMS_SCHEDULER包的简单使用及参数说明.</p>
<span id="more"></span>

<h3 id="DBMS-SCHDULER"><a href="#DBMS-SCHDULER" class="headerlink" title="DBMS_SCHDULER"></a><strong>DBMS_SCHDULER</strong></h3><p>首先,DBMS_SCHDULER(<strong>Oracle10g引入 的新功能,10g之前则可以使用DBMS_JOB包创建任务,见下文</strong>)可以创建两种任务,但也可以说是有三种,当中指的‘任务’可以是<strong>存储过程,可以是匿名PL&#x2F;SQL语句块,还可以是操作系统层面的命令</strong>,在oracle10g之后,对操作系统层面的命令的支持更加完善.</p>
<blockquote>
<ul>
<li>DBMS_SCHDULER.CREATE_JOB</li>
<li>DBMS_SCHDULER.CREATE_SCHEDULE</li>
<li>DBMS_SCHDULER.CREATE_CREATE_PROGRAM</li>
</ul>
</blockquote>
<h4 id="DBMS-SCHDULER-CREATE-JOB"><a href="#DBMS-SCHDULER-CREATE-JOB" class="headerlink" title="DBMS_SCHDULER.CREATE_JOB"></a><strong>DBMS_SCHDULER.CREATE_JOB</strong></h4><p>这里以定时执行某一存储过程为例:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">DBMS_SCHEDULER.CREATE_JOB (</span><br><span class="line">job_name <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;job_proc&#x27;</span>,</span><br><span class="line">job_type <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;STORED_PROCEDURE&#x27;</span>,</span><br><span class="line">job_action <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;proc_name&#x27;</span>,</span><br><span class="line"><span class="comment">-- start_date =&gt; null</span></span><br><span class="line"><span class="comment">-- end_date =&gt; null </span></span><br><span class="line">repeat_interval <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;FREQ=HOURLY; BYHOUR=9,12,15; BYMINUTE=5,15&#x27;</span>,</span><br><span class="line">auto_drop <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">FALSE</span>,</span><br><span class="line">enabled <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">TRUE</span>,</span><br><span class="line">comments <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;proc automated exec&#x27;</span>);</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<p>说明:</p>
<blockquote>
<ul>
<li><p>DBMS_SCHEDULER.CREATE_JOB:创建一个scheduler的job,创建job至少需要create_job这个系统权限.</p>
</li>
<li><p>job_name: 给job指定一个名字</p>
</li>
<li><p>job_type: job的类型,在3种值</p>
<blockquote>
<ol>
<li>PL&#x2F;SQL块: PLSQL_BLOCK</li>
<li>存储过程: STORED_PROCEDURE</li>
<li>外部程序: EXECUTABLE (外部程序可以是一个shell脚本,也可以是操作系统级别的指令),需要create external job 权限</li>
</ol>
</blockquote>
</li>
<li><p>job_action: 对应要执行的命令</p>
</li>
<li><p>start_date&#x2F;end_date: 开始执行与结束执行的时间,如果不指定或者指定为null,则表示立即执行,参数start_date和end_date都是TIMESTAMP 类型，在输入的時候要遵循它们的格式</p>
</li>
<li><p><strong>repeat_interval: 重复执行的频率,支持两种写法</strong></p>
<blockquote>
<ol>
<li><p>兼容老版本的PL&#x2F;SQL方式:例如SYSDATE+1, SYSDATE + 30&#x2F;24*60,见下文DBMS_JOB部分</p>
</li>
<li><p>日历的表达方式: 分为三部分: 第一部分是频率,也就是”FREQ”这个关键字,它是必须指定的; 第二部分是时间间隔,也就是”INTERVAL”这个关键字，取值范围是1-999. 它是可选的参数; 最后一部分是附加的参数,可用于<br>精确地指定日期和时间,它也是可选的参数,例如下面这些值都是合的:</p>
<p>BYMONTH,BYWEEKNO,BYYEARDAY,BYMONTHDAY,BYDAY<br>BYHOUR,BYMINUTE,BYSECOND</p>
<blockquote>
<ul>
<li>repeat_interval &#x3D;&gt; ‘FREQ&#x3D;HOURLY; INTERVAL&#x3D;2’: 每隔2小时运行一次job</li>
<li>repeat_interval &#x3D;&gt; ‘FREQ&#x3D;DAILY’: 每天运行一次job</li>
<li>repeat_interval &#x3D;&gt; ‘FREQ&#x3D;WEEKLY; BYDAY&#x3D;MON,WED,FRI”: 每周的1,3,5运行job</li>
<li>repeat_interval &#x3D;&gt; ‘FREQ&#x3D;HOURLY; BYHOUR&#x3D;9,12,15; BYMINUTE&#x3D;5,15’: 每天的9:05, 9:15, 12:05, 12:15, 15:05 and 15:15运行job</li>
<li>FREQ&#x3D;MONTHLY;BYMONTHDAY&#x3D;1,5,-1: 每月的1号,5号,该月的最后一天执行,<strong>-1表示最后的意思</strong></li>
</ul>
</blockquote>
</li>
</ol>
</blockquote>
</li>
<li><p>auto_drop: 任务执行完之后是否drop掉</p>
</li>
<li><p><strong>enable: 默认创建的job是不会自动执行的,需要手动执行一次后才能自动执行,如果需要创建完就自动执行,则必须指定enable: true</strong></p>
</li>
<li><p>comment: 添加注释&#x2F;备注</p>
</li>
</ul>
</blockquote>
<h4 id="DBMS-SCHDULER-CREATE-SCHDULER"><a href="#DBMS-SCHDULER-CREATE-SCHDULER" class="headerlink" title="DBMS_SCHDULER.CREATE_SCHDULER"></a><strong>DBMS_SCHDULER.CREATE_SCHDULER</strong></h4><p>我们可以只定义一个schdule,然后再创建一个Job,这个job指向schdule,相当于schdule是定义,job是执行实体</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Create the schedule.</span></span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">  DBMS_SCHEDULER.create_schedule (</span><br><span class="line">    schedule_name   <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;test_hourly_schedule&#x27;</span>,</span><br><span class="line">    start_date      <span class="operator">=</span><span class="operator">&gt;</span> SYSTIMESTAMP,</span><br><span class="line">    repeat_interval <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;freq=hourly; byminute=0&#x27;</span>,</span><br><span class="line">    end_date        <span class="operator">=</span><span class="operator">&gt;</span> <span class="keyword">NULL</span>,</span><br><span class="line">    comments        <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;Repeats hourly, on the hour, for ever.&#x27;</span>);</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<p>上面我们只是定义了一个schdule,可以看到这里并没有指定具体要执行的是什么,具体执行实体可放在job中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Job defined by an existing program and schedule.</span></span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">  DBMS_SCHEDULER.create_job (</span><br><span class="line">    job_name      <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;test_prog_sched_job_definition&#x27;</span>,</span><br><span class="line">    program_name  <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;test_plsql_block_prog&#x27;</span>,</span><br><span class="line">    schedule_name <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;test_hourly_schedule&#x27;</span>,</span><br><span class="line">    enabled       <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">TRUE</span>,</span><br><span class="line">    comments      <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;Job defined by an existing program and schedule.&#x27;</span>);</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<p>上面的schedule_name即是指向schedule定义的内容(同样适用于program)</p>
<p>当然,可以完全像文章开头介绍DBMS_SCHDULER.CREATE_JOB一样在创建job里就指定执行实体,这样一步到位</p>
<h4 id="DBMS-SCHDULER-CREATE-PROGRAM"><a href="#DBMS-SCHDULER-CREATE-PROGRAM" class="headerlink" title="DBMS_SCHDULER.CREATE_PROGRAM"></a><strong>DBMS_SCHDULER.CREATE_PROGRAM</strong></h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">begin</span></span><br><span class="line">dbms_scheduler.create_program(</span><br><span class="line">program_name<span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;DAILY_BACKUP_SH&#x27;</span>,</span><br><span class="line">program_type<span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;EXECUTABLE&#x27;</span>,</span><br><span class="line">program_action<span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;/home/oracle/script/daily_backup.sh&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<p>参数跟上面类似,在此就不详述</p>
<p>大家可能已经注意到了,上面三个东西大体相似,一个就可以,为何要出这么多包体呢？</p>
<p><strong>program与job区别:</strong></p>
<blockquote>
<ul>
<li>program可以与job分离开来,且program可以被共用,而job是属于具体用户的</li>
<li>从上面的create_program定义可以看出,里面是不包含开始&#x2F;结束、执行重复频率的,这也就说明program可以被用户很自由地选择特定的程序在特定的时间段运行,以及自由的配置程序执行时的参数</li>
<li>一个计划里可以没有program,对于计划来说它是可选的,但是一个计划必须要有一个job</li>
</ul>
</blockquote>
<h4 id="DBMS-SCHEDULER其它相关函数"><a href="#DBMS-SCHEDULER其它相关函数" class="headerlink" title="DBMS_SCHEDULER其它相关函数"></a><strong>DBMS_SCHEDULER其它相关函数</strong></h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 手工执行</span></span><br><span class="line"><span class="comment">-- SQL&gt; EXEC DBMS_SCHEDULER.RUN_JOB(&#x27;DB_BACKUP&#x27;);</span></span><br><span class="line"><span class="comment">-- 默认情况下任务是不启用的,除非创建计划时设置了(enabled=&gt;true),so 如果计划没有启用,首先启用它:</span></span><br><span class="line"><span class="comment">-- EXEC dbms_scheduler.enable(&#x27;DB_BACKUP&#x27;);</span></span><br><span class="line"><span class="comment">-- 删除job</span></span><br><span class="line"><span class="comment">-- SQL&gt; exec dbms_scheduler.drop_job(&#x27;DB_BACKUP&#x27;);</span></span><br><span class="line"><span class="comment">-- 删除schedule</span></span><br><span class="line"><span class="comment">-- SQL&gt; exec dbms_scheduler.drop_schedule(&#x27;DB_BACKUP&#x27;);</span></span><br><span class="line"><span class="comment">-- 停止job</span></span><br><span class="line"><span class="comment">-- SQL&gt; exec dbms_scheduler.stop_job(&#x27;DB_BACKUP&#x27;);</span></span><br></pre></td></tr></table></figure>



<h3 id="DBMS-JOB"><a href="#DBMS-JOB" class="headerlink" title="DBMS_JOB"></a><strong>DBMS_JOB</strong></h3><h4 id="DBMS-JOB-submit"><a href="#DBMS-JOB-submit" class="headerlink" title="DBMS_JOB.submit"></a><strong>DBMS_JOB.submit</strong></h4><p>这里以oracle9i定时生成statspack报告为例:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">declare</span> </span><br><span class="line">	id_job number; </span><br><span class="line"><span class="keyword">begin</span> </span><br><span class="line">	dbms_job.submit(</span><br><span class="line">		job <span class="operator">=</span><span class="operator">&gt;</span> id_job, <span class="comment">-- OUT; the job ID number that will be generated,</span></span><br><span class="line">		what <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;schema_name.procedure_name;&#x27;</span>, <span class="comment">-- IN; the name of the job you wish to run,</span></span><br><span class="line">		next_date <span class="operator">=</span><span class="operator">&gt;</span> trunc(sysdate)<span class="operator">+</span><span class="number">1</span><span class="operator">/</span><span class="number">24</span>,  <span class="comment">-- IN; the first time the job will be run,</span></span><br><span class="line">		<span class="type">interval</span> <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;trunc(sysdate)+1+4/24&#x27;</span> <span class="comment">-- IN; the interval the job will be repeated,</span></span><br><span class="line">        no_parse <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">True</span> <span class="comment">--IN BOOLEAN DEFAULT FALSE,</span></span><br><span class="line">        instance <span class="operator">=</span><span class="operator">&gt;</span> <span class="number">0</span> <span class="comment">--IN BINARY_INTEGER DEFAULT ANY_INSTANCE,</span></span><br><span class="line">        force <span class="operator">=</span><span class="operator">&gt;</span> <span class="literal">False</span> <span class="comment">--IN BOOLEAN DEFAULT FALSE</span></span><br><span class="line">	); </span><br><span class="line"><span class="keyword">end</span>; </span><br></pre></td></tr></table></figure>

<p>说明:</p>
<blockquote>
<ul>
<li>job: 是该job的返回值,代表该job的id号,在修改或者是删除时会用到</li>
<li>what: 对应要执行的命令</li>
<li>next_date: 开始执行命令的时间</li>
<li>interval: 执行的频率,<strong>interval后面的最大字符长度不能超过200个字符</strong></li>
<li>no_parse: 默认为false,如下情景时需要设置为true,如果想job在还没有创建相关的表或者是存储过程之前运行</li>
<li>instance: 默认为0,表示任何实例都能运行job,如果只想某一实例运行的话,这里需要指定实例号,如果指定的是非法的数字或者为null,则会报ora-23319错误</li>
<li>force: 与instance相关,默认为false,表示instance指定的实例必须处于running状态,否则会报ora-23428错误,如果设置为true,则instance可接受任一数字作为job的实例</li>
</ul>
</blockquote>
<p>dbms_job功能上不如dbms_schduler强大,但也相对比较简单,下面是interval常用的examples:</p>
<p><strong>其中trunc(sysdate)+1跟trunc(sysdate+1)是相等的,sysdate表示当前的时间,sysdate+1,这里的单位是天,也就是第二天的当前时间,SYSDATE+1&#x2F;24则表示当前时间的下一个小时,1天24小时,所以1&#x2F;24表示一个小时,同理1&#x2F;24*60则表示1分钟等等</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">VARIABLE  jobno   NUMBER;</span><br><span class="line">VARIABLE  instno  NUMBER;</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">select</span> instance_number <span class="keyword">into</span> :instno <span class="keyword">from</span> v$instance;</span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">-- Submit job to begin at 0600 and run every hour.</span></span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    DBMS_JOB.SUBMIT (</span><br><span class="line">        :jobno</span><br><span class="line">      , <span class="string">&#x27;statspack.snap;&#x27;</span></span><br><span class="line">      , TRUNC(sysdate<span class="operator">+</span><span class="number">1</span>)<span class="operator">+</span><span class="number">6</span><span class="operator">/</span><span class="number">24</span></span><br><span class="line">      , <span class="string">&#x27;TRUNC(SYSDATE+1/24,&#x27;&#x27;HH&#x27;&#x27;)&#x27;</span></span><br><span class="line">      , <span class="literal">TRUE</span></span><br><span class="line">      , :instno);</span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">-- Submit job to begin at 0900 and run 12 hours later</span></span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    DBMS_JOB.SUBMIT (</span><br><span class="line">        :jobno</span><br><span class="line">      , <span class="string">&#x27;statspack.snap;&#x27;</span></span><br><span class="line">      , TRUNC(sysdate<span class="operator">+</span><span class="number">1</span>)<span class="operator">+</span><span class="number">9</span><span class="operator">/</span><span class="number">24</span></span><br><span class="line">      , <span class="string">&#x27;TRUNC(SYSDATE+12/24,&#x27;&#x27;HH&#x27;&#x27;)&#x27;</span></span><br><span class="line">      , <span class="literal">TRUE</span></span><br><span class="line">      , :instno);</span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">-- Submit job to begin at 0600 and run every 10 minutes</span></span><br><span class="line">    <span class="comment">-- ------------------------------------------------------------</span></span><br><span class="line">    DBMS_JOB.SUBMIT (</span><br><span class="line">        :jobno</span><br><span class="line">      , <span class="string">&#x27;statspack.snap;&#x27;</span></span><br><span class="line">      , TRUNC(sysdate<span class="operator">+</span><span class="number">1</span>)<span class="operator">+</span><span class="number">6</span><span class="operator">/</span><span class="number">24</span></span><br><span class="line">      , <span class="string">&#x27;TRUNC(sysdate+10/1440,&#x27;&#x27;MI&#x27;&#x27;)&#x27;</span></span><br><span class="line">      , <span class="literal">TRUE</span></span><br><span class="line">      , :instno);</span><br><span class="line">    <span class="comment">-- ----------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">-- Submit job to begin at 0600 and run every hour, Monday - Friday</span></span><br><span class="line">    <span class="comment">-- ----------------------------------------------------------------</span></span><br><span class="line">    DBMS_JOB.SUBMIT (</span><br><span class="line">        :jobno</span><br><span class="line">      , <span class="string">&#x27;statspack.snap;&#x27;</span></span><br><span class="line">      , TRUNC(sysdate<span class="operator">+</span><span class="number">1</span>)<span class="operator">+</span><span class="number">6</span><span class="operator">/</span><span class="number">24</span></span><br><span class="line">      , <span class="string">&#x27;TRUNC(</span></span><br><span class="line"><span class="string">             LEAST(</span></span><br><span class="line"><span class="string">                 NEXT_DAY(sysdate,&#x27;&#x27;MONDAY&#x27;&#x27;)</span></span><br><span class="line"><span class="string">               , NEXT_DAY(sysdate,&#x27;&#x27;TUESDAY&#x27;&#x27;)</span></span><br><span class="line"><span class="string">               , NEXT_DAY(sysdate,&#x27;&#x27;WEDNESDAY&#x27;&#x27;)</span></span><br><span class="line"><span class="string">               , NEXT_DAY(sysdate,&#x27;&#x27;THURSDAY&#x27;&#x27;)</span></span><br><span class="line"><span class="string">               , NEXT_DAY(sysdate,&#x27;&#x27;FRIDAY&#x27;&#x27;)</span></span><br><span class="line"><span class="string">             ) + 1/24</span></span><br><span class="line"><span class="string">        , &#x27;&#x27;HH&#x27;&#x27;)&#x27;</span></span><br><span class="line">      , <span class="literal">TRUE</span></span><br><span class="line">      , :instno);</span><br><span class="line">    <span class="keyword">COMMIT</span>;</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<p>从上面的例子我们可以看出,对于这样的需求从早上8点开始到下午17点结束,dbms_job就不如dbms_scheduler直观,</p>
<p>这里需要使用CASE子句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--oracle9i,每天早上8:00am到下午5:00pm这段高峰期,每隔一个小时生成一次snapshot</span></span><br><span class="line">VARIABLE id_job number;</span><br><span class="line">VARIABLE inst_num number;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">SELECT</span> instance_number <span class="keyword">INTO</span> :inst_num <span class="keyword">FROM</span> v$instance;</span><br><span class="line">	dbms_job.submit(</span><br><span class="line">		job <span class="operator">=</span><span class="operator">&gt;</span> id_job, <span class="comment">-- OUT; </span></span><br><span class="line">		what <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;statspack.snap;&#x27;</span>, </span><br><span class="line">        next_date <span class="operator">=</span><span class="operator">&gt;</span> TRUNC(SYSDATE<span class="operator">+</span><span class="number">1</span><span class="operator">/</span><span class="number">24</span>)</span><br><span class="line">        inverval <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;case when to_char(SYSDATE, &#x27;&#x27;hh24:mi&#x27;&#x27;) between &#x27;&#x27;08:00&#x27;&#x27; and &#x27;&#x27;17:00&#x27;&#x27; then TRUNC(SYSDATE+1/24,&#x27;&#x27;HH&#x27;&#x27;) else trunc(SYSDATE+1)+(8/24) end&#x27;</span></span><br><span class="line">        instance <span class="operator">=</span><span class="operator">&gt;</span> :inst_num</span><br><span class="line">      );</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="operator">/</span></span><br></pre></td></tr></table></figure>

<h4 id="DBMS-JOB其它相关函数"><a href="#DBMS-JOB其它相关函数" class="headerlink" title="DBMS_JOB其它相关函数"></a><strong>DBMS_JOB其它相关函数</strong></h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--dbms_job.run(job_id): job_id为dbms_job.submit()函数返回的job id号</span></span><br><span class="line"><span class="comment">--dbms_job.remove(job_id): 移除job</span></span><br><span class="line"><span class="comment">--dbms_job.change(...): 修改一个已经存在的job的所有属性</span></span><br><span class="line"><span class="comment">--dbms_job.what(job_id,what对应的命令):只修改指定job中执行的命令</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://dev-notes.com/code.php?q=109">Scheduling Jobs with Oracle 9i DBMS_JOB Package</a></li>
<li><a href="http://www.idevelopment.info/data/Oracle/DBA_tips/SQL/SQL_4.shtml">DBA Tips Archive for Oracle</a></li>
<li><a href="https://oracle-base.com/articles/10g/scheduler-10g">DBMS_SCHEDULER in Oracle Database 10g</a></li>
<li><a href="https://docs.oracle.com/cd/B10501_01/appdev.920/a96612/d_job.htm">DBMS_JOB官方文档</a></li>
<li><a href="https://docs.oracle.com/cd/A97630_01/server.920/a96521/jobq.htm">DBMS_JOB官方文档2</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Opentelemetry调研实践一(可观测性到底在说什么)</title>
    <url>/2021/10/19/OpenTelemetry-what-is-observability/</url>
    <content><![CDATA[<p>可观测性(Observability)并不是一个新词，而在几十年前被广泛地用于控制理论，用它来描述和理解⾃我调节系统。随着容器技术、微服务、⽆服务器迅速流行，使得系统间的访问越来越复杂，在云上、本地或两者上可能会运⾏数千个进程， 使用传统的监控技术和⼯具很难跟踪这些分布式架构中的通信路径和相互依赖关系。系统内部的可见性就变得非常重要。</p>
<p>那可观测性到底在说什么呢?</p>
<span id="more"></span>



<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ol>
<li>你的应用真的是可观测的吗？</li>
<li>日志系统只是用于查询日志的吗，有没有可能日志系统与监控系统是联动的？</li>
<li>可以很容易统计一条请求的总耗时，但如何统计具体某个函数的执行时间、该函数调用次数?</li>
</ol>
<p>上述3个问题如果你都能出”Yes”回答，那么作者建议直接滑走不用看下文。</p>
<p>如果你脱口而出，不就是日志平台跟监控系统能解决吗？</p>
<p>对，其实也不对</p>
<p>好，首先来说明一下可观测性跟监控系统的一些区别</p>
<h3 id="问题域"><a href="#问题域" class="headerlink" title="问题域"></a>问题域</h3><p>可观测性跟监控系统确实很像，可观测性跟监控系统本质上是一样的，都是在解决一个问题，：度量你的基础设施、平台和应用程序，以了解它是如何运行。</p>
<p>但两者的问题域却完全不同，<strong>监控告诉我们系统的哪些部分是工作的,可观测性告诉我们那里为什么不工作了.</strong></p>
<p><strong>度量</strong>,是个<code>程度</code>可深可浅的词，比如回到<strong>问题一: 你的应用是可观测的吗</strong>,很多人会给出肯定的回答，在某些人的理解中，不就是监控应用的状态吗？对于k8s应用来讲，prometheus就可以开箱即用地监控它，没错，状态是能够被监控的，通过监控系统我们可以知道某个时刻<code>it works</code></p>
<p>但是这个应用是可观测的吗？当然不是，因它在出现问题的时候，通过监控系统可能没办法判断它在哪个函数中crash了</p>
<p>如果要知道它哪里出了问题，那么就需要在应用内部实现可见性，通过埋点或者是字节码注入的方式，让应用暴露它的业务、性能指标，比如函数的时延、调用次数、调用错误等，借助这些指标再结合旁路分析系统就可以很清晰地展现应用的全貌。</p>
<p>这就属于可观测性</p>
<h3 id="三板斧"><a href="#三板斧" class="headerlink" title="三板斧"></a>三板斧</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20211008140156.png"></p>
<p>logging、trace、metrics是实现可观测性的三板斧,每一个都有很多的解决方案可选，</p>
<p>Logging中使用较多的有EFK、loki</p>
<p>Trace中有Jaeger、Tempo</p>
<p>Metrics中有Prometheus，Netdata</p>
<p>通过这些工具收集的数据统称为<strong>遥测数据</strong></p>
<p>这些工具基本开箱即用的特性，作者相信给很多企业带来了便利性。</p>
<p>想用好这三个工具业界还是有一些指导思想的，对于logging跟metrics很多人会相对熟悉一些，作者在这里想简单说一说trace，trace一般为全链路追踪，表示请求通过分布式系统的端到端的过程， trace有两个很重要的属性: </p>
<ol>
<li>TraceID: 一次请求中的所有路径都会共用一个唯一的TraceID，这个TraceID一般由初始节点产生，然后传递到所有节点</li>
<li>SpanID: TraceID只能够串起所有节点，但节点之间的调用顺序需要由SpanID来产生</li>
</ol>
<p>由于篇幅有限，trace将做为后续文章介绍的重点，不在这里展开</p>
<h3 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h3><p>为什么要实现可观测性呢?</p>
<p>可能有些人会提出: 我没有做任何可观测性方面的工作，系统也一直<code>work fine</code>呢</p>
<p>确实，在小公司或者系统不那么复杂的情况下，落地可观测性是有成本的，毕竟需要花人力物力去搭建那么些系统同时需要有可运维能力</p>
<p>但是如果业务链路足够长、逻辑足够复杂，长远来看，可观测性的落地是非常有必要的，有以下几点原因:</p>
<ol>
<li>现在到处吹捧云原生，从纯技术角度上来说，实现可观测性是趋势，也是技术追求</li>
<li>整个架构的可观测性对开发⼈员和运维⼯程师更友好、定位问题更快</li>
<li>可以更好地提供业务属性数据</li>
</ol>
<h3 id="痛苦"><a href="#痛苦" class="headerlink" title="痛苦"></a>痛苦</h3><p>同时，也会发现一个很痛苦的问题:这些开源项目大多是孤立的，之间没有多大的关联性，大公司可能有能力研发自己的可观测平台实现大一统，对于小公司来说，需要在多个平台之前进行切换，如果有架构调整，那切换的成本相信是巨大的。</p>
<p><strong>观测性能否做到一统呢?</strong></p>
<p>这也其实也是【思考】中第2个问题，简单地说，能不能通过日志系统中的关键字，跳转到trace系统中看这条日志所处的请求的所有链路。</p>
<p>答案是肯定的，grafana就做的非常不错，统一了agent，从logging、trace、metrics三个纬度进行采集，loki实现了日志的存储，新贵tempo实现trace的追踪，metrics则原生支持prometheus，grafana则提供了统一的可视化界面，从agent到view集成度非常高。</p>
<p>但grafana不是作者要介绍的主角，client端的实现才是作者要深入调研实践的主角，这就是CNCF牵头开源的新项目: <strong>OpenTelemetry</strong></p>
<p>总结一下，以上主要介绍了可观性测是什么以及为什么需要可观测性，但没有回答一个很重要的问题:<strong>如何实现可观测性</strong>，这其实是接下来要分享的内容，会详细介绍借用OpenTelemetry如何实现可观测性</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1801643">https://cloud.tencent.com/developer/article/1801643</a></li>
<li><a href="https://opentelemetry.io/">https://opentelemetry.io</a></li>
<li><a href="https://grafana.com/">https://grafana.com</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(prometheus operator使用)</title>
    <url>/2020/04/15/Prometheus-operator/</url>
    <content><![CDATA[<p>在很多的一段时间内, 业务一直都是使用单独的prometheus进行集群业务的监控,通过prometheus提供的服务发现能力, 在业务svc加入指定的annotations即可实现数据收集, 但还是会面临时常需要修改prometheus的配置文件(比如static scrape)，prometheus虽然也提供了配置热加载的能力, 久而久之也烦,如果能够实现配置的动态加载, 对运维人员无感操作, 是不是就方便地多, prometheus operator即可实现.</p>
<p>对于平台来说， 上面跑着若干产品线，prometheus operator能够将更多的自由度移位到业务侧, 比如抓取时间，超时时间等，同时也使得平台运维同事的工作量能够解脱出来.</p>
<span id="more"></span>



<p>需要对prometheus及operator的工作机制有所了解</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Prometheus Operator 架构图如下：</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200709213701.png"></p>
<p>其中<code>Operator</code>是最核心的部分，作为一个控制器，他会去创建<code>Prometheus</code>、<code>ServiceMonitor</code>、<code>AlertManager</code>以及<code>PrometheusRule</code>4个<code>CRD</code>资源对象，然后会一直监控并维持这4个资源对象的状态</p>
<p>其中创建的<code>prometheus</code>这种资源对象就是作为<code>Prometheus Server</code>存在, 而serviceMonitor则是通过label来选择需要监控的一组service对象, prometheus也通过label来选择serviceMonitor</p>
<p>因此，每个业务线都是可以使用单独的prometheus server，这在海量监控数据下是推荐的用法，如果平台监控量不是很大也可以使用一个prometheus server， 然后serviceMonitor都关联该prometheus即可</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>部署非常简单, 官方提供了一个yaml文件，只需要根据实际情况修改内容即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f bundle.yaml</span><br></pre></td></tr></table></figure>



<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="deployment-x2F-service"><a href="#deployment-x2F-service" class="headerlink" title="deployment&#x2F;service"></a>deployment&#x2F;service</h4><p>首先，需要一个deployment、service, 这里以最简单的nginx为例</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">example-app</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">example-app</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example-app</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">fabxc/instrumented_app</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-app</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">example-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">example-app</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这两个资源对象没什么好说的</p>
<h4 id="servicemonitor"><a href="#servicemonitor" class="headerlink" title="servicemonitor"></a>servicemonitor</h4><p>接下来定义一个<code>serviceMonitor</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceMonitor</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-app</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">team:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span> <span class="comment"># 这里指定关联的service， 即上面声明的nginx service 也有app: example-app</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">example-app</span></span><br><span class="line">  <span class="attr">endpoints:</span>  <span class="comment"># 这里指定需要从哪个端口抓取metrices, 这里使用端口名</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="string">web</span></span><br></pre></td></tr></table></figure>

<p>这样通过servicemonitor就将service暴露了出来, 那又将如何将其加入到prometheus呢</p>
<h4 id="prometheus"><a href="#prometheus" class="headerlink" title="prometheus"></a>prometheus</h4><p>这里就需要定义一个prometheus对象，一个prometheus对象就是一个prometheus server</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Prometheus</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">prometheus</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">prometheus</span></span><br><span class="line">  <span class="attr">serviceMonitorSelector:</span> <span class="comment"># 这里就指定需要关联的servicemonitor的label即上面声明的servicemonitor也有team: frontend</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">team:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">memory:</span> <span class="string">400Mi</span></span><br><span class="line">  <span class="attr">enableAdminAPI:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>这样创建之后就会生成一个statefulset的prometheus server的实例，同时动态生成scrape config</p>
<p>这里要说明的是, <code>如果是每个业务线单独使用prometheus, 则业务线自己去定义prometheus, 而如果是共用一个prometheus server,则可以事先将prometheus创建出来, 约定好servicemonitorselector所用的label, 然后要求所有业务线创建servicemonitor时使用这个label即可实现servicemonitor与prometheus自动关联</code></p>
<h4 id="alertmanager"><a href="#alertmanager" class="headerlink" title="alertmanager"></a>alertmanager</h4><p>上面是实现了prometheus抓取nginx的metrics，那又如何实现报警规则的添加呢?</p>
<p>先创建出一个alertmanager 实例出来，名字叫example, 通过alertmanager-examples可以访问</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Alertmanager</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span> <span class="comment"># 下文解释</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">alertmanager-example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">30903</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">9093</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">alertmanager:</span> <span class="string">example</span></span><br></pre></td></tr></table></figure>

<p>在传统的prometheus中都会在配置文件中指定alertmanager的地址, 同样, 使用operator的情况下也需要指定alertmanager，如下所示</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Prometheus</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span> <span class="comment"># 下文解释</span></span><br><span class="line">  <span class="attr">alerting:</span> <span class="comment"># 这里即绑定alertmanager</span></span><br><span class="line">    <span class="attr">alertmanagers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">alertmanager-example</span></span><br><span class="line">      <span class="attr">port:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">serviceMonitorSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">team:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">ruleSelector:</span>  <span class="comment"># 这里是关联prometheusRule的，见下文 </span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">alert-rules</span></span><br><span class="line">      <span class="attr">prometheus:</span> <span class="string">example</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="prometheusRule"><a href="#prometheusRule" class="headerlink" title="prometheusRule"></a>prometheusRule</h4><p>上面prometheus只是关联了alertmanager，但是还没有一个具体的告警规则, 这就需要使用prometheusrule了</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PrometheusRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">prometheus:</span> <span class="string">example</span></span><br><span class="line">    <span class="attr">role:</span> <span class="string">alert-rules</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">prometheus-example-rules</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">./example.rules</span></span><br><span class="line">    <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">ExampleAlert</span> <span class="comment"># 这里真正的告警规则</span></span><br><span class="line">      <span class="attr">expr:</span> <span class="string">vector(1)</span></span><br></pre></td></tr></table></figure>

<p>可以发现它的labels与prometheus中指定的ruleSelector是相同的，prometheus也正是通过ruleSelector也关联prometheusRule规则，所以如果需要再不回几个告警，使用prometheus需要的label即可.</p>
<p>prometheus operator也支持直接对pod进行监控, 使用podMonitor，原理是一样的，不再细说</p>
<h3 id="多副本"><a href="#多副本" class="headerlink" title="多副本"></a>多副本</h3><p>其实在上面的例子中, prometheus与alertmanager都可以使用多个副本部署, 实际上是以statefulset的形式运行</p>
<p>当指定<code>prometheus replicas 大于1时</code>,也就是说多个实例的configmwp文件是一样的，也就意味着多个实例需要抓取相同的配置，但是多个实例不一定会同时抓取，也就导致有某个时间点上数据不完全相同, 如何更好的解决prometheus高可用也官方的roadmap中</p>
<p>因此官方建议是使用服务分片的机制，可以按照业务区， 一个prometheus实例只管一个业务线, 当然如果有更高要加的场景下, 可以使用<code>thanos</code>架构来实现prometheus架构，不推荐使用prometheus的联邦方式</p>
<p>而对于alertmanager， operator在资源允许的情况下推荐<code>replicas 大于1</code>，alertmanager会以集群的方式运行，执行相同的配置，在prometheus产生的告警后，alertmanager实例之间会使用gossip协议来实现告警只触发一次， 避免告警在多个实例发送.</p>
<p>目前平台对告警数据的高可用并没有很大的需求，因此使用的是一个高配置的prometheus实例, 各业务线自己编写serviceMonitor及prometheusRule, 只需要包含特定的labels即可.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.qikqiak.com/post/first-use-prometheus-operator/">https://www.qikqiak.com/post/first-use-prometheus-operator/</a></li>
<li><a href="https://www.qikqiak.com/post/prometheus-operator-custom-alert/">https://www.qikqiak.com/post/prometheus-operator-custom-alert/</a></li>
<li><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/high-availability.md">https://github.com/coreos/prometheus-operator/blob/master/Documentation/high-availability.md</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(CKA及CKAD考试)</title>
    <url>/2019/12/08/Kubernetes%E5%AD%A6%E4%B9%A0(CKA%E5%8F%8ACKAD%E8%80%83%E8%AF%95)/</url>
    <content><![CDATA[<p>CKA及CKAD是CNCF专门为Kubernetes认证考虑，考试通过即被认为能熟练使用Kubernetes.</p>
<p>但这两门考试又有所不同.</p>
<span id="more"></span>

<p>惯例用图说话:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200117224635.png"></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200117224737.png"></p>
<h3 id="CKA"><a href="#CKA" class="headerlink" title="CKA"></a><strong>CKA</strong></h3><p>Certified Kubernetes Administrator, 从Administrator字眼也能看出这个考试更偏向于对于Kubernetes集群管理方面的要求，要对Kubernetes的原理需要有全面的认知，所以考试会有几道大题涉及的范围相对较广, 比如安装k8s集群，Bootstrap Node, 证书的使用等,有些知识点其它在工作上也用不太到，所以我们很容易忽略这些点，比如Bootstrap Token机制，想弄明白还是需要花点时间的，但是CKA考试也会考察.</p>
<p>CKA考试3小时, 24道题, 75%的正确率即可通过</p>
<h3 id="CKAD"><a href="#CKAD" class="headerlink" title="CKAD"></a><strong>CKAD</strong></h3><p>Certified Kubernetes Application Developer, 这门考试更倾向于能够在Kubernetes部署应用即可, 即能够熟练使用一些常用的资源对象即可，不需要像CKA那样还得会管理k8s集群</p>
<p>CKAD考试2小时，19道题, 66%的正确率即可通过</p>
<p>因为有保密协议，所以不能泄露考试题目，不过整体还是相对容易的，因为不需要考满分, 考试会比较严格, 时间比较紧,所以考试期间对题目的取舍很重要，另外，<strong>就算没过的话一年之内都可以免费补考一次.</strong></p>
<p>这两个考试出来也有几年了，从证书的编号来看，全球通过的人数不多，CKAD的更少，但现在出了很多的培训班，号称3天可以让一个从来没有接触过k8s的人通过考试，还是很恐怖的，这种明显就是对题目进行专门的训练</p>
<p>所以说， <strong>如果打算考的话还是早考点好，目前还是有点实用价值，等培训大军一到，人手一证，就没什么含金量了,当然，不屑于考证的同学请略过</strong></p>
<p>最后，每年的<strong>BlackMonday</strong>前后CNCF都会有促销活动，对这两门考试的考试费用的打折力度还是蛮大的，准备考试的同学可以注意下.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>理解NUMA架构</title>
    <url>/2022/06/02/System-Understanding-NUMA-Architecture/</url>
    <content><![CDATA[<p>最近在研究kubernetes下NUMA架构的支持, 看到一篇关于NUMA的博文,就尝试翻译成了中文, 原文地址: <a href="https://linuxhint.com/understanding_numa_architecture/">Understanding NUMA Architecture</a></p>
<span id="more"></span>



<p>设计电脑总是一种妥协。计算机的四个基本部件——中央处理单元(CPU)或处理器、内存、存储器和连接部件的电路板(I&#x2F;O总线系统)——被尽可能巧妙地组合在一起，以创造出一台既划算又强大的机器。设计过程主要涉及处理器(协处理器、多核设置)、内存类型和数量、存储(磁盘、文件系统)以及价格的优化。协同处理器和多核架构背后的理念是，<strong>在尽可能小的空间中，将操作分配到尽可能多的单个计算单元</strong>，并使并行执行计算指令更容易获得和负担得起。就内存而言，这是一个可以由单个计算单元处理的数量或大小的问题，以及哪种内存类型具有尽可能低的延迟。存储属于外部内存，其性能取决于磁盘类型、正在使用的文件系统、线程、传输协议、通信结构和附加的内存设备的数量。</p>
<p>I&#x2F;O总线的设计代表了计算机的主干，并显著地决定了上面列出的单个组件之间可以交换多少数据和多快的数据。排名第一的是用于高性能计算(HPC)领域的组件。截至2020年年中，高性能计算的当代代表产品有英伟达特斯拉和DGX、Radeon Instinct和英特尔Xeon Phi gpu加速器产品(产品对比见[引用1,2])。</p>
<h3 id="理解NUMA"><a href="#理解NUMA" class="headerlink" title="理解NUMA"></a>理解NUMA</h3><p>非统一内存访问(NUMA)描述了当代多处理系统中使用的共享内存架构。NUMA是一个由多个节点组成的计算系统，所有节点共享聚合的内存:<strong>每个CPU被分配自己的本地内存，并且可以从系统中的其他CPU访问内存</strong>，见[引用12,7]。</p>
<p>NUMA是一个巧妙的系统，用于将多个中央处理单元(CPU)连接到计算机上任何数量的可用内存。单个NUMA节点通过可伸缩网络(I&#x2F;O总线)连接，这样CPU就可以系统地访问与其他NUMA节点关联的内存。</p>
<p>本地内存是CPU在特定NUMA节点中使用的内存。外部或远程内存是CPU从另一个NUMA节点获取的内存。术语NUMA比率描述了访问外部内存成本与访问本地内存成本的比率。比例越大，成本就越大，因此访问内存所需的时间就越长。</p>
<p>但是，它所花费的时间比CPU访问它自己的本地内存要长。本地内存访问是一个主要的优势，因为它结合了低延迟和高带宽。相比之下，访问属于任何其他CPU的内存具有更高的延迟和更低的带宽性能。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220602110808.png"></p>
<h3 id="回顾-共享内存多处理器的发展"><a href="#回顾-共享内存多处理器的发展" class="headerlink" title="回顾:共享内存多处理器的发展"></a>回顾:共享内存多处理器的发展</h3><p>Frank Dennemann[引用8]指出，现代系统架构不允许真正的统一内存访问(UMA)，即使这些系统是专门为此目的而设计的。简单地说，并行计算的思想是让一组处理器协同计算给定的任务，从而加快传统的顺序计算。</p>
<p>正如Frank Dennemann[引用8]所解释的那样，在20世纪70年代早期，随着关系数据库系统的引入，“对能够服务多个并发用户操作和过多数据生成的系统的需求成为主流”。“尽管单处理器的性能令人印象深刻，但多处理器系统能够更好地处理这种工作负载。为了提供性价比高的系统，共享内存地址空间成为研究的重点。早期，使用交叉开关的系统被提倡，然而随着这种设计的复杂性随着处理器的增加而增加，这使得基于总线的系统更有吸引力。总线系统中的处理器(可以)通过在总线上发送请求来访问整个内存空间，这是一种尽可能优化地使用可用内存的非常划算的方式。”</p>
<p>然而，基于总线的计算机系统有一个瓶颈——<strong>有限的带宽会导致可伸缩性问题</strong>。系统中添加的cpu越多，每个节点可用的带宽就越少。此外，添加的cpu越多，总线就越长，因此延迟就越高。</p>
<p>大多数cpu都是在二维平面上构建的。cpu还必须添加集成内存控制器。对于每个CPU核心，有四个内存总线(上、下、左、右)的简单解决方案允许完全可用的带宽，但仅此而已。cpu在很长一段时间内都停滞在4核状态。当芯片变成3D时，在上面和下面添加痕迹允许直接总线穿过对角线相反的cpu。在卡上放置一个四核CPU，然后连接到总线，这是合乎逻辑的下一步。</p>
<p>如今，每个处理器都包含许多核心，这些核心都有一个共享的片上缓存和片外内存，并且在服务器内不同内存部分的内存访问成本是可变的。</p>
<p>提高数据访问效率是当前CPU设计的主要目标之一。每个CPU核都被赋予了一个较小的一级缓存(32 KB)和一个较大的二级缓存(256 KB)。各个核心随后共享几个MB的3级缓存，其大小随着时间的推移而大幅增长。</p>
<p>为了避免缓存丢失(请求不在缓存中的数据)，需要花费大量的研究时间来寻找合适的CPU缓存数量、缓存结构和相应的算法。关于缓存snoop协议[引用4]和缓存一致性[引用3,5]的更详细的解释，以及NUMA背后的设计思想，请参见[引用8]。</p>
<h3 id="NUMA的软件支持"><a href="#NUMA的软件支持" class="headerlink" title="NUMA的软件支持"></a>NUMA的软件支持</h3><p>有两种软件优化措施可以提高支持NUMA架构的系统的性能—处理器关联性和数据放置。正如[引用19]中解释的那样，“处理器关联[…]允许将进程或线程绑定到单个CPU或一系列CPU，以便进程或线程只在指定的CPU或CPU上执行，而不是在任何CPU上执行。”术语“数据放置”是指将代码和数据尽可能地保存在内存中的软件修改。</p>
<p>不同的UNIX和UNIX相关操作系统通过以下方式支持NUMA(以下列表来自[引用14]):</p>
<ul>
<li>Silicon Graphics IRIX支持ccNUMA架构超过1240 CPU与Origin服务器系列。</li>
<li>Microsoft Windows 7和Windows Server 2008 R2增加了对64个逻辑核上NUMA架构的支持。</li>
<li>Linux内核的2.5版本已经包含了基本的NUMA支持，在后续的内核版本中得到了进一步的改进。Linux内核的3.8版带来了新的NUMA基础，允许在以后的内核版本[引用13]中开发更有效的NUMA策略。Linux内核的3.13版本带来了大量的策略，旨在将进程放在靠近其内存的位置，以及对各种情况的处理，例如在进程之间共享内存页，或使用透明的巨大页;新的系统控制设置允许启用或禁用NUMA平衡，以及配置各种NUMA内存平衡参数[引用15]。</li>
<li>Oracle和OpenSolaris都采用了引入逻辑组的NUMA架构。</li>
<li>FreeBSD在11.0版本中添加了初始NUMA亲和性和策略配置。</li>
</ul>
<p>蔡宁在《Computer Science and Technology, Proceedings of the International Conference (CST2016)》一书中提出NUMA架构的研究主要集中在高端计算环境，提出了NUMA-aware Radix Partitioning (NaRP)，优化NUMA节点中的共享缓存的性能，以加速商业智能应用程序。因此，NUMA代表共享内存(SMP)系统与几个处理器[引用6]之间的中间地带。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20220602110349.png"></p>
<h3 id="NUMA和Linux"><a href="#NUMA和Linux" class="headerlink" title="NUMA和Linux"></a>NUMA和Linux</h3><p>如上所述，Linux内核从版本2.5开始就支持NUMA了。Debian GNU&#x2F;Linux和Ubuntu都提供numactl[引用16]和numad[引用17]两个软件包对进程优化的NUMA支持。在numactl命令的帮助下，您可以列出系统[引用18]中可用NUMA节点的清单:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># numactl --hardware</span></span><br><span class="line">available: 2 nodes (0-1)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23</span><br><span class="line">node 0 size: 8157 MB</span><br><span class="line">node 0 free: 88 MB</span><br><span class="line">node 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31</span><br><span class="line">node 1 size: 8191 MB</span><br><span class="line">node 1 free: 5176 MB</span><br><span class="line">node distances:</span><br><span class="line">node 0 1</span><br><span class="line">0: 10 20</span><br><span class="line">1: 20 10</span><br></pre></td></tr></table></figure>

<p>NumaTop是Intel开发的一个有用的工具，用于监视运行时内存位置和分析NUMA系统中的进程[引用10,11]。该工具可以识别潜在的NUMA相关性能瓶颈，从而帮助重新平衡内存&#x2F;CPU分配，以最大限度地发挥NUMA系统的潜力。有关更详细的描述，请参阅[引用9]。</p>
<p><img src="https://linuxhint.com/wp-content/uploads/2020/10/understanding_numa_architecture_1.jpg" alt="img"></p>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>支持NUMA技术的计算机允许所有cpu直接访问整个内存——cpu将其视为一个单一的线性地址空间。这可以更有效地使用64位寻址方案，从而更快地移动数据、更少地复制数据、更容易地进行编程。</p>
<p>NUMA系统对于服务器端应用程序非常有吸引力，比如数据挖掘和决策支持系统。此外，有了这种架构，为游戏和高性能软件编写应用程序变得更加容易。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>总之，NUMA架构解决了可伸缩性问题，这是它的主要优点之一。在NUMA CPU中，一个节点将拥有更高的带宽或更低的延迟来访问同一节点上的内存(例如，本地CPU在远程访问的同时请求内存访问;优先级在本地CPU上)。如果将数据本地化到特定的进程(以及处理器)，这将显著提高内存吞吐量。缺点是将数据从一个处理器移动到另一个处理器的成本较高。只要这种情况不经常发生，NUMA系统将优于具有更传统架构的系统</p>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ol>
<li>Compare NVIDIA Tesla vs. Radeon Instinct, <a href="https://www.itcentralstation.com/products/comparisons/nvidia-tesla_vs_radeon-instinct">https://www.itcentralstation.com/products/comparisons/nvidia-tesla_vs_radeon-instinct</a></li>
<li>Compare NVIDIA DGX-1 vs. Radeon Instinct, <a href="https://www.itcentralstation.com/products/comparisons/nvidia-dgx-1_vs_radeon-instinct">https://www.itcentralstation.com/products/comparisons/nvidia-dgx-1_vs_radeon-instinct</a></li>
<li>Cache coherence, Wikipedia, <a href="https://en.wikipedia.org/wiki/Cache_coherence">https://en.wikipedia.org/wiki/Cache_coherence</a></li>
<li>Bus snooping, Wikipedia, <a href="https://en.wikipedia.org/wiki/Bus_snooping">https://en.wikipedia.org/wiki/Bus_snooping</a></li>
<li>Cache coherence protocols in multiprocessor systems, Geeks for geeks, <a href="https://www.geeksforgeeks.org/cache-coherence-protocols-in-multiprocessor-system">https://www.geeksforgeeks.org/cache-coherence-protocols-in-multiprocessor-system</a>&#x2F;</li>
<li>Computer science and technology – Proceedings of the International Conference (CST2016), Ning Cai (Ed.), World Scientific Publishing Co Pte Ltd, ISBN: 9789813146419</li>
<li>Daniel P. Bovet and Marco Cesati: Understanding NUMA architecture in Understanding the Linux Kernel, 3rd edition, O’Reilly, <a href="https://www.oreilly.com/library/view/understanding-the-linux/0596005652/">https://www.oreilly.com/library/view/understanding-the-linux/0596005652/</a></li>
<li>Frank Dennemann: NUMA Deep Dive Part 1: From UMA to NUMA, <a href="https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/">https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/</a></li>
<li>Colin Ian King: NumaTop: A NUMA system monitoring tool, <a href="https://smackerelofopinion.blogspot.com/2015/09/numatop-numa-system-monitoring-tool.html">http://smackerelofopinion.blogspot.com/2015/09/numatop-numa-system-monitoring-tool.html</a></li>
<li>Numatop, <a href="https://github.com/intel/numatop">https://github.com/intel/numatop</a></li>
<li>Package numatop for Debian GNU&#x2F;Linux, <a href="https://packages.debian.org/buster/numatop">https://packages.debian.org/buster/numatop</a></li>
<li>Jonathan Kehayias: Understanding Non-Uniform Memory Access&#x2F;Architectures (NUMA), <a href="https://www.sqlskills.com/blogs/jonathan/understanding-non-uniform-memory-accessarchitectures-numa/">https://www.sqlskills.com/blogs/jonathan/understanding-non-uniform-memory-accessarchitectures-numa/</a></li>
<li>Linux Kernel News for Kernel 3.8, <a href="https://kernelnewbies.org/Linux_3.8">https://kernelnewbies.org/Linux_3.8</a></li>
<li>Non-uniform memory access (NUMA), Wikipedia, <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">https://en.wikipedia.org/wiki/Non-uniform_memory_access</a></li>
<li>Linux Memory Management Documentation, NUMA, <a href="https://www.kernel.org/doc/html/latest/vm/numa.html">https://www.kernel.org/doc/html/latest/vm/numa.html</a></li>
<li>Package numactl for Debian GNU&#x2F;Linux, <a href="https://packages.debian.org/sid/admin/numactl">https://packages.debian.org/sid/admin/numactl</a></li>
<li>Package numad for Debian GNU&#x2F;Linux, <a href="https://packages.debian.org/buster/numad">https://packages.debian.org/buster/numad</a></li>
<li>How to find if NUMA configuration is enabled or disabled?, <a href="https://www.thegeekdiary.com/centos-rhel-how-to-find-if-numa-configuration-is-enabled-or-disabled/">https://www.thegeekdiary.com/centos-rhel-how-to-find-if-numa-configuration-is-enabled-or-disabled/</a></li>
<li>Processor affinity, Wikipedia, <a href="https://en.wikipedia.org/wiki/Processor_affinity">https://en.wikipedia.org/wiki/Processor_affinity</a></li>
</ol>
<h3 id="感谢"><a href="#感谢" class="headerlink" title="感谢"></a>感谢</h3><p>作者要感谢Gerold Rupprecht在准备本文时的支持。</p>
<h3 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h3><blockquote>
<ul>
<li>普莱克斯·内翰达(Plaxedes Nehanda)是一位多才多艺、自我驱动的多面手，他身兼多种职务，其中包括活动策划、虚拟助理、转录员以及热心的研究员，现居南非约翰内斯堡。</li>
<li>Prince K. Nehanda是位于津巴布韦哈拉雷的paflow Metering公司的仪器和控制(计量)工程师。</li>
<li>弗兰克·霍夫曼(Frank Hofmann)在路上工作——最好是来自德国柏林、瑞士日内瓦和南非开普敦——他是Linux- user和Linux Magazine等杂志的开发者、培训师和作者。他也是Debian包管理书籍(<a href="http://www.dpmb.org)的合著者./">http://www.dpmb.org)的合著者。</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title>使用crash查看Linux系统异常重启原因</title>
    <url>/2020/09/27/System-kernel-panic-debug/</url>
    <content><![CDATA[<p>最近有一台物理机间歇性的发生重启, 本以为是偶发事件就没多在意，今天又给重启了, 因此决定探探究竟</p>
<span id="more"></span>

<p>首先看一下kdump服务是否开启, kdump主要用于内核发生crash时记录相关上下文的. 用来转储运行内存的一个工具</p>
<p>简言之: 系统一旦崩溃，内核就没法正常工作了，这个时候将由kdump提供一个用于捕获当前运行信息的内核，</p>
<p>该内核会将此时内存中的所有运行状态和数据信息收集到一个dump core文件中以便之后分析崩溃原因</p>
<p>之后系统便会重启.</p>
<p>关于kdump在内核已经崩溃的前提下还能够捕获到异常信息, 原理可<a href="https://blog.csdn.net/zhangskd/article/details/38084337">参考</a></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927180148.png"></p>
<p>这里引用<a href="https://blog.csdn.net/zhangskd/article/details/38084337">一张图</a></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927185450.png"></p>
<p>要分析core文件，可以使用crash工具，首先是安装:</p>
<p>注意, 要安装与当前内核对应的版本, 要不然crash无法使用.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-693.el7.x86_64.rpm</span><br><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-693.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-693.el7.x86_64.rpm</span><br><span class="line">rpm -ivh kernel-debuginfo-3.10.0-693.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">yum install crash</span><br></pre></td></tr></table></figure>

<p>通常, 内核crash后保存的core文件保存在<code>/var/crash</code>目录下，在该目录下找到最近一次crash的时间，使用以下命令启动crash分析dump.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crash vmcore /usr/lib/debug/lib/modules/3.10.0-693.el7.x86_64/vmlinux</span><br></pre></td></tr></table></figure>

<p>启动之后就进入到了crash命令行，很明显地显示出crash的原因,如下图红框所示:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927175237.png"></p>
<p>这里简要地反应出内核crash时的一些内存信息快照，包含一些物理信息, tasks数量，最重要的就是导致crash的command与panic的原因</p>
<p>这里可直接使用ps查看进程的相关信息,跟linux 命令行下的ps功能相似</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927175207.png"></p>
<p>还可以使用bt命令来查看trace信息, 当然都是一些汇编层的内容，需要有一定的专业知识，理解起来比较困难.</p>
<p>从上面已经知道了panic的原因后就可对症下药.</p>
<p>另外crash支持很多的子命令,大家可使用man crash查看，比较常用的为</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crash ps <span class="comment"># 查看进程</span></span><br><span class="line">crash sys <span class="comment"># 查看系统信息</span></span><br><span class="line">crash bt <span class="comment"># 查看堆栈信息</span></span><br><span class="line">crash files <span class="comment"># 查看异常时进程打开的文件</span></span><br><span class="line">crash task <span class="comment"># 查看指定task的信息</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.linuxtechi.com/how-to-enable-kdump-on-rhel-7-and-centos-7/">https://www.linuxtechi.com/how-to-enable-kdump-on-rhel-7-and-centos-7/</a></li>
<li><a href="https://www.cnblogs.com/doctormo/p/12619485.html">https://www.cnblogs.com/doctormo/p/12619485.html</a></li>
<li><a href="https://blog.csdn.net/zhangskd/article/details/38084337">https://blog.csdn.net/zhangskd/article/details/38084337</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下使用lvm将多块盘合并</title>
    <url>/2020/09/15/System-use-lvm-manager-disks/</url>
    <content><![CDATA[<p>需求: 将vdb vdc这两个500G的盘合并成一个1000G的盘，然后新建一个目录挂载到大盘上，当大盘出现磁盘紧张的时候还可以自动扩容.</p>
<p>由于部门里有基础服务的同事，很少有机会直接接触lvm，刚好最近有几台物理服务器，借这个机会，就尝试自己实践了一番</p>
<span id="more"></span>



<h3 id="lsblk"><a href="#lsblk" class="headerlink" title="lsblk"></a>lsblk</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用lsblk 查看当前挂载的磁盘</span></span><br><span class="line">lsblk</span><br><span class="line"></span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0   50G  0 disk</span><br><span class="line">└─vda1 253:1    0   50G  0 part /</span><br><span class="line">vdb    253:16   0  500G  0 disk /data</span><br><span class="line">vdc    253:32   0  500G  0 disk</span><br></pre></td></tr></table></figure>

<p>这里将vdb vdc这两个500G的盘合并成一个1000G的盘，然后新建一个目录挂载到大盘上</p>
<h3 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用fdisk进行盘的格式化</span></span><br><span class="line">fdisk /dev/vdb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下是交互输出结果</span></span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">Building a new DOS disklabel with disk identifier 0xadfbfcb4.</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n <span class="comment"># 新建分区</span></span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p  <span class="comment"># 待定主分区</span></span><br><span class="line">Partition number (1-4, default 1): 1 <span class="comment"># 序号</span></span><br><span class="line">First sector (2048-1048575999, default 2048): <span class="comment"># 直接回车</span></span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-1048575999, default 1048575999): <span class="comment"># 直接回车</span></span><br><span class="line">Using default value 1048575999</span><br><span class="line">Partition 1 of <span class="built_in">type</span> Linux and of size 500 GiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): p <span class="comment"># 确认分区情况</span></span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk label <span class="built_in">type</span>: dos</span><br><span class="line">Disk identifier: 0xadfbfcb4</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vdb1            2048  1048575999   524286976   83  Linux</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): t <span class="comment"># 选择系统id</span></span><br><span class="line">Selected partition 1</span><br><span class="line">Hex code (<span class="built_in">type</span> L to list all codes): 8e <span class="comment"># 8e指定的是使用LVM</span></span><br><span class="line">Changed <span class="built_in">type</span> of partition <span class="string">&#x27;Linux&#x27;</span> to <span class="string">&#x27;Linux LVM&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w <span class="comment"># 保存</span></span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br></pre></td></tr></table></figure>

<p>上面用注释标注的是需要在交互提示时输入的参数，完成之后使用fdisk -l查看确认</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201014114920.png"></p>
<p>使用相同的办法处理<code>/dev/vdc</code></p>
<h3 id="LVM"><a href="#LVM" class="headerlink" title="LVM"></a>LVM</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pvcreate如果提示命令不存在，则需要安装lvm2</span></span><br><span class="line">yum install lvm2 -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pvcreate 创建物理卷</span></span><br><span class="line">pvcreate /dev/vdb1</span><br><span class="line"><span class="comment"># 提示 Physical volume &quot;/dev/vdb1&quot; successfully created.</span></span><br><span class="line"><span class="comment"># 使用pvs或者 pvdisplay 查看结果</span></span><br><span class="line">pvs</span><br><span class="line"><span class="comment">#  PV         VG Fmt  Attr PSize    PFree</span></span><br><span class="line"><span class="comment">#  /dev/vdb1     lvm2 ---  &lt;500.00g &lt;500.00g</span></span><br><span class="line"><span class="comment"># 使用同样的办法处理/dev/vdc1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用vgcreate创建卷组，用于管理PV以及LV, 这里vgdata是随意的名字</span></span><br><span class="line">vgcreate vgdata /dev/vdc1 /dev/vdb1  </span><br><span class="line"><span class="comment"># 使用vgs 查看vg, vgdisplay的信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用lvcreate创建逻辑卷</span></span><br><span class="line"><span class="comment"># -L 后面是大小， -n 后面是逻辑卷名称， vgdata对应上面的卷组</span></span><br><span class="line">lvcreate -L 900G -n data vgdata</span><br><span class="line"><span class="comment"># 使用lvdisplay 查看结果</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201014115754.png"></p>
<h3 id="Mkfs"><a href="#Mkfs" class="headerlink" title="Mkfs"></a>Mkfs</h3><p>完成上述操作后，对该卷组进行格式化，这里格式化成xfs</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 格式化成xfs, /dev/vgdata/data为上面 LV Path</span></span><br><span class="line">mkfs.xfs /dev/vgdata/data</span><br></pre></td></tr></table></figure>

<p>再次查看会发现vgdata-data确实是900G了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@spring-7 /]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda             253:0    0   50G  0 disk</span><br><span class="line">└─vda1          253:1    0   50G  0 part /</span><br><span class="line">vdb             253:16   0  500G  0 disk</span><br><span class="line">└─vdb1          253:17   0  500G  0 part</span><br><span class="line">  └─vgdata-data 252:0    0  900G  0 lvm</span><br><span class="line">vdc             253:32   0  500G  0 disk</span><br><span class="line">└─vdc1          253:33   0  500G  0 part</span><br><span class="line">  └─vgdata-data 252:0    0  900G  0 lvm</span><br></pre></td></tr></table></figure>



<h3 id="Mount"><a href="#Mount" class="headerlink" title="Mount"></a>Mount</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /data</span><br><span class="line">mount /dev/vgdata/data /data</span><br></pre></td></tr></table></figure>

<p>查看:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@spring-7 data]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda             253:0    0   50G  0 disk</span><br><span class="line">└─vda1          253:1    0   50G  0 part /</span><br><span class="line">vdb             253:16   0  500G  0 disk</span><br><span class="line">└─vdb1          253:17   0  500G  0 part</span><br><span class="line">  └─vgdata-data 252:0    0  900G  0 lvm  /data</span><br><span class="line">vdc             253:32   0  500G  0 disk</span><br><span class="line">└─vdc1          253:33   0  500G  0 part</span><br><span class="line">  └─vgdata-data 252:0    0  900G  0 lvm  /data</span><br></pre></td></tr></table></figure>



<h3 id="Auto-mount"><a href="#Auto-mount" class="headerlink" title="Auto mount"></a>Auto mount</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为了防止机器重启，需要将挂载操作写入到/etc/fstab中，加入下面一行即可</span></span><br><span class="line">/dev/vgdata/data                          /data                   xfs      defaults        1 1</span><br></pre></td></tr></table></figure>



<p>整个过程倒是不复杂, 以后有这需要直接上个自动化脚本, 非常方便.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://juejin.im/post/6844903959199481869">https://juejin.im/post/6844903959199481869</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(编写业务Exporter)</title>
    <url>/2020/01/19/Prometheus-Write-Exporter/</url>
    <content><![CDATA[<p>Prometheus官网维护了大量的Exporter, 很多都是开箱即用，非常方便，但有时业务要接入Prometheus时, 还需要业务端开发Exporter,好在Prometheus定制了一套流程的Client开发流程，非常容易接入.</p>
<span id="more"></span>

<h3 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a><strong>实现方式</strong></h3><p>编写 Exporter 的两种方式，分别是：</p>
<ul>
<li>自己的代码定时刷新值</li>
<li>在 Prometheus 抓取的时候实时获取值</li>
</ul>
<p>这两种方式本质上没有差别，但是在采集时有一些差别：</p>
<ul>
<li>方式一可以以几乎忽略的延迟返回监控数据, 但无法保证数据是最新值.</li>
<li>方式二可能会因为一些难以获取的值而超时或者很久才会返回值</li>
</ul>
<p>一般情况下都是使用第二种方式,只需要我们在业务代码中实现<strong>Collect接口</strong>即可</p>
<p>Prometheus的官网写明了实现自己的exporter需要的流程, 看<a href="https://prometheus.io/docs/instrumenting/writing_exporters/">writing_exporters</a></p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a><strong>Example</strong></h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/prometheus/client_golang/prometheus&quot;</span></span><br><span class="line">    <span class="string">&quot;net/http&quot;</span></span><br><span class="line">    <span class="string">&quot;math/rand&quot;</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    <span class="comment">//版本信息</span></span><br><span class="line">    version = prometheus.NewGauge(prometheus.GaugeOpts&#123;</span><br><span class="line">        Name: <span class="string">&quot;fake_version&quot;</span>,</span><br><span class="line">        Help: <span class="string">&quot;Version information about this binary&quot;</span>,</span><br><span class="line">        ConstLabels: <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;</span><br><span class="line">            <span class="string">&quot;version&quot;</span>: <span class="string">&quot;v1.0&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;)</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">type</span> Exporter <span class="keyword">struct</span> &#123;</span><br><span class="line">    gauge          prometheus.Gauge</span><br><span class="line">    gaugeVec       prometheus.GaugeVec</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewExporter</span><span class="params">(metricsPrefix <span class="type">string</span>)</span></span> *Exporter &#123;</span><br><span class="line">    gauge := prometheus.NewGauge(prometheus.GaugeOpts&#123;</span><br><span class="line">        Namespace: metricsPrefix,</span><br><span class="line">        Name:      <span class="string">&quot;gauge_metric&quot;</span>,</span><br><span class="line">        Help:      <span class="string">&quot;This is a gauge metric&quot;</span>&#125;)</span><br><span class="line"> </span><br><span class="line">    gaugeVec := *prometheus.NewGaugeVec(prometheus.GaugeOpts&#123;</span><br><span class="line">        Namespace: metricsPrefix,</span><br><span class="line">        Name:      <span class="string">&quot;gauge_vec_metric&quot;</span>,</span><br><span class="line">        Help:      <span class="string">&quot;This is a gauga vece metric&quot;</span>&#125;,</span><br><span class="line">        []<span class="type">string</span>&#123;<span class="string">&quot;myLabel&quot;</span>&#125;)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> &amp;Exporter&#123;</span><br><span class="line">        gauge:        gauge,</span><br><span class="line">        gaugeVec:     gaugeVec,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *Exporter)</span></span> Collect(ch <span class="keyword">chan</span>&lt;- prometheus.Metric) &#123;</span><br><span class="line">        <span class="comment">// --------</span></span><br><span class="line">        <span class="comment">// 业务逻辑</span></span><br><span class="line">        timestamp := time.Now().Unix()</span><br><span class="line">        fmt.Println(<span class="string">&quot;Timestamp: &quot;</span>, timestamp)</span><br><span class="line">        rand.Seed(timestamp)</span><br><span class="line">        ranint := rand.Intn(<span class="number">10000</span>)</span><br><span class="line">        fmt.Println(<span class="string">&quot;Random: &quot;</span>, ranint)</span><br><span class="line">        e.gauge.Set(<span class="type">float64</span>(timestamp))</span><br><span class="line">        e.gaugeVec.WithLabelValues(<span class="string">&quot;helloworld&quot;</span>).Set(<span class="type">float64</span>(ranint))</span><br><span class="line">        <span class="comment">// --------</span></span><br><span class="line">        <span class="comment">// Called use a concurrency safe way</span></span><br><span class="line">        e.gauge.Collect(ch)</span><br><span class="line">        e.gaugeVec.Collect(ch)</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// metric 描述, 可被重写</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *Exporter)</span></span> Describe(ch <span class="keyword">chan</span>&lt;- *prometheus.Desc) &#123;</span><br><span class="line">    e.gauge.Describe(ch)</span><br><span class="line">    e.gaugeVec.Describe(ch)</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">   fmt.Println(<span class="string">`</span></span><br><span class="line"><span class="string">        prometheus exporter example,</span></span><br><span class="line"><span class="string">        metrics expose at http://:18081/metrics</span></span><br><span class="line"><span class="string">    `</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Define parameters</span></span><br><span class="line">    metricsPath := <span class="string">&quot;/metrics&quot;</span></span><br><span class="line">    listenAddress := <span class="string">&quot;0.0.0.0:18081&quot;</span></span><br><span class="line">    metricsPrefix := <span class="string">&quot;fake&quot;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Register exporter to Prometheus, call Collect</span></span><br><span class="line">    exporter := NewExporter(metricsPrefix)</span><br><span class="line">    prometheus.MustRegister(exporter)</span><br><span class="line">    prometheus.MustRegister(version)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Launch http service</span></span><br><span class="line">    http.Handle(metricsPath, prometheus.Handler())</span><br><span class="line">    http.Handle(<span class="string">&quot;/&quot;</span>, prometheus.Handler())</span><br><span class="line">    fmt.Println(http.ListenAndServe(listenAddress, <span class="literal">nil</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这是个最简单的例子，Prometheus官网推荐我们参考haproxy_exporter实现,大家可参考<a href="https://github.com/prometheus/haproxy_exporter">这里</a></p>
<p>Node_exporter算是比较复杂的实现，有兴趣也可以看看.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/prometheus/haproxy_exporter">haproxy_exporter</a></li>
<li><a href="https://prometheus.io/docs/instrumenting/writing_exporters/">writing_exporters</a></li>
<li><a href="http://terrence.logdown.com/posts/6867841">新手玩GO語言，自己動手寫一個prometheus的exporter</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP协议中四次分手的time_wait状态</title>
    <url>/2017/11/12/TCP%E5%8D%8F%E8%AE%AE%E4%B8%AD%E5%9B%9B%E6%AC%A1%E5%88%86%E6%89%8B%E7%9A%84time_wait%E7%8A%B6%E6%80%81/</url>
    <content><![CDATA[<p>今天在测试服务器上压测时发现并发数不在正常范围,使用netstat查看tcp连接存在很多连接的状态为time_wait状态,比established多很多,这是个不正常的现象,time_wait是什么鬼？</p>
<p><code>netstat -n | awk &#39;/^tcp/ &#123;++state[$NF]&#125; END &#123;for(key in state) print key,&quot;\t&quot;,state[key]&#125;&#39;</code></p>
<span id="more"></span>

<h3 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a><strong>三次握手</strong></h3><p>这里还得从tcp的四次分手说起,在这之前,稍微带上三次握手的状态转变:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/tcp_timewait1.png" alt="tcp_timewait1"></p>
<p>三次握手不是这篇笔记的重点,这里只扯一个问题:</p>
<p><code>为什么需要3次握手而不是2次就够?</code></p>
<p>以下引用谢希仁版《计算机网络》中的话:</p>
<p>“已失效的连接请求报文段”的产生在这样一种情况下:client发出的第一个连接请求报文段并没有丢失,而是在某个网络结点长时间的滞留了,以致延误到连接释放以后的某个时间才到达server.本来这是一个早已失效的报文段.但server收到此失效的连接请求报文段后,就误认为是client再次发出的一个新的连接请求.于是就向client发出确认报文段,同意建立连接.假设不采用“三次握手”,那么只要server发出确认,新的连接就建立了.由于现在client并没有发出建立连接的请求,因此不会理睬server的确认,也不会向server发送数据.但server却以为新的运输连接已经建立,并一直等待client发来数据.这样,server的很多资源就白白浪费掉了.采用“三次握手”的办法可以防止上述现象发生.例如刚才那种情况,client不会向server的确认发出确认.server由于收不到确认,就知道client并没有要求建立连接.”</p>
<p>总结一句话就是:</p>
<p><strong>防止已经失效的请求再次到达服务端导致服务器连接资源浪费.</strong></p>
<p>注:失效的连接请求：若客户端向服务端发送的连接请求丢失,客户端等待应答超时后就会再次发送连接请求,此时,上一个连接请求就是『失效的』</p>
<h3 id="四次分手"><a href="#四次分手" class="headerlink" title="四次分手"></a><strong>四次分手</strong></h3><p>tcp的四次分手过程如下:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/tcp_timewait2.png" alt="tcp_timewait2"></p>
<p>这里说一下,在实际环境下,FIN-WAIT1的状态其实很难看见,这是因为当有一端请求close时,会很快得到对方ACK,所以FIN-WAIT1有状态很快的会变成FIN-WAIT2,我们这里重点关注Time-Wait状态的2MSL(Maximum Segment Lifetime )</p>
<h3 id="TIME-WAIT"><a href="#TIME-WAIT" class="headerlink" title="TIME-WAIT"></a><strong>TIME-WAIT</strong></h3><p>每个具体TCP实现必须选择一个报文段最大生存时间MSL(Maximum Segment Lifetime),它是任何报文段被丢弃前在网络内的最长时间,我们知道这个时间是有限的,因为TCP报文段以IP数据报在网络内传输,而IP数据报则有限制其生存时间的TTL(time to live)字段,TTL为8位,所以最大值为255,我们来看下ip数据报头的格式:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/tcp_timewait3.png" alt="tcp_timewait3"></p>
<p>我们姑且认为ip数据报在网络中每经过一个跳数TTL字段-1,最后减到0时则会丢弃这个数据报,(或者这样理解:一个IP数据报可以经过的最大路由数,每经过一个处理他的路由器此值就减1,当此值为0则数据报将被丢弃,同时发送ICMP报文通知源主机)一般情况下,具有最大跳数(255)的分组在网络中的存活时间不可能超过MSL,所以这里设置为2MSL就可以确保TCP的可靠性.</p>
<h3 id="必要性"><a href="#必要性" class="headerlink" title="必要性"></a><strong>必要性</strong></h3><blockquote>
<ul>
<li>我们没有任何机制保证最后的一个ACK能够正常传输</li>
<li>网络上仍然有可能有残余的数据包(wandering duplicates),我们也必须能够正常处理TCP是建立在不可靠网络上的可靠的协议.</li>
</ul>
</blockquote>
<p>1.假设最后一个ACK丢失了,被动关闭一方会重发它的FIN。主动关闭一方必须维持一个有效状态信息（TIMEWAIT状态下维持）,以便能够重发ACK。如果主动关闭的socket不维持这种状态而进入CLOSED状态,那么主动关闭的socket在处于CLOSED状态时,接收到FIN后将会响应一个RST。被动关闭一方接收到RST后会认为出错了。如果TCP协议想要正常完成必要的操作而终止双方的数据流传输,就必须完全正确的传输四次握手的四个节,不能有任何的丢失。这就是为什么socket在关闭后,仍然处于TIME_WAIT状态的第一个原因,因为他要等待以便重发ACK。<br>2.假设目前连接的通信双方都已经调用了close(),双方同时进入CLOSED的终结状态,而没有走TIME_WAIT状态。会出现如下问题,现在有一个新的连接被建立起来,使用的IP地址与端口与先前的完全相同,后建立的连接是原先连接的一个完全复用。还假定原先的连接中有数据报残存于网络之中,这样新的连接收到的数据报中有可能是先前连接的数据报。为了防止这一点,TCP不允许新连接复用TIME_WAIT状态下的socket。处于TIME_WAIT状态的socket在等待两倍的MSL时间以后（之所以是两倍的MSL,是由于MSL是一个数据报在网络中单向发出到认定丢失的时间,一个数据报有可能在发送途中或是其响应过程中成为残余数据报,确认一个数据报及其响应的丢弃的需要两倍的MSL）,将会转变为CLOSED状态。这就意味着,一个成功建立的连接,必然使得先前网络中残余的数据报都丢失了.</p>
<h3 id="负作用"><a href="#负作用" class="headerlink" title="负作用"></a><strong>负作用</strong></h3><p>处于Time-Wait状态的连接必须等待2MSL的时间才能变成CLOSED,RFC 793指出MSL时间为2分钟,2MSL就为4分钟,在这个时间内,这个连接处于不可用状态,也就是不能被复用,只能干等着这个时间结束后再新建新的连接,这对应用程序来说是个巨大的浪费,特别是对于短连接类的应用来说,更是性能的一大损耗,那有什么办法来缩短这个时间或者复用这个连接呢?</p>
<p>当然是有的,linux下可通过修改内核的配置来做到快回收重复用time-wait:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">vim /etc/sysctl.conf</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">主要是就是上面两个,下面的这些参数可根据实际情况是否开启</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_fin_timeout = 30</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_keepalive_time = 1800</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_syncookies = 1</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.ip_local_port_range = 1024 65000</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_max_syn_backlog = 8192</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_max_tw_buckets = 5000</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.route.gc_timeout = 100</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_syn_retries = 1</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">net.ipv4.tcp_synack_retries = 1</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、开启tcp_timestamp是开启tcp_tw_reuse的前提条件.</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、但是在nat模式下,不用将tcp_tw_recycle和tcp_timestamp同时开启,这会造成tcp超时引发故障</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用sysctl -p 重启生效</span></span><br></pre></td></tr></table></figure>

<p>内核参数说明:</p>
<ol>
<li>net.ipv4.tcp_syncookies &#x3D; 1 表示开启SYN Cookies.当出现SYN等待队列溢出时,启用cookies来处理,可防范少量SYN攻击,默认为0,表示关闭,打开这个syncookies的目的实际上是：“在服务器资源（并非单指端口资源,拒绝服务有很多种资源不足的情况）不足的情况下,尽量不要拒绝TCP的syn（连接）请求,尽量把syn请求缓存起来,留着过会儿有能力的时候处理这些TCP的连接请求。</li>
<li>net.ipv4.tcp_tw_reuse &#x3D; 1 表示开启重用.允许将TIME-WAIT sockets重新用于新的TCP连接,默认为0,表示关闭</li>
<li>net.ipv4.tcp_tw_recycle &#x3D; 1 表示开启TCP连接中TIME-WAIT sockets的快速回收,默认为0,表示关闭.</li>
<li>net.ipv4.tcp_fin_timeout &#x3D; 30 表示如果套接字由本端要求关闭,这个参数决定了它保持在FIN-WAIT-2状态的时间.</li>
<li>net.ipv4.tcp_keepalive_time &#x3D; 1800 表示当keepalive起用的时候,TCP发送keepalive消息的频度.缺省是2小时,改为20分钟.</li>
<li>net.ipv4.ip_local_port_range &#x3D; 1024  65000 表示用于向外连接的端口范围.缺省情况下很小:32768到61000,改为1024到65000.</li>
<li>net.ipv4.tcp_max_syn_backlog &#x3D; 8192 表示SYN队列的长度,默认为1024,加大队列长度为8192,可以容纳更多等待连接的网络连接数.</li>
<li>net.ipv4.tcp_max_tw_buckets &#x3D; 5000 表示系统同时保持TIME_WAIT套接字的最大数量,如果超过这个数字,TIME_WAIT套接字将立刻被清除并打印警告信息.默认为180000,改为5000.对于Apache、Nginx等服务器,上几行的参数可以很好地减少TIME_WAIT套接字数量</li>
<li>net.ipv4.route.gc_timeout &#x3D; 100 路由缓存刷新频率,当一个路由失败后多长时间跳到另一个,默认是300</li>
<li>net.ipv4.tcp_syn_retries &#x3D; 1 对于一个新建连接,内核要发送多少个 SYN 连接请求才决定放弃.不应该大于255,默认值是5,对应于180秒左右.</li>
</ol>
<p>这样一来,状态为time-wait的连接数就刷刷的下来了.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://blog.csdn.net/sctq8888/article/details/11180519">修改Linux内核参数,解决TCP连接中的TIME-WAIT socket</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP内核参数详解</title>
    <url>/2020/07/06/TCP-DEEP-LEARN/</url>
    <content><![CDATA[<p>本文为转载文章,主要对tcp的内核相关参数做了一个比较详细的介绍, 写的非常不错, 值得推荐,  <strong>原文地址 <a href="https://mp.weixin.qq.com/s/Z50N9JpvHgWCYGCZuTH6Pg">mp.weixin.qq.com</a></strong></p>
<span id="more"></span>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>TCP 性能的提升不仅考察 TCP 的理论知识，还考察了对于操作系统提供的内核参数的理解与应用。</p>
<p>TCP 协议是由操作系统实现，所以操作系统提供了不少调节 TCP 的参数。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Vf5DSosG7GpApZ3MiaveuoBxVNmibnh9J4BUgFvNQ22ZBF0d4grxwXweA/640?wx_fmt=png">Linux </p>
<p>TCP 参数</p>
<p>如何正确有效的使用这些参数，来提高 TCP 性能是一个不那么简单事情。我们需要针对 TCP 每个阶段的问题来对症下药，而不是病急乱投医。</p>
<p>接下来，将以三个角度来阐述提升 TCP 的策略，分别是：</p>
<ul>
<li><p>TCP 三次握手的性能提升；</p>
</li>
<li><p>TCP 四次挥手的性能提升；</p>
</li>
<li><p>TCP 数据传输的性能提升；</p>
</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VNibZtTj2S0Uv8zulJxs99ank1DYN9PMeVko3Y37KJialMrx0mDOIy5Dg/640?wx_fmt=png">本节提纲</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="01-TCP-三次握手的性能提升"><a href="#01-TCP-三次握手的性能提升" class="headerlink" title="01 TCP 三次握手的性能提升"></a>01 TCP 三次握手的性能提升</h3><p>TCP 是面向连接的、可靠的、双向传输的传输层通信协议，所以在传输数据之前需要经过三次握手才能建立连接。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VLPcVD30JQ2hZKPOgCesbok6sNedqib1e0ibYJEI8qurabNKsjosVOhow/640?wx_fmt=png">三次握手与数据传输</p>
<p>那么，三次握手的过程在一个 HTTP 请求的平均时间占比 10% 以上，在网络状态不佳、高并发或者遭遇 SYN 攻击等场景中，如果不能有效正确的调节三次握手中的参数，就会对性能产生很多的影响。</p>
<p>如何正确有效的使用这些参数，来提高 TCP 三次握手的性能，这就需要理解「三次握手的状态变迁」，这样当出现问题时，先用 <code>netstat</code> 命令查看是哪个握手阶段出现了问题，再来对症下药，而不是病急乱投医。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Vdq0Su2TWGMHn1nM1KZlebujJyMDngyQIia0neP3bhmrIZzeRzA3d7sQ/640?wx_fmt=png">TCP 三次握手的状态变迁</p>
<p>客户端和服务端都可以针对三次握手优化性能。主动发起连接的客户端优化相对简单些，而服务端需要监听端口，属于被动连接方，其间保持许多的中间状态，优化方法相对复杂一些。</p>
<p>所以，客户端（主动发起连接方）和服务端（被动连接方）优化的方式是不同的，接下来分别针对客户端和服务端优化。</p>
<h4 id="客户端优化"><a href="#客户端优化" class="headerlink" title="客户端优化"></a>客户端优化</h4><p>三次握手建立连接的首要目的是「同步序列号」。</p>
<p>只有同步了序列号才有可靠传输，TCP 许多特性都依赖于序列号实现，比如流量控制、丢包重传等，这也是三次握手中的报文称为 SYN 的原因，SYN 的全称就叫 _Synchronize Sequence Numbers_（同步序列号）。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Vicx6iczviaNZwib06MTSjIFe4lwfTOaJU7PmAE4YjB96YcJ9rUZgtbjPww/640?wx_fmt=png">TCP 头部</p>
<blockquote>
<p>SYN_SENT 状态的优化</p>
</blockquote>
<p>客户端作为主动发起连接方，首先它将发送 SYN 包，于是客户端的连接就会处于 <code>SYN_SENT</code> 状态。</p>
<p>客户端在等待服务端回复的 ACK 报文，正常情况下，服务器会在几毫秒内返回 SYN+ACK ，但如果客户端长时间没有收到 SYN+ACK 报文，则会重发 SYN 包，<strong>重发的次数由 tcp_syn_retries 参数控制</strong>，默认是 5 次：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VHDic6bsueW6k27ibea5U6GRcOWOnF8YtTR4EApQzCWs65e29PNalBzFw/640?wx_fmt=png"></p>
<p>通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，<strong>每次超时的时间是上一次的 2 倍</strong>。</p>
<p>当第五次超时重传后，会继续等待 32 秒，如果仍然服务端没有回应 ACK，客户端就会终止三次握手。</p>
<p>所以，总耗时是 1+2+4+8+16+32&#x3D;63 秒，大约 1 分钟左右。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VA5Y8s7UWYLIjx7TsaNBNX46H5BQSGVBzX8clDJLvz4X5IhEX9O4tNg/640?wx_fmt=png">SYN 超时重传</p>
<p>你可以根据网络的稳定性和目标服务器的繁忙程度修改 SYN 的重传次数，调整客户端的三次握手时间上限。比如内网中通讯时，就可以适当调低重试次数，尽快把错误暴露给应用程序。</p>
<h4 id="服务端优化"><a href="#服务端优化" class="headerlink" title="服务端优化"></a>服务端优化</h4><p>当服务端收到 SYN 包后，服务端会立马回复 SYN+ACK 包，表明确认收到了客户端的序列号，同时也把自己的序列号发给对方。</p>
<p>此时，服务端出现了新连接，状态是 <code>SYN_RCV</code>。在这个状态下，Linux 内核就会建立一个「半连接队列」来维护「未完成」的握手信息，当半连接队列溢出后，服务端就无法再建立新的连接。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VFlFykJPoibADSWRc10HiaX04pPb9a0TzRwpSAibb2k63oEraQMOJp6jYQ/640?wx_fmt=png">半连接队列与全连接队列</p>
<p>SYN 攻击，攻击的是就是这个半连接队列。</p>
<blockquote>
<p>如何查看由于 SYN 半连接队列已满，而被丢弃连接的情况？</p>
</blockquote>
<p>我们可以通过该 <code>netstat -s</code> 命令给出的统计结果中，  可以得到由于半连接队列已满，引发的失败次数：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VvX6ZX7cya8iaibM1e7HMWtMhxn9yFGERykW9iaftuJlYaK0LNVCkibQN3Q/640?wx_fmt=png"></p>
<p>上面输出的数值是<strong>累计值</strong>，表示共有多少个 TCP 连接因为半连接队列溢出而被丢弃。<strong>隔几秒执行几次，如果有上升的趋势，说明当前存在半连接队列溢出的现象</strong>。</p>
<blockquote>
<p>如何调整 SYN 半连接队列大小？</p>
</blockquote>
<p>要想增大半连接队列，<strong>不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大 accept 队列。否则，只单纯增大 tcp_max_syn_backlog 是无效的。</strong></p>
<p>增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VRpEPdtiaOrav5AaXgoibqv994ibKibIA1aib0Dg4LLbAXfZZXcRd6sMdoEQ/640?wx_fmt=png"></p>
<p>增大 backlog 的方式，每个 Web 服务都不同，比如 Nginx 增大 backlog 的方法如下：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VY2NtMNCPyBcNu4cyXcbtGf7ZZQJ9dYbrFMhqvrkRtbkxe21ZtU8Msw/640?wx_fmt=png"></p>
<p>最后，改变了如上这些参数后，要重启 Nginx 服务，因为 SYN 半连接队列和 accept 队列都是在 <code>listen()</code> 初始化的。</p>
<blockquote>
<p>如果 SYN 半连接队列已满，只能丢弃连接吗？</p>
</blockquote>
<p>并不是这样，<strong>开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接</strong>。</p>
<p>syncookies 的工作原理：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出，当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功，如下图所示。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VkPlfkHuTM0nicT1nF1ED11FJm7TcjoiczO7QBZOGMHH58yL1HyqoMdSg/640?wx_fmt=png">开启 syncookies 功能</p>
<p>syncookies 参数主要有以下三个值：</p>
<ul>
<li><p>0 值，表示关闭该功能；</p>
</li>
<li><p>1 值，表示仅当 SYN 半连接队列放不下时，再启用它；</p>
</li>
<li><p>2 值，表示无条件开启功能；</p>
</li>
</ul>
<p>那么在应对 SYN 攻击时，只需要设置为 1 即可：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VvptUXMjpAq8zibC1o3ZAId9tiaiastPqlbQ18ibCezu5bgeTlSuGkZAwHA/640?wx_fmt=png"></p>
<blockquote>
<p>SYN_RCV 状态的优化</p>
</blockquote>
<p>当客户端接收到服务器发来的 SYN+ACK 报文后，就会回复 ACK 给服务器，同时客户端连接状态从 SYN_SENT 转换为 ESTABLISHED，表示连接建立成功。</p>
<p>服务器端连接成功建立的时间还要再往后，等到服务端收到客户端的 ACK 后，服务端的连接状态才变为 ESTABLISHED。</p>
<p>如果服务器没有收到 ACK，就会重发 SYN+ACK 报文，同时一直处于 SYN_RCV 状态。</p>
<p>当网络繁忙、不稳定时，报文丢失就会变严重，此时应该调大重发次数。反之则可以调小重发次数。<strong>修改重发次数的方法是，调整 tcp_synack_retries 参数</strong>：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VASiaO11drOmhUvGjMX0zYCibDrvp2n05oWoUBuKz67ekYFrmLzBm542A/640?wx_fmt=png"></p>
<p>tcp_synack_retries 的默认重试次数是 5 次，与客户端重传 SYN 类似，它的重传会经历 1、2、4、8、16 秒，最后一次重传后会继续等待 32 秒，如果服务端仍然没有收到 ACK，才会关闭连接，故共需要等待 63 秒。</p>
<p>服务器收到 ACK 后连接建立成功，此时，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。</p>
<p>如果进程不能及时地调用 accept 函数，就会造成 accept 队列（也称全连接队列）溢出，最终导致建立好的 TCP 连接被丢弃。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VKoOYfc57ONXcqLa3FBvNicibxl3SJv30lfKZ80yEXIzONbqaoJhdYcLw/640?wx_fmt=png">accept 队列溢出</p>
<blockquote>
<p>accept 队列已满，只能丢弃连接吗？</p>
</blockquote>
<p>丢弃连接只是 Linux 的默认行为，我们还可以选择向客户端发送 RST 复位报文，告诉客户端连接已经建立失败。打开这一功能需要将 tcp_abort_on_overflow 参数设置为 1。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VSsUHGjUL9j234x7wGJVhzRyUesgnNbia4j23z8eBMq0vIGZQPibWJDYQ/640?wx_fmt=png"></p>
<p>tcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：</p>
<ul>
<li><p>0 ：如果 accept 队列满了，那么 server 扔掉 client  发过来的 ack ；</p>
</li>
<li><p>1 ：如果 accept 队列满了，server 发送一个 <code>RST</code> 包给 client，表示废掉这个握手过程和这个连接；</p>
</li>
</ul>
<p>如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把 tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 <code>connection reset by peer</code> 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。</p>
<p>通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。</p>
<p>举个例子，当 accept 队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，客户端进程就在建立好的连接上发送请求。只要服务器没有为请求回复 ACK，客户端的请求就会被多次「重发」。<strong>如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 accept 队列有空位时，再次接收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VVMauhYxIb2B5KoRdCThOxhQNZ3pv0fJduEXic4Q38x0RuzLkggZ7tIw/640?wx_fmt=png">tcp_abort_on_overflow 为 0 可以应对突发流量</p>
<p>所以，tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 TCP 全连接队列会长期溢出时，才能设置为 1 以尽快通知客户端。</p>
<blockquote>
<p>如何调整 accept 队列的长度呢？</p>
</blockquote>
<p>accept 队列的长度取决于 somaxconn 和 backlog 之间的最小值，也就是 min(somaxconn, backlog)，其中：</p>
<ul>
<li><p>somaxconn 是 Linux 内核的参数，默认值是 128，可以通过 <code>net.core.somaxconn</code> 来设置其值；</p>
</li>
<li><p>backlog 是 <code>listen(int sockfd, int backlog)</code> 函数中的 backlog 大小；</p>
</li>
</ul>
<p>Tomcat、Nginx、Apache 常见的 Web 服务的 backlog 默认值都是 511。</p>
<blockquote>
<p>如何查看服务端进程 accept 队列的长度？</p>
</blockquote>
<p>可以通过 <code>ss -ltn</code> 命令查看：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V29Pc73XLliaAWWeibqwzw6VgKJAkZic7QvkNJ0wicjZg5VmnichTced0Vog/640?wx_fmt=png"></p>
<ul>
<li><p>Recv-Q：当前 accept 队列的大小，也就是当前已完成三次握手并等待服务端 <code>accept()</code> 的 TCP 连接；</p>
</li>
<li><p>Send-Q：accept 队列最大长度，上面的输出结果说明监听 8088 端口的 TCP 服务，accept 队列的最大长度为 128；</p>
</li>
</ul>
<blockquote>
<p>如何查看由于 accept 连接队列已满，而被丢弃的连接？</p>
</blockquote>
<p>当超过了 accept 连接队列，服务端则会丢掉后续进来的 TCP 连接，丢掉的 TCP 连接的个数会被统计起来，我们可以使用 netstat -s 命令来查看：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VBjYgHiaYqHsyVgWib7Oibe2tKTa0Vw5yKXpIhxDpgCj4dNFrJzZh6uGkw/640?wx_fmt=png"></p>
<p>上面看到的 41150 times ，表示 accept 队列溢出的次数，注意这个是累计值。可以隔几秒钟执行下，如果这个数字一直在增加的话，说明 accept 连接队列偶尔满了。</p>
<p>如果持续不断地有连接因为 accept 队列溢出被丢弃，就应该调大 backlog 以及 somaxconn 参数。</p>
<h4 id="如何绕过三次握手？"><a href="#如何绕过三次握手？" class="headerlink" title="如何绕过三次握手？"></a>如何绕过三次握手？</h4><p>以上我们只是在对三次握手的过程进行优化，接下来我们看看如何绕过三次握手发送数据。</p>
<p>三次握手建立连接造成的后果就是，HTTP 请求必须在一个 RTT（从客户端到服务器一个往返的时间）后才能发送。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V9FKdRKnqibJK3Yy78l0ZqMEg1jXd6Oau9PJQahaGaibSZykFecYribeWQ/640?wx_fmt=png">常规 HTTP 请求</p>
<p>在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。</p>
<blockquote>
<p>接下来说说，TCP Fast Open 功能的工作方式。</p>
</blockquote>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Vu6FONrqDCbf9aVRFWEnPWOTRhGolqOX1nia02pwjqM06wmrskNNGAPA/640?wx_fmt=png">开启 TCP Fast Open 功能</p>
<p>在客户端首次建立连接时的过程：</p>
<ol>
<li><p>客户端发送 SYN 报文，该报文包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；</p>
</li>
<li><p>支持 TCP Fast Open 的服务器生成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；</p>
</li>
<li><p>客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。</p>
</li>
</ol>
<p>所以，第一次发起 HTTP GET 请求的时候，还是需要正常的三次握手流程。</p>
<p>之后，如果客户端再次向服务器建立连接时的过程：</p>
<ol>
<li><p>客户端发送 SYN 报文，该报文包含「数据」（对于非 TFO 的普通 TCP 握手过程，SYN 报文中不包含「数据」）以及此前记录的 Cookie；</p>
</li>
<li><p>支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「数据」递送至相应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；</p>
</li>
<li><p>如果服务器接受了 SYN 报文中的「数据」，服务器可在握手完成之前发送「数据」，<strong>这就减少了握手带来的 1 个 RTT 的时间消耗</strong>；</p>
</li>
<li><p>客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报文中发送的「数据」没有被确认，则客户端将重新发送「数据」；</p>
</li>
<li><p>此后的 TCP 连接的数据传输过程和非 TFO 的正常情况一致。</p>
</li>
</ol>
<p>所以，之后发起 HTTP GET 请求的时候，可以绕过三次握手，这就减少了握手带来的 1 个 RTT 的时间消耗。</p>
<p>注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直至服务器认为 Cookie 无效（通常为过期）。</p>
<blockquote>
<p>Linux 下怎么打开 TCP Fast Open 功能呢？</p>
</blockquote>
<p>在 Linux 系统中，可以通过<strong>设置 tcp_fastopn 内核参数，来打开 Fast Open 功能</strong>：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VwzGZbjy6Wv2IXuibTCRXveHazhGKm9MIULSDsOXj7ZmUMK7WkYy4yKA/640?wx_fmt=png"></p>
<p>tcp_fastopn 各个值的意义:</p>
<ul>
<li><p>0 关闭</p>
</li>
<li><p>1 作为客户端使用 Fast Open 功能</p>
</li>
<li><p>2 作为服务端使用 Fast Open 功能</p>
</li>
<li><p>3 无论作为客户端还是服务器，都可以使用 Fast Open 功能</p>
</li>
</ul>
<p><strong>TCP Fast Open 功能需要客户端和服务端同时支持，才有效果。</strong></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>本小结主要介绍了关于优化 TCP 三次握手的几个 TCP 参数。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VMKTGbut3ibP8YXz8Jf1Xz5xFZia0EHHpxUOm8kkd5ic2o1upcacyYjlCg/640?wx_fmt=png">三次握手优化策略</p>
<blockquote>
<p>客户端的优化</p>
</blockquote>
<p>当客户端发起 SYN 包时，可以通过 <code>tcp_syn_retries</code> 控制其重传的次数。</p>
<blockquote>
<p>服务端的优化</p>
</blockquote>
<p>当服务端 SYN 半连接队列溢出后，会导致后续连接被丢弃，可以通过 <code>netstat -s</code> 观察半连接队列溢出的情况，如果 SYN 半连接队列溢出情况比较严重，可以通过 <code>tcp_max_syn_backlog、somaxconn、backlog</code> 参数来调整 SYN 半连接队列的大小。</p>
<p>服务端回复 SYN+ACK 的重传次数由 <code>tcp_synack_retries</code> 参数控制。如果遭受 SYN 攻击，应把 <code>tcp_syncookies</code> 参数设置为 1，表示仅在 SYN 队列满后开启 syncookie 功能，可以保证正常的连接成功建立。</p>
<p>服务端收到客户端返回的 ACK，会把连接移入 accpet 队列，等待进行调用 accpet() 函数取出连接。</p>
<p>可以通过 <code>ss -lnt</code> 查看服务端进程的 accept 队列长度，如果 accept 队列溢出，系统默认丢弃 ACK，如果可以把 <code>tcp_abort_on_overflow</code> 设置为 1 ，表示用 RST 通知客户端连接建立失败。</p>
<p>如果 accpet 队列溢出严重，可以通过 listen 函数的 <code>backlog</code> 参数和 <code>somaxconn</code> 系统参数提高队列大小，accept 队列长度取决于 min(backlog, somaxconn)。</p>
<blockquote>
<p>绕过三次握手</p>
</blockquote>
<p>TCP Fast Open 功能可以绕过三次握手，使得 HTTP 请求减少了 1 个 RTT 的时间，Linux 下可以通过 <code>tcp_fastopen</code> 开启该功能，同时必须保证服务端和客户端同时支持。</p>
<h3 id="02-TCP-四次挥手的性能提升"><a href="#02-TCP-四次挥手的性能提升" class="headerlink" title="02 TCP 四次挥手的性能提升"></a>02 TCP 四次挥手的性能提升</h3><p>接下来，我们一起看看针对 TCP 四次挥手关不连接时，如何优化性能。</p>
<p>在开始之前，我们得先了解四次挥手状态变迁的过程。</p>
<p>客户端和服务端双方都可以主动断开连接，<strong>通常先关闭连接的一方称为主动方，后关闭连接的一方称为被动方。</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V1KDt5CUY9FjNQJzOps6UZbYYkh4P2Ykfb8YfNWsobWVqGcmNmqWNkQ/640?wx_fmt=png">客户端主动关闭</p>
<p>可以看到，<strong>四次挥手过程只涉及了两种报文，分别是 FIN 和 ACK</strong>：</p>
<ul>
<li><p>FIN 就是结束连接的意思，谁发出 FIN 报文，就表示它将不会再发送任何数据，关闭这一方向上的传输通道；</p>
</li>
<li><p>ACK 就是确认的意思，用来通知对方：你方的发送通道已经关闭；</p>
</li>
</ul>
<p>四次挥手的过程:</p>
<ul>
<li><p>当主动方关闭连接时，会发送 FIN 报文，此时发送方的 TCP 连接将从 ESTABLISHED 变成 FIN_WAIT1。</p>
</li>
<li><p>当被动方收到 FIN 报文后，内核会自动回复 ACK 报文，连接状态将从 ESTABLISHED 变成 CLOSE_WAIT，表示被动方在等待进程调用 close 函数关闭连接。</p>
</li>
<li><p>当主动方收到这个 ACK 后，连接状态由 FIN_WAIT1 变为 FIN_WAIT2，也就是表示<strong>主动方的发送通道就关闭了</strong>。</p>
</li>
<li><p>当被动方进入 CLOSE_WAIT 时，被动方还会继续处理数据，等到进程的 read 函数返回 0 后，应用程序就会调用 close 函数，进而触发内核发送 FIN 报文，此时被动方的连接状态变为 LAST_ACK。</p>
</li>
<li><p>当主动方收到这个 FIN 报文后，内核会回复 ACK 报文给被动方，同时主动方的连接状态由 FIN_WAIT2 变为 TIME_WAIT，<strong>在 Linux 系统下大约等待 1 分钟后，TIME_WAIT 状态的连接才会彻底关闭</strong>。</p>
</li>
<li><p>当被动方收到最后的 ACK 报文后，<strong>被动方的连接就会关闭</strong>。</p>
</li>
</ul>
<p>你可以看到，每个方向都需要<strong>一个 FIN 和一个 ACK</strong>，因此通常被称为<strong>四次挥手</strong>。</p>
<p>这里一点需要注意是：<strong>主动关闭连接的，才有 TIME_WAIT 状态。</strong></p>
<p>主动关闭方和被动关闭方优化的思路也不同，接下来分别说说如何优化他们。</p>
<h4 id="主动方的优化"><a href="#主动方的优化" class="headerlink" title="主动方的优化"></a>主动方的优化</h4><p>关闭的连接的方式通常有两种，分别是 RST 报文关闭和 FIN 报文关闭。</p>
<p>如果进程异常退出了，内核就会发送 RST 报文来关闭，它可以不走四次挥手流程，是一个暴力关闭连接的方式。</p>
<p>安全关闭连接的方式必须通过四次挥手，它由进程调用 <code>close</code> 和 <code>shutdown</code> 函数发起 FIN 报文（shutdown 参数须传入 SHUT_WR 或者 SHUT_RDWR 才会发送 FIN）。</p>
<blockquote>
<p>调用 close 函数 和 shutdown 函数有什么区别？</p>
</blockquote>
<p>调用了 close 函数意味着完全断开连接，<strong>完全断开不仅指无法传输数据，而且也不能发送数据。此时，调用了 close 函数的一方的连接叫做「孤儿连接」，如果你用 netstat -p 命令，会发现连接对应的进程名为空。</strong></p>
<p>使用 close 函数关闭连接是不优雅的。于是，就出现了一种优雅关闭连接的 <code>shutdown</code> 函数，<strong>它可以控制只关闭一个方向的连接</strong>：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VybOEcSGjd0OjclmQmNMLLod8DRYfVkApic6U13bqY6NFjS9s81XBxcw/640?wx_fmt=png"></p>
<p>第二个参数决定断开连接的方式，主要有以下三种方式：</p>
<ul>
<li><p>SHUT_RD(0)：<strong>关闭连接的「读」这个方向</strong>，如果接收缓冲区有已接收的数据，则将会被丢弃，并且后续再收到新的数据，会对数据进行 ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了。</p>
</li>
<li><p>SHUT_WR(1)：<strong>关闭连接的「写」这个方向</strong>，这就是常被称为「半关闭」的连接。如果发送缓冲区还有未发送的数据，将被立即发送出去，并发送一个 FIN 报文给对端。</p>
</li>
<li><p>SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各一次，<strong>关闭套接字的读和写两个方向</strong>。</p>
</li>
</ul>
<p>close 和 shutdown 函数都可以关闭连接，但这两种方式关闭的连接，不只功能上有差异，控制它们的 Linux 参数也不相同。</p>
<blockquote>
<p>FIN_WAIT1 状态的优化</p>
</blockquote>
<p>主动方发送 FIN 报文后，连接就处于 FIN_WAIT1 状态，正常情况下，如果能及时收到被动方的 ACK，则会很快变为 FIN_WAIT2 状态。</p>
<p>但是当迟迟收不到对方返回的 ACK 时，连接就会一直处于 FIN_WAIT1 状态。此时，<strong>内核会定时重发 FIN 报文，其中重发次数由 tcp_orphan_retries 参数控制</strong>（注意，orphan 虽然是孤儿的意思，该参数却不只对孤儿连接有效，事实上，它对所有 FIN_WAIT1 状态下的连接都有效），默认值是 0。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Vv86D0Y3AF9x52SXBQibD8LYU8pBKnXB62o7cdicjMRNye0KIdWPbpm8A/640?wx_fmt=png"></p>
<p>你可能会好奇，这 0 表示几次？<strong>实际上当为 0 时，特指 8 次</strong>，从下面的内核源码可知：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V1yAYXaXIAs1DTON8lSQvWvctCMtIk78Ja4dYxe6ibmjEuPzkhHaW1zw/640?wx_fmt=png"></p>
<p>如果 FIN_WAIT1 状态连接很多，我们就需要考虑降低 tcp_orphan_retries 的值，当重传次数超过 tcp_orphan_retries 时，连接就会直接关闭掉。</p>
<p>对于普遍正常情况时，调低 tcp_orphan_retries 就已经可以了。如果遇到恶意攻击，FIN 报文根本无法发送出去，这由 TCP 两个特性导致的：</p>
<ul>
<li><p>首先，TCP 必须保证报文是有序发送的，FIN 报文也不例外，当发送缓冲区还有数据没有发送时，FIN 报文也不能提前发送。</p>
</li>
<li><p>其次，TCP 有流量控制功能，当接收方接收窗口为 0 时，发送方就不能再发送数据。所以，当攻击者下载大文件时，就可以通过接收窗口设为 0 ，这就会使得 FIN 报文都无法发送出去，那么连接会一直处于 FIN_WAIT1 状态。</p>
</li>
</ul>
<p>解决这种问题的方法，是<strong>调整 tcp_max_orphans 参数，它定义了「孤儿连接」的最大数量</strong>：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VqUCNRibFu32AkicOOUCtd7qyYks8h4nYrkzh6daFNLsibgFKbA9M0GpIQ/640?wx_fmt=png"></p>
<p>当进程调用了 <code>close</code> 函数关闭连接，此时连接就会是「孤儿连接」，因为它无法在发送和接收数据。Linux 系统为了防止孤儿连接过多，导致系统资源长时间被占用，就提供了 <code>tcp_max_orphans</code> 参数。如果孤儿连接数量大于它，新增的孤儿连接将不再走四次挥手，而是直接发送 RST 复位报文强制关闭。</p>
<blockquote>
<p>FIN_WAIT2 状态的优化</p>
</blockquote>
<p>当主动方收到 ACK 报文后，会处于 FIN_WAIT2 状态，就表示主动方的发送通道已经关闭，接下来将等待对方发送 FIN 报文，关闭对方的发送通道。</p>
<p>这时，<strong>如果连接是用 shutdown 函数关闭的，连接可以一直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收数据。但对于 close 函数关闭的孤儿连接，由于无法在发送和接收数据，所以这个状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长</strong>，默认值是 60 秒：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V5yUIH2yQFaOLIFcmia6UVhq6FMLgOPEGS8HL3FvicN9dOIZWUibZS4WQQ/640?wx_fmt=png"></p>
<p>它意味着对于孤儿连接（调用 close 关闭的连接），如果在 60 秒后还没有收到 FIN 报文，连接就会直接关闭。</p>
<p>这个 60 秒不是随便决定的，它与 TIME_WAIT 状态持续的时间是相同的，后面我们在来说说为什么是 60 秒。</p>
<blockquote>
<p>TIME_WAIT 状态的优化</p>
</blockquote>
<p>TIME_WAIT 是主动方四次挥手的最后一个状态，也是最常遇见的状态。</p>
<p>当收到被动方发来的 FIN 报文后，主动方会立刻回复 ACK，表示确认对方的发送通道已经关闭，接着就处于 TIME_WAIT 状态。在 Linux 系统，TIME_WAIT 状态会持续 60 秒后才会进入关闭状态。</p>
<p>TIME_WAIT 状态的连接，在主动方看来确实快已经关闭了。然后，被动方没有收到 ACK 报文前，还是处于 LAST_ACK 状态。如果这个 ACK 报文没有到达被动方，被动方就会重发 FIN 报文。重发次数仍然由前面介绍过的 tcp_orphan_retries 参数控制。</p>
<p>TIME-WAIT 的状态尤其重要，主要是两个原因：</p>
<ul>
<li><p>防止具有相同「四元组」的「旧」数据包被收到；</p>
</li>
<li><p>保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭；</p>
</li>
</ul>
<p><em>原因一：防止旧连接的数据包</em></p>
<p>TIME-WAIT 的一个作用是<strong>防止收到历史数据，从而导致数据错乱的问题。</strong></p>
<p>假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Veuzbfy6pZQNiavDH6ue03MC0Ij0euZ97yLQSwUCJywbrYibWyGqxckDA/640?wx_fmt=png">接收到历史数据的异常  </p>
<ul>
<li><p>如上图黄色框框服务端在关闭连接之前发送的 <code>SEQ = 301</code> 报文，被网络延迟了。</p>
</li>
<li><p>这时有相同端口的 TCP 连接被复用后，被延迟的 <code>SEQ = 301</code> 抵达了客户端，那么客户端是有可能正常接收这个过期的报文，这就会产生数据错乱等严重的问题。</p>
</li>
</ul>
<p>所以，TCP 就设计出了这么一个机制，经过 <code>2MSL</code> 这个时间，<strong>足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。</strong></p>
<p><em>原因二：保证连接正确关闭</em></p>
<p>TIME-WAIT 的另外一个作用是<strong>等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。</strong></p>
<p>假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Voo03UXicFjeywGE5DEsPBMugJwW8bgqKDMbqoygVTd7nVRArvCQKAAQ/640?wx_fmt=png">没有确保正常断开的异常  </p>
<ul>
<li><p>如上图红色框框客户端四次挥手的最后一个 <code>ACK</code> 报文如果在网络中被丢失了，此时如果客户端 <code>TIME-WAIT</code> 过短或没有，则就直接进入了 <code>CLOSE</code> 状态了，那么服务端则会一直处在 <code>LASE-ACK</code> 状态。</p>
</li>
<li><p>当客户端发起建立连接的 <code>SYN</code> 请求报文后，服务端会发送 <code>RST</code> 报文给客户端，连接建立的过程就会被终止。</p>
</li>
</ul>
<p>我们再回过头来看看，为什么 TIME_WAIT 状态要保持 60 秒呢？这与孤儿连接 FIN_WAIT2 状态默认保留 60 秒的原理是一样的，<strong>因为这两个状态都需要保持 2MSL 时长。MSL 全称是 Maximum Segment Lifetime，它定义了一个报文在网络中的最长生存时间</strong>（报文每经过一次路由器的转发，IP 头部的 TTL 字段就会减 1，减到 0 时报文就被丢弃，这就限制了报文的最长存活时间）。</p>
<p>为什么是 2 MSL 的时长呢？这其实是相当于<strong>至少允许报文丢失一次</strong>。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。</p>
<p>为什么不是 4 或者 8 MSL 的时长呢？你可以想象一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比。</p>
<p><strong>因此，TIME_WAIT 和 FIN_WAIT2 状态的最大时长都是 2 MSL，由于在 Linux 系统中，MSL 的值固定为 30 秒，所以它们都是 60 秒。</strong></p>
<p>虽然 TIME_WAIT 状态有存在的必要，但它毕竟会消耗系统资源。<strong>如果发起连接一方的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。</strong></p>
<ul>
<li><p><strong>客户端受端口资源限制</strong>：如果客户端 TIME_WAIT 过多，就会导致端口资源被占用，因为端口就 65536 个，被占满就会导致无法创建新的连接；</p>
</li>
<li><p><strong>服务端受系统资源限制</strong>：由于一个 四元组表示 TCP 连接，理论上服务端可以建立很多连接，服务端确实只监听一个端口 但是会把连接扔给处理线程，所以理论上监听的端口可以继续监听。但是线程池处理不了那么多一直不断的连接了。所以当服务端出现大量 TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连接；</p>
</li>
</ul>
<p>另外，<strong>Linux 提供了 tcp_max_tw_buckets 参数，当 TIME_WAIT 的连接数量超过该参数时，新关闭的连接就不再经历 TIME_WAIT 而直接关闭：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V7kCLpXAqZ9r4K9gWCibkQ3k91X2Bu4Iq1zJicI2lZyBIbTUbhiaDzJRxA/640?wx_fmt=png"></p>
<p>当服务器的并发连接增多时，相应地，同时处于 TIME_WAIT 状态的连接数量也会变多，此时就应当调大 <code>tcp_max_tw_buckets</code> 参数，减少不同连接间数据错乱的概率。</p>
<p>tcp_max_tw_buckets 也不是越大越好，毕竟内存和端口都是有限的。</p>
<p><strong>有一种方式可以在建立新连接时，复用处于 TIME_WAIT 状态的连接，那就是打开 tcp_tw_reuse 参数。但是需要注意，该参数是只用于客户端（建立连接的发起方），因为是在调用 connect() 时起作用的，而对于服务端（被动连接方）是没有用的。</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VxVmia7uZOr4hROGf4bibShaOI2KBIHT2IUlKR7MOJut9WRthhIKnCialg/640?wx_fmt=png"></p>
<p>tcp_tw_reuse 从协议角度理解是安全可控的，可以复用处于 TIME_WAIT 的端口为新的连接所用。</p>
<p>什么是协议角度理解的安全可控呢？主要有两点：</p>
<ul>
<li><p>只适用于连接发起方，也就是 C&#x2F;S 模型中的客户端；</p>
</li>
<li><p>对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。</p>
</li>
</ul>
<p>使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持（对方也要打开 ）：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VUWtC4SWkRwKSm3qT0LfXMuiaIT6zNbspJRkJOqvnTX92ydI8sUxcydQ/640?wx_fmt=png"></p>
<p>由于引入了时间戳，它能带来了些好处：</p>
<ul>
<li><p>我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃；</p>
</li>
<li><p>同时，它还可以防止序列号绕回，也是因为重复的数据包会由于时间戳过期被自然丢弃；</p>
</li>
</ul>
<p>老版本的 Linux 还提供了 tcp_tw_recycle 参数，但是当开启了它，就有两个坑：</p>
<ul>
<li><p><strong>Linux 会加快客户端和服务端 TIME_WAIT 状态的时间</strong>，也就是它会使得 TIME_WAIT 状态会小于 60 秒，很容易导致数据错乱；</p>
</li>
<li><p>另外，<strong>Linux 会丢弃所有来自远端时间戳小于上次记录的时间戳（由同一个远端发送的）的任何数据包</strong>。就是说要使用该选项，则必须保证数据包的时间戳是单调递增的。那么，问题在于，此处的时间戳并不是我们通常意义上面的绝对时间，而是一个相对时间。很多情况下，我们是没法保证时间戳单调递增的，比如使用了 NAT，LVS 等情况；</p>
</li>
</ul>
<p>所以，不建议设置为 1 ，建议关闭它：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6Va7Y5QyNWP0E6tKzLFLvvaKgbqxJEVp8zibkhymrlqibk5g5J4vtiakwHQ/640?wx_fmt=png"></p>
<p>在 Linux 4.12 版本后，Linux 内核直接取消了这一参数。</p>
<p>另外，我们可以在程序中设置 socket 选项，来设置调用 close 关闭连接行为。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VFr2victHZBGXhFw3OIAHee3g7bNkIlLTzPlktZwAmCl8icyqgwhTvRBQ/640?wx_fmt=png"></p>
<p>如果<code>l_onoff</code>为非 0， 且<code>l_linger</code>值为 0，<strong>那么调用 close 后，会立该发送一个 RST 标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了 TIME_WAIT 状态，直接关闭。</strong></p>
<p>但这为跨越 TIME_WAIT 状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。</p>
<h4 id="被动方的优化"><a href="#被动方的优化" class="headerlink" title="被动方的优化"></a>被动方的优化</h4><p>当被动方收到 FIN 报文时，内核会自动回复 ACK，同时连接处于 CLOSE_WAIT 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。</p>
<p>内核没有权利替代进程去关闭连接，因为如果主动方是通过 shutdown 关闭连接，那么它就是想在半关闭连接上接收数据或发送数据。因此，Linux 并没有限制 CLOSE_WAIT 状态的持续时间。</p>
<p>当然，大多数应用程序并不使用 shutdown 函数关闭连接。所以，<strong>当你用 netstat 命令发现大量 CLOSE_WAIT 状态。就需要排查你的应用程序，因为可能因为应用程序出现了 Bug，read 函数返回 0 时，没有调用 close 函数。</strong></p>
<p>处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文关闭发送通道，同时连接进入 LAST_ACK 状态，等待主动方返回 ACK 来确认连接关闭。</p>
<p>如果迟迟收不到这个 ACK，内核就会重发 FIN 报文，重发次数仍然由 tcp_orphan_retries 参数控制，这与主动方重发 FIN 报文的优化策略一致。</p>
<p>还有一点我们需要注意的，<strong>如果被动方迅速调用 close 函数，那么被动方的 ACK 和 FIN 有可能在一个报文中发送，这样看起来，四次挥手会变成三次挥手，这只是一种特殊情况，不用在意。</strong></p>
<blockquote>
<p>如果连接双方同时关闭连接，会怎么样？</p>
</blockquote>
<p>由于 TCP 是双全工的协议，所以是会出现两方同时关闭连接的现象，也就是同时发送了 FIN 报文。</p>
<p>此时，上面介绍的优化策略仍然适用。两方发送 FIN 报文时，都认为自己是主动方，所以都进入了 FIN_WAIT1 状态，FIN 报文的重发次数仍由 tcp_orphan_retries 参数控制。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VwnRFnFribrznkfDy3FiaerLG59wIR72ZxL2w3ntkSQEsUW9quibiabQksQ/640?wx_fmt=png">同时关闭</p>
<p>接下来，<strong>双方在等待 ACK 报文的过程中，都等来了 FIN 报文。这是一种新情况，所以连接会进入一种叫做 CLOSING 的新状态，它替代了 FIN_WAIT2 状态</strong>。接着，双方内核回复 ACK 确认对方发送通道的关闭后，进入 TIME_WAIT 状态，等待 2MSL 的时间后，连接自动关闭。</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p>针对 TCP 四次挥手的优化，我们需要根据主动方和被动方四次挥手状态变化来调整系统 TCP 内核参数。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VWCJcNgpHTGbbREc03Vq09wwlFXftDqbic2klZd9FTXtNkuGuxcEeLBw/640?wx_fmt=png">四次挥手的优化策略</p>
<blockquote>
<p>主动方的优化</p>
</blockquote>
<p>主动发起 FIN 报文断开连接的一方，如果迟迟没收到对方的 ACK 回复，则会重传 FIN 报文，重传的次数由 <code>tcp_orphan_retries</code> 参数决定。</p>
<p>当主动方收到 ACK 报文后，连接就进入 FIN_WAIT2 状态，根据关闭的方式不同，优化的方式也不同：</p>
<ul>
<li><p>如果这是 close 函数关闭的连接，那么它就是孤儿连接。如果 <code>tcp_fin_timeout</code> 秒内没有收到对方的 FIN 报文，连接就直接关闭。同时，为了应对孤儿连接占用太多的资源，<code>tcp_max_orphans</code> 定义了最大孤儿连接的数量，超过时连接就会直接释放。</p>
</li>
<li><p>反之是 shutdown 函数关闭的连接，则不受此参数限制；</p>
</li>
</ul>
<p>当主动方接收到 FIN 报文，并返回 ACK 后，主动方的连接进入 TIME_WAIT 状态。这一状态会持续 1 分钟，为了防止 TIME_WAIT 状态占用太多的资源，<code>tcp_max_tw_buckets</code> 定义了最大数量，超过时连接也会直接释放。</p>
<p>当 TIME_WAIT 状态过多时，还可以通过设置 <code>tcp_tw_reuse</code> 和 <code>tcp_timestamps</code> 为 1 ，将 TIME_WAIT 状态的端口复用于作为客户端的新连接，注意该参数只适用于客户端。</p>
<blockquote>
<p>被动方的优化</p>
</blockquote>
<p>被动关闭的连接方应对非常简单，它在回复 ACK 后就进入了 CLOSE_WAIT 状态，等待进程调用 close 函数关闭连接。因此，出现大量 CLOSE_WAIT 状态的连接时，应当从应用程序中找问题。</p>
<p>当被动方发送 FIN 报文后，连接就进入 LAST_ACK 状态，在未等到 ACK 时，会在 <code>tcp_orphan_retries</code> 参数的控制下重发 FIN 报文。</p>
<h3 id="03-TCP-传输数据的性能提升"><a href="#03-TCP-传输数据的性能提升" class="headerlink" title="03 TCP 传输数据的性能提升"></a>03 TCP 传输数据的性能提升</h3><p>在前面介绍的是三次握手和四次挥手的优化策略，接下来主要介绍的是 TCP 传输数据时的优化策略。</p>
<p>TCP 连接是由内核维护的，内核会为每个连接建立内存缓冲区：</p>
<ul>
<li><p>如果连接的内存配置过小，就无法充分使用网络带宽，TCP 传输效率就会降低；</p>
</li>
<li><p>如果连接的内存配置过大，很容易把服务器资源耗尽，这样就会导致新连接无法建立；</p>
</li>
</ul>
<p>因此，我们必须理解 Linux 下 TCP 内存的用途，才能正确地配置内存大小。</p>
<h3 id="滑动窗口是如何影响传输速度的？"><a href="#滑动窗口是如何影响传输速度的？" class="headerlink" title="滑动窗口是如何影响传输速度的？"></a>滑动窗口是如何影响传输速度的？</h3><p>TCP 会保证每一个报文都能够抵达对方，它的机制是这样：报文发出去后，必须接收到对方返回的确认报文 ACK，如果迟迟未收到，就会超时重发该报文，直到收到对方的 ACK 为止。</p>
<p><strong>所以，TCP 报文发出去后，并不会立马从内存中删除，因为重传时还需要用到它。</strong></p>
<p>由于 TCP 是内核维护的，所以报文存放在内核缓冲区。如果连接非常多，我们可以通过 free 命令观察到 <code>buff/cache</code> 内存是会增大。</p>
<p>如果 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。这个模式就有点像我和你面对面聊天，你一句我一句，但这种方式的缺点是效率比较低的。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VgIxh6EvfubELsqPFAmzngKehxmYdia4Gic1iaQWo3AvzBdffqUCn1Llcg/640?wx_fmt=png">按数据包进行确认应答</p>
<p>所以，这样的传输方式有一个缺点：数据包的<strong>往返时间越长，通信的效率就越低</strong>。</p>
<p><strong>要解决这一问题不难，并行批量发送报文，再批量确认报文即刻。</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6V3j746BfJ0QMibtAcUeQibP58JiaL2WfULj4Au66wM8sfQXriad8X6Kf9HA/640?wx_fmt=png">并行处理</p>
<p>然而，这引出了另一个问题，发送方可以随心所欲的发送报文吗？<strong>当然这不现实，我们还得考虑接收方的处理能力。</strong></p>
<p>当接收方硬件不如发送方，或者系统繁忙、资源紧张时，是无法瞬间处理这么多报文的。于是，这些报文只能被丢掉，使得网络效率非常低。</p>
<p><strong>为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是滑动窗口的由来。</strong></p>
<p>接收方根据它的缓冲区，可以计算出后续能够接收多少字节的报文，这个数字叫做接收窗口。当内核接收到报文时，必须用缓冲区存放它们，这样剩余缓冲区空间变小，接收窗口也就变小了；当进程调用 read 函数后，数据被读入了用户空间，内核缓冲区就被清空，这意味着主机可以接收更多的报文，接收窗口就会变大。</p>
<p>因此，接收窗口并不是恒定不变的，接收方会把当前可接收的大小放在 TCP 报文头部中的<strong>窗口字段</strong>，这样就可以起到窗口大小通知的作用。</p>
<p>发送方的窗口等价于接收方的窗口吗？如果不考虑拥塞控制，发送方的窗口大小「约等于」接收方的窗口大小，因为窗口通知报文在网络传输是存在时延的，所以是约等于的关系。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VlYAtbju32eiaB20wI5xTEHBU6gh6s9tVA11vkGT8oSLV3hyt38oOWrQ/640?wx_fmt=png">TCP 头部</p>
<p>从上图中可以看到，窗口字段只有 2 个字节，因此它最多能表达 65535 字节大小的窗口，也就是 64KB 大小。</p>
<p>这个窗口大小最大值，在当今高速网络下，很明显是不够用的。所以后续有了扩充窗口的方法：<strong>在 TCP 选项字段定义了窗口扩大因子，用于扩大 TCP 通告窗口，使 TCP 的窗口大小从 2 个字节（16 位） 扩大为 30 位，所以此时窗口的最大值可以达到 1GB（2^30）。</strong></p>
<p>Linux 中打开这一功能，需要把 tcp_window_scaling 配置设为 1（默认打开）：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VghTuMKx92UaOon40ekeZI9aRuFGzl2vBuhCpe9LA33FqTuDsa1iafpw/640?wx_fmt=png"></p>
<p>要使用窗口扩大选项，通讯双方必须在各自的 SYN 报文中发送这个选项：</p>
<ul>
<li><p>主动建立连接的一方在 SYN 报文中发送这个选项；</p>
</li>
<li><p>而被动建立连接的一方只有在收到带窗口扩大选项的 SYN 报文之后才能发送这个选项。</p>
</li>
</ul>
<p>这样看来，只要进程能及时地调用 read 函数读取数据，并且接收缓冲区配置得足够大，那么接收窗口就可以无限地放大，发送方也就无限地提升发送速度。</p>
<p><strong>这是不可能的，因为网络的传输能力是有限的，当发送方依据发送窗口，发送超过网络处理能力的报文时，路由器会直接丢弃这些报文。因此，缓冲区的内存并不是越大越好。</strong></p>
<h4 id="如果确定最大传输速度？"><a href="#如果确定最大传输速度？" class="headerlink" title="如果确定最大传输速度？"></a>如果确定最大传输速度？</h4><p>在前面我们知道了 TCP 的传输速度，受制于发送窗口与接收窗口，以及网络设备传输能力。其中，窗口大小由内核缓冲区大小决定。如果缓冲区与网络传输能力匹配，那么缓冲区的利用率就达到了最大化。</p>
<p>问题来了，如何计算网络的传输能力呢？</p>
<p>相信大家都知道网络是有「带宽」限制的，带宽描述的是网络传输能力，它与内核缓冲区的计量单位不同:</p>
<ul>
<li><p>带宽是单位时间内的流量，表达是「速度」，比如常见的带宽 100 MB&#x2F;s；</p>
</li>
<li><p>缓冲区单位是字节，当网络速度乘以时间才能得到字节数；</p>
</li>
</ul>
<p>这里需要说一个概念，就是带宽时延积，它决定网络中飞行报文的大小，它的计算方式：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VTia5knGzShHeFYPmFJRjT2ASaWJLrhawjbO2CBPRYaDtswnZkFLcoZA/640?wx_fmt=png"></p>
<p>比如最大带宽是 100 MB&#x2F;s，网络时延（RTT）是 10ms 时，意味着客户端到服务端的网络一共可以存放 100MB&#x2F;s * 0.01s &#x3D; 1MB 的字节。</p>
<p>这个 1MB 是带宽和时延的乘积，所以它就叫「带宽时延积」（缩写为 BDP，Bandwidth Delay Product）。同时，这 1MB 也表示「飞行中」的 TCP 报文大小，它们就在网络线路、路由器等网络设备上。如果飞行报文超过了 1 MB，就会导致网络过载，容易丢包。</p>
<p><strong>由于发送缓冲区大小决定了发送窗口的上限，而发送窗口又决定了「已发送未确认」的飞行报文的上限。因此，发送缓冲区不能超过「带宽时延积」。</strong></p>
<p>发送缓冲区与带宽时延积的关系：</p>
<ul>
<li><p>如果发送缓冲区「超过」带宽时延积，超出的部分就没办法有效的网络传输，同时导致网络过载，容易丢包；</p>
</li>
<li><p>如果发送缓冲区「小于」带宽时延积，就不能很好的发挥出网络的传输效率。</p>
</li>
</ul>
<p>所以，发送缓冲区的大小最好是往带宽时延积靠近。</p>
<h4 id="怎样调整缓冲区大小？"><a href="#怎样调整缓冲区大小？" class="headerlink" title="怎样调整缓冲区大小？"></a>怎样调整缓冲区大小？</h4><p>在 Linux 中发送缓冲区和接收缓冲都是可以用参数调节的。设置完后，Linux 会根据你设置的缓冲区进行<strong>动态调节</strong>。</p>
<blockquote>
<p>调节发送缓冲区范围</p>
</blockquote>
<p>先来看看发送缓冲区，它的范围通过 tcp_wmem 参数配置；</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VibxhzMZM9RbmW68JG6zW07WuD2ENOp3icqCib8UTcf47Eqxnt2Vr1VxSQ/640?wx_fmt=png"></p>
<p>上面三个数字单位都是字节，它们分别表示：</p>
<ul>
<li><p>第一个数值是动态范围的最小值，4096 byte &#x3D; 4K；</p>
</li>
<li><p>第二个数值是初始默认值，87380 byte ≈ 86K；</p>
</li>
<li><p>第三个数值是动态范围的最大值，4194304 byte &#x3D; 4096K（4M）；</p>
</li>
</ul>
<p><strong>发送缓冲区是自行调节的</strong>，当发送方发送的数据被确认后，并且没有新的数据要发送，就会把发送缓冲区的内存释放掉。</p>
<blockquote>
<p>调节接收缓冲区范围</p>
</blockquote>
<p>而接收缓冲区的调整就比较复杂一些，先来看看设置接收缓冲区范围的 tcp_rmem 参数：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VnQk4W87CzoqZJjdFz3EpIWQRxEISic2oRV07Jzqfx8zkexesUJAiaibiag/640?wx_fmt=png"></p>
<p>上面三个数字单位都是字节，它们分别表示：</p>
<ul>
<li><p>第一个数值是动态范围的最小值，表示即使在内存压力下也可以保证的最小接收缓冲区大小，4096 byte &#x3D; 4K；</p>
</li>
<li><p>第二个数值是初始默认值，87380 byte ≈ 86K；</p>
</li>
<li><p>第三个数值是动态范围的最大值，6291456 byte &#x3D; 6144K（6M）；</p>
</li>
</ul>
<p><strong>接收缓冲区可以根据系统空闲内存的大小来调节接收窗口：</strong></p>
<ul>
<li><p>如果系统的空闲内存很多，就可以自动把缓冲区增大一些，这样传给对方的接收窗口也会变大，因而提升发送方发送的传输数据数量；</p>
</li>
<li><p>反正，如果系统的内存很紧张，就会减少缓冲区，这虽然会降低传输效率，可以保证更多的并发连接正常工作；</p>
</li>
</ul>
<p>发送缓冲区的调节功能是自动开启的，<strong>而接收缓冲区则需要配置 tcp_moderate_rcvbuf 为 1 来开启调节功能</strong>：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VU9rvtpQ0RDxwJLbhzw2EVqH9u9M3ICbx2qic4aPib8PUKrsB7mFVVLxw/640?wx_fmt=png"></p>
<blockquote>
<p>调节 TCP 内存范围</p>
</blockquote>
<p>接收缓冲区调节时，怎么知道当前内存是否紧张或充分呢？这是通过 tcp_mem 配置完成的：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VM77JxcchqRZvJ4tia1ZkzR6GKtwP2iccgOiaiaibqn3wYc7412lmbQwKxiaA/640?wx_fmt=png"></p>
<p>上面三个数字单位不是字节，而是「页面大小」，1 页表示 4KB，它们分别表示：</p>
<ul>
<li><p>当 TCP 内存小于第 1 个值时，不需要进行自动调节；</p>
</li>
<li><p>在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的大小；</p>
</li>
<li><p>大于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是无法建立的；</p>
</li>
</ul>
<p>一般情况下这些值是在系统启动时根据系统内存数量计算得到的。根据当前 tcp_mem 最大内存页面数是 177120，当内存为 (177120 * 4) &#x2F; 1024K ≈ 692M 时，系统将无法为新的 TCP 连接分配内存，即 TCP 连接将被拒绝。</p>
<blockquote>
<p>根据实际场景调节的策略</p>
</blockquote>
<p>在高并发服务器中，为了兼顾网速与大量的并发连接，<strong>我们应当保证缓冲区的动态调整的最大值达到带宽时延积，而最小值保持默认的 4K 不变即可。而对于内存紧张的服务而言，调低默认值是提高并发的有效手段。</strong></p>
<p>同时，如果这是网络 IO 型服务器，那么，<strong>调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力</strong>。需要注意的是，tcp_wmem 和 tcp_rmem 的单位是字节，而 tcp_mem 的单位是页面大小。而且，<strong>千万不要在 socket 上直接设置 SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能。</strong></p>
<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p>本节针对 TCP 优化数据传输的方式，做了一些介绍。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZciat6yMSZJ2QYWIldpAXY6VRHkVqDLvfa8VKqtbWJho3u5AWdqRQFbJjGZT6OwLJUcsSs9KKKDibZg/640?wx_fmt=png">数据传输的优化策略</p>
<p>TCP 可靠性是通过 ACK 确认报文实现的，又依赖滑动窗口提升了发送速度也兼顾了接收方的处理能力。</p>
<p>可是，默认的滑动窗口最大值只有 64 KB，不满足当今的高速网络的要求，要想要想提升发送速度必须提升滑动窗口的上限，在 Linux 下是通过设置 <code>tcp_window_scaling</code> 为 1 做到的，此时最大值可高达 1GB。</p>
<p>滑动窗口定义了网络中飞行报文的最大字节数，当它超过带宽时延积时，网络过载，就会发生丢包。而当它小于带宽时延积时，就无法充分利用网络带宽。因此，滑动窗口的设置，必须参考带宽时延积。</p>
<p>内核缓冲区决定了滑动窗口的上限，缓冲区可分为：发送缓冲区 tcp_wmem 和接收缓冲区 tcp_rmem。</p>
<p>Linux 会对缓冲区动态调节，我们应该把缓冲区的上限设置为带宽时延积。发送缓冲区的调节功能是自动打开的，而接收缓冲区需要把 tcp_moderate_rcvbuf 设置为 1 来开启。其中，调节的依据是 TCP 内存范围 tcp_mem。</p>
<p>但需要注意的是，如果程序中的 socket 设置 SO_SNDBUF 和 SO_RCVBUF，则会关闭缓冲区的动态整功能，所以不建议在程序设置它俩，而是交给内核自动调整比较好。</p>
<p>有效配置这些参数后，既能够最大程度地保持并发性，也能让资源充裕时连接传输速度达到最大值。</p>
<h3 id="巨人的肩膀"><a href="#巨人的肩膀" class="headerlink" title="巨人的肩膀"></a>巨人的肩膀</h3><p>[1] 系统性能调优必知必会. 陶辉. 极客时间.</p>
<p>[2] 网络编程实战专栏. 盛延敏. 极客时间.</p>
<p>[3] <a href="http://www.blogjava.net/yongboy/archive/2013/04/11/397677.html">http://www.blogjava.net/yongboy/archive/2013/04/11/397677.html</a></p>
<p>[4] <a href="http://blog.itpub.net/31559359/viewspace-2284113/">http://blog.itpub.net/31559359/viewspace-2284113/</a></p>
<p>[5] <a href="https://blog.51cto.com/professor/1909022">https://blog.51cto.com/professor/1909022</a></p>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>关于业务监控重试机制</title>
    <url>/2020/01/19/about-retry/</url>
    <content><![CDATA[<p>在使用监控系统的时候，一般在定义报警条件的时候都会在当某某报警连续出现N次之后才会被定义为报警(当然核心服务可能只出现一次就报警)， 当监控系统无法满足复杂业务需求的时候，可能就得自己写监控了, 那么必然经常会面临异常的逻辑发生, 最典型的比如网络不稳定、机房割接等操作，都有可能产生网络抖动，在这样的情况下并非服务不可用, 如何使监控更加健硕, 而不是告警满天飞就变得很有必要.</p>
<span id="more"></span>

<p>那么重试机制又该如何保障能够真正发现生产上的问题呢? 因为有可能频繁地重试会对服务产生影响.</p>
<h3 id="接口重试"><a href="#接口重试" class="headerlink" title="接口重试"></a>接口重试</h3><p>首先，要指出的是，这是说到的需要的重试的过程可以分为两类:</p>
<blockquote>
<ul>
<li>如果是网络协议出现异常, 比如说, http协议层面相关的问题, <strong>连接错误、连接超时</strong>, 这类的可以直接使用网络框架来做重试, 这个大部分都有参数能够指定</li>
</ul>
</blockquote>
<p>比如代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Operator_Requests</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        self.conn = self.req_conn(kwargs.get(<span class="string">&quot;max_retries&quot;</span>, HTTP_MAX_RETRIES)) <span class="comment"># 重试次数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">req_conn</span>(<span class="params">max_retries</span>):</span><br><span class="line">        conn = requests.session()</span><br><span class="line">        conn.mount(<span class="string">&quot;http://&quot;</span>, HTTPAdapter(max_retries=max_retries))</span><br><span class="line">        conn.mount(<span class="string">&quot;https://&quot;</span>, HTTPAdapter(max_retries=max_retries))</span><br><span class="line">        <span class="keyword">return</span> conn</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">type_map</span>(<span class="params">url, method, wrong, code, msg, rnt</span>):</span><br><span class="line">        msg = &#123;</span><br><span class="line">            <span class="string">&quot;ConnUrl&quot;</span>: url,</span><br><span class="line">            <span class="string">&quot;UrlMethod&quot;</span>: method,</span><br><span class="line">            <span class="string">&quot;ErrorType&quot;</span>: wrong,</span><br><span class="line">            <span class="string">&quot;HttpCode&quot;</span>: code,</span><br><span class="line">            <span class="string">&quot;ResponseMsg&quot;</span>: msg,</span><br><span class="line">            <span class="string">&quot;rntCode&quot;</span>: rnt</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> msg</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">http_get</span>(<span class="params">self, url, data=<span class="literal">None</span>, headers=<span class="literal">None</span>, timeout=HTTP_TIMEOUT</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = self.conn.get(url=url,</span><br><span class="line">                                params=data,</span><br><span class="line">                                headers=headers,</span><br><span class="line">                                timeout=timeout)</span><br><span class="line">            en_code = res.encoding <span class="keyword">if</span> res.encoding <span class="keyword">else</span> <span class="string">&quot;utf-8&quot;</span></span><br><span class="line">        <span class="keyword">except</span> (requests.exceptions.ConnectionError,</span><br><span class="line">                requests.exceptions.HTTPError) <span class="keyword">as</span> e:</span><br><span class="line">            msg = self.type_map(url, <span class="string">&quot;GET&quot;</span>, <span class="string">&quot;ConnError&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="built_in">str</span>(e), <span class="number">3</span>)</span><br><span class="line">            LOGGER.error(msg)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>, msg</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.Timeout <span class="keyword">as</span> e:</span><br><span class="line">            msg = self.type_map(url, <span class="string">&quot;GET&quot;</span>, <span class="string">&quot;ConnTimeout&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="built_in">str</span>(e), <span class="number">5</span>)</span><br><span class="line">            LOGGER.error(msg)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">5</span>, msg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="number">200</span> == res.status_code:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>, res</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                msg = self.type_map(</span><br><span class="line">                    url, <span class="string">&quot;GET&quot;</span>, <span class="string">&quot;NOT200&quot;</span>,</span><br><span class="line">                    <span class="built_in">str</span>(res.status_code) <span class="keyword">if</span> res.status_code <span class="keyword">else</span> <span class="string">&quot;Null&quot;</span>,</span><br><span class="line">                    res.text.encode(en_code) <span class="keyword">if</span> res.text <span class="keyword">else</span> <span class="string">&quot;&quot;</span>, <span class="number">7</span>)</span><br><span class="line">                LOGGER.error(msg)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">7</span>, msg</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">http_post</span>(<span class="params">self, url, data, headers=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = self.conn.post(url=url, data=data, headers=headers)</span><br><span class="line">            en_code = res.encoding <span class="keyword">if</span> res.encoding <span class="keyword">else</span> <span class="string">&quot;utf-8&quot;</span></span><br><span class="line">        <span class="keyword">except</span> (requests.exceptions.ConnectionError,</span><br><span class="line">                requests.exceptions.HTTPError) <span class="keyword">as</span> e:</span><br><span class="line">            msg = self.type_map(url, <span class="string">&quot;POST&quot;</span>, <span class="string">&quot;ConnError&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="built_in">str</span>(e), <span class="number">3</span>)</span><br><span class="line">            LOGGER.error(msg)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>, msg</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.Timeout <span class="keyword">as</span> e:</span><br><span class="line">            msg = self.type_map(url, <span class="string">&quot;POST&quot;</span>, <span class="string">&quot;ConnTimeout&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="built_in">str</span>(e), <span class="number">5</span>)</span><br><span class="line">            LOGGER.error(msg)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">5</span>, msg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="number">200</span> == res.status_code:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>, res</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                msg = self.type_map(</span><br><span class="line">                    url, <span class="string">&quot;POST&quot;</span>, <span class="string">&quot;NOT200&quot;</span>,</span><br><span class="line">                    <span class="built_in">str</span>(res.status_code) <span class="keyword">if</span> res.status_code <span class="keyword">else</span> <span class="string">&quot;Null&quot;</span>,</span><br><span class="line">                    res.text.encode(en_code) <span class="keyword">if</span> res.text <span class="keyword">else</span> <span class="string">&quot;&quot;</span>, <span class="number">7</span>)</span><br><span class="line">                LOGGER.error(msg)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">7</span>, msg</span><br></pre></td></tr></table></figure>

<p>这里相当于又封装了一层http常用的方法, <code>HTTP_MAX_RETRIES</code>指定了http接口的重试次数, 这个重试遵循http协议的相关标准, 一般不需要太关注</p>
<p><strong>http接口上出现的错误，在返回码上就很容易判断</strong>，从上面的代码就可以看出</p>
<h3 id="功能重试"><a href="#功能重试" class="headerlink" title="功能重试"></a>功能重试</h3><p>对于单个接口上的重试依托底层协议去完成. 重要的是功能重试</p>
<p><strong>因为对于某个具体的功能业说，只有人才真正明白涉及的逻辑</strong></p>
<p>比如想要监控<strong>页面登录</strong>这个功能是否正常, 那么需要定义好以下内容:</p>
<ol>
<li>要完成登录操作的输入是什么?</li>
<li>登录成功后的输出又是什么? </li>
<li>如何定义监控预期?</li>
</ol>
<p>关于第3点，**监控预期指的是怎么判断脚本执行完登录这个操作后是成功还是失败?**那么必定存在一个判断逻辑</p>
<p>现如今，微服务大行其道, 登录功能可能涉及到多个服务间的调用, <strong>任何一个服务出了问题,都将导致登录失败.</strong></p>
<p>那是不是每个服务都需要去写判断逻辑呢?</p>
<p>答案是否定的，<strong>所谓的功能监控，也就是黑盒监控(Blackbox)，是不需要关心这个功能会调用多少个服务的，把整个服务看成是个整体，单一的输入, 单一的输出, 然后用期望值与输出进行比较来判断登录功能是否正常</strong></p>
<p>在功能上重试重试，很容易想到的二个参数: <strong>重试次数、间隔时间</strong></p>
<p>有了这两个条件, 可以很大程度上减少产生误报的情况.</p>
<p>可以写一个实现重试功能的<strong>装饰器</strong>，这样就不用在每个业务逻辑上都把这个重试函数都实现一遍</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">do_retry</span>(<span class="params">retry_times=FUNC_RETRIES, delay_seconds=FUNC_RETRY_DELAY_SECONDS</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Decorator for function retry .</span></span><br><span class="line"><span class="string">    @param retry_times: retry times</span></span><br><span class="line"><span class="string">    @param delay_seconds: sleep time after per try</span></span><br><span class="line"><span class="string">    @return: the return value of the called function</span></span><br><span class="line"><span class="string">    @raise Exception: raises an exception when an error occurred after last time</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deco</span>(<span class="params">f</span>):</span><br><span class="line"><span class="meta">        @wraps(<span class="params">f</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            _retry_times, _delay_seconds = retry_times, delay_seconds</span><br><span class="line">            _err = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">while</span> _retry_times &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    _err = <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">return</span> f(*args, **kwargs)</span><br><span class="line">                <span class="keyword">except</span> RetryException <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="comment"># e is a RetryException instance</span></span><br><span class="line">                    <span class="comment"># so can call e.get_err()</span></span><br><span class="line">                    _err = e</span><br><span class="line">                    <span class="keyword">if</span> _delay_seconds &gt; <span class="number">0</span>:</span><br><span class="line">                        time.sleep(_delay_seconds)</span><br><span class="line">                    _retry_times -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> _err:</span><br><span class="line">                event = _err.get_err()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;_err--&gt;&quot;</span>, <span class="built_in">type</span>(event), event)</span><br><span class="line">                call_back(event)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> deco</span><br></pre></td></tr></table></figure>

<p>那么, 在业务逻辑上需要进行重试的时候, 只需要加入这个装饰器即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">    @do_retry(<span class="params"><span class="number">2</span>， <span class="number">10</span></span>)</span></span><br><span class="line"><span class="comment"># @run_time()  # @run_time(10)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">login</span>():</span><br><span class="line">  rnt, res = http_post(url=url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">if</span> rnt &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># httpcode is NOT 200</span></span><br><span class="line">        <span class="comment"># res:</span></span><br><span class="line">        <span class="comment">#   error msg</span></span><br><span class="line">        <span class="comment">#   type: dict</span></span><br><span class="line">        <span class="keyword">raise</span> RetryException(res, rnt)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># httpcode 200</span></span><br><span class="line">        _json = res.json()</span><br><span class="line">        <span class="comment"># but response is not as expect</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> != _json.get(<span class="string">&quot;error_code&quot;</span>):</span><br><span class="line">            event = self._buildup(_json)</span><br><span class="line">            <span class="keyword">raise</span> RetryException(event, rnt)</span><br></pre></td></tr></table></figure>

<p>这里使用了二层结构:</p>
<ol>
<li>如果http状态码不为200,则抛出<code>exception</code>重试</li>
<li>如果返回码为200,但是返回的数据不是期望值,抛出<code>exception</code>重试,这里的期望值为<code>登录成功后的可以从response中get到error_code=0</code></li>
</ol>
<p>如果<strong>任一模块出现问题</strong>, 则error_code必不会为0, 抛出<code>exception</code>, 这个<code>exception</code>在<code>do_retry</code>中被捕获后进行重试操作, 这里指定了重试次数及间隔时间. 在<code>do_retry</code>中，如果超过了重试次数后还是发生异常, 则会调用<code>call_back</code>函数, 那么就可以自定义触发的动作, 比如邮件报警等</p>
<p>这里，使用了<code>exception</code>, 也更加自由地对错误进行定制.</p>
<p>当然, <strong>生产环境下对功能进行监控，最好能够隔离生产数据</strong>, 比如使用一个只用于登录监控的用户名与密码, 这个用户可能没用任何权限, 只能够登录就行.</p>
<p>另一个见人见智了,最好能够做到功能监控随时能禁, 随时能启用, 或者说告警随时能屏蔽.</p>
<p>从这里也可以看出, 如果我只是对登录这个功能进行监控, 是不需要care登录这个功能涉及多少个模块的, 只需要关心最后收到的response是否符合，如果不符合预期，说明登录这个功能是异常的.</p>
<p>至于登录进行之后再做其它的操作有问题, 则不属于<code>登录</code>这个功能监控的范畴.</p>
<p>上述代码在本人写的大部分的功能监控一直使用, 后来发现个更强大的<code>Retry库</code><a href="https://github.com/zhoushuke/tenacity">tenacity</a>, 自由性更高, github在<a href="https://github.com/zhoushuke/tenacity">这里</a>,开箱即用.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/zhoushuke/tenacity">https://github.com/zhoushuke/tenacity</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>方法论</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title>解决Alpine镜像的not found解决</title>
    <url>/2020/12/13/alpine-run-go-binary/</url>
    <content><![CDATA[<p>CI阶段使用的镜像size要尽可能地小，这样在CI pull时使用的时间也会相应的短，所以有些base镜像都是基于alpine编译出来的</p>
<p>今天在<code>alpine</code>的<code>base</code>镜像中使用<code>mongoimport</code>时，提示了<code>not fuond</code>的奇怪问题，记录一下解决过程</p>
<span id="more"></span>

<p><code>alpine</code>使用的镜像为最新的<code>alpine:1.13</code>, <code>mongoimport</code>为官方使用go 1.15编译出来的二进制文件</p>
<p><code>Dockfile</code>文件如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM alpine/git:latest</span><br><span class="line"><span class="comment"># ... 省略</span></span><br><span class="line">COPY mongoimport /usr/bin/mongoimport</span><br><span class="line"><span class="comment"># ... 省略</span></span><br></pre></td></tr></table></figure>

<p>镜像编译完成之后运行起来后，会提示以下的错误:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">main.sh: workflow.sh: line 133: mongoimport: not found</span><br></pre></td></tr></table></figure>

<p><code>not found</code>这个很奇怪,我开始以为是没有找到<code>mongoimport</code>这个命令，但使用<code>ls或者which</code>后发现在<code>/usr/bin</code>是存在<code>mongoimport</code></p>
<p>但这个<code>not found</code>指的是什么呢？</p>
<p>一番<code>google</code>后发现在使用<code>alpine</code>时，如果按正常的go编译命令编译出来的二进制文件或多或少地地出现这个问题，原因是什么呢?</p>
<p>这个就要从<code>alpine</code>使用的musl libc说起，而对应的则是gnu libc，这两者有什么区别呢?</p>
<p><code>gnu libc</code>是标准的libc的库，我们大部分时间编译出来的都是可以使用这个库</p>
<p>而Alpine<code>采用了</code>musl libc<code>和</code>busybox&#96; 以减小系统的体积和运行时资源消耗</p>
<p>所以用alpine做base的镜像体积非常的小， 很适合用于CI阶段</p>
<p><strong>musl libc只能说是部分兼容&#96;gnu libc，因此也不是所有的go编译出来的二进制都无法使用</strong></p>
<p>这两者详细的区别大家可考虑这些链接:</p>
<p><a href="https://blog.csdn.net/liumiaocn/article/details/89702529">https://blog.csdn.net/liumiaocn/article/details/89702529</a></p>
<p>回到问题本身，上面提示<code>not found</code>是不是也是因为<code>mongoimport</code>编译的时候使用的是动态库,要怎么解决呢?</p>
<p>从网上的解决方案来看,有以下几种解决方案:</p>
<ol>
<li><code>RUN mkdir /lib64 &amp;&amp; ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2</code></li>
<li>RUN apk –no-cache add  libc6-compat libgcc libstdc++</li>
<li>go编译时指定<code>CGO_ENABLED=0 go build .</code></li>
</ol>
<p> 第一种方式我尝试过，可惜，可以解决<code>not found</code>的报错，也就是说可以找到二进制命令了，但是命令中如果引用了其它的动态库，则还是会有问题，会提示如下的报错:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error loading shared library libgssapi_krb5.so.2: No such file or directory (needed by /usr/bin/mongoimport)</span><br><span class="line">Error loading shared library libkrb5.so.3: No such file or directory (needed by /usr/bin/mongoimport)</span><br><span class="line">Error relocating /usr/bin/mongoimport: gss_release_buffer: symbol not found</span><br><span class="line">Error relocating /usr/bin/mongoimport: gss_init_sec_context: symbol not found</span><br></pre></td></tr></table></figure>

<p>开始的时候为了保证镜像的size尽可能的小，还打算将错误提示中的so手工放到镜像中，考虑后续不可维护，就没有再尝试</p>
<p>对于方式2我也尝试了，<code>libc6-compat</code>是libc的兼容包, 大部分提示<code>not found</code>的问题都可以通过安装这个包进行解决,本人未解决</p>
<p>由于mongodb官方提供的tools版本没有基于alpine可用的，对于方式3本人未尝试，原因在于<code>mongodb-tools</code>提供的编译工具相对比较麻烦, 而且我只是需要一个<code>mongoimport</code>，官方的编译工具没办法只编译<code>mongoimpmort</code>,理论来说指定<code>CGO_ENABLED=0</code>编译出来的二进制是可用的</p>
<p>本人使用最后一种方式，这里偷机了一把, 随便run一个alpine的镜像，然后在镜像中<code>add mongodb-tools</code>,最后在<code>/usr/bin/mongoimport</code>这单个二进制复制出来，然后放到base镜像中，这样避免直接在base中安装，降低size</p>
<p>之前网上就看到要慎用alpine，会出现各种各样奇怪的问题, 看来是真的</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://pkgs.alpinelinux.org/">https://pkgs.alpinelinux.org/</a></li>
<li><a href="https://github.com/mongodb/mongo-tools">https://github.com/mongodb/mongo-tools</a></li>
<li><a href="https://blog.zjyl1994.com/post/alpine-cgo/">https://blog.zjyl1994.com/post/alpine-cgo/</a></li>
<li><a href="https://studygolang.com/articles/26851">https://studygolang.com/articles/26851</a></li>
<li><a href="https://blog.csdn.net/liumiaocn/article/details/89702529">https://blog.csdn.net/liumiaocn/article/details/89702529</a></li>
<li><a href="https://www.mongodb.com/try/download/database-tools?tck=docs_databasetools">https://www.mongodb.com/try/download/database-tools?tck=docs_databasetools</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title>argoCD学习(argocd是如何绑定资源的)</title>
    <url>/2021/04/02/argoCD-how-to-bind-resource/</url>
    <content><![CDATA[<p>目前团队使用argo-cd做为统一发布平台，承接所有的环境的发布，最近碰到一个很有意思的问题(下文描述)，跟随这个问题引出另一个问题: argo-cd是如何绑定资源的呢？用了argo-cd这么久了，感叹自己从来都没细致研究过，呵呵了.</p>
<span id="more"></span>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p><strong>简单描述就是，argo上的应用出现了本不属于它资源</strong></p>
<p>这里使用kustomize，kustomization.yaml文件如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">configmap.yml.j2</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">deployment.yml.j2</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">service.yml.j2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">test</span></span><br></pre></td></tr></table></figure>

<p>正常来说，发布到k8s中的资源对象，只会有resources下指定的，但是在某个时间点，argo ui上却出现了其它资源，状态是这样的</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210405221303.png"></p>
<p>作者非常确定这个资源对象不包含在上述3个文件中，那这个对象是怎么来的呢?</p>
<h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>作者记得之前应该出现过一二次，但由于当时没有时间仔细排查，还以为是开发团队自己的原因，即没有深究，这次又出现，作者感到绝非偶然，是时候认真排查一下了</p>
<p>作者又想起，另一个组员跟我反映过，测试环境某个服务的NodePort好几次都莫名地被删除了，测试环境开发是没有权限做删除操作的，但也是作者小组内的同事删的，那又是怎么回事呢?</p>
<p>由于作者在发布某应用，使用的命令如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">argocd --prune</span><br></pre></td></tr></table></figure>

<p>其中 –prune会将当前集群中存在的但是在git上不存在的资源进行delete.</p>
<p>嗯，这二者有关系</p>
<p>从上面的图中来看，这个资源对象的名称非常符合作者对使用NodePort的统一命名规范，即应用的名字-nodeport,可以肯定的是，这个资源是组员为了开发的某个需求创建的，且是在rancher UI(一种开源的k8s集群管理平台)上进行创建的，这里要说明一下，rancher ui上支持直接对常用资源比如deployment,service等对象进行clone</p>
<p>跟同事确认了，这个资源也确实是这么创建的，那这个资源又是如何同步反应到argo上的呢?</p>
<p>因为在作者看来，argo只会关联kustomization.yaml中resource指定的资源，这个不速之客显然不属于</p>
<p>这就引入一个作者想错或者未细致地了解的问题: argo到底是根据什么绑定资源的呢?</p>
<p>带着这个疑问，开始找官方文档，作者从将argo-cd引入到生产环境之前，已经将argo-cd官方的文档翻了好几遍，好像没有映象有这方面的介绍，也确实没有找到相关文档</p>
<p>作者这里犯了个错误，没有第一时间检查clone资源有没有差异，而是直接从argo-cd上查找答案，导致将问题复杂化了.</p>
<p>作者从这个<a href="https://github.com/argoproj/argo-cd/issues/5792">issue</a>上找到了问题所在，然后再回头查看clone的资源，然后做了个实验确认事实即是如此</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>由于argo controller是根据资源中的annotations里的key(argocd.argoproj.io&#x2F;instance)来判定这个资源是不是属于某个app,在日常使用的过程，由于团队中使用的是rancher做为容器管理平台，rancher上提供了非常便利的对资源的clone手段，就会产生clone后的资源也会带上相同的annotations，但是在原app下的manifest并不包含这个资源，因此在app下就会出现这个资源且这个资源会被标注为<code>require pruning</code>,那么在每次sync后，这个clone后的资源就会被删除, 这也就解释了为什么会出现NodePort会莫名其妙地被删了</p>
<p>那么解决也很简单，在clone时将annotations删除掉即可，这样argo-controller就不会将其绑定到某应用下.</p>
<h3 id="误区"><a href="#误区" class="headerlink" title="误区"></a>误区</h3><p>这个有个作者的误区，即作者以为argo-cd只会关联kustomization.yaml中resource指定的资源(这里以kustomize为例，argo-cd支持的其它的如helm原理也是如此)，但具体如何关联，作者没有深究，也没有第一时间想到，argo-cd其实也是通过CRD controller机制实现</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/argoproj/argo-cd/issues/5792">https://github.com/argoproj/argo-cd/issues/5792</a></li>
<li><a href="https://argoproj.github.io/argo-cd/">https://argoproj.github.io/argo-cd/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>argoCD学习(使用argoCD实现多集群应用同步)</title>
    <url>/2020/12/04/argoCD-Usage/</url>
    <content><![CDATA[<p>运维中经常会存在多套的环境，开发、测试、stagging、prod等，这么多的环境，对于开发同学，如何通过一次部署多环境上线，打通开发与测试间的gap，而对于运维同学来说，则关心如何保障环境之间应用版本一致，而argoCD就是这样一个工具，配合GitOps思路，可以实现对多集群的应用版本管理.</p>
<span id="more"></span>

<h3 id="argoCD"><a href="#argoCD" class="headerlink" title="argoCD"></a>argoCD</h3><p>Declarative GitOps CD for Kubernetes</p>
<p>官方的说明很简单，为k8s提供GitOps能力</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://argoproj.github.io/argo-cd/assets/argocd_architecture.png" alt="架构图"></p>
<p>上面是官方的提供的架构图, 可以很清晰的看到argocd的工作机制.</p>
<p>argoCD通过发布到k8s的crd控制器连续监视正在运行的应用程序,并将当前的活动状态与所需的目标状态（在Git存储库中指定）进行比较</p>
<p>下面就从一个demo来体验一下，先从安装开始</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create namespace argocd</span><br><span class="line">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</span><br></pre></td></tr></table></figure>

<h4 id="helm安装"><a href="#helm安装" class="headerlink" title="helm安装"></a>helm安装</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm repo add argo https://argoproj.github.io/argo-helm</span><br><span class="line">helm install argocd -n argocd argo/argo-cd --values values.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat values.yaml,可以在安装的时候就指定使用ingress</span></span><br><span class="line">server:</span><br><span class="line">  ingress:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">    annotations: </span><br><span class="line">      kubernetes.io/ingress.class: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">      nginx.ingress.kubernetes.io/force-ssl-redirect: <span class="string">&quot;true&quot;</span></span><br><span class="line">      nginx.ingress.kubernetes.io/ssl-passthrough: <span class="string">&quot;true&quot;</span></span><br><span class="line">      nginx.ingress.kubernetes.io/backend-protocol: <span class="string">&quot;HTTPS&quot;</span></span><br><span class="line">    hosts:</span><br><span class="line">    - argocd.local</span><br></pre></td></tr></table></figure>

<p>所有pod起来之后即可访问web页面</p>
<p>默认的用户名为 admin，密码为 server Pod 的名称，可以通过如下所示的命令来获取：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | <span class="built_in">cut</span> -d<span class="string">&#x27;/&#x27;</span> -f 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用以下命令暴露argocd ui， (不建议)</span></span><br><span class="line">kubectl port-forward service/argocd-server -n argocd 38888:443</span><br></pre></td></tr></table></figure>

<h4 id="ingress"><a href="#ingress" class="headerlink" title="ingress"></a>ingress</h4><p>添加ingress用于暴露argo cd UI</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: argocd-server-ingress</span><br><span class="line">  namespace: argocd</span><br><span class="line">  annotations:</span><br><span class="line">    cert-manager.io/cluster-issuer: letsencrypt-prod</span><br><span class="line">    kubernetes.io/ingress.class: nginx</span><br><span class="line">    kubernetes.io/tls-acme: <span class="string">&quot;true&quot;</span></span><br><span class="line">    nginx.ingress.kubernetes.io/ssl-passthrough: <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="comment"># If you encounter a redirect loop or are getting a 307 response code</span></span><br><span class="line">    <span class="comment"># then you need to force the nginx ingress to connect to the backend using HTTPS.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</span></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: argocd.local</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: argocd-server</span><br><span class="line">          servicePort: https</span><br><span class="line">        path: /</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - argocd.example.com</span><br><span class="line">    secretName: argocd-secret <span class="comment"># do not change, this is provided by Argo CD</span></span><br></pre></td></tr></table></figure>



<h3 id="多集群支持"><a href="#多集群支持" class="headerlink" title="多集群支持"></a>多集群支持</h3><p>目前无法直接在UI上添加其它集群，可使用以下命令添加外部集群，这样就可<strong>使用一套argoCD来同步到多个集群环境</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先进行argocd的登录</span></span><br><span class="line">argocd login argocd.example.com:4433 --grpc-web</span><br><span class="line"><span class="comment"># 输入用户名密码即可，出现successfully</span></span><br><span class="line">argocd cluster add mysql-cluster --server <span class="string">&#x27;argocd.example.com:4433&#x27;</span></span><br><span class="line"><span class="comment"># 出现以下提示则说明添加成功</span></span><br><span class="line">INFO[0000] ServiceAccount <span class="string">&quot;argocd-manager&quot;</span> already exists <span class="keyword">in</span> namespace <span class="string">&quot;kube-system&quot;</span></span><br><span class="line">INFO[0000] ClusterRole <span class="string">&quot;argocd-manager-role&quot;</span> updated</span><br><span class="line">INFO[0000] ClusterRoleBinding <span class="string">&quot;argocd-manager-role-binding&quot;</span> updated</span><br><span class="line">Cluster <span class="string">&#x27;https://xxx.xxx.xxx.xxx:6443&#x27;</span> added</span><br></pre></td></tr></table></figure>

<p>那么对于多集群发同步来说，最终实现的效果为:</p>
<p><img src="https://www.gitops.tech/images/multiple.png"></p>
<p>当然，每个集群的下的应用的触发逻辑可以在CI阶段定义, 这样可以跟测试stag更好地集成,从而实现真正的一次Push，多次发布.</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>通过上面的方式安装完成之后，即可访问argoCD的web页，可以按照不同的环境将app进行分类，同时argocd提供了project的概念, 可以将你认为可以属于同一个project的所有应用划为一类，这样可以避免同一个应用要在多个环境下发布需要多次添加的问题.</p>
<h4 id="添加Project"><a href="#添加Project" class="headerlink" title="添加Project"></a>添加Project</h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201121212553.png"></p>
<p>主要的就是sources跟destinations</p>
<p>sources指定的是git repo的地址，可以添加多个</p>
<p>destinations指定集群，也可以指定多个</p>
<p>这样添加之后在新增app时即可从project中选择了</p>
<h4 id="添加Repository"><a href="#添加Repository" class="headerlink" title="添加Repository"></a>添加Repository</h4><p>repository指的是git repo的地址，在发布时argocd会连接到该git上拉取资源, 这里需要提供git的认证方式，常用的为用户名、密码.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201121211926.png"></p>
<p>要注意的是: <strong>repository URL需要以.gti结尾， 要不然连接的时候会提示301</strong></p>
<p>参考 : <a href="https://argoproj.github.io/argo-cd/user-guide/private-repositories/">https://argoproj.github.io/argo-cd/user-guide/private-repositories/</a></p>
<h4 id="添加APP"><a href="#添加APP" class="headerlink" title="添加APP"></a>添加APP</h4><p>app做为GitOps的最小发布单元, 一个app中包含了所有的跟发布相关的资源文件，比如策略，代码，配置，甚至监控事件和版本控制等</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201121212553.png"></p>
<p>上面要指定git地址及要发布的kubernetes集群及git上的目录，这个目录用于argoCD检测是否具有<code>更新</code>.</p>
<p>argoCD支持了目前常用的资源管理工具，比如helm， kustomize, directory, Ksonnet, Jsonnet等工具，我这里使用的是Kustomize,k8s原生就支持，没有任何依赖，非常方便.</p>
<p><strong>最重要的是指定是自动同步还是手动同步</strong>, 不同的同步策略使用场景不同</p>
<p>添加成功后如下图所示:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201206154807.png"></p>
<p>应用发布成功后，效果如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201122003139.png"></p>
<p>从上面可以看到应用的发布历史，出于数据存储限制，默认情况下只会保留最新的10条，如果觉得不合理可以通过修改app进行调整，同时也能看到同步的状态及发布是否成功</p>
<p>这里要注意的是<code>Sync OK</code>只是代表从git上同步资源成功了，但能不能发布成功是通过<code>Healthy</code>来看的</p>
<h3 id="同步策略及发布状态"><a href="#同步策略及发布状态" class="headerlink" title="同步策略及发布状态"></a>同步策略及发布状态</h3><h4 id="同步策略"><a href="#同步策略" class="headerlink" title="同步策略"></a>同步策略</h4><p>argoCD应用可以选择是自动同步还是手工同步来触发发布上线，这个就要看具体的场合了</p>
<p>自动同步: 指的是git如果发生变化则直接触发发布,不需要人介入， 这个比较适合用于持续发布的测试环境</p>
<p>手工同步: 指的则是人为地去页面上点击同步的方式进行,比较适合生产类稳定环境</p>
<p>同时，还有一些同步策略，比如是否强制对线上资源进行强制删除后再更新、指定需要同步的文件、同步时间窗口等等比较人性化的策略</p>
<p>详情可参考<a href="https://argoproj.github.io/argo-cd/user-guide/selective_sync/">这里</a></p>
<h4 id="发布状态"><a href="#发布状态" class="headerlink" title="发布状态"></a>发布状态</h4><p>在同步完成之后便会触发发布上线，状态分为Processing、unHealty、Healty</p>
<p>Processing表示正在更新资源</p>
<p>unHealty表示发布失败, 发布失败可以通过点击详情进行查看原因</p>
<p>Healty则表示发布成功, 所有的资源都以git上的版本运行.</p>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>相信大家都有这样的需求: 同一个应用需要在多个集群中做发布，目前对于单应用多集群的部署官方支持得还不是很好，因为应用名必须唯一，官方有个issue讨论了为何不支持这种场景，主要<strong>担心多个集群的查询性能</strong>，感兴趣的可以参考<a href="https://github.com/argoproj/argo-cd/issues/1673">support multiple clusters (destinations) for an Application</a></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>执行login 登录命令时提示:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">argocd login argocd.local:4433</span><br></pre></td></tr></table></figure>

<p>出现以下错误</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201106184926.png"></p>
<p>原因: </p>
<p>参考: <a href="https://github.com/argoproj/argo-cd/issues/1415">https://github.com/argoproj/argo-cd/issues/1415</a></p>
<p>解决:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">argocd login argocd.local:4433 --grpc-web</span><br></pre></td></tr></table></figure>

<p>如果添加集群的时候提示以下错误，则需要先使用上面的命令进行登录，登录成功后再进行集群的添加操作.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201106185339.png"></p>
<ol start="2">
<li>创建token时提示以下错误:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to generate new token: account <span class="string">&#x27;admin&#x27;</span> does not have apiKey capability</span><br></pre></td></tr></table></figure>

<p>原因: argocd的用户也是通过角色来进行划分的，创建token的用户需要具有 apiKey的权限，这个可以从argocd部署的configmap中查看使用的用户是否具有什么权限.</p>
<ol start="3">
<li>回滚时提示以下错误:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to load data: revision HEAD must be resolved</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201122185428.png"></p>
<p>原因: 如果使用argocd命令行的方式同步应用，会导致revision没有commit值，如上图，而回滚时需要revision，因此造成上面的报错.</p>
<h3 id="CLI常用命令"><a href="#CLI常用命令" class="headerlink" title="CLI常用命令"></a>CLI常用命令</h3><p>argoCD也提供了CLI工具，可以直接使用该工具来对argoCD进行操作，常用的命令如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改账号密码,然后根据提示完成</span></span><br><span class="line">argocd account update-password</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录</span></span><br><span class="line">argocd login argocd.local:4433 --grpc-web --username admin --password xxx --insecure</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看应用状态</span></span><br><span class="line">argocd app get guestbook</span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">argocd app <span class="built_in">set</span> argocd-kustomize-frontend --kustomize-image xxx/argocd-kustomize-frontend:testwithtag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步应用</span></span><br><span class="line">argocd app <span class="built_in">sync</span> guestbook</span><br></pre></td></tr></table></figure>



<p>这里只是体验了一下argoCD的使用过程，很多有趣的功能还没有介绍，下次再更.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://argoproj.github.io/argo-cd/">https://argoproj.github.io/argo-cd/</a></li>
<li><a href="https://www.gitops.tech/#:~:text=GitOps%20is%20a%20way%20of,Git%20and%20Continuous%20Deployment%20tools">https://www.gitops.tech/#:~:text=GitOps%20is%20a%20way%20of,Git%20and%20Continuous%20Deployment%20tools</a></li>
<li><a href="https://www.jianshu.com/p/eec8e201b7e9">https://www.jianshu.com/p/eec8e201b7e9</a></li>
<li><a href="https://www.qikqiak.com/post/gitlab-ci-argo-cd-gitops/">https://www.qikqiak.com/post/gitlab-ci-argo-cd-gitops/</a></li>
<li><a href="https://argoproj.github.io/argo-cd/user-guide/private-repositories/">https://argoproj.github.io/argo-cd/user-guide/private-repositories/</a></li>
<li><a href="https://github.com/argoproj/argo-cd/issues/1673">https://github.com/argoproj/argo-cd/issues/1673</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible学习(ansible基础使用)</title>
    <url>/2019/02/23/ansible-basic-theory/</url>
    <content><![CDATA[<p>Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs.</p>
<span id="more"></span>



<ul>
<li><a href="#Ansible">Ansible</a><ul>
<li><a href="#%E5%B7%A5%E4%BD%9C%E6%9E%B6%E6%9E%84">工作架构</a></li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8">基本使用</a></li>
<li><a href="#Playbook/Role">Playbook&#x2F;Role</a></li>
<li><a href="#%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6">执行机制</a></li>
</ul>
</li>
</ul>
<p><code>Ansible is a radically simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs.</code></p>
<blockquote>
<ul>
<li>simple, based on python&#x2F;jinja</li>
<li>no agent, based on ssh(default)</li>
<li>modular design</li>
<li>idempotency</li>
</ul>
</blockquote>
<h4 id="工作架构"><a href="#工作架构" class="headerlink" title="工作架构"></a><strong>工作架构</strong></h4><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191122154626140.png"></p>
<p><code>ansible只是一种框架</code>, 真正执行任务是各种模块:</p>
<blockquote>
<ul>
<li>connection plugins：连接插件，负责和被监控端实现通信</li>
<li>host inventory：主机清单，是一个配置文件里面定义监控的主机</li>
<li>modules：ansible自身核心模块、自定义模块</li>
<li>Plugins：借助于插件完成记录日志邮件等功能</li>
<li>playbook：剧本执行多个任务</li>
</ul>
</blockquote>
<h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a><strong>基本使用</strong></h4><p><code>在实际使用中, 为了安全一般不会直接在配置中使用明文, 都会提前配置好免密登录.</code></p>
<p>常用ansible的执行有以下两种方式(<strong>ansible-galaxy没用过,不讨论</strong>):</p>
<h5 id="ad-hoc"><a href="#ad-hoc" class="headerlink" title="ad-hoc"></a><strong>ad-hoc</strong></h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ansible  [ -i  inventory文件 ]  分组名  -m  模块名  -a <span class="string">&#x27;模块参数&#x27;</span>  ansible参数</span><br></pre></td></tr></table></figure>

<p><a href="https://docs.ansible.com/ansible/latest/modules/modules_by_category.html">官方模块列表</a></p>
<p>示例：</p>
<p><code>cat /etc/ansible/hosts</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">[<span class="string">all-machines</span>]</span><br><span class="line"><span class="number">172.16</span><span class="number">.104</span><span class="number">.210</span></span><br><span class="line"><span class="number">172.16</span><span class="number">.104</span><span class="number">.211</span></span><br><span class="line"></span><br><span class="line">[<span class="string">echo_date</span>]</span><br><span class="line"><span class="number">172.16</span><span class="number">.104</span><span class="number">.210</span></span><br></pre></td></tr></table></figure>

<p>Hosts文件称之为<code>host inventory</code>文件, 该文件指定机器组别.</p>
<p><code>ansible -i /etc/ansible/hosts echo_date -m shell -a &#39;echo $(ifconfig|grep eth0)&#39;</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191122161721880.png"></p>
<p>这里使用了免密登录, 所以可以直接执行, 如果没有免密登录的情况下, 可以在执行时输入密码(由于只能输入一次密码，这种情况要求所有机器上所使用的密码一致)或者直接在主机清单文件中直接指定用户名密码</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">k8snode-1</span> <span class="string">ansible_host=172.16.104.210</span> <span class="string">ansible_user=root</span> <span class="string">ansible_ssh_pass=&#x27;xxxx&#x27;</span></span><br><span class="line">[<span class="string">echo_date</span>]</span><br><span class="line"><span class="string">k8snode-1</span></span><br></pre></td></tr></table></figure>

<h5 id="playbook"><a href="#playbook" class="headerlink" title="playbook"></a><strong>playbook</strong></h5><p><code>cat prepare.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">hosts:</span> <span class="string">echo_date</span></span><br><span class="line"> <span class="attr">roles:</span></span><br><span class="line">   <span class="bullet">-</span> &#123; <span class="attr">role:</span> <span class="string">common</span> &#125;</span><br></pre></td></tr></table></figure>

<p><code>ansible-playbook prepare.yml</code></p>
<p>playbook详见下文</p>
<h4 id="Playbook-x2F-Role"><a href="#Playbook-x2F-Role" class="headerlink" title="Playbook&#x2F;Role"></a><strong>Playbook&#x2F;Role</strong></h4><p><code>Playbook及role是ansible为了实现更加复杂的编排任务及更好地组织对象而设定的抽象对象.</code></p>
<p>role主要将整个任务进行封装从而实现复用</p>
<p>playbook则明确定义要完成这个任务需要执行哪些步骤</p>
<h5 id="一个role的标准目录"><a href="#一个role的标准目录" class="headerlink" title="一个role的标准目录"></a><strong>一个role的<code>标准目录</code></strong></h5><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/unknown-4411877.png"></p>
<p>那么在其它有需要执行这个工作的地方则可以直接引用这个角色(或者角色中的部分步骤).</p>
<p>playbook则明确定义了完成这个role需要执行的tasks.</p>
<h5 id="playbook的样例"><a href="#playbook的样例" class="headerlink" title="playbook的样例"></a><strong>playbook的样例</strong></h5><p><code>cat roles/common/tasks/main.yml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;make sure system:public-info-viewer cluster role manifests&quot;</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">src:</span> <span class="string">&quot;system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line">    <span class="attr">dest:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; kube_config_dir &#125;&#125;</span>/kubernetes-api-system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;enabled kubernetes api /version,/version/ auth&quot;</span></span><br><span class="line">  <span class="attr">shell:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; kubectl &#125;&#125;</span> apply -f <span class="template-variable">&#123;&#123; kube_config_dir &#125;&#125;</span>/kubernetes-api-system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line"> <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>两个大括号代表的是变量,在执行playbook前由jinja语法进行渲染, 执行时会被真正的value替换.</p>
<p>当然,playbook中配合各类module,可以实现更加复杂的逻辑</p>
<p><a href="https://docs.ansible.com/ansible/latest/modules/modules_by_category.html">官方模块列表</a></p>
<h6 id="var"><a href="#var" class="headerlink" title="var"></a><strong>var</strong></h6><p>ansible中的变量规则比较灵活, 允许我们在<code>多处定义同一个变量</code>, 再根据优先级决定执行时使用哪个变量.</p>
<p>常用的有以下几种定义变量方式(<code>优先级从高到低</code>):</p>
<blockquote>
<ul>
<li>set_fact&#x2F;registry_vars</li>
<li>roles&#x2F;vars&#x2F;main.yml    </li>
<li>group_vars&#x2F;all.yaml</li>
<li>roles&#x2F;defaults&#x2F;main.yml</li>
</ul>
</blockquote>
<p>其它方式定义的变量优先级参考<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#variable-precedence-where-should-i-put-a-variable">官方文档</a></p>
<h6 id="tag"><a href="#tag" class="headerlink" title="tag"></a><strong>tag</strong></h6><p><code>cat roles/common/tasks/main.yml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;make sure system:public-info-viewer cluster role manifests&quot;</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">src:</span> <span class="string">&quot;system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line">    <span class="attr">dest:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; kube_config_dir &#125;&#125;</span>/kubernetes-api-system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;enabled kubernetes api /version,/version/ auth&quot;</span></span><br><span class="line">  <span class="attr">shell:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; kubectl &#125;&#125;</span> apply -f <span class="template-variable">&#123;&#123; kube_config_dir &#125;&#125;</span>/kubernetes-api-system-public-info-viewer-cluster-role.yml&quot;</span></span><br><span class="line">  <span class="attr">tag:</span> <span class="string">enabled_kubernetes</span></span><br></pre></td></tr></table></figure>

<p>进行引用时可以指定存在tag的是执行还是跳过</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">hosts:</span> <span class="string">kube-master[0]</span></span><br><span class="line">  <span class="attr">roles:</span></span><br><span class="line">    <span class="bullet">-</span> &#123; <span class="attr">role:</span> <span class="string">common</span> &#125;</span><br><span class="line">  <span class="attr">tags:</span> <span class="string">enabled_kubernetes</span></span><br></pre></td></tr></table></figure>

<p>ansible中内置4种tag: always、tagged、untagged、all 是四个系统内置的tag，有自己的特殊意义</p>
<blockquote>
<ul>
<li>always: 指定这个tag 后，task任务将永远被执行，除非明确使用–skip-tags always标记</li>
<li>tagged: 当 –tags 指定为它时，则只要有tags标记的task都将被执行</li>
<li>untagged: 当 –tags 指定为它时，则所有没有tag标记的task 将被执行</li>
<li>all: 这个标记无需指定，ansible-playbook 默认执行的时候就是这个标记.所有task都被执行</li>
</ul>
</blockquote>
<p><code>通过tag机制可以实现执行部分role功能.</code></p>
<h6 id="template"><a href="#template" class="headerlink" title="template"></a><strong>template</strong></h6><p><code>cat roles/common/template/docker_auth.j2</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">&quot;auths&quot;:</span> &#123;</span><br><span class="line">		<span class="string">&quot;<span class="template-variable">&#123;&#123; docker_registry &#125;&#125;</span>&quot;</span><span class="string">:</span> &#123;</span><br><span class="line">			<span class="attr">&quot;auth&quot;:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; docker_auth &#125;&#125;</span>&quot;</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>template通过jinja2语法进行渲染, 模块中的变量由参数传递过来, 参数<a href="#var">var</a></p>
<p>templates机制可以让我们把共性的部分固定,将可变的部分由参数传递进模块,实现参数化.</p>
<h4 id="执行机制"><a href="#执行机制" class="headerlink" title="执行机制"></a><strong>执行机制</strong></h4><p>ansible执行分为同步和异步:</p>
<h5 id="同步模式"><a href="#同步模式" class="headerlink" title="同步模式"></a><strong>同步模式</strong></h5><p>也是默认值,<code>按批执行</code>, 即<code>先在一批机器上(取决于fork的数量)执行playbook中第一个任务, 直到这批机器都执行完(默认配置下就算有机器提前执行完也需要等待所有机器都执行完这个任务)之后再在下一批机器执行第一个任务,待该任务在所有的机器上都执行完成之后，然后重新回到第一批机器开始执行playbook的下一个动作,依次循环</code></p>
<p><strong>ansible2.0+可指定<code>strategy=free</code> 即可让提前完成的机器无需等待同批机器一同完成而快速地让下批机器执行.</strong></p>
<p>默认情况下的playbook执行过程:</p>
<p>从这里也可以看出<code>playbook中的task是所有机器都执行完之后再执行下一个task</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/ansible-tasks.jpg"></p>
<h5 id="异步模式"><a href="#异步模式" class="headerlink" title="异步模式"></a><strong>异步模式</strong></h5><p>ansible直接将机器的任务(取决于fork的数量)放在后台执行, 并每隔一段时间去检查这些节点的执行完成情况(通过job-id),同样直到这批机器都执行完(就算有机器提前执行完也需要等待所有机器执行完),ansible才会将下一批机器放在后台执行. 异步通过<code>async跟poll</code>两个参数决定</p>
<p><code>同步跟异步的主要区别就是异步是通过轮询状态来判断任务是否执行成功的.</code></p>
<p><code>特殊地: 如果poll参数被指定为0, 则ansible在将机器的任务放在后台执行后立刻返回, 迅速地在下一批机器开始执行</code></p>
<p><code>也就是说ansible此时并不会管各机器的任务是否执行成功.</code></p>
<p>通过以上我们知道ansible默认是通过ssh远程登录执行的, 这个时间其实是受ssh的timeout控制, 如果任务的执行时间超过ssh的timeout时间, ssh会被断开, ansible会认为该任务失败.</p>
<p>因此, 我们可以直接将任务放到后台执行, 后续定时地去轮询任务执行完成与否. 这样即不受ssh Timeout限制.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">host:</span> <span class="string">all</span></span><br><span class="line"><span class="comment">#strategy: free/linear</span></span><br><span class="line"><span class="attr">max_fail_percentage:</span> <span class="number">30</span>    <span class="comment">#serial指定的节点个数中执行失败的host大于这个百分比,则终止这个任务.</span></span><br><span class="line"><span class="attr">serial:</span> <span class="number">5</span>    <span class="comment">#指定一次执行多少host</span></span><br><span class="line"><span class="attr">tasks:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&#x27;YUM - async task&#x27;</span></span><br><span class="line">      <span class="attr">yum:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">docker-io</span></span><br><span class="line">        <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line">      <span class="attr">async:</span> <span class="number">100</span>     <span class="comment">#参数值代表了这个任务执行时间的上限值。即任务执行所用时间如果超出这个时间，则认为任务失败</span></span><br><span class="line">      <span class="attr">poll:</span> <span class="number">0</span>        <span class="comment">#任务异步执行时轮询的时间间隔,默认为10s,如果为0, 则表示不关心任务执行结果,只将任务推送到机器上执行，然后立即执行下一个任务</span></span><br><span class="line">      <span class="attr">register:</span> <span class="string">yum_sleeper</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&#x27;YUM - check on async task&#x27;</span></span><br><span class="line">      <span class="attr">async_status:</span>   <span class="comment">#根据job_id获取任务执行状态</span></span><br><span class="line">        <span class="attr">jid:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; yum_sleeper.ansible_job_id &#125;&#125;</span>&quot;</span></span><br><span class="line">      <span class="attr">register:</span> <span class="string">job_result</span></span><br><span class="line">      <span class="attr">until:</span> <span class="string">job_result.finished</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">30</span></span><br></pre></td></tr></table></figure>

<p><code>--fork指定了同一时刻一个task能在多少个host上执行</code></p>
<p><code>--serial指定了同一时刻能够有多少个host执行完整个playbook(在strategy=free效果明显)</code></p>
<h5 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a><strong>错误处理</strong></h5><p>正常情况下, playbook中的任何一个任务失败都将导致整个playbook终止运行, 我们可以使用<code>ignore_errors: true</code>来忽略这种场景, 即产生了错误也继续执行.</p>
<p>同时,也可使用fail&#x2F;failed_when来捕获错误异常</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">- hosts: xxx</span><br><span class="line">  remote_user: root</span><br><span class="line">  tasks:</span><br><span class="line">  - debug:</span><br><span class="line">      msg: &quot;I execute normally&quot;</span><br><span class="line">    ignore_errors: true</span><br><span class="line">  - shell: &quot;echo &#x27;This is a string for testing error&#x27;&quot;</span><br><span class="line">    register: return_value  #保留命令执行结果.</span><br><span class="line">    failed_when: &#x27; &quot;error&quot; in return_value.stdout&#x27;</span><br><span class="line">  - debug: </span><br><span class="line">      msg: &quot;I never execute,Because the playbook has stopped&quot;</span><br></pre></td></tr></table></figure>

<p>最后的debug不会被执行.因为第2个task failed了.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://docs.ansible.com/ansible/latest/modules_by_category.html">http://docs.ansible.com/ansible/latest/modules_by_category.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>ArgoCD学习(argoCD之ResourceCheck及Notifications机制)</title>
    <url>/2021/01/09/argoCD-ResourceHook-Notifications/</url>
    <content><![CDATA[<p>在之<a href="https://izsk.me/2020/12/04/argoCD-Usage/">前一篇</a>体验一把使用argocd对多k8s集群进行gitops的实践，在团队使用过程中会发现，argocd的sync状态跟app health状态是两个步骤，经常出现资源同步成功了但是deployment在k8s一起起不来，在argocd app status 一直都是<code>Processing</code>,如何处理这种情况呢? argocd如何做 resource check,有没有必要引用notification机制</p>
<span id="more"></span>



<h3 id="Resource-Check"><a href="#Resource-Check" class="headerlink" title="Resource Check"></a>Resource Check</h3><p>argocd官网文档: <a href="https://argoproj.github.io/argo-cd/operator-manual/health/">health</a></p>
<p>首先解决一下比较常见的 app的状态一直是<code>Processing</code></p>
<p>这里就要区别说一下argocd的两个命令:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. argocd app sync </span></span><br><span class="line"><span class="comment"># app sync只是用于对资源进行同步，也就是说，将git上的资源同步到argocd -- k8s, 一般情况下只要跟集群的连接、权限没有问题，sync一般都会成功，但是这个命令并不会验证资源运行能不能work,因此无法根据这个状态来判断整个pipeline是否成功</span></span><br><span class="line"><span class="comment"># 可以指定超时 --timeout, 单位: 秒</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. argocd app wait --health --timeout</span></span><br><span class="line"><span class="comment"># app wait 则是等待资源达到某种状态, 这个是整个app的状态</span></span><br><span class="line"><span class="comment"># --health 表示等待整个app达到health的状态，health状态表示所有同步的资源都运行成功</span></span><br><span class="line"><span class="comment"># --timeout表示超时，如果超过这个时间，app还没有达到health状态，则表示失败</span></span><br><span class="line"><span class="comment"># 比如deployment一直重启，则过了timeout时间后，argocd会发现app未是health状态，则可以让整个pipeline会失败</span></span><br></pre></td></tr></table></figure>

<p>作者最开始时，只使用<code>app sync</code>,这就忽略了对整个app的health状态的判断，因此会出现CI&#x2F;CD所有stage都成功了，但是pod一直处于重启状态</p>
<p>为解决这个问题，比较简单一点的做法就是在<code>app sync</code>后使用<code>app wait --health --timeout</code>来对<code>check app health</code>，如果不成功，则<code>pipeline exit 1</code>,这样可以在CI&#x2F;CD阶段就暴露资源异常问题</p>
<p>但这也有明显的缺点: 如果资源异常，则必须要等<code>timeout</code>时间之后才能发现，这个问题则可以结合k8s集群的监控发现，还能接受.</p>
<p>从官网的文档来，argocd本身提供了对k8s中的常用资源对象的健康检查，比如对于deployment，argocd详细地说明了如何判断其达到health状态的.</p>
<p>同时，也提供了自定义的检查机制，有2种形式，本人看了一下，第一种使用LUA语言写的，也就几行代码，倒也不难，作者没有采用，原因首先现在的业务大部分都是k8s常用资源对象，argocd本身支持度非常高，不需要</p>
<p>第二种则需要以特殊的目录结构来组织，作者也没有采用，实在是在开发端维护这些跟业务无法的文件，会引起复杂性.</p>
<p>取舍作者最后还是采用 <code>app wait</code>的方式，这还有一个好处是，不需要做额外的工作就可以将通知精准地发送给对应的enduser，比如gitlab本身的ci&#x2F;cd,如果整个pipeline的失败会直接给发起pipeline的人发送邮件</p>
<h3 id="notifications"><a href="#notifications" class="headerlink" title="notifications"></a>notifications</h3><p>这里还是需要分析一下argocd里的通知机制， argocd本身不提供notification，要依赖第三方提供的机制, 参考<a href="https://argoproj.github.io/argo-cd/operator-manual/notifications/">notifications</a></p>
<h4 id="resource-hook"><a href="#resource-hook" class="headerlink" title="resource hook"></a>resource hook</h4><p>resource hook是argocd本身提供的功能，简单来说，就是在资源同步前，中，后提供hoo脚本来执行相应的动作, 那么想在资源同步后获取app的状态，然后根据状态进行通知就非常简单了，通知可以是很简单的curl命令</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">PreSync:</span> <span class="string">在同步之前执行相关操作，这个一般用于比如数据库操作等</span></span><br><span class="line"><span class="attr">Sync:</span> <span class="string">同步时执行相关操作，主要用于复杂应用的编排</span></span><br><span class="line"><span class="attr">PostSync:</span> <span class="string">同步之后且app状态为health执行相关操作</span></span><br><span class="line"><span class="attr">SyncFail:</span> <span class="string">同步失败后执行相关操作，同步失败一般不常见</span></span><br></pre></td></tr></table></figure>

<p>用的最多的就是PostSync，用来获取app是否成功启动，hook一般是放在job中执行，比如</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">generateName:</span> <span class="string">app-slack-notification-</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">argocd.argoproj.io/hook:</span> <span class="string">PostSync</span></span><br><span class="line">    <span class="attr">argocd.argoproj.io/hook-delete-policy:</span> <span class="string">HookSucceeded</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">slack-notification</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">curlimages/curl</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;curl&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;-X&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;POST&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;--data-urlencode&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;payload=&#123;\&quot;channel\&quot;: \&quot;#somechannel\&quot;, \&quot;username\&quot;: \&quot;hello\&quot;, \&quot;text\&quot;: \&quot;App Sync succeeded\&quot;, \&quot;icon_emoji\&quot;: \&quot;:ghost:\&quot;&#125;&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;https://hooks.slack.com/services/...&quot;</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>这个job比较容易理解，就是sync同步成功且app状态为health后发送一个slack消息</p>
<p>结合业务来说， 作者并没有采用这种方法来发送通知，原因如下:</p>
<ol>
<li>对于PostSync可以发送成功的通知，但对于状态为Processing的无法判断</li>
<li>存量的业务都需要将这个job加入到git中，最大的考量在于，通知还是没有办法做到<strong>谁执行的pipeline,谁接收通知的原则</strong>，比如上面的通知收件人，没有办法很好地更细粒度地指定，要么指定一个组，大家都收，要么指定一个人，但是模块都是多人开发，只发给一个人不合适</li>
</ol>
<h4 id="argocd-notifications"><a href="#argocd-notifications" class="headerlink" title="argocd-notifications"></a>argocd-notifications</h4><p>argocd-notification是argocd自家开发的通知系统,也是argocd推荐之一</p>
<p>Argocd-notifications v1.0之前的版本与v1.0+版本有很大的不一样，建议大家认真阅读<a href="https://github.com/argoproj-labs/argocd-notifications/blob/d0da61d206/docs/upgrading/0.x-1.0.md">update</a></p>
<p>安装很简单，就一条命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-notifications/stable/manifests/install.yaml</span><br></pre></td></tr></table></figure>

<p>起来之后就一个controller应用，这里以邮件为例</p>
<p>首先，配置一下邮箱</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: argocd-notifications-secret</span><br><span class="line">  namespace: argocd</span><br><span class="line">stringData:</span><br><span class="line">  service.email: |</span><br><span class="line">    host: smtp.xxx.com</span><br><span class="line">    port: 587</span><br><span class="line">    from: xxx</span><br><span class="line">    username: xxx</span><br><span class="line">    password: xxx</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure>

<p>然后添加相应状态通知:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">argocd-notifications-cm</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="comment"># email的参数可以来自上面的secret</span></span><br><span class="line">  <span class="attr">service.email:</span> <span class="string">|    </span></span><br><span class="line"><span class="string">    host: smtp.gmail.com</span></span><br><span class="line"><span class="string">    port: 587</span></span><br><span class="line"><span class="string">    from: &lt;myemail&gt;@gmail.com</span></span><br><span class="line"><span class="string">    username: $email-username</span></span><br><span class="line"><span class="string">    password: $email-password</span></span><br><span class="line"><span class="string"></span>  </span><br><span class="line">  <span class="comment"># 触发条件</span></span><br><span class="line">  <span class="attr">trigger.on-sync-status-unknown:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    - when: app.status.sync.status == &#x27;Unknown&#x27;  # 指定条件</span></span><br><span class="line"><span class="string">      send: [my-custom-template]								 # 指定模板</span></span><br><span class="line"><span class="string"></span>  </span><br><span class="line">  <span class="comment"># 发送模板</span></span><br><span class="line">  <span class="attr">template.my-custom-template:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    title: &lt;argocd&gt; Sync Application &#123;&#123;.app.metadata.name&#125;&#125; Status &#123;&#123;.app.status.health.status&#125;&#125;.</span></span><br><span class="line"><span class="string">    body: |</span></span><br><span class="line"><span class="string">      App Name: &#123;&#123;.app.metadata.name&#125;&#125;</span></span><br><span class="line"><span class="string">      Cluster Name: &#123;&#123;.context.clusterName&#125;&#125;</span></span><br><span class="line"><span class="string">      Sync Status: &#123;&#123;.app.status.health.status&#125;&#125; </span></span><br><span class="line"><span class="string">      Following Error: &#123;&#123;.app.status.operationState.message&#125;&#125;</span></span><br><span class="line"><span class="string">      Repo URL: &#123;&#123;.repo.FullNameByRepoURL&#125;&#125;</span></span><br><span class="line"><span class="string">      Commit Msg: &#123;&#123;.repo.GetCommitMetadata&#125;&#125;</span></span><br><span class="line"><span class="string"></span>  </span><br><span class="line">  <span class="comment"># 订阅者</span></span><br><span class="line">  <span class="attr">subscriptions:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    - recipients:</span></span><br><span class="line"><span class="string">      - email:test@gmail.com</span></span><br><span class="line"><span class="string">      triggers:</span></span><br><span class="line"><span class="string">      - on-sync-status-unknown</span></span><br></pre></td></tr></table></figure>

<p>如上面的配置，用于监控argocd同步状态为 Unknown的情况，发送邮件告警</p>
<p>当然， 在argocd部署应用的时候，也可以添加订阅者，直接在annotations中添加即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">notifications.argoproj.io/subscribe.on-app-synched.eamil: xxx@xxx.com</span><br></pre></td></tr></table></figure>

<p>argocd-notification每隔一分钟就会扫描所有argocd app，感觉还是比较频繁，如果应用一多的话，不知道是不是影响性能，这个时间目前不支持通过参数覆盖，如果需要调整只能修改源码后重新编译</p>
<p>作者最后也没有采用这种形式，原因在于argocd-notification还是用于监控argocd 的sync状态，并不关心app 是否health</p>
<h4 id="argo-kube-notifier"><a href="#argo-kube-notifier" class="headerlink" title="argo-kube-notifier"></a><a href="https://github.com/argoproj-labs/argo-kube-notifier">argo-kube-notifier</a></h4><p>最后argocd也推荐使用<a href="https://github.com/argoproj-labs/argo-kube-notifier">argo-kube-notifier</a>，但感觉这个跟argocd本身就没多大关系了，完且是发布后的health check,这个主要用于k8s集群应用状态的check，实现这个目的的组件很多，而且<a href="https://github.com/argoproj-labs/argo-kube-notifier">argo-kube-notifier</a>还是没办法实现<strong>谁执行的pipeline,谁接收通知的原则</strong></p>
<p>综上，还是直接使用<code>argo app wait</code>简单粗暴，加两行代码，贴合需求</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2020/12/04/argoCD-Usage/">https://izsk.me/2020/12/04/argoCD-Usage/</a></li>
<li><a href="https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/webhook.md">https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/webhook.md</a></li>
<li><a href="https://github.com/argoproj-labs/argocd-notifications/blob/d0da61d206/docs/upgrading/0.x-1.0.md">https://github.com/argoproj-labs/argocd-notifications/blob/d0da61d206/docs/upgrading/0.x-1.0.md</a></li>
<li><a href="https://www.jianshu.com/p/a0613951c4b4">https://www.jianshu.com/p/a0613951c4b4</a></li>
<li><a href="https://argoproj.github.io/argo-cd/operator-manual/health/">https://argoproj.github.io/argo-cd/operator-manual/health/</a></li>
<li><a href="https://argoproj.github.io/argo-cd/operator-manual/notifications/">https://argoproj.github.io/argo-cd/operator-manual/notifications/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>bootstrap3适配IE8</title>
    <url>/2017/07/29/bootstrap3%E9%80%82%E9%85%8DIE8/</url>
    <content><![CDATA[<p>最近在做一个内部用的在线生成oracle性能数据的报表工具,那段时期刚好在看flask框架,所以整个项目是用flask来做,前端使用的bootstrap3,由于工作环境需要,大部分的机器还停留在IE9及以下阶段(什么年代了还IE9,真的好想骂人),直接使用的bootstrap会出现页面排版错位情况,bootstrap3开始已放弃对IE9的支持,只得另想办法,好在我们都站在巨人的肩膀上,办法还是有的,在这记录一下.</p>
<span id="more"></span>

<h3 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a><strong>Bootstrap</strong></h3><p>本人对前端知识不是很熟悉,也只是停留在需要的时候上网查下资料的程序,在这之前并没有接触过bootstrap,不敢妄加评说,这里就不过多介绍了,大家上bootstrap的官网,首页第一句话就是: <strong>bootstrap4 is comming</strong>.虽然bootstrap当初是给移动端设计的,但是给我的感受就一句话:<strong>用过的都说好</strong></p>
<p>其实bootstrap的资料还是挺多的,IE低版本的兼容问题在<a href="http://getbootstrap.com/getting-started/">bootstrap官网的Get Start</a>就有很详细的说明:</p>
<p>bootstrap3其实是支持IE8的,只不过css3有某些属性和HTML5的某些元素对于一些浏览器不能完全兼容,下图是官方说法:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/bootstrap-ie91.PNG" alt="bootstrap-ie91"></p>
<p>由于工作只需要兼容IE9,IE8,而如果是想适配IE8及以下,可以使用<a href="https://github.com/ddouble/bsie">bsie</a>,当然该项目只适用于bootstrap2.</p>
<h3 id="兼容IE8"><a href="#兼容IE8" class="headerlink" title="兼容IE8"></a><strong>兼容IE8</strong></h3><p>IE9以下的版本是不支持 CSS3 Media Queries的, 所以IE9以下要使用bootstrap3,必须要引入两个js文件:</p>
<p><strong><a href="https://github.com/aFarkas/html5shiv">html5shiv.js</a></strong></p>
<p><strong><a href="https://github.com/scottjehl/Respond">Respond.js</a></strong></p>
<p>html5shiv.js的作用是让那些不完全运行html5元素的浏览器支持html5</p>
<p>而Respond.js则是让IE8及以下版本支持CSS3 Media Queries.更详细的介绍请移步上面的GitHub链接</p>
<p>所以,要让bootstrap3适配IE8,要做以下几步:</p>
<blockquote>
<ul>
<li><p>申明使用html5</p>
<p> 在每个html渲染版本文件(或者父模板)中以<!DOCTYPE html>开头,而且该行与下一行之间不能有空行</p>
</li>
<li><p>加入meta标签</p>
<p>   在html文件的metas块中加入以下几句话,而且必须是在head块的最开头位置(网上有人提到过,但是具体的链接找不到了),其中,IE&#x3D;edge是强制浏览器使用最新版本内核渲染</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">&#123;% block head %&#125;</span><br><span class="line">&#123;% block metas %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;Content-Type&quot;</span> <span class="attr">content</span>=<span class="string">&quot;text/html; charset=UTF-8&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=edge&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1&quot;</span> /&gt;</span>   </span><br><span class="line">&#123;%- endblock metas %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>&#123;% block title %&#125; Report Form On Line &#123;%- endblock title %&#125;<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>引入html5shiv.min.js和respond.min.js</p>
<p> 这两个js的作用已在上面说过,具体入如下,先判断IE的版本,低于IE9的则引用,且不能使用如@import或者file:&#x2F;&#x2F;形式引入进来</p>
</li>
</ul>
</blockquote>
<blockquote>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">&#123;% block styles %&#125;  </span><br><span class="line">  <span class="comment">&lt;!--[if lt IE 9]&gt;</span></span><br><span class="line"><span class="comment">      &lt;script src=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;js/html5shiv.js&#x27;)&#125;&#125;&quot;&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="comment">      &lt;script src=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;js/respond.js&#x27;)&#125;&#125;&quot;&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="comment">  &lt;![endif]--&gt;</span></span><br><span class="line">&#123;%- endblock styles %&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>使用1.x版本的jquery.js ,且jquery.js一般需要在其它js之前引用</p>
</li>
<li><p>html中对css文件的引用要先于js文件的引用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">虽然折腾到最后在IE8下勉强能用,但有些地方的样式还是不尽人意,直接放弃了.</span><br><span class="line"></span><br><span class="line">下面贴个我完整的base.html</span><br><span class="line"></span><br><span class="line">```html</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">&#123;% block html %&#125;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">   &#123;% block head %&#125;</span><br><span class="line">   &#123;% block metas %&#125;</span><br><span class="line">       &lt;meta charset=&quot;utf-8&quot;&gt;</span><br><span class="line">       &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;</span><br><span class="line">       &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt;</span><br><span class="line">       &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;   </span><br><span class="line">    &#123;%- endblock metas %&#125;</span><br><span class="line">       &lt;title&gt;&#123;% block title %&#125; Report Form On Line &#123;%- endblock title %&#125;&lt;/title&gt;</span><br><span class="line"> &#123;% block styles %&#125;</span><br><span class="line"></span><br><span class="line">     &lt;!--[if lt IE 9]&gt;</span><br><span class="line">         &lt;script src=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;js/html5shiv.js&#x27;)&#125;&#125;&quot;&gt;&lt;/script&gt;</span><br><span class="line">         &lt;script src=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;js/respond.js&#x27;)&#125;&#125;&quot;&gt;&lt;/script&gt;</span><br><span class="line">     &lt;![endif]--&gt;</span><br><span class="line"></span><br><span class="line">     &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;css/bootstrap.css&#x27;)&#125;&#125;&quot;&gt;</span><br><span class="line">     &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;css/classic.css&#x27;)&#125;&#125;&quot;&gt;</span><br><span class="line">     &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123;url_for(&#x27;static&#x27;,filename=&#x27;css/bootstrap-datetimepicker.min.css&#x27;)&#125;&#125;&quot;&gt;</span><br><span class="line">     &lt;link rel=&quot;shortcut icon&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;,filename=&#x27;favicon.ico&#x27;) &#125;&#125;&quot; type=&quot;image/x-icon&quot;&gt;</span><br><span class="line">     &lt;link rel=&quot;icon&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;,filename=&#x27;favicon.ico&#x27;) &#125;&#125;&quot; type=&quot;image/x-icon&quot;&gt;</span><br><span class="line">     &lt;style type=&quot;text/css&quot;&gt;  </span><br><span class="line">       .navbar .nav &gt; li .dropdown-menu &#123;  </span><br><span class="line">           margin: 0;  </span><br><span class="line">       &#125;  </span><br><span class="line">       .navbar .nav &gt; li:hover .dropdown-menu &#123;  </span><br><span class="line">           display: block;  </span><br><span class="line">       &#125;</span><br><span class="line">     &lt;/style&gt;</span><br><span class="line"> &#123;%- endblock styles %&#125;</span><br><span class="line"> &#123;%- endblock head %&#125;</span><br><span class="line">&lt;/head&gt; </span><br><span class="line">&lt;body&gt;</span><br><span class="line">   &#123;% block body -%&#125;</span><br><span class="line">   &#123;% block navbar %&#125;</span><br><span class="line">   &#123;%- endblock navbar %&#125;</span><br><span class="line">   &#123;% block content -%&#125;</span><br><span class="line"></span><br><span class="line">   &#123;% with messages = get_flashed_messages(with_categories=true) %&#125;</span><br><span class="line">   &lt;!-- Categories: success (green), info (blue), warning (yellow), danger (red) --&gt;</span><br><span class="line">     &#123;% if messages %&#125;</span><br><span class="line">       &#123;% for category,message in messages %&#125;</span><br><span class="line">         &lt;div class=&quot;alert alert-&#123;&#123; category &#125;&#125;&quot;&gt;</span><br><span class="line">           &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot; aria-label=&quot;Close&quot;&gt;&lt;span aria-hidden=&quot;true&quot;&gt;&gt;×&lt;/span&gt;&lt;/button&gt;</span><br><span class="line">           &lt;h4&gt;&lt;strong&gt;&#123;&#123;category&#125;&#125;:&#123;&#123; message &#125;&#125;&lt;/strong&gt;&lt;/h4&gt;</span><br><span class="line">         &lt;/div&gt;</span><br><span class="line">       &#123;% endfor %&#125;</span><br><span class="line">     &#123;% endif %&#125;</span><br><span class="line">   &#123;% endwith %&#125;</span><br><span class="line"></span><br><span class="line">   &#123;%- endblock content %&#125;</span><br><span class="line">   &#123;% block scripts %&#125;</span><br><span class="line">   &#123;%- endblock scripts %&#125;</span><br><span class="line">   &#123;%- endblock body %&#125;</span><br><span class="line"> &lt;/body&gt;</span><br><span class="line">&#123;%- endblock html %&#125;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</blockquote>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://getbootstrap.com/getting-started/">Bootstrap官网</a></li>
<li><a href="http://www.zendei.com/article/1806.html">bootstrap 相容 IE8 浏览器</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>捣鼓手册</category>
      </categories>
      <tags>
        <tag>捣鼓手册</tag>
      </tags>
  </entry>
  <entry>
    <title>cilium在kubernetes中的生产实践一(cilium介绍)</title>
    <url>/2023/04/01/cilium-on-kubernetes-introduction/</url>
    <content><![CDATA[<p>在前东家的时候其实就有意将cilium强大的链路追踪能力集成到生产环境中,各种因素导致没有很大信心落地, 经过深入调研(也就把官网docs翻了四五遍)及测试, 终于有机会在生产kubernetes集群中(目前一个集群规模不算很大,2w+核心,持续增长)使用cilium做为cni,同时替换kube-proxy, 到现在已经有一段时间了，也算是有生产经验可以跟大家聊一聊这个工具，使用体验总结一句话: 轻松愉悦.<br>分享一下整个落地过程,同时也总结下方方面面, 工作之余尽量更新.<br>此篇为: cilium在kubernetes中的生产实践一(cilium介绍)</p>
<span id="more"></span>

<p>总体分为以下几块内容:<br>cilium在kubernetes中的生产实践一(cilium介绍)<br>cilium在kubernetes中的生产实践二(cilium部署)<br>cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)<br>cilium在kubernetes中的生产实践四(cilium网络模型之生产实践)<br>cilium在kubernetes中的生产实践五(cilium网络策略)<br>cilium在kubernetes中的生产实践六(cilium排错指南)<br>cilium在kubernetes中的生产实践七(cilium中的bpf hook)</p>
<h3 id="eBPF"><a href="#eBPF" class="headerlink" title="eBPF"></a>eBPF</h3><p>现在很火的技术eBPF,相信大家都有所耳闻，cilium就是在eBPF的技术之上构建的功能强大工具, eBPF技术不是本文的重点，因此不详细展开，引用网上一个比较通俗的概述:<br>**eBPF(extended Berkeley Packet Filter)**是一套通用执行引擎，提供了可基于系统或程序事件高效安全执行特定代码的通用能力，通用能力的使用者不再局限于内核开发者；eBPF 可由执行字节码指令、存储对象和 Helper 帮助函数组成，字节码指令在内核执行前必须通过 BPF Verfier 的验证，同时在启用 BPF JIT 模式的内核中，会直接将字节码指令转成内核可执行的本地指令运行</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230401155149.png"></p>
<p>由于不需要修改内核就可以完成原本要在内核才能做到的事情，因此功能强大，eBPF也逐渐在观测、跟踪、性能调优、安全和网络等领域发挥重要的角色</p>
<p><a href="https://www.cnblogs.com/charlieroro/p/13403672.html">eBPF更多详细介绍</a></p>
<h3 id="cilium"><a href="#cilium" class="headerlink" title="cilium"></a>cilium</h3><p>回到本文的主角: <strong>cilium</strong><br>一个多么痛的领悟: 当kubernetes发展到一定规模后，维护的成本也跟随升高, 特别是在某些链路很长，网络很复杂的场景下，<br>(对，说的就是本人目前在维的集群)对在网络的deubg, 链路可观测方面提出更高要求， 但这恰恰是eBPF的强项,<br>Google更是宣布使用Cilium 作为 GKE 的下一代数据面, <strong>作为第一个通过ebpf实现了kube-proxy所有功能的网络插件</strong>,那cilium到底有什么魔力呢?<br>摘一段官网的介绍：<br><strong>Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230401160445.png"></p>
<p>因此: 当在kubernetes中需要开箱即用解决以下问题时:</p>
<ol>
<li><p>服务依赖关系和通信图</p>
<blockquote>
<ul>
<li>哪些服务正在相互通信？有多频繁？服务依赖关系图是什么样子的？</li>
<li>正在进行哪些HTTP调用？服务从哪些Kafka主题消费或生产到哪些Kafka主题？</li>
</ul>
</blockquote>
</li>
<li><p>网络监控和警报</p>
<blockquote>
<ul>
<li>是否有网络通信故障？为什么通信失败？是DNS吗？是应用程序问题还是网络问题？第4层（TCP）或第7层（HTTP）上的通信是否中断？</li>
<li>哪些服务在过去5分钟内遇到了DNS解析问题？哪些服务最近遇到了TCP连接中断或连接超时？未应答TCP SYN请求的速率是多少？</li>
</ul>
</blockquote>
</li>
<li><p>应用程序监控</p>
<blockquote>
<ul>
<li>特定服务或所有群集中5xx或4xx HTTP响应代码的比率是多少？</li>
<li>在集群中，HTTP请求和响应之间的第95和第99百分位延迟是多少？哪些服务性能最差？两个服务之间的延迟是多少？</li>
</ul>
</blockquote>
</li>
<li><p>安全可观察性</p>
<blockquote>
<ul>
<li>哪些服务的连接因网络策略而被阻止？从群集外部访问了哪些服务？哪些服务解析了特定的DNS名称？</li>
</ul>
</blockquote>
</li>
</ol>
<p>借助cilium，都可以得到答案。</p>
<p>上一张神图来说明cilium的优越性:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230407170643.png"></p>
<p>cilium由好几部分构成:</p>
<blockquote>
<ul>
<li>cilium-agent: cilium-agent在集群中的每个节点上运行(通常以daemonset方式运行)agent通过Kubernetes或API接受配置，这些配置描述了网络，服务负载平衡，网络策略以及可见性和监控要求。<br>Cilium agent监听来自编排系统（如Kubernetes）的事件，以了解容器或工作负载何时启动和停止。它管理Linux内核用来控制进出这些容器的所有网络访问的eBPF程序。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>cilium-client: Cilium clienet是随Cilium-agent一起安装的命令行工具(即部署在cilium-agent的容器中)。它与运行在同一节点上的Cilium agent的REST API交互。同时允许检查本地代理的状态。它还提供了直接访问eBPF映射以验证其状态的工具。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>cilium-cli: 官方还出了一个命令行工具, 就叫cilium, 可以直接运行在集群之外(类似kubectl这类的客户端, 通过token或者kubeconfig方式访问apiserver), 它与cilium-client的命令有些区别</li>
</ul>
</blockquote>
<p>注: cilium-client是 get from client-agent，而clium-cli是整个cilium cluster的命令行工具</p>
<blockquote>
<ul>
<li>cilium-operator: Cilium Operator负责管理集群(逻辑上是为整个集群处理一次，而不是为集群中的每个节点处理一次),Cilium operator不在任何转发或网络策略决策的关键路径中。如果operator暂时不可用，群集通常会继续运行。但是，根据配置的不同，operator的可用性故障可能导致：</li>
<li>IP地址管理（IPAM）延迟，因此，如果要求operator分配新的IP地址，则会延迟新工作负载的调度</li>
<li>更新kvstore心跳密钥失败，这将导致代理程序声明kvstore不健康并重新启动。<br><strong>要注意，cilium operator与Kubernetes中的operator概念不同, cilium operator不生成子对象，它负责全局的一些配置等</strong></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>cni plugin: cilium自身可做为kubernetes cni插件存在，且可完全替代kube-proxy的功能(这个功能是后续介绍的重点)</li>
</ul>
</blockquote>
<p>以上算是cilium自身的components,还有hubble, hubble是存储、展示从cilium获取的相关数据:</p>
<blockquote>
<ul>
<li>hubble-server: 在每个节点上运行，并从Cilium检索基于eBPF的可见性。它被嵌入到Cilium agent中，以实现高性能和低开销。它提供了一个gRPC服务来检索流和Prometheus指标。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>hubble-relay: hubble-relay是一个独立的组件，它可以感知所有正在运行的Hubble服务器，并通过连接到它们各自的gRPC API并提供表示集群中所有服务器的API来提供集群范围的可见性。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>hubble-cli: 是一个命令行工具，能够连接到hubble-relay的gRPC API或本地服务器以检索流事件。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>hubble-ui: 图形用户界面，可以在该界面上查看网络调用关系。</li>
</ul>
</blockquote>
<p><strong>虽然有些hubble功能被内嵌到cilium中，但hubble对于cilium来说不是必需的</strong></p>
<p>另外, cilium也有数据库(data-store),用来在代理之间传播状态。它支持以下数据存储：</p>
<blockquote>
<ul>
<li>Kubernetes crds（默认）<br>存储任何数据和传播状态的默认选择是使用Kubernetes自定义资源定义（CRD）。Kubernetes为集群组件提供CRD，以通过Kubernetes资源表示配置和状态。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>key-value存储<br>状态存储和传播的所有要求都可以通过Cilium默认配置中配置的Kubernetes CRD来满足。键值存储库可以可选地用作优化以提高集群的可扩展性，因为直接使用键值存储库会使更改通知和存储需求更有效。<br>可使用etcd做为key-value数据库，当然可以直接使用Kubernetes的etcd集群，也可以维护一个专用的etcd集群</li>
</ul>
</blockquote>
<p>以上是cilium的介绍，算是对cilium有个基本认识，接下来是cilium在kubernetes中的安装部署，由于eBPF在不同版本的linux内核中的支持不同，因此cilium中某些功能对linux内核有版本要求，详见下文</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://cilium.io/">https://cilium.io</a></li>
<li><a href="https://izsk.me/2023/06/03/cilium-on-kubernetes-install/">https://izsk.me/2023/06/03/cilium-on-kubernetes-install/</a></li>
<li><a href="https://www.cnblogs.com/charlieroro/p/13403672.html">https://www.cnblogs.com/charlieroro/p/13403672.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>cilium在kubernetes中的生产实践二(cilium部署)</title>
    <url>/2023/06/03/cilium-on-kubernetes-install/</url>
    <content><![CDATA[<p>在前东家的时候其实就有意将cilium强大的链路追踪能力集成到生产环境中,各种因素导致没有很大信心落地, 经过深入调研(也就把官网docs翻了四五遍)及测试, 终于有机会在生产kubernetes集群中(其中一个集群规模不算很大,2w+核心,持续增长)使用cilium做为cni,同时替换kube-proxy, 到现在已经有一段时间了，也算是有生产经验可以跟大家聊一聊这个工具，使用体验总结一句话: 轻松愉悦.<br>分享一下整个落地过程,同时也总结下方方面面, 工作之余尽量更新.<br>此篇为: cilium在kubernetes中的生产实践二(cilium部署)</p>
<span id="more"></span>

<p>总体分为以下几块内容:<br><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">cilium在kubernetes中的生产实践一(cilium介绍)</a><br>cilium在kubernetes中的生产实践二(cilium部署)<br>cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)<br>cilium在kubernetes中的生产实践四(cilium网络模型之生产实践)<br>cilium在kubernetes中的生产实践五(cilium网络策略)<br>cilium在kubernetes中的生产实践六(cilium排错指南)<br>cilium在kubernetes中的生产实践七(cilium中的bpf hook)</p>
<h3 id="内核要求"><a href="#内核要求" class="headerlink" title="内核要求"></a>内核要求</h3><p>由于eBPF是个较新的特性, 因此对内核版本有要求,建议最少4.18+以上的内核版本,cilium有些功能必须要5.0+之上的内核版本，生产环境下大多数都不会满足</p>
<p>相关参数如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">os:</span> <span class="string">Rockey</span> <span class="string">Linux</span> <span class="string">release</span> <span class="number">8.7</span></span><br><span class="line"><span class="attr">kernel:</span> <span class="number">4.18</span><span class="number">.0</span><span class="number">-425.10</span><span class="number">.1</span><span class="string">.el8_7.x86_64</span></span><br><span class="line"><span class="attr">k8s:</span> <span class="string">v1.22.13</span></span><br><span class="line"><span class="attr">docker:</span> <span class="string">v20.10.18</span> </span><br></pre></td></tr></table></figure>

<p>cilium功能对内核版本要求，查看如下列表:</p>
<p><a href="https://docs.cilium.io/en/v1.12/operations/system_requirements/#required-kernel-versions-for-advanced-features">System Requirements &amp;mdash; Cilium 1.12.10 documentation</a></p>
<p>基于该列表再结合已有环境的内核版本来决定是否开启相关功能</p>
<h3 id="k8s安装"><a href="#k8s安装" class="headerlink" title="k8s安装"></a>k8s安装</h3><p>这里简单带一下k8s使用cilium替换kube-proxy的安装，kubeadm集群初始化配置文件参考: <code>kubeadm-kubeproxy-free.conf</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system:bootstrappers:kubeadm:default-node-token</span></span><br><span class="line">  <span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line">  <span class="attr">ttl:</span> <span class="string">24h0m0s</span></span><br><span class="line">  <span class="attr">usages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">signing</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">authentication</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line">  <span class="attr">advertiseAddress:</span> <span class="string">xxx.xxx.xxx.xxx</span> <span class="comment"># 指定本机的ip地址用于advertise</span></span><br><span class="line">  <span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span> </span><br><span class="line">  <span class="attr">kubeletExtraArgs:</span></span><br><span class="line">    <span class="attr">node-ip:</span> <span class="string">xxx.xxx.xxx.xxx</span> <span class="comment"># 指定本机的ip地址,用于kubelet向master注册,同advertiseAddress</span></span><br><span class="line">  <span class="attr">criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">master-1</span></span><br><span class="line">  <span class="attr">taints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">    <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">dns:</span> &#123;&#125;</span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">dataDir:</span> <span class="string">/mnt/nvme0/etcd</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">harbor.local/k8s</span> </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="number">1.22</span><span class="number">.13</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="string">xxx.xxx.xxx.xxx:6443</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">extraArgs:</span></span><br><span class="line">    <span class="attr">authorization-mode:</span> <span class="string">Node,RBAC</span></span><br><span class="line">  <span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line">  <span class="attr">certSANs:</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="string">domain.xxx</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">vip.k8s</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">nodelist-xxx</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line">  <span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line">  <span class="attr">podSubnet:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line"><span class="attr">scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line">  <span class="attr">anonymous:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheTTL:</span> <span class="string">30s</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">x509:</span></span><br><span class="line">    <span class="attr">clientCAFile:</span> <span class="string">/etc/kubernetes/pki/ca.crt</span></span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line">  <span class="attr">mode:</span> <span class="string">Webhook</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheAuthorizedTTL:</span> <span class="string">10s</span></span><br><span class="line">    <span class="attr">cacheUnauthorizedTTL:</span> <span class="string">10s</span></span><br><span class="line"><span class="attr">clusterDNS:</span></span><br><span class="line"><span class="bullet">-</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line"><span class="attr">clusterDomain:</span> <span class="string">cluster.local</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line"><span class="attr">healthzPort:</span> <span class="number">10248</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">systemd</span> </span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">250</span></span><br><span class="line"><span class="attr">rotateCertificates:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">staticPodPath:</span> <span class="string">/etc/kubernetes/manifests</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="number">5</span><span class="string">%</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeproxy.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeProxyConfiguration</span></span><br><span class="line"><span class="attr">mode:</span> <span class="string">ipvs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 说明</span></span><br><span class="line"><span class="comment"># 使用kubeadm-kubeproxy-free.conf做为集群的初始化配置文件, 注意修改如下参数:</span></span><br><span class="line"><span class="comment"># 1. InitConfiguration: </span></span><br><span class="line"><span class="comment">#   a. localAPIEndpoint</span></span><br><span class="line"><span class="comment">#   b. name</span></span><br><span class="line"><span class="comment"># 2. ClusterConfiguration(在集群中存在于Kube-system为configmap： kubeadm-config): </span></span><br><span class="line"><span class="comment">#   a. etcd的存储路劲</span></span><br><span class="line"><span class="comment">#   b. imageRepository的路径</span></span><br><span class="line"><span class="comment">#   c. controlPlaneEndpoint的地址，这里使用了kube-vip提供的高可用ip </span></span><br><span class="line"><span class="comment">#   d. certSANs，这里需要加上所有的master的节点ip及主机名，以及集群的域名</span></span><br></pre></td></tr></table></figure>

<p>集群初始化</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm init --upload-certs --config kubeadm-kubeproxy-free.conf --skip-phases=addon/kube-proxy --apiserver-advertise-address xxx.xxx.xxx.xxx</span><br><span class="line"><span class="comment"># --skip-phases=addon/kube-proxy 不安装kube-proxy组件</span></span><br><span class="line"><span class="comment"># --apiserver-advertise-address 指定需要绑定的ip，在机器有多网卡的时候用</span></span><br></pre></td></tr></table></figure>

<h3 id="cilium安装"><a href="#cilium安装" class="headerlink" title="cilium安装"></a>cilium安装</h3><p>Cilium在kubernetes安装有2种运行模式，一种是完全替换kube-proxy, 如果底层 Linux 内核版本低，可以替换kube-proxy的部分功能，与原来的kube-proxy 共存。因此这里要注意kubeProxyReplacement参数,从官网上查看有如下几个选项:</p>
<blockquote>
<ul>
<li>kubeProxyReplacement&#x3D;strict: 该选项期望使用无kube-proxy的Kubernetes设置，其中Cilium有望完全替代所有kube-proxy功能。 Cilium代理启动并运行后，将负责处理类型为ClusterIP，NodePort，ExternalIP和LoadBalancer的Kubernetes服务。如果不满足基本内核版本要求（请参阅不带kube-proxy注释的Kubernetes），则Cilium代理将在启动时退出并显示一条错误消息。</li>
<li>kubeProxyReplacement&#x3D;probe: 此选项适用于混合设置，即kube-proxy在Kubernetes集群中运行，其中Cilium部分替换并优化了kube-proxy功能。一旦Cilium agent启动并运行，它就会在基础内核中探查所需BPF内核功能的可用性，如果不存在，则依靠kube-proxy补充其余的Kubernetes服务处理，从而禁用BPF中的部分功能。在这种情况下，Cilium代理将向其日志中发送一条信息消息。例如，如果内核不支持 Host-Reachable Services，则节点的主机名空间的ClusterIP转换是通过kube-proxy的iptables规则完成的。</li>
<li>kubeProxyReplacement&#x3D;partial: 与探针类似，此选项用于混合设置，即kube-proxy在Kubernetes集群中运行，其中Cilium部分替换并优化了kube-proxy功能。与探查基础内核中可用的BPF功能并在缺少内核支持时自动禁用负责BPF kube-proxy替换的组件的探针相反，该部分选项要求用户手动指定应替换BPF kube-proxy的组件。与严格模式类似，如果不满足基本内核要求，则Cilium代理将在启动时通过错误消息进行援助。对于细粒度的配置，可以将global.hostServices.enabled，global.nodePort.enabled和global.externalIPs.enabled设置为true。默认情况下，所有三个选项均设置为false。</li>
</ul>
</blockquote>
<p>由于k8s集群是全新安装，所以这里直接使用<strong>kubeProxyReplacement&#x3D;strict</strong>, 使用helm安装, cilium 版本 v1.12.0</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm install cilium cilium/cilium --version 1.12.0 --namespace kube-system --<span class="built_in">set</span> kubeProxyReplacement=strict --<span class="built_in">set</span> k8sServiceHost=xxx.xxx.xxx.xxx --<span class="built_in">set</span> k8sservicePort=6443 --<span class="built_in">set</span> ipam.Operator.clusterPoolIPv4PodCIDRList=10.244.0.0/16 --<span class="built_in">set</span> enable-bpf-masquerade=<span class="literal">true</span> --<span class="built_in">set</span> ipam.Operator.ClusterPoolIPv4MaskSize=24 --<span class="built_in">set</span> devices=ens192 --<span class="built_in">set</span> hubble.Relay.Enabled=<span class="literal">true</span> --<span class="built_in">set</span> hubble.Ui.Enabled=<span class="literal">true</span> --<span class="built_in">set</span> prometheus.Enabled=<span class="literal">true</span> --<span class="built_in">set</span> operator.Prometheus.Enabled=<span class="literal">true</span> --<span class="built_in">set</span> hubble.Enabled=<span class="literal">true</span> --<span class="built_in">set</span> hubble.Metrics.Enabled=<span class="string">&quot;&#123;dns, drop, tcp, flow, port-distribution, icmp, http&#125;&quot;</span> --<span class="built_in">set</span> image.UseDigest=<span class="literal">false</span> --<span class="built_in">set</span> operator.image.useDigest=<span class="literal">false</span> --<span class="built_in">set</span> operator.Image.Repository=harbor.io/k8s/cilium/operator --<span class="built_in">set</span> image.Repository=harbor.io/k8s/cilium/cilium --<span class="built_in">set</span> hubble.Rela y.Image.UseDigest=<span class="literal">false</span> --<span class="built_in">set</span> hubble.Relay.Image.Repository=harbor.io/k8s/cilium/hubble-relay --<span class="built_in">set</span> hubble.ui.Backend.Image.Repository=harbor.io/k8s/cilium/hubble-ui-backend --<span class="built_in">set</span> hubble.Ui.Backend.Image.Tag=v0.9.0 --<span class="built_in">set</span> hubble.ui.Frontend.Image.Repository=harbor.io/k8s/cilium/hubble-ui --<span class="built_in">set</span> hubble.ui.Frontend.Image.Tag=v0.9.0</span><br></pre></td></tr></table></figure>

<p>其中:</p>
<ol>
<li>kubeProxyReplacement: cilium在哪种方式下运行，上文已进行了解释, 这里选择kube-proxy-free模式</li>
<li>k8sServiceHost: k8s集群的地址,如果是集群，则一般填写负载均衡地址(vip地址)</li>
<li>k8sservicePort: k8s集群的端口,如果是集群，则一般填写负载均衡端口(vip端口)</li>
<li>ipam.Operator.clusterPoolIPv4PodCIDRList: pod的CIDR</li>
<li><strong>enable-bpf-masquerade: 是否开启ip的masquerade, 下文解释</strong></li>
<li><strong>devices: 非常重要的参数,下文解释</strong></li>
<li>hubble.xx: 跟hubble相关的参数,能开启的尽量开启hubble吧</li>
</ol>
<p>后续如果需要对配置项进行更新，则可使用以下命令复用之前的配置(<code>--reuse-values</code>)，只更新给的配置项即可, 比如将将<code>host routing 由 legacy 改成 bpf</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm upgrade cilium --version 1.12.0 --namespace kube-system --reuse-values --<span class="built_in">set</span> --bpf.Masquerade=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>因为cilium号称可以完全取代kube-proxy, 但也需要支持kube-proxy已存在的场景，所以cilium会自动根据情况进行设置，还是比较人性化的</p>
<p>默认情况下cilium是使用kubernetes CRD-based来进行状态管理的，当然也支持使用外部的key-value数据库来存储状态, 一般情况下都会使用默认方式</p>
<p>这里要特别注意的参数做如下解释:</p>
<h4 id="enable-bpf-masquerade"><a href="#enable-bpf-masquerade" class="headerlink" title="enable-bpf-masquerade"></a>enable-bpf-masquerade</h4><p><a href="https://docs.cilium.io/en/stable/network/concepts/masquerading/">Masquerading &amp;mdash; Cilium 1.13.3 documentation</a></p>
<p>是否开启bpf的封装, 一张图应该很容易看明白</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230603173157.png"></p>
<p>当在容器内部即访问外部资源时，如何让回来的流量可以再沿着出去的路径，kube-proxy的场景下是基于iptables实现，</p>
<p>而bpf比iptables高效的多, 它需要Linux内核4.19+的版本上才支持</p>
<p>当然，它需要devices参数的配合，即能访问外部资源所在的网卡必须在devices列表里</p>
<p>cilium还能容器哪些CIDR不被封装</p>
<h4 id="devices"><a href="#devices" class="headerlink" title="devices"></a>devices</h4><p>如果k8s节点存在多张网卡，如果网卡层面上做了网络隔离的话，那么在指定devices就需要注意了</p>
<p>举个生产遇到的例子:</p>
<p>节点上有eth0, eth1, 在部署k8s集群时，如果没有指定网口名，则会选择默认的网口名或者是kubeadm挑出的第一个网卡名，但有时候不一定能选中我们期望的，因此建议最好是使用–apiserver-advertise-address来显式地指定需要绑定的IP地址(networ interface)，但是只能指定一个，比如指定eth0, 这样在集群中启动的容器会有一个eth0 ip, bind node eth0, 它想要访问集群外的资源，最终都会通过node eth0这个网口出入(可能做了masquerade也可能不做)</p>
<p>但有些集群外的资源只能通过eth1这个网段才能访问，这个时候容器里的网络就是不通的, 怎么办？</p>
<p>如果放在以前，那么可能会通过折腾multi-CNI的方式将eth0&#x2F;eth1这两块网卡都映射进容器，即在容器里会同时看到两个ip地址，跟宿主机一样，这样的话，网络上就能通了，</p>
<p>但事实上这种实现方式还是有些复杂，成本较高，</p>
<p>放在cilium可就方便多了，直接devices列表里指定多个网卡即可实现上述能力，</p>
<p>devices参数的作用是: <strong>当请求从容器到达节点时，允许从node的哪些network interface出去</strong></p>
<p>指定多个network interface后, 容器里还是只有一个eth0，但是cilium会自行判断应该从哪个interface出入，是不是简单、清爽了很多. 在网络复杂的情况下，devices参数还是非常有用的</p>
<h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><p>可以单独使用cilium-cli工具查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cilium status --<span class="built_in">wait</span></span><br><span class="line">   /¯¯\</span><br><span class="line">/¯¯\__/¯¯\    Cilium:         OK</span><br><span class="line">\__/¯¯\__/    Operator:       OK</span><br><span class="line">/¯¯\__/¯¯\    Hubble:         disabled</span><br><span class="line">\__/¯¯\__/    ClusterMesh:    disabled</span><br><span class="line">   \__/</span><br><span class="line"></span><br><span class="line">DaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2</span><br><span class="line">Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2</span><br><span class="line">Containers:       cilium-operator    Running: 2</span><br><span class="line">                  cilium             Running: 2</span><br><span class="line">Image versions    cilium             quay.io/cilium/cilium:v1.9.5: 2</span><br><span class="line">                  cilium-operator    quay.io/cilium/operator-generic:v1.9.5: 2</span><br></pre></td></tr></table></figure>

<p>cilium-cli看到的是一些大面的信息，也可以通过cilium-agent查看，会列出一些详细信息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec -it -n kube-system cilium-8wvvp -- cilium status</span></span><br><span class="line">Defaulted container <span class="string">&quot;cilium-agent&quot;</span> out of: cilium-agent, mount-cgroup (init), wait-for-node-init (init), clean-cilium-state (init)</span><br><span class="line">KVStor:                 Ok   Disabled</span><br><span class="line">Kubernetes:             Ok   1.24 (v1.24.0) [linux/amd64]</span><br><span class="line">Kubernetes APIs:        [<span class="string">&quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;</span>, <span class="string">&quot;cilium/v2::CiliumEgressNATPolicy&quot;</span>, <span class="string">&quot;cilium/v2::CiliumLocalRedirectPolicy&quot;</span>, <span class="string">&quot;cilium/v2::CiliumNetworkPolicy&quot;</span>, <span class="string">&quot;cilium/v2::CiliumNode&quot;</span>, <span class="string">&quot;cilium/v2alpha1::CiliumEndpointSlice&quot;</span>, <span class="string">&quot;core/v1::Namespace&quot;</span>, <span class="string">&quot;core/v1::Node&quot;</span>, <span class="string">&quot;core/v1::Pods&quot;</span>, <span class="string">&quot;core/v1::Service&quot;</span>, <span class="string">&quot;discovery/v1::EndpointSlice&quot;</span>, <span class="string">&quot;networking.k8s.io/v1::NetworkPolicy&quot;</span>]</span><br><span class="line">KubeProxyReplacement:   Strict   [eth0 172.16.127.45 (Direct Routing)]</span><br><span class="line">Host firewall:          Disabled</span><br><span class="line">Cilium:                 Ok   1.11.4 (v1.11.4-9d25463)</span><br><span class="line">NodeMonitor:            Disabled</span><br><span class="line">Cilium health daemon:   Ok   </span><br><span class="line">IPAM:                   IPv4: 4/254 allocated from 10.244.0.0/24, </span><br><span class="line">Allocated addresses:</span><br><span class="line">  10.244.0.105 (kube-system/coredns-6d4b75cb6d-wjk9p)</span><br><span class="line">  10.244.0.146 (kube-system/coredns-6d4b75cb6d-956sh)</span><br><span class="line">  10.244.0.225 (router)</span><br><span class="line">  10.244.0.239 (health)</span><br><span class="line">BandwidthManager:       EDT with BPF   [eth0]</span><br><span class="line">Host Routing:           BPF</span><br><span class="line">Masquerading:           BPF   [eth0]   10.244.0.0/16 [IPv4: Enabled, IPv6: Disabled]</span><br><span class="line">Clock Source <span class="keyword">for</span> BPF:   ktime</span><br><span class="line">Controller Status:      30/30 healthy</span><br><span class="line">  Name                                  Last success   Last error   Count   Message</span><br><span class="line">  bpf-map-sync-cilium_ipcache           1s ago         34m13s ago   0       no error   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...省略</span></span><br></pre></td></tr></table></figure>

<p><code>cilium status --verbose</code>可以显示更多的信息，在debug时很有用.</p>
<p>使用cilium后，再来查看<code>iptables-save | grep KUBE-SVC</code>, 干净的很了, <code>ipvsadm -ln</code>也一目了然</p>
<p>cilium在线上集群跑了快一年了，还没有碰到过pod之间访问的奇葩网络问题, 爽.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.cilium.io/en/v1.12/">https://docs.cilium.io/en/v1.12/</a></li>
<li><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/</a></li>
<li><a href="https://juejin.cn/post/7208048755122339898">使用Cilium 完全替换kube-proxy - 掘金</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>argoCD学习(argocd踩坑记)</title>
    <url>/2021/04/02/argoCD-prombles/</url>
    <content><![CDATA[<p>argocd配合GitOps，可以实现对kubernetes多集群的应用版本管理，目前已用于我负责的业务环境中实现统一发布平台.</p>
<p><strong>在这里记录下使用argocd踩过的坑, 不定期更新</strong></p>
<span id="more"></span>



<h3 id="UI上出现了不属于某app的资源"><a href="#UI上出现了不属于某app的资源" class="headerlink" title="UI上出现了不属于某app的资源"></a>UI上出现了不属于某app的资源</h3><p>原因: 由于argo controller是根据资源中的annotations里的key(argocd.argoproj.io&#x2F;instance)来判定这个资源是不是属于某个app,在日常使用的过程，由于团队中使用的是rancher做为容器管理平台，rancher上提供了非常便利的对资源的clone手段，就会产生clone后的资源也会带上相同的annotations，但是在原app下的manifest并不包含这个资源，因此在app下就会出现这个资源且这个资源会被标注为<code>require pruning</code>,那么在每次sync后，这个clone后的资源就会被删除</p>
<p>解决: 在clone时将annotations删除掉即可.</p>
<p>参考: <a href="https://github.com/argoproj/argo-cd/issues/5792">https://github.com/argoproj/argo-cd/issues/5792</a></p>
<h3 id="使用用户密码登录时提示"><a href="#使用用户密码登录时提示" class="headerlink" title="使用用户密码登录时提示"></a>使用用户密码登录时提示</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HTTP response body: &#123;<span class="string">&quot;error&quot;</span>:<span class="string">&quot;no session information&quot;</span>,<span class="string">&quot;code&quot;</span>:16,<span class="string">&quot;message&quot;</span>:<span class="string">&quot;no session information&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>原因: argocd使用密码认证时会使用密码信息调用&#x2F;api&#x2F;session接口生成token信息，然后使用token转换成session， 出现这个错误是由client端没有做这个转换，因此在服务端找不到对应的session信息</p>
<h3 id="添加集群时提示Unauthenticated-desc-x3D-the-server-has-asked-for-the-client-to-provide-credentials"><a href="#添加集群时提示Unauthenticated-desc-x3D-the-server-has-asked-for-the-client-to-provide-credentials" class="headerlink" title="添加集群时提示Unauthenticated desc &#x3D;  the server has asked for the client to provide credentials"></a>添加集群时提示Unauthenticated desc &#x3D;  the server has asked for the client to provide credentials</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210222114258.png"></p>
<p>原因及解决方案: <a href="https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80">https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80</a></p>
<h3 id="kustomize-build-out-of-index"><a href="#kustomize-build-out-of-index" class="headerlink" title="kustomize build out of index"></a>kustomize build out of index</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210219204446.png"></p>
<p>原因: 这个是新版kustomize(v4.0.1)时使用了kyaml的版本的bug, 原因在于kustomizationl.yml中存在非法字符</p>
<p>解决: 回退kustomize为v3.6.1</p>
<h3 id="forbidden-User-“system-anonymouns”-cantnot-get-path"><a href="#forbidden-User-“system-anonymouns”-cantnot-get-path" class="headerlink" title="forbidden: User “system:anonymouns” cantnot get path"></a>forbidden: User “system:anonymouns” cantnot get path</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20210119193206.png"></p>
<p>解决: 重新使用命令加一下集群解决</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rgocd cluster add my-cluster --server <span class="string">&#x27;xxxx&#x27;</span> --upsert</span><br></pre></td></tr></table></figure>



<h3 id="login时提示received-the-unexpected-content-type"><a href="#login时提示received-the-unexpected-content-type" class="headerlink" title="login时提示received the unexpected content-type"></a>login时提示received the unexpected content-type</h3><p>执行login 登录命令时提示:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">argocd login argocd.local:4433</span><br></pre></td></tr></table></figure>

<p>出现以下错误</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201106184926.png"></p>
<p>原因: </p>
<p>参考: <a href="https://github.com/argoproj/argo-cd/issues/1415">https://github.com/argoproj/argo-cd/issues/1415</a></p>
<p>解决:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">argocd login argocd.local:4433 --grpc-web</span><br></pre></td></tr></table></figure>

<p>如果添加集群的时候提示以下错误，则需要先使用上面的命令进行登录，登录成功后再进行集群的添加操作.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201106185339.png"></p>
<h3 id="创建token时提示以下错误-not-have-apikey-capability"><a href="#创建token时提示以下错误-not-have-apikey-capability" class="headerlink" title="创建token时提示以下错误 not have apikey capability"></a>创建token时提示以下错误 not have apikey capability</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to generate new token: account <span class="string">&#x27;admin&#x27;</span> does not have apiKey capability</span><br></pre></td></tr></table></figure>

<p>原因: argocd的用户也是通过角色来进行划分的，创建token的用户需要具有 apiKey的权限，这个可以从argocd部署的configmap中查看使用的用户是否具有什么权限.</p>
<h3 id="回滚时提示以下错误revision-must-be-resolved"><a href="#回滚时提示以下错误revision-must-be-resolved" class="headerlink" title="回滚时提示以下错误revision must be resolved"></a>回滚时提示以下错误revision must be resolved</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unable to load data: revision HEAD must be resolved</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20201122185428.png"></p>
<p>原因: 如果使用argocd命令行的方式同步应用，会导致revision没有commit值，如上图，而回滚时需要revision，因此造成上面的报错.</p>
<h3 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h3><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/argoproj/argo-cd/issues/2802">https://github.com/argoproj/argo-cd/issues/2802</a></li>
<li><a href="https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80">https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80</a></li>
<li><a href="https://github.com/argoproj/argo-cd/issues/5792">https://github.com/argoproj/argo-cd/issues/5792</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Prometheus监控consul提示415 Unsupported Media Type错误</title>
    <url>/2020/03/03/consul-415-error/</url>
    <content><![CDATA[<p>虽然现业务架构上了kubernetes，因为历史遗留问题, 现在还存在着使用consul来做service discovery组件, 预计还得在生产中存在一段时间, 因此配上了Prometheus监控, 在部署过程中出现了一个Unsupported Media Type错</p>
<span id="more"></span>



<p>当然, 因为consul的3个节点都是二进制部署的, 因此在这里是直接使用了static discovery，很简单, 只需要在prometheus的配置文件中加入节点即可，consul官网也是很清楚的说明,参考<a href="https://www.consul.io/docs/">这里</a></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">consul-k8s</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">format:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">prometheus</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">20s</span></span><br><span class="line">  <span class="attr">scrape_timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">/v1/agent/metrics</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">static_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.220</span><span class="string">:8500</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.222</span><span class="string">:8500</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.104</span><span class="number">.224</span><span class="string">:8500</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是必须要指定format为Prometheus. 热启动Prometheus后，在Prometheus发现3个节点都是Down.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/7EE1F24A-8789-4D6E-BE25-B4D19BBDCE06.png"></p>
<p><strong>415 Unsupported Media Type</strong>, 直接翻译就是说, 返回的response是不支持的类型</p>
<p>登录一个consul节点使用以下命令查看是否有数据返回.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl <span class="string">&#x27;localhost:8500/v1/agent/metrics?format=prometheus&#x27;</span></span><br></pre></td></tr></table></figure>

<p>结果:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/21BAAB63-A351-4C2A-90B9-7794E18F15C7.png"></p>
<p>这就很奇怪了, 从提示来看, 是因为 <strong>retention time</strong></p>
<p>从consul的官网查看这个信息, 发现有个<strong>prometheus_retention_time</strong><a href="https://www.consul.io/docs/agent/options.html">配置</a>, 该参数目前节点的配置是没有配置的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">prometheus_retention_time If the value is greater than 0s (the default), this enables Prometheus <span class="built_in">export</span> of metrics. The duration can be expressed using the duration semantics and will aggregates all counters <span class="keyword">for</span> the duration specified (it might have an impact on Consul<span class="string">&#x27;s memory usage). A good value for this parameter is at least 2 times the interval of scrape of Prometheus, but you might also put a very high retention time such as a few days (for instance 744h to enable retention to 31 days). Fetching the metrics using prometheus can then be performed using the /v1/agent/metrics?format=prometheus endpoint. The format is compatible natively with prometheus. When running in this mode, it is recommended to also enable the option disable_hostname to avoid having prefixed metrics with hostname. Consul does not use the default Prometheus path, so Prometheus must be configured as follows. Note that using ?format=prometheus in the path won&#x27;</span>t work as ? will be escaped, so it must be specified as a parameter</span><br></pre></td></tr></table></figure>

<p>也就是说, 如果要用prometheus来监控consul, 则必须将这个值配置成大小0的值, 否则就会出现如上的错误</p>
<p>那这个值到底有何用呢, 从上面的解释来看, 这个参数指定了prometheus指标在内存中保存的时间, 如果不设置的话, 默认为0, 即不存在轮转时间, 上面还建议这个时间要是prometheus抓取的2倍, 当然这个时间越长, 使用的内存越多, 因此可酌情处理. </p>
<p>根据这个, 在consul原有配置文件的基础上加入以下配置, 然后再重启consul</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;telemetry&quot;:</span> &#123;</span><br><span class="line">  <span class="attr">&quot;prometheus_retention_time&quot;:</span> <span class="string">&quot;744h&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;disable_hostname&quot;:</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意: 在实践过程中发现使用consul reload命令重启consul发现配置不生效, 最好能够kill掉再启动, 或者systemd</p>
<p>重启之后再看prometheus发现就正常了.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200303141730.png"></p>
<p>grafana上抓取的数据也正常了.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200303141856.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.consul.io/docs/">https://www.consul.io/docs/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)</title>
    <url>/2023/09/26/cilium-on-kubernetes-network-models-config/</url>
    <content><![CDATA[<p>在前东家的时候其实就有意将cilium强大的链路追踪能力集成到生产环境中,各种因素导致没有很大信心落地, 经过深入调研(也就把官网docs翻了四五遍)及测试, 终于有机会在生产kubernetes集群中(目前一个集群规模不算很大,2w+核心,持续增长)使用cilium做为cni,同时替换kube-proxy, 到现在已经有一段时间了，也算是有生产经验可以跟大家聊一聊这个工具，使用体验总结一句话: 轻松愉悦.<br>分享一下整个落地过程,同时也总结下方方面面, 工作之余尽量更新.<br>此篇为: cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)</p>
<span id="more"></span>

<p>总体分为以下几块内容:<br><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">cilium在kubernetes中的生产实践一(cilium介绍)</a><br><a href="https://izsk.me/2023/06/03/cilium-on-kubernetes-install/">cilium在kubernetes中的生产实践二(cilium部署)</a><br>cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)<br>cilium在kubernetes中的生产实践四(cilium网络模型之生产实践)<br>cilium在kubernetes中的生产实践五(cilium网络策略)<br>cilium在kubernetes中的生产实践六(cilium排错指南)<br>cilium在kubernetes中的生产实践七(cilium中的bpf hook)</p>
<p>本节，会详细介绍一下cilium的网络模型中的一些关键参数及相关参数的配置及这些参数的必要条件, 在一些场景下，这些参数的启停对网络性能有很大影响</p>
<h3 id="Routing-路由"><a href="#Routing-路由" class="headerlink" title="Routing(路由)"></a>Routing(路由)</h3><p>开始先说一说路由, 各节点之前互通是kubernetes对CNI的通用要求， 但集群节点上的pod通信要通过什么样的方式路由到其它节点上呢? 在cilium中有多种选择:</p>
<ol>
<li>Encapsulation(封装)<br>这个相信对大家不陌生了, 最常见的vxlan就属于这种<br>在未提供任何配置的情况下，Cilium会自动以这种模式运行，因为这种模式<strong>对底层网络基础设施的要求最低</strong><br>在这种模式下，所有集群节点都会使用基于 UDP 的封装协议VXLAN或Geneve形成网状隧道。<strong>Cilium 节点之间的所有流量都经过封装.</strong><br>不过这种模式的缺点,由于要封装额外的协议头，提高了MTU开销,不过这个可以通过开启巨帧网络来降低影响.<br>由于增加了封装头，有效载荷可用的 MTU 要低于本地路由（VXLAN 每个网络数据包 50 字节）。这导致特定网络连接的最大吞吐率降低。</li>
</ol>
<p>注:<br>这里简单说一下Encapsulation vs Masquerading(下文会介绍)的区别:<br>Encapsulation翻译过来是封装&#x2F;包装,就是把AAA封装到BBB里, 它是<strong>一种叠加的过程</strong><br>而Masquerading翻译过来是伪装的, 它则是把AAA转变成BBB，是<strong>一种替换的过程</strong>, 最常见的如NAT,存在IP伪装</p>
<ol start="2">
<li>Native-Routing(原生路由)<br>Native-Routing网上的文档有不同的译法，被翻译成原生路由或者本地路由，作者觉得原生路由更贴合一些<br>在原生路由模式下，Cilium将把所有<strong>未寻址到</strong>另一个本地端点的数据包委托给Linux内核的路由子系统。这意味着数据包将被路由，就像本地进程发出数据包一样。因此，连接群集节点的网络必须能够路由PodCIDR。<br>再简单来说就是<strong>所有节点对所有pod的ip是可寻址的</strong><br>可寻址以通过两种方式实现：<blockquote>
<ol>
<li>节点本身不知道如何路由所有pod IP，但网络上有路由器知道如何到达所有其他pod。在这种情况下，Linux 节点被配置为包含指向此类路由器的默认路由。这种模式用于云提供商网络集成</li>
<li>每个节点都知道所有其他节点的所有pod IP，并在Linux内核路由表中插入路由来表示这一点。<br>如果<strong>所有节点共享一个L2网络</strong>,则可以启用选项<code>auto-direct-node-routes: true</code>来解决这个问题。如果kubernetes节点并非在同一个L2网络上，则需要BGP daemon组件的辅助。</li>
</ol>
</blockquote>
</li>
</ol>
<p><strong>配置原生路由时,Cilium会自动在Linux内核中启用IP转发</strong><br><strong>开启原生路由,表明cilium不能工作在任一隧道模式中</strong></p>
<p>开启native-routing的方式如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--<span class="built_in">set</span> tunnel=disabled</span><br><span class="line">--<span class="built_in">set</span> routing-mode=native</span><br></pre></td></tr></table></figure>

<p>注:<br>在cilium中还有一个词很常见: host-routing(主机路由,下文会介绍), 也是个很重要的性能优化点,这里只简单说一说host-routing vs native-routing区别:<br>native-routing说的是<strong>跨主机的pod间是如何导址访问的</strong><br>而host-routing则说的是<strong>网络数据是否会绕过iptables和上层主机堆栈，以实现比常规veth 设备操作更快的网络命名空间切换</strong><br>所以这两个完全是不同的二个概念, 很容易混淆</p>
<ol start="3">
<li>还有一些是在特定云厂商集群环境下才会用的路由方式，这里不过多介绍.</li>
</ol>
<p>所以总结来说，如果想要<strong>更佳的网络性能，选择native-routing是个不错的选择</strong><br>如果集群网络复杂, 可以使用<code>Encapsulation</code>，虽然有点性能损耗，好在任何网络都可满足条件, 只需要node间互通.</p>
<h3 id="IPAM"><a href="#IPAM" class="headerlink" title="IPAM"></a>IPAM</h3><p>cilium做为CNI, 自然需要具备为集群node上的pod分配地址段的能力, 这部分直接使用cilium默认的选项即可<br>在部署时指定<code>ipam.Operator.clusterPoolIPv4PodCIDRList=10.244.0.0/16</code>及<code>ipam.Operator.ClusterPoolIPv4MaskSize=24</code>即可<br>最终, 每个node所具备的pod ip pool将会以CiliumNode这种CRD的形式存在,可通过以下命令查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get cn  <span class="comment"># cn代表ciliumNode</span></span><br></pre></td></tr></table></figure>

<h3 id="Masquerading-伪装"><a href="#Masquerading-伪装" class="headerlink" title="Masquerading(伪装)"></a>Masquerading(伪装)</h3><p>通常情况下, <strong>pod的ip在集群外是不可寻址的</strong>,所以从pod里访问集群外的资源, 在网络协议里需要把pod IP masquerading成node IP, 这样网络请求来回时才可对应上, 这是典型的<code>iptables mode</code>,会降低网络性能<br>而基于<code>eBPF mode</code>是一种更加高效的masquerading技术 </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20230928163323.png"></p>
<p>eBPF-based masquerading时需要具备如下条件:</p>
<blockquote>
<ul>
<li>Linux kernel 4.19+</li>
<li>BPF NodePort特性(即在相应的网络接口上需要开启nodeport)</li>
</ul>
</blockquote>
<p>在安装cilium时通过指定如下参数: <code>bpf.masquerade=true</code>开启<br>如果上述条件不满足，就算是开启了bpf.masquerade&#x3D;true,也无法使用, cilium会自动回退到iptables的方式</p>
<p>可通过如下命令确认masquerading属处的mode</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cilium status --verbose</span><br><span class="line"><span class="comment"># 如下字段</span></span><br><span class="line">Masquerading: BPF</span><br></pre></td></tr></table></figure>

<h3 id="Host-Routing-主机路由"><a href="#Host-Routing-主机路由" class="headerlink" title="Host-Routing(主机路由)"></a>Host-Routing(主机路由)</h3><p>即使Cilium使用eBPF执行网络路由，默认情况下，网络数据包仍然会遍历节点的常规网络堆栈的某些部分。这就导致了所有数据包仍能通过所有iptables钩子而增加了开销。<br>在Cilium 1.9中引入了基于eBPF的主机路由，以<strong>完全绕过iptables和上层主机堆栈，并实现比常规veth设备操作更快的网络命名空间切换.</strong><br>host-routing 分为<code>Legacy mode及BPF mode</code>, </p>
<p>BPF mode要求如下:</p>
<blockquote>
<ul>
<li>Kernel &gt;&#x3D; 5.10</li>
<li>Direct-routing configuration or tunneling</li>
<li>eBPF-based kube-proxy replacement</li>
<li>eBPF-based masquerading</li>
</ul>
</blockquote>
<p>可通过如下命令确认masquerading属处的mode</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cilium status --verbose</span><br><span class="line"><span class="comment"># 如下字段</span></span><br><span class="line">Host Routing: BPF</span><br></pre></td></tr></table></figure>

<p>如果内核支持BPF.则会自动启用它.如果不满足BPF mode,则会自动回退到Legacy mode<br>不过从实践来看, Kernel&gt;&#x3D;5.10应该不是必须, 作者的集群 node kernel&#x3D;4.19时, 也可开启host-routing BPF</p>
<h3 id="DSR-直接服务返回"><a href="#DSR-直接服务返回" class="headerlink" title="DSR(直接服务返回)"></a>DSR(直接服务返回)</h3><p><strong>默认情况下，Cilium的eBPF NodePort实现在SNAT模式下运行</strong>。也就是说，当节点外部流量到达节点LoadBalancer、NodePort或具有externalIP的服务后端位于远程节点时(即该服务的pod没有部署在请求到达的这台node上)，则节点通过执行SNAT将请求重定向到远程后端。这不需要任何额外的MTU更改。代价是来自后端的回复需要进行额外的跳回到该节点，以在将数据包直接返回到外部客户端之前在那里执行反向SNAT转换。<br>可以通过loadBalancer.mode Helm选项将此设置更改为dsr，以便让Cilium的eBPFNodePort实现在DSR模式下运行。<strong>在DSR模式下，后端直接回复外部客户端，而不需要额外的跳，这意味着后端使用服务IP&#x2F;端口作为源进行回复。</strong></p>
<p>DSR的前置要求为:</p>
<blockquote>
<ul>
<li>需要cilimi开启Native-Routing.</li>
</ul>
</blockquote>
<p><strong>DSR模式的另一个优点是客户端的源IP被保留</strong>，因此策略可以在后端节点上与之匹配。在SNAT模式下，这是不可能的。给定一个特定的后端可以被多个服务使用，后端需要知道它们需要回复的服务IP&#x2F;端口。因此，Cilium在Cilium特定的IPv4选项或IPv6目的地选项扩展报头中编码此信息，代价是较低的MTU。对于TCP服务，Cilium只对SYN数据包的服务IP&#x2F;端口进行编码，而不对后续数据包进行编码。后者还允许在混合模式下操作Cilium，其中DSR用于TCP，SNAT用于UDP，以避免另外需要的MTU减少</p>
<p>请注意，DSR模式在某些公共云提供商环境中可能无法使用，原因是底层网络结构可能会丢弃特定于Cilium的IP选项。如果后端位于处理给定NodePort请求的节点的远程节点上的服务的连接问题，首先检查NodePort请求是否实际到达包含后端的节点。如果不是这种情况，则建议切换回默认SNAT模式作为一种解决方法</p>
<p>总结就是, <strong>DSR有利于提高服务响应时间</strong></p>
<h3 id="socketLB"><a href="#socketLB" class="headerlink" title="socketLB"></a>socketLB</h3><p>socketLB也是一个很实用的功能，旨在将BPF程序attach到socket的系统调用hooks，使客户端直接和后端pod建连和通信, 但socketLB需要cgroup v2的支持, 相信大多数的生产环境还是在用cgroup v1, 因此这里将不进行socketlb的实践.</p>
<h3 id="vxlan下的虚拟网口"><a href="#vxlan下的虚拟网口" class="headerlink" title="vxlan下的虚拟网口"></a>vxlan下的虚拟网口</h3><p>在默认部署方式下，使用vxlan的overlay组网情况，主机上的网络会发生了以下变化，在主机的 root 命名空间，新增了如下图所示的四个虚拟网络接口，</p>
<blockquote>
<ul>
<li>cilium_vxlan: 主要是对数据包进行vxlan封装和解封装操作</li>
<li>cilium_net和cilium_host: 是一对 veth-pair, 一端插在主机上，一端插在容器里</li>
<li>cilium_host: 作为该节点所管理的Cluster IP子网的网关,容器中使用ip a看到的eth0的网卡的形式就eth0@xxx, xxx与宿主机上的一个lxcxxxx@ifxxx相对应，一般为正负1的关系<br>可以通过以下cilium命令查看相关的vxlan tunnel</li>
</ul>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cilium bpf tunnel list <span class="comment"># 查看各个节点的vxlan tunnel</span></span><br></pre></td></tr></table></figure>

<h3 id="暴露服务"><a href="#暴露服务" class="headerlink" title="暴露服务"></a>暴露服务</h3><p>还有一个很常见的场景是，需要在集群外访问集群内的服务, 当然直接使用原生的nodeport是最直接的, 在cilium中还提供了一个很重要的选择: lbExternalClusterIP, 用于是否开启集群外对集群内资源的访问,<br>也很简单，只需要二步:</p>
<ol>
<li>集群外可访问得到集群内资源的ip(podIP或者是serviceIP)</li>
<li>设置cilium的<code>bpf.lbExternalClusterIP=true</code></li>
</ol>
<p>但需要注意的是<strong>这种方式是开启所有服务可在集群外访问，相对于nodeport按需开启在安全方面会弱一些</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://cilium.io/">https://cilium.io</a></li>
<li><a href="https://www.infoq.cn/article/p9vG2G9T49KpvHrckFwu">https://www.infoq.cn/article/p9vG2G9T49KpvHrckFwu</a></li>
<li><a href="https://www.cnblogs.com/charlieroro/p/13403672.html">https://www.cnblogs.com/charlieroro/p/13403672.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/404278920">https://zhuanlan.zhihu.com/p/404278920</a></li>
<li><a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/</a></li>
<li><a href="https://podsbook.com/posts/cilium/xdp/">https://podsbook.com/posts/cilium/xdp/</a></li>
<li><a href="https://mdnice.com/writing/4429d86bf8dc42d1a42f60ae7b873f6a">https://mdnice.com/writing/4429d86bf8dc42d1a42f60ae7b873f6a</a></li>
<li><a href="https://mp.weixin.qq.com/s/KSWrvOmKeX-74RU5d6NUlQ">https://mp.weixin.qq.com/s/KSWrvOmKeX-74RU5d6NUlQ</a></li>
<li><a href="http://www.sel.zju.edu.cn/blog/2022/12/01/cilium-kind-setup/">http://www.sel.zju.edu.cn/blog/2022/12/01/cilium-kind-setup/</a></li>
<li><a href="https://blog.51cto.com/liujingyu/5285535">https://blog.51cto.com/liujingyu/5285535</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/594084783">https://zhuanlan.zhihu.com/p/594084783</a></li>
<li><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/</a></li>
<li><a href="https://izsk.me/2023/06/03/cilium-on-kubernetes-install/">https://izsk.me/2023/06/03/cilium-on-kubernetes-install/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask学习(flask-login使用)</title>
    <url>/2017/07/01/flask-login%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>近期在做一个数据库后台管理系统,功能非常简单,就是定时获取oracle数据库的awr性能报告数据,结合highcharts生成图表.因为之前已经用python写了个模块,这次有个功能刚好可以直接用,就也直接想用python写了,这种小系统当然是用python的flask框架了,简单轻便,项目中有用到用户登陆功能,使用了flask-login,对这种内部小型的不需要太复杂权限应用系统实乃必备</p>
<span id="more"></span>

<h3 id="flask-login"><a href="#flask-login" class="headerlink" title="flask-login"></a><strong>flask-login</strong></h3><p>Flask-Login 为 Flask 提供了用户会话管理,它处理了日常的登入、登出并且长时间记住用户的会话,主要使用的是session机制</p>
<p>它可以轻松实现以下功能:</p>
<blockquote>
<ul>
<li>在会话中存储当前活跃的用户 ID，让你能够自由地登入和登出</li>
<li>让你限制登入(或者登出)用户可以访问的视图</li>
<li>处理让人棘手的 “记住我” 功能</li>
<li>帮助你保护用户会话免遭 cookie 被盗的牵连</li>
<li>可以与以后可能使用的 Flask-Principal 或其它认证扩展集成</li>
</ul>
</blockquote>
<p>但是,它不会:</p>
<blockquote>
<ul>
<li>限制你使用特定的数据库或其它存储方法。如何加载用户完全由你决定</li>
<li>限制你使用用户名和密码,OpenIDs,或者其它的认证方法</li>
<li>处理超越 “登入或者登出” 之外的权限</li>
<li>处理用户注册或者账号恢复</li>
</ul>
</blockquote>
<p>使用它也是非常简单</p>
<blockquote>
<ol>
<li>引用flask-login</li>
<li>在应用中配置login_manager</li>
<li>实现User(db.Model,UserMixin)</li>
<li>完成load_user(user_id)回调函数</li>
<li>login_user保存用户信息</li>
<li>使用@login_required</li>
</ol>
</blockquote>
<p>当然,以上几点并不是它的顺序,而是各自对应一种操作,下面一一介绍:</p>
<h4 id="初始化flask-login"><a href="#初始化flask-login" class="headerlink" title="初始化flask-login"></a><strong>初始化flask-login</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask_login <span class="keyword">import</span> UserMixin,LoginManager,login_user,logout_user,login_required</span><br><span class="line"></span><br><span class="line">login_manager = LoginManager()                        <span class="comment">####LoginManager对象</span></span><br><span class="line">login_manager.login_view = <span class="string">&quot;login&quot;</span>                    <span class="comment">####表示如果用户没有登陆时跳转指向的路由</span></span><br><span class="line">login_manager.session_protection = <span class="string">&quot;strong&quot;</span>           <span class="comment">####session的保护等级,如果发现异常会登出用户</span></span><br><span class="line">login_manager.login_message = <span class="string">&quot;请先登陆&quot;</span>               <span class="comment">####用户没有登陆时访问需要登陆才能访问的页面时的提示</span></span><br><span class="line">login_manager.login_message_category = <span class="string">&quot;warning&quot;</span>      <span class="comment">####上述提示的提示等级,跟flask的flash消息分类等同</span></span><br><span class="line">login_manager.init_app(app)                           <span class="comment">####初始化</span></span><br></pre></td></tr></table></figure>

<p>登录管理(login manager)包含了让你的应用和 Flask-Login 协同工作的代码，比如怎样从一个 ID 加载用户，当用户需要登录的时候跳转到哪里等等。一旦实际的应用对象创建后，就可以使用最后一句话在应用中初始化了</p>
<h4 id="实现User类"><a href="#实现User类" class="headerlink" title="实现User类"></a><strong>实现User类</strong></h4><p>使用flask-login验证用户登录时需要如下属性:</p>
<blockquote>
<ol>
<li><p>is_authenticated</p>
<p>当用户通过验证时,也即提供有效证明时返回 True .（只有通过验证的用户会满足 login_required 的条件)</p>
</li>
<li><p>is_active</p>
<p>如果这是一个活动用户且通过验证,账户也已激活,未被停用,也不符合任何你 的应用拒绝一个账号的条件,返回 True .不活动的账号可能不会登入（当然, 是在没被强制的情况下）</p>
</li>
<li><p>is_anonymous</p>
<p>如果是一个匿名用户,返回 True (真实用户应返回 False)</p>
</li>
<li><p>get_id()</p>
<p>返回一个能唯一识别用户的,并能用于从 user_loader 回调中加载用户的 unicode .注意着 必须 是一个 unicode,如           果 ID 原本是 一个 int 或其它类型,你需要把它转换为 unicode.</p>
</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">User</span>(db.Model,UserMixin):</span><br><span class="line">    __tablename__ = <span class="string">&#x27;XT_REGIST_USER&#x27;</span></span><br><span class="line">    rowid = db.Column(db.Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    username = db.Column(db.String(<span class="number">64</span>), unique=<span class="literal">True</span>)</span><br><span class="line">    password = db.Column(db.String(<span class="number">255</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rowid=<span class="literal">None</span>, username=<span class="literal">None</span>, password=<span class="literal">None</span></span>):</span><br><span class="line">        self.rowid = rowid</span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = self.set_password(password)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_password</span>(<span class="params">self, password</span>):</span><br><span class="line">        <span class="keyword">return</span> generate_password_hash(password,method=<span class="string">&quot;pbkdf2:sha1:100&quot;</span>,salt_length=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_password</span>(<span class="params">self, password</span>):</span><br><span class="line">        <span class="keyword">return</span> check_password_hash(self.password, password)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_active</span>():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_anonymous</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_id</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.rowid</span><br><span class="line"></span><br><span class="line"><span class="comment">####登陆视图</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/login/&quot;</span>,methods=[<span class="string">&quot;GET&quot;</span>,<span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">login</span>():</span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">&quot;POST&quot;</span>:</span><br><span class="line">        me = User.query.filter_by(username=request.form.get(<span class="string">&#x27;username&#x27;</span>)).first()</span><br><span class="line">        <span class="keyword">if</span> me <span class="keyword">and</span> me.check_password(request.form.get(<span class="string">&#x27;password&#x27;</span>)):</span><br><span class="line">            session[<span class="string">&quot;spasswd&quot;</span>] = request.form.get(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line">            login_user(me,remember=request.form.get(<span class="string">&#x27;rememberme&#x27;</span>))</span><br><span class="line">            <span class="keyword">return</span> redirect(url_for(<span class="string">&quot;online&quot;</span>,_external=<span class="literal">True</span>) <span class="keyword">or</span> request.args.get(<span class="string">&quot;next&quot;</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            flash(<span class="string">&quot;用户不存在或密码错误&quot;</span>,<span class="string">&#x27;warning&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&quot;login.html&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这里我们是直接继承UserMixin,它提供了对所有这些方法的默认 实现,当然可以自己实现用户类的.通过werkzeug.security包中的generate_password_hash与check_password_hash来实现密码的加密解密.</p>
<p>我们看到在login视图中,先通过用户名在库中查找到该用户信息(注意这里使用了User类),如果存在该用户而且输入的密码也正确,那么就使用login_user()函数来登录用户,这时用户在会话中的状态就是登录状态了</p>
<h4 id="remember-me"><a href="#remember-me" class="headerlink" title="remember_me"></a><strong>remember_me</strong></h4><p>“记住我”的功能很难实现,但是，Flask-Login 几乎透明地实现它 - 只要把 remember&#x3D;True 传递给 login_user。一个 cookie 将会存储在用户计算机中，如果用户会话中没有用户 ID 的话，Flask-Login 会自动地从 cookie 中恢复用户 ID。cookie 是防纂改的，因此如果用户纂改过它(比如，使用其它的一些东西来代替用户的 ID)，它就会被拒绝，就像不存在。该层功能是被自动实现的。但你能（且应该，如果你的应用处理任何敏感的数据）提供 额外基础工作来增强你记住的 cookie 的安全性</p>
<h4 id="load-user"><a href="#load-user" class="headerlink" title="load_user()"></a><strong>load_user()</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@login_manager.user_loader</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_user</span>(<span class="params">rowid</span>):</span><br><span class="line">    <span class="keyword">return</span> User.query.get(<span class="built_in">int</span>(rowid))</span><br></pre></td></tr></table></figure>

<p>这是一个回调函数,根据session中存储的rowid来reload User object,返回User对象,第一行,其实是一个装饰器,这个装饰器表示,每次有请求时都会调用它所装饰的函数,说的更直白一点就是,<strong>每次请求到来时,都会调用这个方法,检查id是否在当前session中,如果存在返回id对应的User给login_user(),不存在则返回None</strong>.这里有更详细的说明<a href="https://stackoverflow.com/questions/12075535/flask-login-cant-understand-how-it-works">flask-login: can’t understand how it works</a></p>
<h4 id="login-required"><a href="#login-required" class="headerlink" title="@login_required"></a><strong>@login_required</strong></h4><p>如果需要让页面只可让已登陆的用户访问,可使用login_required装饰路由函数,未登陆的请求将会跳转到上面loginManager.login_view设置的登陆页面路由</p>
<h4 id="logout-user"><a href="#logout-user" class="headerlink" title="logout_user()"></a><strong>logout_user()</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/logout/&#x27;</span>,methods=[<span class="string">&#x27;GET&#x27;</span>, <span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logout</span>():</span><br><span class="line">    logout_user()</span><br><span class="line">    <span class="keyword">return</span> redirect(url_for(<span class="string">&#x27;index&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p>logout_user()函数功能就是将缓存的用户信息清楚，将Remember me标记位设置为清空状态</p>
<h4 id="current-user-is-authenticated"><a href="#current-user-is-authenticated" class="headerlink" title="current_user.is_authenticated()"></a><strong>current_user.is_authenticated()</strong></h4><p>current_user.is_authenticated()是用来获取当前登陆用户的,可直接在模板中判断用户级别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> current_user.is_authenticated() <span class="keyword">and</span> current_user.username == <span class="string">&#x27;Admin&#x27;</span> %&#125;</span><br><span class="line">  Hi &#123;&#123; current_user.name &#125;&#125;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.jianshu.com/p/06bd93e21945">使用Flask实现用户登陆认证的详细过程</a></li>
<li><a href="http://www.cnblogs.com/alima/p/5818852.html">关于flask-login中各种API使用实例</a></li>
<li><a href="http://www.voidcn.com/blog/agmcs/article/p-3750636.html">Flask学习记录之Flask-Login</a></li>
<li><a href="http://www.pythondoc.com/flask-login/">Flask-Login</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>使用fluentd收集Kubernetes容器日志</title>
    <url>/2019/12/17/container-log-fluentd-elasticsearch/</url>
    <content><![CDATA[<p>使用fluentd收集kubernetes的容器日志，由elastricsearch存储，并由kibana view. 这里不仅收集业务容器的日志, 同时也收集kuberntes集群组件的日志, 由于业务容器跟集群组件的日志打印格式不一致，因此需要单独使用正则进行处理. </p>
<span id="more"></span>



<h3 id="dockerd配置"><a href="#dockerd配置" class="headerlink" title="dockerd配置"></a>dockerd配置</h3><p>kubernetes组件本身也是以容器的方式部署</p>
<p>容器本身需要将日志都重定向到标准输出，同时指定dockerd的日志打印格式为json,这个可以全局修改dockerd的启动参数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /etc/docker/daemon.json|grep &quot;log-driver&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;log-driver&quot;</span>: <span class="string">&quot;json-file&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 日志格式如下:</span></span><br><span class="line">&#123;<span class="string">&quot;log&quot;</span>:<span class="string">&quot;64 bytes from 14.215.177.39: seq=34 ttl=55 time=7.067 ms\r\n&quot;</span>,<span class="string">&quot;stream&quot;</span>:<span class="string">&quot;stdout&quot;</span>,<span class="string">&quot;time&quot;</span>:<span class="string">&quot;2019-05-16T14:14:15.030612567Z&quot;</span>&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>所有的容器日志都能在目录**&#x2F;var&#x2F;log&#x2F;containers&#x2F;*.log**找到</p>
<p>因些fluentd就是检测这个目录下的日志变化，类似于tail -f的机制实时获取新增日志.</p>
<p> fluentd本身也是容器. 配置文件是以configmap的形式存在，如下:</p>
<h3 id="fluentd-conf"><a href="#fluentd-conf" class="headerlink" title="fluentd.conf"></a>fluentd.conf</h3><details> <summary>fluentd.conf</summary>
<pre><code>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fluentd-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">system.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    &lt;system&gt;</span></span><br><span class="line"><span class="string">      &lt;log&gt;</span></span><br><span class="line"><span class="string">        format json</span></span><br><span class="line"><span class="string">        log_level warn</span></span><br><span class="line"><span class="string">        time_format %Y-%m-%dT%H:%M:%S</span></span><br><span class="line"><span class="string">      &lt;/log&gt;</span></span><br><span class="line"><span class="string">      root_dir /tmp/fluentd-buffers/</span></span><br><span class="line"><span class="string">    &lt;/system&gt;</span></span><br><span class="line"><span class="string"></span>  <span class="attr">containers.input.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    &lt;source&gt;</span></span><br><span class="line"><span class="string">      @id fluentd-k8s-containers.log</span></span><br><span class="line"><span class="string">      @type tail</span></span><br><span class="line"><span class="string">      path /var/log/containers/*.log</span></span><br><span class="line"><span class="string"></span><span class="string">​</span>     <span class="string">exclude_path</span> [<span class="string">&quot;/var/log/containers/*install-cni*&quot;</span>, <span class="string">&quot;/var/log/containers/*rke*&quot;</span>]</span><br><span class="line"><span class="string">​</span>     <span class="string">pos_file</span> <span class="string">/var/log/fluentd-k8s-containers.log.pos</span></span><br><span class="line"><span class="string">​</span>     <span class="string">tag</span> <span class="string">kubernetes.*</span></span><br><span class="line"><span class="string">​</span>     <span class="string">&lt;parse&gt;</span></span><br><span class="line"><span class="string">​</span>       <span class="string">@type</span> <span class="string">multi_format</span></span><br><span class="line"><span class="string">​</span>       <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>         <span class="string">format</span> <span class="string">json</span></span><br><span class="line"><span class="string">​</span>         <span class="string">time_key</span> <span class="string">time</span> </span><br><span class="line"><span class="string">​</span>         <span class="string">time_format</span> <span class="string">%Y-%m-%dT%H:%M:%S.%NZ</span></span><br><span class="line"><span class="string">​</span>       <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>     <span class="string">&lt;/parse&gt;</span></span><br><span class="line"><span class="string">​</span>   <span class="string">&lt;/source&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># Enriches records with Kubernetes metadata</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">filter_kubernetes_metadata</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">kubernetes_metadata</span></span><br><span class="line"><span class="string">​</span>      <span class="string">skip_labels</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="comment">#skip_container_metadata true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">skip_master_url</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">skip_namespace_metadata</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/filter&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># Transfer docker logging date to Chinese time. 8H issue.</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">record_transformer</span></span><br><span class="line"><span class="string">​</span>      <span class="string">enable_ruby</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;record&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">service_name</span> <span class="string">$&#123;record[&#x27;kubernetes&#x27;][&#x27;container_name&#x27;]&#125;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">docker_stamp</span> <span class="string">$&#123;time.to_i</span> <span class="string">+</span> <span class="number">3600</span> <span class="string">*</span> <span class="number">8</span><span class="string">&#125;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/record&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/filter&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># Fixes json fields in Elasticsearch</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;filter</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">k8s_filter_parser</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">parser</span></span><br><span class="line"><span class="string">​</span>      <span class="string">key_name</span> <span class="string">log</span></span><br><span class="line"><span class="string">​</span>      <span class="string">reserve_data</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">remove_key_name_field</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;parse&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">@type</span> <span class="string">multi_format</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>          <span class="string">format</span> <span class="string">json</span></span><br><span class="line"><span class="string">​</span>          <span class="string">time_key</span> <span class="string">time</span></span><br><span class="line"><span class="string">​</span>          <span class="string">keep_time_key</span></span><br><span class="line"><span class="string">​</span>          <span class="string">time_format</span> <span class="string">%Y-%m-%dT%H:%M:%S</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#k8s-Component</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#&#123;&quot;log&quot;:&quot;I0710 04:12:31.540733       1 vxlan_network.go:60] watching for new subnet leases\n&quot;,&quot;stream&quot;:&quot;stderr&quot;,&quot;time&quot;:&quot;2019-07-10T04:12:31.540798651Z&quot;&#125;</span></span><br><span class="line"><span class="string">​</span>          <span class="string">format</span> <span class="string">/^[A-Z]+(?&lt;logtime&gt;.*)[\s]+(?&lt;request_file&gt;[0-9]+.*)\]</span> <span class="string">(?&lt;msg&gt;.*)$/</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#cattle-node-agent</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#&#123;&quot;log&quot;:&quot;time=\&quot;2019-09-20T18:06:03Z\&quot; level=info msg=\&quot;Starting plan monitor\&quot;\n&quot;,&quot;stream&quot;:&quot;stderr&quot;,&quot;time&quot;:&quot;2019-09-20T18:06:03.149115054Z&quot;&#125;</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#format /^time=.&#123;2&#125;(?&lt;time&gt;.*Z).&#123;2&#125; level=(?&lt;level&gt;.*) msg=(?&lt;log&gt;.*)$/</span></span><br><span class="line"><span class="string">​</span>          <span class="string">format</span> <span class="string">/^(?&lt;logtime&gt;.*)</span> <span class="string">level=(?&lt;level&gt;.*)</span> <span class="string">msg=(?&lt;msg&gt;.*)$/</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#fluentd-container</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#&#123;&quot;log&quot;:&quot;/var/lib/gems/2.3.0/gems/fluentd-1.6.3/lib/fluent/plugin/parser_regexp.rb:50: warning: regular expression has &#x27;]&#x27; without escape\n&quot;,&quot;stream&quot;:&quot;stderr&quot;, &quot;time&quot;: &quot;2019-09-20T18:06:03.149115054Z&quot;&#125;</span></span><br><span class="line"><span class="string">​</span>          <span class="string">format</span> <span class="string">/^(?&lt;request_file&gt;.*[0-9]+):</span> <span class="string">(?&lt;level&gt;.*):</span> <span class="string">(?&lt;msg&gt;.*)$/</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;pattern&gt;</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#nginx-container error log</span></span><br><span class="line"><span class="string">​</span>          <span class="comment">#2019/10/18 12&quot;00:00 [warn] 123#123: *xxxyyy zzz...</span></span><br><span class="line"><span class="string">​</span>          <span class="string">format</span> <span class="string">/^(?&lt;logtime&gt;.*)</span> <span class="string">\[(?&lt;level&gt;.*)\]</span> <span class="string">(?&lt;pid&gt;[0-9]+)#(?&lt;tid&gt;[0-9]+):</span> <span class="string">(?&lt;msg&gt;.*)$/</span></span><br><span class="line"><span class="string">​</span>        <span class="string">&lt;/pattern&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/parse&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/filter&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># Modify tag to container name</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;match</span> <span class="string">kubernetes.**&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">rewrite_tag_filter</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;rule&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">key</span> <span class="string">$[&#x27;kubernetes&#x27;][&#x27;container_name&#x27;]</span></span><br><span class="line"><span class="string">​</span>        <span class="string">pattern</span> <span class="string">^(.+)$</span></span><br><span class="line"><span class="string">​</span>        <span class="string">tag</span> <span class="string">$1</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/rule&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/match&gt;</span></span><br><span class="line">  <span class="attr">system.input.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">​</span>    <span class="comment">#Logs from systemd-journal for interesting services.</span></span><br><span class="line"><span class="string">​</span>    <span class="comment">#TODO(random-liu): Remove this after cri container runtime rolls out.</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">journald-docker</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">systemd</span></span><br><span class="line"><span class="string">​</span>      <span class="string">matches</span> [&#123; <span class="attr">&quot;_SYSTEMD_UNIT&quot;:</span> <span class="string">&quot;docker.service&quot;</span> &#125;]</span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;storage&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">@type</span> <span class="string">local</span></span><br><span class="line"><span class="string">​</span>        <span class="string">persistent</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">path</span> <span class="string">/var/log/journald-docker.pos</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/storage&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;entry&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_strip_underscores</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_lowercase</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/entry&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">tag</span> <span class="string">docker</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">journald-container-runtime</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">systemd</span></span><br><span class="line"><span class="string">​</span>      <span class="string">matches</span> [&#123; <span class="attr">&quot;_SYSTEMD_UNIT&quot;:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; fluentd_container_runtime_service &#125;&#125;</span>.service&quot;</span> &#125;]</span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;storage&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">@type</span> <span class="string">local</span></span><br><span class="line"><span class="string">​</span>        <span class="string">persistent</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">path</span> <span class="string">/var/log/journald-container-runtime.pos</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/storage&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;entry&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_strip_underscores</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_lowercase</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/entry&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">tag</span> <span class="string">container-runtime</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span>   </span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">kernel</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">systemd</span></span><br><span class="line"><span class="string">​</span>      <span class="string">matches</span> [&#123; <span class="attr">&quot;_TRANSPORT&quot;:</span> <span class="string">&quot;kernel&quot;</span> &#125;]</span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;storage&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">@type</span> <span class="string">local</span></span><br><span class="line"><span class="string">​</span>        <span class="string">persistent</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">path</span> <span class="string">/var/log/kernel.pos</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/storage&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;entry&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_strip_underscores</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>        <span class="string">fields_lowercase</span> <span class="literal">true</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/entry&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">tag</span> <span class="string">kernel</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span></span><br><span class="line">  <span class="attr">forward.input.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># Takes the messages sent over TCP</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">forward</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">forward</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span></span><br><span class="line">  <span class="attr">monitoring.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">​</span>    <span class="comment">#  Prometheus Exporter Plugin</span></span><br><span class="line"><span class="string">​</span>    <span class="comment"># input plugin that exports metrics</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">prometheus</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">prometheus</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">monitor_agent</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">monitor_agent</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span>   </span><br><span class="line"><span class="string">​</span>    <span class="comment"># input plugin that collects metrics from MonitorAgent</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">prometheus_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">prometheus_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;labels&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">host</span> <span class="string">$&#123;hostname&#125;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/labels&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span>   </span><br><span class="line"><span class="string">​</span>    <span class="comment"># input plugin that collects metrics for output plugin</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">prometheus_output_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">prometheus_output_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;labels&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">host</span> <span class="string">$&#123;hostname&#125;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/labels&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span>    </span><br><span class="line"><span class="string">​</span>    <span class="comment"># input plugin that collects metrics for in_tail plugin</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;source&gt;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@id</span> <span class="string">prometheus_tail_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">@type</span> <span class="string">prometheus_tail_monitor</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;labels&gt;</span></span><br><span class="line"><span class="string">​</span>        <span class="string">host</span> <span class="string">$&#123;hostname&#125;</span></span><br><span class="line"><span class="string">​</span>      <span class="string">&lt;/labels&gt;</span></span><br><span class="line"><span class="string">​</span>    <span class="string">&lt;/source&gt;</span></span><br><span class="line">  <span class="attr">output.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    &lt;match **&gt;</span></span><br><span class="line"><span class="string">      @id elasticsearch</span></span><br><span class="line"><span class="string">      @type elasticsearch</span></span><br><span class="line"><span class="string">      type_name _doc</span></span><br><span class="line"><span class="string">      include_tag_key true</span></span><br><span class="line"><span class="string">      host your-es-cluster-endpoint</span></span><br><span class="line"><span class="string">      port 9200</span></span><br><span class="line"><span class="string">      user your-es-account</span></span><br><span class="line"><span class="string">      password your-es-password</span></span><br><span class="line"><span class="string">      logstash_format true</span></span><br><span class="line"><span class="string">      logstash_prefix your-es-index-prefix.$&#123;tag&#125;</span></span><br><span class="line"><span class="string">      request_timeout 30s</span></span><br><span class="line"><span class="string">      &lt;buffer&gt;</span></span><br><span class="line"><span class="string">        @type file</span></span><br><span class="line"><span class="string">        path /var/log/fluentd-buffers/kubernetes.system.buffer</span></span><br><span class="line"><span class="string">        chunk_limit_size 64MB</span></span><br><span class="line"><span class="string">        total_limit_size 32GB</span></span><br><span class="line"><span class="string">        flush_mode interval</span></span><br><span class="line"><span class="string">        retry_type exponential_backoff</span></span><br><span class="line"><span class="string">        flush_thread_count 2</span></span><br><span class="line"><span class="string">        flush_interval 5s</span></span><br><span class="line"><span class="string">        retry_forever</span></span><br><span class="line"><span class="string">        retry_max_interval 30</span></span><br><span class="line"><span class="string">        queue_limit_length 8</span></span><br><span class="line"><span class="string">        overflow_action block</span></span><br><span class="line"><span class="string">      &lt;/buffer&gt;</span></span><br><span class="line"><span class="string">    &lt;/match&gt;</span></span><br></pre></td></tr></table></figure>
</code></pre>
</details>



<p>从上面可以看到，这里不仅收集业务容器的日志, 同时也收集kuberntes集群组件的日志, 由于业务容器跟集群组件的日志虽然都是json格式，但为了更细粒度的进行数据分析，使用正则进行处理.</p>
<p>最后需要修改es的集群地址，用户、密码，索引前缀等信息.</p>
<p>这里没有使用kafka进行缓存, 一来因为使用kafka后，又需要一个logstash进行过度到es，增加了一层,又得维护一层配置，后续增加索引时不是很方便</p>
<p>二来数据量没达到一个量级, 没有kafka，es也能够抗住.</p>
<p>如果需要使用kafka的话，也可以使用kafka-connector机制来直接对接elasticsearch， <a href="https://github.com/confluentinc/kafka-connect-elasticsearch">github</a>上已经有现成的工具, 大家可参考使用, 我这里没用过, </p>
<h3 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h3><p>在体验的时候，由于各个组件打印的日志格式都不尽相同, 为了接收更多的组件日志定位问题，在使用正则表达式匹配的时候花的时候最长, 同时，大家也可开启fluentd的debug日志，或者将收集到的日志直接打印在本地，对定位问题方便一点，最后切记将debug关掉即可, 不然，磁盘会扛不住</p>
<p>Fluentd本地保存收集日志的配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  outputfile.conf: |- </span></span><br><span class="line"><span class="comment">#    &lt;match **&gt;</span></span><br><span class="line"><span class="comment">#      @type file</span></span><br><span class="line"><span class="comment">#      path /var/log/$&#123;tag&#125;.fluentd</span></span><br><span class="line"><span class="comment">#      &lt;buffer tag&gt;</span></span><br><span class="line"><span class="comment">#        @type file</span></span><br><span class="line"><span class="comment">#        path /var/log/xxyy</span></span><br><span class="line"><span class="comment">#      &lt;/buffer&gt;</span></span><br><span class="line"><span class="comment">#    &lt;/match&gt;</span></span><br></pre></td></tr></table></figure>

<p>fluentd开启es的debug, 更多参数可参考<a href="https://github.com/uken/fluent-plugin-elasticsearch">这里</a></p>
<p>遇到一个elasticsearch 索引mapping问题，感兴趣的可参考<a href="https://izsk.me/2019/10/06/ES-FGC-Fix/">这里</a> </p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.fluentd.org/">https://www.fluentd.org/</a></li>
<li><a href="https://www.cnblogs.com/operationhome/p/10907591.html">https://www.cnblogs.com/operationhome/p/10907591.html</a></li>
<li><a href="https://github.com/confluentinc/kafka-connect-elasticsearch">https://github.com/confluentinc/kafka-connect-elasticsearch</a></li>
<li><a href="https://github.com/uken/fluent-plugin-elasticsearch">https://github.com/uken/fluent-plugin-elasticsearch</a></li>
<li><a href="https://izsk.me/2019/10/06/ES-FGC-Fix/">https://izsk.me/2019/10/06/ES-FGC-Fix/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>FELK</category>
      </categories>
      <tags>
        <tag>FELK</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask学习(flask应用)</title>
    <url>/2017/08/12/flask%E5%AD%A6%E4%B9%A0%E4%BA%8C(flask%E5%BA%94%E7%94%A8)/</url>
    <content><![CDATA[<p>距离<a href="https://izsk.me/2017/08/12/falsk%E5%AD%A6%E4%B9%A0(-)">上一篇</a>有一段时间了,上次扯了点跟flask相关的内容,这次抽空记录下flask的机制.flask之所于上手容易,是因为我们对python语言稍微了解的话,简单几步就可以把一个小应用跑起来,不需要搭建额外的东西,所以对于并发量没有要求的话flask可以轻松应对.</p>
<span id="more"></span>

<h3 id="flask单应用"><a href="#flask单应用" class="headerlink" title="flask单应用"></a><strong>flask单应用</strong></h3><p>我们在网上看到的flask教程基本都会有这样一句代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">app = Flask(__name__)</span><br></pre></td></tr></table></figure>

<p>每个 Flask 应用必须创建一个Flask类的实例,并且把模块的名称传递给该实例,<strong>flask支持多应用共存,甚至是可以跟django实例共存</strong>这个后续再说,这里只是指定了一个flask实例,并不能运行起来.</p>
<p>Flask()更多的参数请见<a href="http://flask.pocoo.org/docs/0.12/api/#application-object">这里</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">19090</span>, debug=<span class="literal">True</span>, passthrough_errors=<span class="literal">True</span>, threaded=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>有了上面这句话之后,flask实例才真正的运行起来,有几个重要的参数这里要说下:</p>
<blockquote>
<ul>
<li>host&#x3D;0.0.0.0:表示其它主机都能访问</li>
<li>debug&#x3D;True:开启调度模式,一般用于测试环境</li>
<li>passthrough_errors&#x3D;True: 禁用错误捕获</li>
<li>threaded&#x3D;True: 则是开启多线程模式,让应用能够同时处理多个请求</li>
</ul>
</blockquote>
<p>更多的参数请见<a href="http://werkzeug.pocoo.org/docs/0.11/serving/#werkzeug.serving.run_simple">这里</a></p>
<p>最基本的例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask,render_template</span><br><span class="line">app = Flask(__name__)  <span class="comment">#Flask的一个对象,是一个app,flask支持多app共存</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span>,methods=[<span class="string">&quot;GET&quot;</span>]</span>) </span><span class="comment">#@是装饰器的语法糖,route则是上面说的路由转发模块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&quot;index.html&quot;</span>,pass_to_template=<span class="string">&quot;index&quot;</span>) <span class="comment">#把pass_to_template变量传递给index.html模板</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">19090</span>, passthrough_errors=<span class="literal">True</span>, threaded=<span class="literal">True</span>) <span class="comment">#使用flask自带的wsgi-server,监听本机的19090端口,threaded=True则开启多线程模式,自带的wsgi-server性能不好,生产环境不会这么用,一般都使用并发量更大的wsgi架构如uwsgi,tornado等</span></span><br></pre></td></tr></table></figure>

<p>现在,打开浏览器输入<code>http://ip:19090/index.html</code>即可访问index页面</p>
<p>面对多个请求时 ,flask是如何做到请求隔离的呢,这里要提一个flask的两个上下文机制</p>
<h3 id="两个上下文"><a href="#两个上下文" class="headerlink" title="两个上下文"></a><strong>两个上下文</strong></h3><p>这里不深入源代码一探究竟,一句话:<strong>临时全局变量</strong>,flask很机智的实现了thread.local类似<code>线程隔离</code>功能,并配置<code>栈</code>这种数据结构让其很轻松的可以将对象推入、弹出、快速获取栈顶对象,当然这些操作也是线程隔离的.</p>
<p>也就是说,<strong>在一次请求的一个线程中可以将其设置为全局变量,但是仅限于请求的这个线程内部,不同线程通过“线程标识符”来区别,这样就不会影响到其他线程的请求.</strong></p>
<h4 id="请求上下文"><a href="#请求上下文" class="headerlink" title="请求上下文"></a><strong>请求上下文</strong></h4><p>实现线程隔离后,为了在一个线程中更加方便使用这些变量,flask中还有一种堆栈的数据结构(通过werkzeug的LocalStack实现),可以处理这些变量,但是并不直接处理这些变量.假如有一个程序得到一个请求,那么flask会将这个请求的所有相关信息进行打包,打包形成的东西就是处理请求的一个环境.flask将这种环境称为<code>请求上下文</code>(request context),之后flask会将这个请求上下文对象放到堆栈中.</p>
<p>这样,请求发生时,我们一般都会指向堆栈中的“请求上下文”对象,这样可以通过请求上下文获取相关对象并直接访问,例如<code>request、session、current_app,g</code>(后两者为应用上下文).还可以通过调用对象的方法或者属性获取其他信息,例如request.method,等请求结束后,请求上下文会被销毁,堆栈重新等待新的请求上下文对象被放入.</p>
<h4 id="应用上下文"><a href="#应用上下文" class="headerlink" title="应用上下文"></a><strong>应用上下文</strong></h4><p><code>应用上下文的概念是在flask 0.9中增加的</code></p>
<p>当在一个应用的请求上下文环境中,需要嵌套处理另一个应用的相关操作时(这种情况更多的是用于测试或者在console中对多个应用进行相关处理),“请求上下文”显然就不能很好地解决问题了,因为魔法current_app无法确定当前处理的到底是哪个应用.如何让请求找到“正确”的应用呢？我们可能会想到,可以再增加一个请求上下文环境,并将其推入栈中.由于两个上下文环境的运行是独立的,不会相互干扰,所以通过调用栈顶对象的app属性或者调用current_app(current_app一直指向栈顶的对象)也可以获得当前上下文环境正在处理哪个应用.这种办法在一定程度上可行,但是如果说对第二个应用的处理不涉及到相关请求,那也就无从谈起“请求上下文”,更不可能建立请求上下文环境了.为了应对这个问题,Flask中将应用相关的信息单独拿出来,形成一个“应用上下文”对象.这个对象可以和“请求上下文”一起使用,也可以单独拿出来使用.不过有一点需要注意的是：在创建“请求上下文”时一定要创建一个“应用上下文”对象.有了“应用上下文”对象,便可以很容易地确定当前处理哪个应用,这就是魔法<code>current_app</code>.在0.1版本中,current_app是对_request_ctx_stack.top.app的引用,而在0.9版本中current_app是对_app_ctx_stack.top.app的引用.其中_request_ctx_stack和_app_ctx_stack分别是存储请求上下文和应用上下文的栈</p>
<p><strong>请求上下文: request、session</strong></p>
<p><strong>应用上下文: g、current_app</strong></p>
<h3 id="实现高并发"><a href="#实现高并发" class="headerlink" title="实现高并发"></a><strong>实现高并发</strong></h3><p>上面说了flask自带的wsgi的性能不好,一般都会选择其它能够实现高并发的http server,如gunicorn,greenlet,uswgi.</p>
<p>其中<a href="http://gunicorn.org/">gunicorn</a>几乎不需要什么配置就可直接整合到flask项目中,但是gunicorn只支持unix,而uswgi的配置比较复杂,但是性能比gunicorn稍好.</p>
<p>上面启动flask是直接在主函数中使用app.run(),这里我们结合gunicorn启动app(假设已pip install gunicorn)</p>
<p>gunicorn基于‘pre-fork worker’模型,意味着有一个中心主控master进程,用它来管理一组worker进程.</p>
<p>worker进程可以支持不同的IO方式（sync,gevent,eventlet,tornado等）</p>
<p>命令非常简洁:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gunicorn -D -w <span class="number">4</span> -b <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">4000</span> myproject:app</span><br></pre></td></tr></table></figure>

<p>其中:</p>
<blockquote>
<ul>
<li>-D: 以deamon形式后台运行</li>
<li>-w: 指定work进程数为4</li>
<li>-b: 指定ip 和端口</li>
<li>myproject:app 程序入口,前为工程入口文件,后为flask启动的应用</li>
</ul>
</blockquote>
<p>这样flask就以gunicorn的方式启动了,并发数也大大提高.</p>
<p>gunicorn处理静态资源的能力不是很好,所以一般都会在前面再部署个nginx,只需要将proxy_pass 指向gunicorn的监听端口即可,可以参考<a href="http://www.cnblogs.com/Ray-liang/p/4837850.html">这里</a></p>
<p>更多的flask实现高并发部署方式请见<a href="http://flask.pocoo.org/docs/0.12/deploying/#deployment">这里</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.zhihu.com/question/33970027">flask框架中应用上下文跟请求上下文是什么意思</a></li>
<li><a href="http://flask.pocoo.org/docs/0.12/">flask-docs</a></li>
<li><a href="http://www.cnblogs.com/Ray-liang/p/4837850.html">Flask + Gunicorn + Nginx 部署</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask学习(flask介绍)</title>
    <url>/2017/08/04/flask%E5%AD%A6%E4%B9%A0%E4%B8%80(flask%E4%BB%8B%E7%BB%8D)/</url>
    <content><![CDATA[<p>Flask是由Python语言编写开发而成的轻量级的Web开发框架,它的作者Armin Ronacher是个大神级人物,同样也是jinja,werkzeug的作者,所以不奇怪flask使用jiaja做为模板渲染,使用werkzeub做为wsgi,flask非常的小巧,也够简单,上手很快,在用了一段时间flask之后反过来学习flask原理,之前很多不理解的地方有了一个更清晰的认识,收益坡多,这里也不过多的去剖析源码,毕竟不是专业研发人士,先理解几个概念:</p>
<span id="more"></span>

<h3 id="Web服务"><a href="#Web服务" class="headerlink" title="Web服务"></a><strong>Web服务</strong></h3><p>前面说flask是一个轻量级的Web开发框架,首先我们需要大致了解一下web服务的原理,在这里引用该<a href="http://9124573.blog.51cto.com/9114573/1723134">博客</a>里的一张图片说明:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/flask1-httpserver.jpg" alt="falsk1-httpserver"></p>
<p>中间可能还涉及到其它的如负载均衡,高可用配置等,但不管整体是用什么框架,<strong>客户端请求&lt;&#x3D;&#x3D;&gt;[业务逻辑处理]&lt;&#x3D;&#x3D;&gt;服务器响应</strong>都逃不过这个流程(如果是静态资源的话可能没有业务逻辑处理就返回给了客户端),flask做为一个web也不例外,只不过flask使用了很简单的核心,其它功能则通过扩展插件来实现.</p>
<h3 id="WSGI"><a href="#WSGI" class="headerlink" title="WSGI"></a><strong>WSGI</strong></h3><p>WSGI (web server gateway interface),全称叫应用服务网关接口,是为 Python 语言定义的Web服务器和Web应用程序之间的一种简单而通用的接口.从名字就可以看出来,这东西是一个Gateway,也就是网关.网关的作用就是在协议之间进行转换.</p>
<p>WSGI将web服务我们可以分为2层:<strong>服务器+应用程序</strong>,服务器只负责处理一些跟底层相关的接口,屏蔽底层的具体实现,如接收http请求,返回http请求,而应用程序则负责业务逻辑的处理,WSGI工作流程如下图所示:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/flask1-wsgi1.jpg" alt="flask1-wsgi1"></p>
<h3 id="Jinja"><a href="#Jinja" class="headerlink" title="Jinja"></a><strong>Jinja</strong></h3><p>flask使用jinja做为模板渲染,现发展到了jinja2,所谓的模板渲染,一般情况下,虽然请求不同,但是响应中的数据的展示方式是相同的,通俗点说就是除了我们请求获得的数据不一样外,其他都是一样的,那么我们就可以设计一个模板(除了数据内容可以改动,其他都是固定的HTML文件).这样我们就不必为每一个请求都写一个页面,我们只需要把一类相同的请求转发到一个模板文件即可,这个模板可以是单独的一个模板,也可以通过模板继承而来.jiaja有自己的语法,同样支持像python一样的宏、循环、判断等语法结构.这个有机会后续再说,jinja2有流程如下:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/flask1-wsgi2.jpg" alt="flask1-wsgi2"></p>
<h3 id="路由转发"><a href="#路由转发" class="headerlink" title="路由转发"></a><strong>路由转发</strong></h3><p>路由转发其它就是当一个请求到达服务器之后,需要如何确认该请求要有哪个业务模块处理的过程呢,查找url跟视图的对应关系的程序就叫路由,flask使用werkzeug做路由转发,说白了就是flask内部会维护一个url跟视图函数的对应关系(这个对应关系其实分为2层),http请求传递过来的url,通过这个对应关系找视图函数,路由转发使用大量的装饰器,非常的简洁,具体的细节,大家可以参考这篇<a href="http://blog.csdn.net/bestallen/article/details/54342120">Flask源码解读 &lt;1&gt; — 浅谈Flask基本工作流程</a>,写得挺不错。</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a><strong>Example</strong></h3><p>在Flask中,我们处理一个请求的流程就是,首先根据用户提交的URL来决定由哪个业务逻辑函数来处理,然后在函数中进行操作,取得所需的数据.再将取得的数据传给相应的模板文件中,由Jinja2负责渲染得到HTTP响应内容,即HTTP响应的HTML文件,然后由Flask返回响应内容.</p>
<p>flask如此的简单,就算我们不知道上述的这些概念,看几个官方的例子也能了解一二</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask,render_template</span><br><span class="line">app = Flask(__name__)  <span class="comment">#Flask的一个对象,是一个app,flask支持多app共存</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span>,methods=[<span class="string">&quot;GET&quot;</span>]</span>) </span><span class="comment">#@是装饰器的语法糖,route则是上面说的路由转发模块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&quot;index.html&quot;</span>,pass_to_template=<span class="string">&quot;index&quot;</span>) <span class="comment">#把pass_to_template变量传递给index.html模板</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">19090</span>, passthrough_errors=<span class="literal">True</span>, threaded=<span class="literal">True</span>) <span class="comment">#使用flask自带的wsgi-server,监听本机的19090端口,threaded=True则开启多线程模式,自带的wsgi-server性能不好,生产环境不会这么用,一般都使用并发量更大的wsgi架构如uwsgi,tornado等</span></span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://9124573.blog.51cto.com/9114573/1723134">http协议及web服务原理</a></li>
<li><a href="http://krzer.com/2016/11/22/flask-introduction/">简谈Flask及实现原理</a></li>
<li><a href="http://blog.csdn.net/bestallen/article/details/54342120">Flask源码解读 &lt;1&gt; — 浅谈Flask基本工作流程</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>cilium在kubernetes中的生产实践六(cilium排错指南)之api-rate-limit</title>
    <url>/2024/08/10/cilium-on-kubernetes-errors-apiratelimit/</url>
    <content><![CDATA[<p>在前东家的时候其实就有意将cilium强大的链路追踪能力集成到生产环境中,各种因素导致没有很大信心落地, 经过深入调研(也就把官网docs翻了四五遍)及测试, 终于有机会在生产kubernetes集群中(目前一个集群规模不算很大,2w+核心,持续增长)使用cilium做为cni,同时替换kube-proxy, 到现在已经有一段时间了，也算是有生产经验可以跟大家聊一聊这个工具，使用体验总结一句话: 轻松愉悦.<br>分享一下整个落地过程,同时也总结下方方面面, 工作之余尽量更新.<br>此篇归属于: cilium在kubernetes中的生产实践六(cilium排错指南)</p>
<span id="more"></span>

<p>总体分为以下几块内容:<br><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">cilium在kubernetes中的生产实践一(cilium介绍)</a><br><a href="https://izsk.me/2023/06/03/cilium-on-kubernetes-install/">cilium在kubernetes中的生产实践二(cilium部署)</a><br><a href="https://izsk.me/2023/09/26/cilium-on-kubernetes-network-models-config/">cilium在kubernetes中的生产实践三(cilium网络模型之关键配置)</a><br>cilium在kubernetes中的生产实践四(cilium网络模型之生产实践)<br>cilium在kubernetes中的生产实践五(cilium网络策略)<br>cilium在kubernetes中的生产实践六(cilium排错指南)<br>cilium在kubernetes中的生产实践七(cilium中的bpf hook)</p>
<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>业务高峰期创建的pod数可能上万+, 某天就出现了在大量pod创建失败的情况, 这些pod处于<code>ContainerCreating</code>状态, 第一感觉以为是节点上运行的pod数超过指定的max-pods(默认pod数最大110)导致的失败,简单排查后发现不是, 找到这类pod所在的节点, 发现kubelet有类似如下日志:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to <span class="built_in">set</span> up sandbox container <span class="string">&quot;a89f1f10c0655325f909ba937ce1b5d8db70fbc0a9fa67daed62dca3de5a8f73&quot;</span> network <span class="keyword">for</span> pod <span class="string">&quot;es-white-list-cronjob-1616127480-nbczk&quot;</span>: networkPlugin cni failed to <span class="built_in">set</span> up pod <span class="string">&quot;es-white-list-cronjob-1616127480-nbczk_caibin1&quot;</span> network: Unable to create endpoint: response status code does not match any response statuses defined <span class="keyword">for</span> this endpoint <span class="keyword">in</span> the swagger spec (status 429): &#123;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h3><p>从上述日志看有几个关键字: <code>networkPlugin cni</code>, ‘create endpoint (status 429)’.<br>很显然, kubelet反应的这个报错是cni返回的, 而集群中的cni为<code>cilium</code><br>所以查看节点上的cilium agent ds, 发现有如下报错:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">level=warning msg=<span class="string">&quot;Not processing API request. Wait duration for maximum parallel requests exceeds maximum&quot;</span> error=<span class="string">&quot;context deadline exceeded&quot;</span> maxWaitDuration=15s name=endpoint-create parallelRequests=3 subsys=rate uuid=efabe2d6-886a-11eb-832b-3cecef001f42</span><br><span class="line">level=info msg=<span class="string">&quot;API call has been processed&quot;</span> error=<span class="string">&quot;timed out while waiting to be served with 3 parallel requests: context deadline exceeded&quot;</span> name=endpoint-create processingDuration=0s subsys=rate totalDuration=15.000634348s uuid=efabe2d6-886a-11eb-832b-3cecef001f42 waitDurationTotal=0s</span><br><span class="line">level=info msg=<span class="string">&quot;API call has been processed&quot;</span> error=<span class="string">&quot;request would have to wait 18.736302926s to be served (maximum wait duration: 622.727216ms)&quot;</span> name=endpoint-create processingDuration=0s subsys=rate totalDuration=16.396271422s uuid=ee181d17-886a-11eb-832b-3cecef001f42 waitDurationTotal=14.377272784s</span><br><span class="line">level=info msg=<span class="string">&quot;API call has been processed&quot;</span> error=<span class="string">&quot;request would have to wait 16.717671024s to be served (maximum wait duration: 18.52909ms)&quot;</span> name=endpoint-create processingDuration=0s subsys=rate totalDuration=17.000479009s uuid=edbbea7f-886a-11eb-832b-3cecef001f42 waitDurationTotal=14.98147091s</span><br><span class="line">level=info msg=<span class="string">&quot;Processing API request with rate limiter&quot;</span> maxWaitDuration=15s name=endpoint-create parallelRequests=3 subsys=rate uuid=ee181d17-886a-11eb-832b-3cecef001f42</span><br><span class="line">level=info msg=<span class="string">&quot;regenerating all endpoints&quot;</span> reason=<span class="string">&quot;one or more identities created or deleted&quot;</span> subsys=endpoint-manager</span><br></pre></td></tr></table></figure>

<p>因此可以确认是cilium agent的问题, <code>cilium的功能是以subsys区分的</code>, 上述日志中提及<code>subsys=rate</code>,<br>同时kubelet中还存在<code>endpoint (status 429)</code>的报错.<br><code>status 429</code>的一般用于表示<code>请求太多</code>,这与<code>subsys=rate</code>刚好对应上了<br>所以，问题出在cilium的rate配置, 作者对cilium中的rate部分并没有特别的进行配置, 所以直接去<a href="https://docs.cilium.io/en/v1.12/configuration/api-rate-limiting/">官方文档</a>查看:</p>
<p>默认的rate配置如下, 这里简单说明:</p>
<table>
<thead>
<tr>
<th>API Call</th>
<th>Limit</th>
<th>Burst</th>
<th>Max Parallel</th>
<th>Min Parallel</th>
<th>Max Wait Duration</th>
<th>Auto Adjust</th>
<th>Estimated Processing Duration</th>
</tr>
</thead>
<tbody><tr>
<td><code>PUT /endpoint/&#123;id&#125;</code></td>
<td>0.5&#x2F;s</td>
<td>4</td>
<td>4</td>
<td></td>
<td>15s</td>
<td>True</td>
<td>2s</td>
</tr>
<tr>
<td><code>DELETE /endpoint/&#123;id&#125;</code></td>
<td></td>
<td></td>
<td>4</td>
<td>4</td>
<td></td>
<td>True</td>
<td>200ms</td>
</tr>
<tr>
<td><code>GET /endpoint/&#123;id&#125;/*</code></td>
<td>4&#x2F;s</td>
<td>4</td>
<td>4</td>
<td>2</td>
<td>10s</td>
<td>True</td>
<td>200ms</td>
</tr>
<tr>
<td><code>PATCH /endpoint/&#123;id&#125;*</code></td>
<td>0.5&#x2F;s</td>
<td>4</td>
<td>4</td>
<td></td>
<td>15s</td>
<td>True</td>
<td>1s</td>
</tr>
<tr>
<td><code>GET /endpoint</code></td>
<td>1&#x2F;s</td>
<td>4</td>
<td>2</td>
<td>2</td>
<td></td>
<td>True</td>
<td>300ms</td>
</tr>
</tbody></table>
<p>还有一张CURD的对应关系表:</p>
<table>
<thead>
<tr>
<th>API Call</th>
<th>Config Name</th>
</tr>
</thead>
<tbody><tr>
<td><code>PUT /endpoint/&#123;id&#125;</code></td>
<td><code>endpoint-create</code></td>
</tr>
<tr>
<td><code>DELETE /endpoint/&#123;id&#125;</code></td>
<td><code>endpoint-delete</code></td>
</tr>
<tr>
<td><code>GET /endpoint/&#123;id&#125;/*</code></td>
<td><code>endpoint-get</code></td>
</tr>
<tr>
<td><code>PATCH /endpoint/&#123;id&#125;*</code></td>
<td><code>endponit-patch</code></td>
</tr>
<tr>
<td><code>GET /endpoint</code></td>
<td><code>endpoint-list</code></td>
</tr>
</tbody></table>
<p>上述的报错可以跟默认配置对应上,包括<code>Max Wait Duration</code><br>然后报错的是<code>create endpoint</code>出现的429, <code>create endpoint</code>属于是<code>PUT endpoint</code>一类，默认配置表中只有0.5&#x2F;s且并发数只有4, 相对于作者的场景还是小了一些, 所以需要进行调整</p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>调整cilium agent中的configamp, 添加如下的配置项</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">api-rate-limit: <span class="string">&quot;endpoint-create=rate-limit:8/s,rate-burst:16,parallel-requests:8&quot;</span></span><br><span class="line"><span class="comment"># rate-limit:8/s 表示每秒允许8个请求</span></span><br><span class="line"><span class="comment"># rate-burst:16 表示可以突发的请求数</span></span><br><span class="line"><span class="comment"># parallel-requests 并发请求数</span></span><br></pre></td></tr></table></figure>

<p>然后再重启cilium agent ds后，问题解决</p>
<h3 id="问题延伸"><a href="#问题延伸" class="headerlink" title="问题延伸"></a>问题延伸</h3><p>有一个问题: 为什么是<code>create endpoint</code>出现了429而不是其它的比如<code>create pod or create ip</code>之类的出现问题?<br>首先, 一个node上的cni可分配的IP段为一个C段网络，即255个ip地址，node上默认最大的pod数量为110, 当node上的pod数量要超过110时，那么新的pod在kube-scheduler中进行调度时会在pre-schedule阶段将这个node排除掉, 所以可以排除是创建pod或者是获取ip问题.<br>每个节点的Cilium代理本质上是事件驱动的。当新的工作负载被调度到节点上时，CNI插件被调用，该节点继而对Cilium代理进行API调用以分配IP地址并创建Cilium端点.</p>
<h3 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h3><p>cilium本身是基于事件驱动的，Cilium agent 执行的工作量在很大程度上取决于它接收外部事件的速率。为了限制Cilium代理消耗的资源, cilium限制了API调用的速率和允许的并行执行数.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.cilium.io/en/v1.12/configuration/api-rate-limiting/">https://docs.cilium.io/en/v1.12/configuration/api-rate-limiting/</a></li>
<li><a href="https://github.com/cilium/cilium/issues/15394">https://github.com/cilium/cilium/issues/15394</a></li>
<li><a href="https://cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/networking?hl=zh-cn#dataplane_v2_cilium">https://cloud.google.com/kubernetes-engine/distributed-cloud/bare-metal/docs/troubleshooting/networking?hl=zh-cn#dataplane_v2_cilium</a></li>
<li><a href="https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/">https://izsk.me/2023/04/01/cilium-on-kubernetes-introduction/</a></li>
<li><a href="https://izsk.me/2023/06/03/cilium-on-kubernetes-install/">https://izsk.me/2023/06/03/cilium-on-kubernetes-install/</a></li>
<li><a href="https://izsk.me/2023/09/26/cilium-on-kubernetes-network-models-config/">https://izsk.me/2023/09/26/cilium-on-kubernetes-network-models-config/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>typora+github+PicGo在MarkDown里实现图床功能</title>
    <url>/2020/01/10/github+PicGo%E5%AE%9E%E7%8E%B0%E5%BC%80%E6%BA%90%E5%9B%BE%E5%BA%8A%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>相信大家写MakeDown文档时一定遇到过上图片显示问题，MakeDown文档里的图片都是本地保存的，这说需要文档里的图片跟着文档一起走，造成了md不能像word那样本身能保存图片数据.</p>
<span id="more"></span>

<p>之前一起使用的是阿里云的oss工具，免费的功能需要有权限但是勉强能用，只不过上传完图片之后复制地址的时候有点麻烦，不能一气呵成，想着github其实不止有保存源码的功能，也能保存图片，借助该的图床神器PicGo,还是会比oss好用一些.</p>
<h3 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a><strong>Typora</strong></h3><p>一个md的工具,个人感觉还是挺简单的.</p>
<h3 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a><strong>GitHub</strong></h3><h4 id="new-repo"><a href="#new-repo" class="headerlink" title="new repo"></a><strong>new repo</strong></h4><p>新建一个repo，用来保存图片, 比如叫zhoushuke&#x2F;BlogPhoto, 下面会用到，这里就不多写了，大家都懂</p>
<h4 id="token"><a href="#token" class="headerlink" title="token"></a><strong>token</strong></h4><p>github头像-&gt;Settings-&gt;Developer settings-&gt;Personnal access tokens-&gt;Generate new token</p>
<p>创建一个personal的token，创建的时候可以设置权限等,这个token将用于PicGo访问github的密钥</p>
<h3 id="PicGO"><a href="#PicGO" class="headerlink" title="PicGO"></a><strong>PicGO</strong></h3><p>PicGo的github在<a href="https://github.com/Molunerfinn/PicGo">这里</a></p>
<p>下载下来之后，可以看到picgo现在已经支持了很多主流的厂商, 这里配置github图床.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200117215949.png"></p>
<p>用星标注的都是必填的，不用多说， token就是上面步骤新建的token,当然，如果repo要用子目录的话，要在这里指定，其它的默认即可.</p>
<p>picgo支持剪切区上传，点击这里就能实现复制区上传，上传完后再点击这里就是复制url了，非常方便</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200117220333.png"></p>
<p>其它详细的用法大家也可以看picgo的github</p>
<p>这样就实现使用github做为图片的存储地了,写md不用再分两步操作了, 挺好.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.itcodemonkey.com/article/14536.html">一款markdown编辑器typora和图床的搭建过程</a></li>
<li><a href="https://github.com/Molunerfinn/PicGo">PicGo</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>捣鼓手册</category>
      </categories>
      <tags>
        <tag>捣鼓手册</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitlab学习(GitOps的最简实践)</title>
    <url>/2020/07/05/gitlab-GitOps-pratice/</url>
    <content><![CDATA[<p><code>任何能够被描述的内容都必须存储在Git库中</code> 这个GitOps的理念, 今天来践行一下，不借助其它devops平台, 看看是不是能够实现工具闭环</p>
<span id="more"></span>

<p>Git是GitOps形成的最基础的内容，就像第一条原则“任何能够被描述的内容都必须存储在Git库中 ”描述的那样：通过使用Git作为存储声明性基础架构和应用程序代码的存储仓库，可以方便地监控集群，以及检查比较实际环境的状态与代码库上的状态是否一致。所以，<strong>我们的目标是描述系统相关的所有内容：策略，代码，配置，甚至监控事件和版本控制等</strong>，并且将这些内容全部存储在版本库中，在通过版本库中的内容构建系统的基础架构或者应用程序的时候，如果没有成功，则可以迅速的回滚，并且重新来过</p>
<p>当然如果团队内有自己的devops平台的话,那其实就没必要接着看了，这篇post主要给大家一种思路，如何借助已有的工具来实现端到端的CI&#x2F;CD, 只适合于团队较小，没有多余的资源来开发自己的平台的场景.</p>
<h3 id="开源工具"><a href="#开源工具" class="headerlink" title="开源工具"></a>开源工具</h3><p>以下3个开源工具, 不在这里介绍, 大家可参考相应的连接</p>
<h4 id="kustomize"><a href="#kustomize" class="headerlink" title="kustomize"></a>kustomize</h4><p>kustomize是kubernetes官方推荐的声明式配置形式</p>
<p><a href="https://izsk.me/2020/07/01/Kubernetes-kustomize/">https://izsk.me/2020/07/01/Kubernetes-kustomize/</a></p>
<h4 id="sealed-secrets"><a href="#sealed-secrets" class="headerlink" title="sealed-secrets"></a>sealed-secrets</h4><p>sealed-secrets主要提供可以在git上以密文的形式存储敏感数据, 当发布到k8s集群时用私钥进行解密,解密之后就变成了正常的secret文件.</p>
<p><a href="https://izsk.me/2020/06/24/Kubernetes-sealed-secrets/">https://izsk.me/2020/06/24/Kubernetes-sealed-secrets/</a></p>
<h4 id="configmapsecret"><a href="#configmapsecret" class="headerlink" title="configmapsecret"></a>configmapsecret</h4><p>kubernetes官方是不支持在configmap中直接引用secret中的内容, 因此configmapsecret主要用于在配置中引用secret或者是configmap中的内容</p>
<p><a href="https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/">https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/</a></p>
<h3 id="Demo实践"><a href="#Demo实践" class="headerlink" title="Demo实践"></a>Demo实践</h3><p>这个demo用于实践以下功能:</p>
<ol>
<li>使用配置端到端的、使用kubernetes推荐的声明式配置管理的方式(kustomize)</li>
<li>敏感的密码之类的数据在git源文件中进行脱敏处理(sealed-secrets)</li>
<li>结合gitlab的ci&#x2F;cd方式完成多环境部署</li>
</ol>
<p>到这一步默认在集群中已安装了以上的开源库.</p>
<h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">app-kustomize</span><br><span class="line">    <span class="comment"># ... 这里省略其它的代码文件</span></span><br><span class="line">    deploy</span><br><span class="line">        base</span><br><span class="line">            config.yaml</span><br><span class="line">            deployment.yaml</span><br><span class="line">            service.yaml</span><br><span class="line">            kustomization.yaml </span><br><span class="line">        overlay</span><br><span class="line">            1box</span><br><span class="line">              patch-deployment.yaml</span><br><span class="line">                kustomization.yaml</span><br><span class="line">            prod</span><br><span class="line">              patch-deployment.yaml</span><br><span class="line">                kustomization.yaml</span><br></pre></td></tr></table></figure>

<p>以上文件的具体内容大家可参考这篇<a href="">post</a></p>
<h4 id="数据加密"><a href="#数据加密" class="headerlink" title="数据加密"></a>数据加密</h4><p>业务中敏感的数据其实不会很多，大多都是k-v的形式，比如mysql的密码，token等，既然要在git上存储这些数据，那自然是要进行脱敏才放心, 以最小的知情范围存在</p>
<p>可以把敏感的数据都放在一个配置文件中，类似下面的形式，怎么简单怎么来</p>
<p><code>cat senserealty-secret-data.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mongodb_password:</span> <span class="string">mysql1234</span></span><br><span class="line"><span class="attr">redis_password:</span> <span class="string">abcdefg</span></span><br><span class="line"><span class="attr">redis_port:</span> <span class="number">6379</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>然后通过<code>sealed-secrets</code>进行加密，发布到集群中, 如何操作, 请参考上面的链接.</p>
<p>发布到集群后会发现已经生成了一个secret文件, senserealty-secret-data用于存储敏感信息</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200701110947.png"></p>
<p>那在配置文件中就可以通过<code>configmapsecret</code>直接引用这些数据了</p>
<h3 id="Kustomize"><a href="#Kustomize" class="headerlink" title="Kustomize"></a>Kustomize</h3><p>通过使用kustomize来组织所有需要发布到集群中的文件, 是非常方便的, 特别是应对多套环境的时候,通过派生的形式来生成配置，大大减少维护成本. </p>
<p>如何使用请参考上面的链接</p>
<h3 id="CI-x2F-CD"><a href="#CI-x2F-CD" class="headerlink" title="CI&#x2F;CD"></a>CI&#x2F;CD</h3><p>使用gitlab自带的CI&#x2F;CD工具同样可以实现很不错的效果, 只需要为其搭建一个k8s集群做为Runner即可, 很大一部分都会围绕gitlab-ci.yaml文件展开, gitlab实现了自己的语法, 拓展性非常强, <a href="https://izsk.me/2020/05/15/gitlab-kubernetes-multi-environment-deploy/">参考</a></p>
<p>这里给一个参考</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">ubuntu:latest</span> <span class="comment"># Pipeline中各个步骤阶段的构建镜像没有指定时，默认使用这里指定的镜像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局配置</span></span><br><span class="line"><span class="attr">workflow:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">if:</span> <span class="string">$CI_MERGE_REQUEST_ID</span></span><br><span class="line">      <span class="attr">when:</span> <span class="string">never</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">when:</span> <span class="string">always</span></span><br><span class="line"></span><br><span class="line"><span class="attr">stages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">makebinary</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">buildpush</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">updateconfig1box</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">deploy1box</span></span><br><span class="line"></span><br><span class="line"><span class="attr">variables:</span></span><br><span class="line"><span class="comment"># 省略 ...</span></span><br><span class="line"></span><br><span class="line"><span class="string">.k8supdateconfig:</span> <span class="meta">&amp;k8supdateconfig</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;[ ! -f $&#123;CONFIGPATH&#125; ] &amp;&amp; (echo &#x27;config file NOT FOUND&#x27; &amp;&amp; exit 1)&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">config.yaml</span> <span class="string">-n</span> <span class="string">$&#123;KUBERNETES_NS&#125;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;kubectl patch deployment $&#123;APP&#125; -n $&#123;KUBERNETES_NS&#125; -p &#x27;&#123;\&quot;spec\&quot;: &#123;\&quot;template\&quot;: &#123;\&quot;metadata\&quot;: &#123;\&quot;annotations\&quot;: &#123;\&quot;com.xxx.configmap/hash\&quot;: \&quot;&#x27;$(date +%s)&#x27;\&quot;&#125;&#125;&#125;&#125;&#125;&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.k8srolloutupgrade:</span> <span class="meta">&amp;k8srolloutupgrade</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kustomize</span> <span class="string">edit</span> <span class="string">set</span> <span class="string">image</span> <span class="string">a/b:c=$&#123;REGISTRY_ADDR&#125;/$&#123;PUSH_REPO&#125;/$&#123;BINARY_NAME&#125;:$&#123;DEPLOY_VERSION&#125;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kustomize</span> <span class="string">build</span> <span class="string">.</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span> <span class="string">--record</span></span><br><span class="line"><span class="comment"># 省略 ...</span></span><br></pre></td></tr></table></figure>

<p><strong>对于只修改配置文件的场景, 由于业务上不想升级一个版本, 因此通过给pod打一个<code>annotation</code>来实现pod的重启.</strong></p>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>进入到容器的<code>/etc/config/config.yml</code>目录会看到生成了两个配置文件，且敏感信息已经被解密成明文.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200703104637.png"></p>
<p>其实还有一种情况: 业务中可能很多的应用都会连接mysql, 那么, 当需要修改mysql的账号时, 还需要一个一个地去只需要这些应用的参数然后重启, 一次还好, 如何碰到安全部门定时修改的要求时更苦逼, 这其实是可以优化的</p>
<p>通过上面的方式, 其它应用都引用的<code>senserealty-secret-data</code>这个secret里的内容, 那只需要修改这一个文件，重新发布一下即可</p>
<p>应用通过watch的方式来监听该文件的变动，一有变动自动重启即可, 这个其实也有开源工具: <a href="https://izsk.me/2020/05/10/Kubernetes-deploy-hot-reload-when-configmap-update/">reloader</a>，有兴趣的可以参考一下</p>
<p>这种方式本人已经在生产环境中得到了验证，非常实用.</p>
<p>当然上面的例子中只是使用了几种最常用的配置, 其实完全可以将其它的资源对象以声明式的形式保存在git中</p>
<p>就像GitOps推荐的一样:<strong>任何能够被描述的内容都必须存储在Git库中</strong></p>
<p>目前有很多的开源库可以直接实现以上功能, 比如Flux，能够同步git的变更直接发布到集群中. 功能上更强大. 感兴趣的可以参考</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li><code>kubectl apply -k </code> 出现<code>error: rawResources failed to read Resources</code></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">error: rawResources failed to <span class="built_in">read</span> Resources: Load from path ../../base/ failed: <span class="string">&#x27;../../base/&#x27;</span> must be a file (got d=<span class="string">&#x27;/Users/zhoushuke/git_uni/gitlab.bj.sensetime.com/gitlabci-golang-demo/deploy/base&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>出现这种情况是因为<code>kustomize</code>与<code>kubectl apply -k</code> 版本不匹配， 可以使用<code>kustomiz build . | kubectl apply -f -</code></p>
<p>另外一个情况下, <code>bases</code>已经在kustomize v2.1.0+ 版本中给去掉了,高版本的会直接将<code>bases</code>转换成<code>resources</code>，这个可以从渲染后的kustomization.yaml中看出</p>
<ol start="2">
<li><p>patch时到底是覆盖还是新增</p>
<p><code>cat base/deployment</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ... 省略</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">p-expoter</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/etc/config/config.yml&quot;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">config.yml</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">alertmanager.yaml</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">config.yml</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">p-expoter-senserealty-cms</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demo-config</span></span><br></pre></td></tr></table></figure>

<p><code>cat overlay/1box/patch-deploy.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ... 省略</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">p-expoter</span> </span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/etc/config/grafana.yml&quot;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">grafana.yml</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">grafana.yaml</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">grafana.yml</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">p-expoter-senserealty-cms</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demo-config</span></span><br></pre></td></tr></table></figure>

<p>最终build的结果为</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ... 省略</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">p-expoter</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/config/grafana.yml</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">grafana.yml</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/config/config.yml</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">config.yml</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo-config</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">grafana.yaml</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">grafana.yml</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">p-expoter-senserealty-cms</span></span><br></pre></td></tr></table></figure>

<p>可以看到 mountPath是合并了, 而volumes则是覆盖了. 这里可以使用在<code>kustomization.yaml</code>中使用<code>patch</code>或者<code>patchJson6902</code>, 参考<a href="https://kubernetes-sigs.github.io/kustomize/api-reference/kustomization/">这里</a>,如果无法确认是合并还是覆盖, 多多使用<code>kustomize build</code>命令</p>
<p>当然这里只是举个例子, 如果想实现叠加的话, 完全可以把base&#x2F;deployment.yaml下的volumes复制一份过来, 不用把结构搞的太复杂</p>
</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#generator">https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#generator</a></li>
<li><a href="https://blog.fleeto.us/post/crud-with-kustomize/">https://blog.fleeto.us/post/crud-with-kustomize/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/92153378">https://zhuanlan.zhihu.com/p/92153378</a></li>
<li><a href="https://izsk.me/2020/06/24/Kubernetes-sealed-secrets/">https://izsk.me/2020/06/24/Kubernetes-sealed-secrets/</a></li>
<li><a href="https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/">https://izsk.me/2020/06/28/Kubernetes-configmap-reference-var-from-secret/</a></li>
<li><a href="https://kubernetes-sigs.github.io/kustomize/api-reference/kustomization/">https://kubernetes-sigs.github.io/kustomize/api-reference/kustomization/</a></li>
<li><a href="https://izsk.me/2020/05/10/Kubernetes-deploy-hot-reload-when-configmap-update/">https://izsk.me/2020/05/10/Kubernetes-deploy-hot-reload-when-configmap-update/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitlab学习(CI中的变量)</title>
    <url>/2020/04/15/gitlab-ci-variables/</url>
    <content><![CDATA[<p>在使用gitlab做ci&#x2F;cd发布时，变量是无法避免的话题，这次来聊一聊gitlab中各类级别的变量</p>
<span id="more"></span>

<p>首先, 变量大致可以分为以下几大类,优先级从高到低:</p>
<blockquote>
<ul>
<li><a href="https://docs.gitlab.com/ee/ci/triggers/README.html#making-use-of-trigger-variables">Trigger variables</a> or <a href="https://docs.gitlab.com/ee/ci/pipelines/schedules.html#using-variables">scheduled pipeline variables</a>.</li>
<li>Project-level <a href="https://docs.gitlab.com/ee/ci/variables/README.html#custom-environment-variables">variables</a> or <a href="https://docs.gitlab.com/ee/ci/variables/README.html#protect-a-custom-variable">protected variables</a>.</li>
<li>Group-level <a href="https://docs.gitlab.com/ee/ci/variables/README.html#group-level-environment-variables">variables</a> or <a href="https://docs.gitlab.com/ee/ci/variables/README.html#protect-a-custom-variable">protected variables</a>.</li>
<li>YAML-defined <a href="https://docs.gitlab.com/ee/ci/yaml/README.html#variables">job-level variables</a>.</li>
<li>YAML-defined <a href="https://docs.gitlab.com/ee/ci/yaml/README.html#variables">global variables</a>.</li>
<li><a href="https://docs.gitlab.com/ee/ci/variables/README.html#deployment-environment-variables">Deployment variables</a>.</li>
<li><a href="https://docs.gitlab.com/ee/ci/variables/predefined_variables.html">Predefined environment variables</a>.</li>
</ul>
</blockquote>
<p>变量无非就是一种作用域跟优先级的问题，优先级高的会自动覆盖同名优先级的变量</p>
<p>我觉得最常用的有<code>Project-level、Group-level、global、job</code></p>
<h3 id="group-x2F-project"><a href="#group-x2F-project" class="headerlink" title="group&#x2F;project"></a>group&#x2F;project</h3><p>这类的变量直接作用于整个组, 在gitlab的这个组下的所有project，都可以使用, 一般源代码管理都是以某一业务组区分，因此group级别的变量多用于整个组内共享</p>
<p>project-level同理</p>
<p>设置方法</p>
<p><code>group --&gt; setting --&gt; ci/cd --&gt; variables</code></p>
<p><code>project --&gt; setting --&gt; ci/cd --&gt; variables</code></p>
<p>这里以project变量为例:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200110161226067.png"></p>
<h4 id="protected"><a href="#protected" class="headerlink" title="protected"></a>protected</h4><p>gitlab的原文如下:</p>
<p><code>Variables can be protected. Whenever a variable is protected, it would only be securely passed to pipelines running on the protected branches or protected tags. The other pipelines would not get any protected variables.</code></p>
<p>翻译一下: 如果把变量选中为<code>protected</code>的话，那么这个就是只会在<code>protected branches or protected tags</code>上安全的传递，其它的情况将不会获取改变量.</p>
<p>但是从我测试的情况来看, 提交的分支为master, 没有设置<code>protected tag</code>, 发现:</p>
<ol>
<li>如果某个变量被选中为<code>protected</code>, commit触发pipeline时都可以获取该变量</li>
<li>但是如果commit时带上tag, 则该变量时为空，当取消<code>protected</code>状态时, 执行正常</li>
</ol>
<p>乍一看是不是感觉跟原文说的有点出路, 其实出现这种情况是正常的, gitlab默认情况下, master分支是受保护的，</p>
<h5 id="protected-branches"><a href="#protected-branches" class="headerlink" title="protected branches"></a>protected branches</h5><p>可以通过项目 <code>Setting--&gt;Repository --&gt; CI/CD --&gt; Protected Branchers</code>中查看，如下图</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200514164539.png"></p>
<p>所以第1种情况能够正确获取<code>protected</code>的变量是正常的,关于<code>protected branch</code>官方说明在<a href="http://gitlab.bj.sensetime.com/help/user/project/protected_branches">protected branch</a></p>
<p>第2种情况无法获取的原因是因为带上了tag, 而<code>protected</code>的变量需要存在<code>protected tag</code>配置才能使用</p>
<p>默认情况下是不设置的, 因此无法获取</p>
<h5 id="protected-tags"><a href="#protected-tags" class="headerlink" title="protected tags"></a>protected tags</h5><p>设置<code>protected tag</code>也非常简单,官方说明<a href="http://gitlab.bj.sensetime.com/help/user/project/protected_tags.md">protected tag</a></p>
<p>项目 <code>Setting--&gt;Repository --&gt; CI/CD --&gt; Protected Tags</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200514165413.png"></p>
<p><code>protected branch</code>跟<code>protected tags</code>都是为了防止对源代码误操作的两种方式</p>
<h4 id="Masked"><a href="#Masked" class="headerlink" title="Masked"></a>Masked</h4><p>而变量的<code>masked</code>则是为了保护变量不回显</p>
<p><code>protected</code>相当于对权限做了一次验证，但是它无法完全的把变量保护起来, 如果在CI中开启<code>CI_DEBUG_TRACE: &#39;true&#39;</code>还是能够看到变量的值的，而<code>masked</code>则完全在log中不打印变量, 一般用于比较敏感的信息</p>
<h4 id="K8s-SECRET"><a href="#K8s-SECRET" class="headerlink" title="K8s_SECRET"></a>K8s_SECRET</h4><p>以<code>K8S_SECRET_</code>开头的变量都会转换成kubernetes的secret</p>
<p>​        如定义了一个变量叫: K8S_SECRET_RAILS_MASTER_KEY</p>
<p>​        最终会在k8s中的形态,在部署应用容器里会最终又被转换成环境变量,可直接通过$RAILS_MASTER_KEY:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">RAILS_MASTER_KEY:</span> <span class="string">MTIzNC10ZXN0</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="number">2018-12-20T01:48:26Z</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">production-secret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">minimal-ruby-app-54</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;429422&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/minimal-ruby-app-54/secrets/production-secret</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">57ac2bfd-03f9-11e9-b812-42010a9400e4</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure>

<p>当然，在运行前pipeline时也可在UI指定环境变量，这些变量只对应某个project. 且只有这次有效</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200110164034852.png"></p>
<h3 id="job"><a href="#job" class="headerlink" title="job"></a>job</h3><p>job级别的变量限定在只在某一具体的job内, 这也是用的最多的一种变量</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">justupdateconfig:</span><br><span class="line">  stage: <span class="built_in">test</span></span><br><span class="line">  dependencies: []</span><br><span class="line">  variables:</span><br><span class="line">    CI_DEBUG_TRACE: <span class="string">&#x27;true // job-level</span></span><br></pre></td></tr></table></figure>

<h3 id="global"><a href="#global" class="headerlink" title="global"></a>global</h3><p>这里说到的global指的是在整个pipeline期间，该类变量都是可以使用的</p>
<p><code>.gitlab-ci.yml</code>中定义全局变量</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">variables:</span><br><span class="line">  KUBERNETES_NS: <span class="string">&#x27;stage&#x27;</span></span><br><span class="line">  APP: <span class="string">&#x27;p-expoter&#x27;</span></span><br><span class="line">  PUSH_REPO: <span class="string">&#x27;test&#x27;</span></span><br><span class="line">  CONFIGPATH: <span class="string">&#x27;config/config.yml&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在pipeline执行期间都是可以直接引用的，<code>$CONFIGPATH</code></p>
<h3 id="predefined"><a href="#predefined" class="headerlink" title="predefined"></a>predefined</h3><p>同时, gitlab在执行时runner也会暴露出很多的自带的变量, 这些变量在做一些逻辑判断的时候非常有用, 比如判断是不是master分支, 当前路径等等</p>
<p>判断是否commit是否存在tag.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$CI_COMMIT_TAG</span>&quot;</span> = <span class="string">&quot;&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  COMMIT_TAG=<span class="string">&quot;<span class="variable">$CI_COMMIT_REF_NAME</span>-<span class="variable">$CI_COMMIT_SHORT_SHA</span>&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  COMMIT_TAG=<span class="string">&quot;<span class="variable">$CI_COMMIT_TAG</span>-<span class="variable">$CI_COMMIT_SHORT_SHA</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>变量列表在<a href="https://docs.gitlab.com/ee/ci/variables/predefined_variables.html">predefined_variables</a></p>
<p>gitlab还支持将变量直接注入到部署的业务容器中, 这个还没有用到过, 有需要的同学可以参考官文.</p>
<h3 id="常用变量"><a href="#常用变量" class="headerlink" title="常用变量"></a>常用变量</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CI_COMMIT_TAG  <span class="comment"># commit tag</span></span><br><span class="line">CI_DEBUG_TRACE <span class="comment"># 开启debug</span></span><br><span class="line">CI_MERGE_REQUEST_ID</span><br><span class="line">CI_JOB_TRIGGERED <span class="comment">#是否由api触发</span></span><br><span class="line">CI_COMMIT_SHA <span class="comment">#git commit产生的id 长格式</span></span><br><span class="line">CI_COMMIT_SHORT_SHA <span class="comment"># 同上, 短格式</span></span><br><span class="line">CI_COMMIT_BRANCH <span class="comment"># 提交分支</span></span><br><span class="line">CI_PROJECT_DIR <span class="comment"># 当前路径</span></span><br><span class="line">CI_MERGE_REQUEST_SOURCE_BRANCH_NAME <span class="comment"># mege request的源分支</span></span><br><span class="line">CI_MERGE_REQUEST_TARGET_BRANCH_NAME <span class="comment"># merge request的目标分支</span></span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://gitlab.bj.sensetime.com/help/user/project/protected_tags.md">http://gitlab.bj.sensetime.com/help/user/project/protected_tags.md</a></li>
<li><a href="http://gitlab.bj.sensetime.com/help/user/project/protected_branches">http://gitlab.bj.sensetime.com/help/user/project/protected_branches</a></li>
<li><a href="http://gitlab.bj.sensetime.com/help/ci/variables/README#variables">http://gitlab.bj.sensetime.com/help/ci/variables/README#variables</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitlab学习(实现多kubernets环境CI/CD)</title>
    <url>/2020/05/15/gitlab-kubernetes-multi-environment-deploy/</url>
    <content><![CDATA[<p>随着环境的多样性, 频繁的集成、测试占用了大量的时间, 大公司都有专门的团队做效率工程, 而我司这种小公司则就只能自养自足. 好在所有的环境都切换到了kubernetes环境, k8s天然的属性结合gitlab end-to-end的能力，实现CI&#x2F;CD也能节省大量的时间.</p>
<span id="more"></span>

<p>目前存在有3个环境<code>dev, gray, prod</code>,都是基于k8s架构, 使用的开发语言大部分都是golang.</p>
<p> <code>dev用于线下环境测试 --&gt;  gray用于灰度环境的联调 --&gt;  prod则是生产环境</code></p>
<p>由于不同环境只是配置文件的不同, CI&#x2F;CD主要存在以下场景:</p>
<ol>
<li>Merge request请求，对于这类请求全都需要执行源码编译, 可由maintainer选择是否同时需要后续流程</li>
<li>dev环境发布, 开发同学自测环境</li>
<li>gray环境发布，所有开发同学都能够发布</li>
<li>prod环境发布， 只有部分同学有权限发布</li>
<li>配置文件的变更，变更权限跟环境走</li>
</ol>
<h3 id="CI-x2F-CD流程"><a href="#CI-x2F-CD流程" class="headerlink" title="CI&#x2F;CD流程"></a>CI&#x2F;CD流程</h3><p><code>dev --&gt; makesource --&gt; imagespush --&gt; --&gt; CD gray --&gt; [自动化测试] --&gt; CD prod --&gt; [Blackbox Monitor]</code></p>
<p>由于dev环境其实是开发自己使用的环境,虽然接入了CI&#x2F;CD，但基于不怎么维护, 因此把dev放在最前面，用于说明开发者自测完成.</p>
<p>目前自动化测试跟功能黑盒测试还在完善.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/gitlab_workflow_example.png"></p>
<h3 id="merge-request"><a href="#merge-request" class="headerlink" title="merge request"></a>merge request</h3><p>由于出于对源码的保护, 对于merge request的请求都需要通过maintainer进行codeview,因此<code>merge request</code> 进行源码编译(代码测试), 编译通过后才能合并到目的分支是，同时gitlab还支持选择当pipeline成功时可自动合并</p>
<p>maintainer也可以选择是否需要配置合并后自动发布流程.</p>
<p>关于gitlab开启自动合并功能, 详见<a href="https://docs.gitlab.com/ee/ci/merge_request_pipelines/">merge_request_pipelines</a></p>
<h3 id="kubernetes认证"><a href="#kubernetes认证" class="headerlink" title="kubernetes认证"></a>kubernetes认证</h3><p>多个kubernetes的认证也是根据环境来做的, 最简单的办法就是通过将各种环境下的kubeconfig文件作为变量,最好做为gitlab的组环境变量，这样所有的repo都能够使用了.</p>
<p>在gitlab-ci.yml中通过不同的环境来选用不同的kubeconfig, 这样就能实现对集群的操作,</p>
<p>比如修改gray环境下的某个应用的配置文件， 主要代码如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">.exportenvgray:</span> <span class="meta">&amp;exportenvgray</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">export</span> <span class="string">MODE=gray</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">KUBE_CONFIG=$&#123;KUBE_CONFIG_GRAY&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">.kubeconfig:</span> <span class="meta">&amp;kubeconfig</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">mkdir</span> <span class="string">$HOME/.kube</span> <span class="string">&amp;&amp;</span> <span class="string">cat</span> <span class="string">$KUBE_CONFIG</span> <span class="string">&gt;</span> <span class="string">$HOME/.kube/config</span> <span class="comment">#需要在gilab中指定为文件类型的变量</span></span><br><span class="line"></span><br><span class="line"><span class="string">.k8supdateconfig:</span> <span class="meta">&amp;k8supdateconfig</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubectl</span> <span class="string">create</span> <span class="string">configmap</span> <span class="string">$&#123;APP&#125;-$&#123;KUBERNETES_NS&#125;-cm</span> <span class="string">--from-file=/tmp/config.yml</span> <span class="string">-n</span> <span class="string">$&#123;KUBERNETES_NS&#125;</span> <span class="string">-o</span> <span class="string">yaml</span> <span class="string">--dry-run</span> <span class="string">|</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="bullet">-</span></span><br><span class="line"></span><br><span class="line"><span class="string">.updateconfig:</span> <span class="meta">&amp;updateconfig</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">kubectl_yaml:v1.15.4</span></span><br><span class="line">  <span class="attr">dependencies:</span> []</span><br><span class="line">  <span class="attr">script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">*kubeconfig</span> <span class="comment"># 获取认证文件</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">*k8supdateconfig</span> <span class="comment"># 到这里就有权限能够更新configmap了</span></span><br><span class="line"></span><br><span class="line"><span class="attr">updateconfig:</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*updateconfig</span></span><br><span class="line">  <span class="attr">stage:</span> <span class="string">updateconfig1box</span></span><br><span class="line">  <span class="attr">before_script:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">*exportenv1gray</span> <span class="comment"># 指定环境变量，通过不同的环境获取不同的kubeconfig变量.</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">if:</span> <span class="string">&#x27;$CI_COMMIT_MESSAGE =~ /\[gray[ _-]uc\]/i &amp;&amp; $CONFIGPATH != &quot;&quot;&#x27;</span> <span class="comment"># 指定执行条件</span></span><br></pre></td></tr></table></figure>

<h3 id="多环境发布流程"><a href="#多环境发布流程" class="headerlink" title="多环境发布流程"></a>多环境发布流程</h3><p>对于多个环境, 其实除了配置不一样之外，其它的操作都是一样的，因此可借用yaml文件的锚定及gitlab的external功能大于减少gitlab-ci.yml配置文件的行数</p>
<p><strong>因为需要将gitlab-ci.yml做成标准化的流程，对于我来说, 我不需要区分是谁触发的pipeline，只需要知道你想触发哪个pipeline, 至于权限问题,则直接丢给了gitlab做管理</strong></p>
<p><strong>有些开发者可能一天commit 100次, 但是他知道前面的90次都是做代码集成, 并不能编译通过, 所以没必要每次的push都执行pipeline， 所以把什么时候需要才真正执行pipeline的选择丢给了开发者,因为开发者才是真正知道代码进度的问题, 当然他想每次都执行也是没问题的.</strong></p>
<p>因此这里大量使用了<code>commit message</code>关键字来区分environment</p>
<p>根据commit message的内容来选择执行对应的pipeline(以下关键字不区分大小写),比如: </p>
<blockquote>
<ul>
<li><p>包含有**[mb]** 时: 会执行源码编译(makebinary),主要用于开发编译集成</p>
</li>
<li><p>包含有**[gray-cd] or [gray_cd] or [gray cd]**时: 发布到1box环境(测试环境)</p>
</li>
<li><p>包含有 **[gray-uc] or [gray_uc] or [gray uc]**时: 说明只需要更新配置文件，但代码并未更新，适用于数据库密码修改等操作</p>
</li>
<li><p>当<code>gray</code>环境回归测试通过后, 将分支合并到主分支，打tag后可使用**[prod-cd]**发布到生产再进行回归</p>
</li>
<li><p>包含有**[skip ci] or [ci skip]**时: 不执行任何pipeline, 适用于比如提交文档修改之类的操作</p>
</li>
</ul>
</blockquote>
<p>对于只需更新配置文件的前提</p>
<blockquote>
<ul>
<li>由于缺少专门的配置平台, 目前使用的配置文件都使用configmap中， 很多的开发者又喜欢直接在平台(rancher)上修改，这样就没办法保证源代码中的配置会得到及时的更新， 为保证配置文件端到端的修改, 尽量禁止了在rancher上直接修改配置文件</li>
<li>配置文件的存放路径需要位于项目跟目录下的config&#x2F;config.yml，yaml格式, demo见下.</li>
<li>开发人员在发布<code>gray</code>环境时, 不应该保存有生产上的配置文件, 生产上则可以保存1box环境的配置,<strong>CI流程会根据发布的环境来选择对应的配置文件</strong></li>
<li>应用启动时可通过命令行参数指定配置文件的路径, 固定位于目录&#x2F;etc&#x2F;${APP}&#x2F;config.yml下</li>
<li>如果应用不存在配置文件(如nodejs等应用)则可在gitlab-ci.yml文件中把<code>CONFIGPATH</code>变量置为空</li>
</ul>
</blockquote>
<h3 id="配置文件处理"><a href="#配置文件处理" class="headerlink" title="配置文件处理"></a>配置文件处理</h3><p>这是一个标准化的流程，由于代码中可能保存有所有环境的配置文件, 为了数据安全, 需要对配置文件进行解析, 即: 根据要发布的环境来选择配置文件段，然后发布成kubernetes configmap的形式,最后挂载到应用中.</p>
<p>这里参考一个开源库<a href="https://github.com/olebedev/config">config</a>,随便改了些代码就实现功能了，即: 通过指定一个环境变量来获取配置文件段.</p>
<p>###<code>config/config.yml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">debug:</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="string">debug</span></span><br><span class="line">    <span class="attr">table:</span> <span class="string">filter</span></span><br><span class="line">    <span class="attr">enabled:</span> [<span class="string">mon</span>,<span class="string">osd</span>]</span><br><span class="line">    <span class="attr">disabled:</span> []</span><br><span class="line">    <span class="attr">ipv4:</span></span><br><span class="line">      <span class="attr">mon:</span></span><br><span class="line">      <span class="attr">priority:</span> <span class="number">100</span></span><br><span class="line">      <span class="attr">tcp:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;-p tcp -m multiport --dport 6789,3300 -m state --state NEW -j ACCEPT&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;-p tcp -m multiport --dport 6789,3300 -m state --state ESTABLISHED -j ACCEPT&quot;</span></span><br><span class="line">    <span class="attr">osd:</span></span><br><span class="line">      <span class="attr">priority:</span> <span class="number">200</span></span><br><span class="line"><span class="attr">1box:</span></span><br><span class="line">    <span class="attr">zsk:</span> <span class="string">&quot;我是ZSK&quot;</span></span><br><span class="line">    <span class="attr">mode:</span> <span class="string">1box</span></span><br><span class="line">    <span class="attr">table:</span> <span class="string">filter</span></span><br><span class="line">    <span class="attr">enabled:</span> [<span class="string">mon</span>,<span class="string">osd</span>]</span><br><span class="line">    <span class="attr">disabled:</span> []</span><br><span class="line">    <span class="attr">ipv4:</span></span><br><span class="line">      <span class="attr">mon:</span></span><br><span class="line">      <span class="attr">priority:</span> <span class="number">100</span></span><br><span class="line">      <span class="attr">tcp:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;-p tcp -m multiport --dport 6789,3300 -m state --state NEW -j ACCEPT&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;-p tcp -m multiport --dport 6789,3300 -m state --state ESTABLISHED -j ACCEPT&quot;</span></span><br><span class="line"><span class="attr">prod:</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>经过几天的研究测试，发现gitlab的CI&#x2F;CD还是很强大的, 除了能够支持很多的语言来写gitlab-ci.yaml外,官方文档也是非常的详细, 我几乎没有再看其它的文档 .</p>
<p>CI&#x2F;CD是个很有技巧的活, 除了标准化之外，跟其它工种的同学怎么协调接入, 比如自动化测、 分级发布、联动发布、黑盒监控等，都需要投入大量的时间来做.</p>
<p><strong>技术从来都不是重点, 推动起来被人认可才是</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.gitlab.com/ee/ci/merge_request_pipelines/">https://docs.gitlab.com/ee/ci/merge_request_pipelines/</a></li>
<li><a href="https://github.com/olebedev/config">https://github.com/olebedev/config</a></li>
<li><a href="http://gitlab.bj.sensetime.com/help/ci/variables/README#variables">http://gitlab.bj.sensetime.com/help/ci/variables/README#variables</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>docker学习记录</title>
    <url>/2016/02/29/docker%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h3 id="docker知识点"><a href="#docker知识点" class="headerlink" title="docker知识点:"></a><strong>docker知识点:</strong></h3><p>1.镜像是只读的，当镜像启动的时候就成了一个容器，容器会在镜像的最外一层加上一个可写层，当我们有一个正在运行的Docker容器时，从运行态到停止态，我们对它所做的一切变更都会永久地写到容器的文件系统中。要切记，对容器的变更是写入到容器的文件系统的，而不是写入到Docker镜像中的.</p>
<span id="more"></span>

<p>我们可以用同一个镜像启动多个Docker容器，这些容器启动后都是活动的，彼此还是相互隔离的。我们对其中一个容器所做的变更只会局限于那个容器本身 如果对容器的底层镜像进行修改，那么当前正在运行的容器是不受影响的，不会发生自动更新现象<br>容器每次启动的时候 ip地址都是随机分配的 如要分配固定的ip,详情请见docker-network部分<br>或<a href="http://www.xiaomastack.com/2015/02/06/docker-static-ip/">参考</a></p>
<p>2.运行镜像 docker run [-t -i –rm -d] image_name:tag_name<br>(其中， -t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开 交互)<br>–rm 表示退出容器后自动删除容器 注意：-d 跟–rm 参数不能同时使用<br>-d 参数则是让镜像后台运行 执行完这条命令后会返回唯一容器号且会随机分配一个容器名(如果没有使用–name显性指定的话) 如果想进入容器 则<br>使用 docker attach container_id(使用容器名不可以) 如果想退出attach而不退出容器的话以下需要注意<br>如果docker run命令使用了 -i参数 则使用Ctrl-c则会退出整个容器,想要只退出attach而不退出容器 使用Ctrl-p+Ctrl-q<br>如果docker run命令没有使用-i参数 使用Ctrl-c则只会退出attach,容器不会退出<br>修改镜像 docker commit -m “add test.txt” -a “hxf0510” 0b4acac89bed centos:latest<br>上传镜像<br>docker push image_name:tag_name<br>下载镜像 docker pull image_name:tag_name (不加tag_name则下载latest)<br>给镜像添加标签 docker tag IMAGE[:TAG] [REGISTRYHOST&#x2F;][USERNAME&#x2F;]NAME[:TAG]<br>查看本地镜像 docker images<br>查看运行的镜像 docker ps -a<br>停止运行容器 docker stop $(docker ps -a)<br>创建容器但是不运行 docker create -t -i image_name<br>启动已终止的镜像 docker start image_name<br>删除容器&#x2F;删除镜像 docker rm container_name&#x2F;docker rmi image_name<br>删除镜像前需要删除容器，即使这个容器没有运行！因为容器保存了这个镜像的运行状态<br>导出容器 docker export container_name &gt;image_name.tar<br>导入容器快照 docker import container_name:[tag] 或者 docker load container_name:[tag]<br>这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息<br>（导入即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文<br>件时可以重新指定标签等元数据信息</p>
<blockquote>
<ul>
<li>docker logs Name&#x2F;ID : 从一个容器中取日志</li>
<li>docker diff Name&#x2F;ID : 列出一个容器里面被改变的文件或者目录</li>
<li>docker top Name&#x2F;ID : 显示一个运行的容器里面的进程信息</li>
<li>docker cp Name:&#x2F;container_path to_path :</li>
</ul>
</blockquote>
<p>从容器里面拷贝文件&#x2F;目录到本地一个路径<br>如果需要在容器中安装软件如vim 时 可能会提示Unable to locate package vim<br>需要先 apt-get update 这个命令的作用是：同步 &#x2F;etc&#x2F;apt&#x2F;sources.list 和 &#x2F;etc&#x2F;apt&#x2F;sources.list.d 中列出的源的索引，这样才能获取到最新的软件包<br>然后再 apt-get install vim<br>查看容器属性如IP: docker inspect –format ‘{{ .NetworkSettings.IPAddress }}‘ container_id (format后面的空格 network后面那个 . 是必需的)<br>3.使用dockerfile创建镜像时如果使用的是同一个dockerfile,再次创建时会使用上一次产生的镜像做为中间镜像,一个镜像最多127层，每一层都产生一个镜像，生成最终镜像后会删除所有的中间镜像<br>4.数据卷&#x2F;数据卷容器<br>一个数据卷就是经过特殊设计的,在一个或多个容器中通过UFS文件系统提供的一些特性</p>
<blockquote>
<ul>
<li>实现数据持久化或共享.</li>
<li>数据卷可以在容器之间共享和重复利用</li>
<li>可以对数据卷里的内容直接进行修改</li>
<li>对镜像的更新不会改变数据卷的内容</li>
<li>卷会一直持续到没有容器使用他们</li>
</ul>
</blockquote>
<p>例如：在nginx容器中挂载本地html目录作为nginx的发布目录(nginx容器默认的目录为&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html)<br>启动容器：docker run -ti –name docker-nginx -p 8008:80 -v &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html:ro -d nginx:latest<br>进入容器：docker exec -ti docker-nginx bash<br>查看nginx容器默认的发布目录已经显示了本地的html文件中,使用了:ro属性则说明在容器中对html目录下的文件进行修改时会提示文件为只读不可修改<br>但是在本地修改html目录下文件时 则修改会同步到nginx容器<br>5.安装docker-compose<br>1.使用pip install -U docker-compose 则先需要安装python-pip(1.8版本)，这个需要python，<br>python2.6需要升级到python3.5<br>步骤：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xf python3.5.tar.xz</span><br><span class="line">cd python3.5</span><br><span class="line">.configure --prefix=/usr/local</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">make clean &amp;&amp; make distclean</span><br><span class="line">/usr/local/bin/python3 -V</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">建立软链接 使系统默认使用python3</span></span><br><span class="line">mv /usr/bin/python /usr/bin/python2.6.6</span><br><span class="line">ln -s /usr/bin/python3 /usr/bin/python</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">更新yum指向的python，因为yum的python已经指向python3,但是yum只能使<span class="comment">#用python2，所以只能指向更名后的python2.6.6</span></span></span><br><span class="line">vim /usr/bin/yum</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">修改<span class="comment">#!/usr/bin/python--&gt;#!/usr/bin/python2.6.6</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装pip</span></span><br><span class="line">wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gz</span><br><span class="line">tar -xvf setuptools-1.4.2.tar.gz</span><br><span class="line">cd setuptools-1.4.2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用 Python 安装 setuptools</span></span><br><span class="line">python setup.py install</span><br><span class="line">curl https://bootstrap.pypa.io/get-pip.py | python</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装docker-compose</span></span><br><span class="line">pip install -U docker-compose</span><br></pre></td></tr></table></figure>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/docker%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951.png" alt="docker951"></p>
<h3 id="遇到的错误"><a href="#遇到的错误" class="headerlink" title="遇到的错误:"></a><strong>遇到的错误:</strong></h3><p>在升级完内核到3.14后启动docker报如下错误：vim &#x2F;etc&#x2F;cfconfig.conf 注释memory那行</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/docker%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952.png" alt="docker952"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://blog.csdn.net/eric_sunah/article/details/46048361">http://blog.csdn.net/eric_sunah/article/details/46048361</a></li>
<li><a href="https://docker.io/">https://docker.io</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(golang中的addressable)</title>
    <url>/2019/10/20/golang-addressable/</url>
    <content><![CDATA[<p><strong>该文章为转载, 原文地址在<a href="https://colobu.com/2018/02/27/go-addressable/">https://colobu.com/2018/02/27/go-addressable/</a></strong></p>
<p>Go语言规范中规定了可寻址(addressable)对象的定义</p>
<blockquote>
<p>For an operand x of type T, the address operation &amp;x generates a pointer of type *T to x. The operand must be addressable, that is, either a variable, pointer indirection, or slice indexing operation; or a field selector of an addressable struct operand; or an array indexing operation of an addressable array. As an exception to the addressability requirement, x may also be a (possibly parenthesized) composite literal. If the evaluation of x would cause a run-time panic, then the evaluation of &amp;x does too.</p>
</blockquote>
<span id="more"></span>

<h3 id="addressable"><a href="#addressable" class="headerlink" title="addressable"></a>addressable</h3><p>对于一个对象<code>x</code>, 如果它的类型为<code>T</code>, 那么<code>&amp;x</code>则会产生一个类型为<code>*T</code>的指针，这个指针指向<code>x</code>, 这是这一段的第一句话，也是我们在开发过程中经常使用的一种获取对象指针的一种方式</p>
<p>上面规范中的这段话规定， <code>x</code>必须是可寻址的， 也就是说，它只能是以下几种方式：</p>
<ul>
<li>一个变量: <code>&amp;x</code></li>
<li>指针引用(pointer indirection): <code>&amp;*x</code></li>
<li>slice索引操作(不管slice是否可寻址): <code>&amp;s[1]</code></li>
<li>可寻址struct的字段: <code>&amp;point.X</code></li>
<li>可寻址数组的索引操作: <code>&amp;a[0]</code></li>
<li>composite literal类型: <code>&amp;struct&#123; X int &#125;&#123;1&#125;</code></li>
</ul>
<p>下列情况<code>x</code>是不可以寻址的，你不能使用<code>&amp;x</code>取得指针：</p>
<ul>
<li>字符串中的字节:</li>
<li>map对象中的元素</li>
<li>接口对象的动态值(通过type assertions获得)</li>
<li>常数</li>
<li>literal值(非composite literal)</li>
<li>package 级别的函数</li>
<li>方法method (用作函数值)</li>
<li>中间值(intermediate value):<ul>
<li>函数调用</li>
<li>显式类型转换</li>
<li>各种类型的操作 （除了指针引用pointer dereference操作*x):<ul>
<li>channel receive operations</li>
<li>sub-string operations</li>
<li>sub-slice operations</li>
<li>加减乘除等运算符</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Tapir Games在他的文章<a href="http://www.tapirgames.com/blog/golang-unofficial-faq#unaddressable-values">unaddressable-values</a>中做了很好的整理。</p>
</blockquote>
<p>有几个点需要解释下：</p>
<ul>
<li>常数为什么不可以寻址?： 如果可以寻址的话，我们可以通过指针修改常数的值，破坏了常数的定义。</li>
<li>map的元素为什么不可以寻址？:两个原因，如果对象不存在，则返回零值，零值是不可变对象，所以不能寻址，如果对象存在，因为Go中map实现中元素的地址是变化的，这意味着寻址的结果是无意义的。</li>
<li>为什么slice不管是否可寻址，它的元素读是可以寻址的？:因为slice底层实现了一个数组，它是可以寻址的。</li>
<li>为什么字符串中的字符&#x2F;字节又不能寻址呢：因为字符串是不可变的。</li>
</ul>
<p>规范中还有几处提到了 <code>addressable</code>:</p>
<ul>
<li>调用一个receiver为指针类型的方法时，使用一个addressable的值将自动获取这个值的指针</li>
<li><code>++</code>、<code>--</code>语句的操作对象必须是addressable或者是map的index操作</li>
<li>赋值语句<code>=</code>的左边对象必须是addressable,或者是map的index操作，或者是<code>_</code></li>
<li>上条同样使用<code>for ... range</code>语句</li>
</ul>
<h3 id="reflect-Value的CanAddr方法和CanSet方法"><a href="#reflect-Value的CanAddr方法和CanSet方法" class="headerlink" title="reflect.Value的CanAddr方法和CanSet方法"></a>reflect.Value<code>的</code>CanAddr<code>方法和</code>CanSet方法</h3><p>在我们使用<code>reflect</code>执行一些底层的操作的时候， 比如编写序列化库、rpc框架开发、编解码、插件开发等业务的时候，经常会使用到<code>reflect.Value</code>的<code>CanSet</code>方法，用来动态的给对象赋值。 <code>CanSet</code>比<code>CanAddr</code>只加了一个限制，就是struct类型的<code>unexported</code>的字段不能<code>Set</code>，所以我们这节主要介绍<code>CanAddr</code>。</p>
<p>并不是任意的<code>reflect.Value</code>的<code>CanAddr</code>方法都返回<code>true</code>,根据它的godoc,我们可以知道：</p>
<blockquote>
<p>CanAddr reports whether the value’s address can be obtained with Addr. Such values are called addressable. A value is addressable if it is an element of a slice, an element of an addressable array, a field of an addressable struct, or the result of dereferencing a pointer. If CanAddr returns false, calling Addr will panic.</p>
</blockquote>
<p>也就是只有下面的类型<code>reflect.Value</code>的<code>CanAddr</code>才是<code>true</code>, 这样的值是<code>addressable</code>:</p>
<ul>
<li>slice的元素</li>
<li>可寻址数组的元素</li>
<li>可寻址struct的字段</li>
<li>指针引用的结果</li>
</ul>
<p>与规范中规定的<code>addressable</code>, <code>reflect.Value</code>的<code>addressable</code>范围有所缩小， 比如对于栈上分配的变量， 随着方法的生命周期的结束， 栈上的对象也就被回收掉了，这个时候如果获取它们的地址，就会出现不一致的结果，甚至安全问题。</p>
<blockquote>
<p>对于栈和堆的对象分配以及逃逸分析，你可以看 William Kennedy 写的系列文章: <a href="https://studygolang.com/articles/12444">Go 语言机制之逃逸分析</a></p>
</blockquote>
<p>所以如果你想通过<code>reflect.Value</code>对它的值进行更新，应该确保它的<code>CanSet</code>方法返回<code>true</code>,这样才能调用<code>SetXXX</code>进行设置。</p>
<p>使用<code>reflect.Value</code>的时候有时会对<code>func Indirect(v Value) Value</code>和<code>func (v Value) Elem() Value</code>两个方法有些迷惑，有时候他们俩会返回同样的值，有时候又不会。</p>
<p>总结一下：</p>
<ol>
<li>如果<code>reflect.Value</code>是一个指针， 那么<code>v.Elem()</code>等价于<code>reflect.Indirect(v)</code></li>
<li>如果不是指针<br>2.1 如果是<code>interface</code>, 那么<code>reflect.Indirect(v)</code>返回同样的值，而<code>v.Elem()</code>返回接口的动态的值<br>2.2 如果是其它值, <code>v.Elem()</code>会panic,而<code>reflect.Indirect(v)</code>返回原值</li>
</ol>
<p>下面的代码列出一些<code>reflect.Value</code>是否可以addressable, 你需要注意数组和struct字段的情况，也就是<code>x7</code>、<code>x9</code>、<code>x14</code>、<code>x15</code>的正确的处理方式。</p>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;reflect&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    checkCanAddr()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">type</span> S <span class="keyword">struct</span> &#123;</span><br><span class="line">    X <span class="type">int</span></span><br><span class="line">    Y <span class="type">string</span></span><br><span class="line">    z <span class="type">int</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">M</span><span class="params">()</span></span> <span class="type">int</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> x0 = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkCanAddr</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 可寻址的情况</span></span><br><span class="line">    v := reflect.ValueOf(x0)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x0: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x0, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x1 = <span class="number">1</span></span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(x1))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x1: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x1, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x2 = &amp;x1</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(x2))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x2: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x2, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x3 = time.Now()</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(x3))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x3: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x3, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x4 = &amp;x3</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(x4))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x4: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x4, v.CanAddr(), v.CanSet()) <span class="comment">// true,true</span></span><br><span class="line">    <span class="keyword">var</span> x5 = []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.ValueOf(x5)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x5: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x5, v.CanAddr(), v.CanSet()) <span class="comment">// false,false</span></span><br><span class="line">    <span class="keyword">var</span> x6 = []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.ValueOf(x6[<span class="number">0</span>])</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x6: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x6[<span class="number">0</span>], v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x7 = []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.ValueOf(x7).Index(<span class="number">0</span>)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x7: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x7[<span class="number">0</span>], v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    v = reflect.ValueOf(&amp;x7[<span class="number">1</span>])</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x7.1: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x7[<span class="number">1</span>], v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x8 = [<span class="number">3</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.ValueOf(x8[<span class="number">0</span>])</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x8: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x8[<span class="number">0</span>], v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="comment">// https://groups.google.com/forum/#!topic/golang-nuts/RF9zsX82MWw</span></span><br><span class="line">    <span class="keyword">var</span> x9 = [<span class="number">3</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(x9).Index(<span class="number">0</span>))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x9: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x9[<span class="number">0</span>], v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x10 = [<span class="number">3</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;x10)).Index(<span class="number">0</span>)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x9: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x10[<span class="number">0</span>], v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x11 = S&#123;&#125;</span><br><span class="line">    v = reflect.ValueOf(x11)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x11: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x11, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x12 = S&#123;&#125;</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;x12))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x12: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x12, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x13 = S&#123;&#125;</span><br><span class="line">    v = reflect.ValueOf(x13).FieldByName(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x13: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x13, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x14 = S&#123;&#125;</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;x14)).FieldByName(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x14: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x14, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x15 = S&#123;&#125;</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;x15)).FieldByName(<span class="string">&quot;z&quot;</span>)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x15: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x15, v.CanAddr(), v.CanSet()) <span class="comment">//true,false</span></span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;S&#123;&#125;))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x15.1: %v \tcan be addressable and set: %t, %t\n&quot;</span>, &amp;S&#123;&#125;, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x16 = M</span><br><span class="line">    v = reflect.ValueOf(x16)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x16: %p \tcan be addressable and set: %t, %t\n&quot;</span>, x16, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x17 = M</span><br><span class="line">    v = reflect.Indirect(reflect.ValueOf(&amp;x17))</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x17: %p \tcan be addressable and set: %t, %t\n&quot;</span>, x17, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x18 <span class="keyword">interface</span>&#123;&#125; = &amp;x11</span><br><span class="line">    v = reflect.ValueOf(x18)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x18: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x18, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">    <span class="keyword">var</span> x19 <span class="keyword">interface</span>&#123;&#125; = &amp;x11</span><br><span class="line">    v = reflect.ValueOf(x19).Elem()</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x19: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x19, v.CanAddr(), v.CanSet()) <span class="comment">//true,true</span></span><br><span class="line">    <span class="keyword">var</span> x20 = [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">    v = reflect.ValueOf([...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;x20: %v \tcan be addressable and set: %t, %t\n&quot;</span>, x20, v.CanAddr(), v.CanSet()) <span class="comment">//false,false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://colobu.com/2018/02/27/go-addressable/">https://colobu.com/2018/02/27/go-addressable/</a></li>
<li><a href="http://www.tapirgames.com/blog/golang-unofficial-faq#unaddressable-values">http://www.tapirgames.com/blog/golang-unofficial-faq#unaddressable-values</a></li>
<li><a href="https://blog.golang.org/laws-of-reflection">https://blog.golang.org/laws-of-reflection</a></li>
<li><a href="https://stackoverflow.com/questions/25384640/why-golang-reflect-makeslice-returns-un-addressable-value">https://stackoverflow.com/questions/25384640/why-golang-reflect-makeslice-returns-un-addressable-value</a></li>
<li><a href="http://grokbase.com/t/gg/golang-nuts/13bwpzedxh/go-nuts-golang-reflect-addressable-is-not-consistency">http://grokbase.com/t/gg/golang-nuts/13bwpzedxh/go-nuts-golang-reflect-addressable-is-not-consistency</a></li>
<li><a href="https://github.com/golang/go/issues/11865">https://github.com/golang/go/issues/11865</a></li>
<li><a href="https://stackoverflow.com/questions/20224478/dereferencing-a-map-index-in-golang">https://stackoverflow.com/questions/20224478/dereferencing-a-map-index-in-golang</a></li>
<li><a href="https://stackoverflow.com/questions/38168329/why-are-map-values-not-addressable">https://stackoverflow.com/questions/38168329/why-are-map-values-not-addressable</a></li>
<li><a href="https://stackoverflow.com/questions/40793289/go-struct-literals-why-is-this-one-addressable">https://stackoverflow.com/questions/40793289/go-struct-literals-why-is-this-one-addressable</a></li>
<li><a href="https://groups.google.com/forum/#!topic/golang-nuts/FnTCX9R_PbM">https://groups.google.com/forum/#!topic/golang-nuts/FnTCX9R_PbM</a></li>
<li><a href="https://stackoverflow.com/questions/24318389/golang-elem-vs-indirect-in-the-reflect-package">https://stackoverflow.com/questions/24318389/golang-elem-vs-indirect-in-the-reflect-package</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(const-iota)</title>
    <url>/2019/08/11/golang-const-iota/</url>
    <content><![CDATA[<p>iota在const中使用的非常广泛, 直接上几个例子吧.</p>
<span id="more"></span>



<p>const比较容易, <strong>唯一值得注意的是 const定义的常量需要在编译值就能够确认.</strong></p>
<p><strong>iota在const关键字出现时将被重置为0(const内部的第一行之前)，从第一行开始, 每新增一行常量声明(不包括空行及注释)将使iota计数一次(iota可理解为const语句块中的行索引)。使用iota能简化定义，主要用于定义枚举，而且很有用</strong></p>
<p><strong>iota还有表达式时, 在没有表达式的行上，将复用上一行的表达式，采用一层一层向上找的策略</strong></p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> ( <span class="comment">// iota在const内部的第一行之前就被初始化为0</span></span><br><span class="line">	mutexLocked = <span class="literal">iota</span> <span class="comment">// iota=0</span></span><br><span class="line">	mutexWoken = <span class="number">2</span> <span class="comment">// iota=1</span></span><br><span class="line">	mutexStarving = <span class="number">3</span> <span class="comment">// iota=2</span></span><br><span class="line">	_  <span class="comment">// iota=3</span></span><br><span class="line">	</span><br><span class="line">  <span class="comment">// 这是一行注释</span></span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span>   <span class="comment">// 所以当到这一行时，共增加了5行，索引从0开始，所以iota为4</span></span><br><span class="line">	xyz</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="literal">iota</span> := <span class="number">33</span>  <span class="comment">//iota不是关键字， 因此也可以被当成一个普通的变量对待，跟其它变量没有区别</span></span><br><span class="line">	fmt.Println(mutexLocked, mutexWoken, mutexStarving, mutexWaiterShift, xyz)</span><br><span class="line">  fmt.Println(<span class="literal">iota</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line"><span class="number">33</span></span><br></pre></td></tr></table></figure>



<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  mutexLocked, mutexWoken = <span class="literal">iota</span> + <span class="number">100</span>, <span class="number">200</span> <span class="comment">// 100 ,200, iota计数是以行为单位的(除去空行跟注释)</span></span><br><span class="line">	mutexStarving, xyz <span class="comment">// 这里只有mutexStarving会使用表达式 iota + 100</span></span><br><span class="line">	_, _ <span class="comment">//前一个等于102，后一个等于200, 这里写一个_会报错：extra expression in const declaration</span></span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(mutexLocked, mutexWoken, mutexStarving, xyz, mutexWaiterShift)</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">101</span> <span class="number">200</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>



<h3 id="例子3"><a href="#例子3" class="headerlink" title="例子3"></a>例子3</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	mutexLocked = -(<span class="literal">iota</span>) <span class="comment">// iota = 0, -0=0</span></span><br><span class="line">  mutexWoken <span class="comment">// iota =1, -(1)= -1</span></span><br><span class="line">  mutexStarving <span class="comment">// iota = 2, -(2) = 2</span></span><br><span class="line">	xyz  <span class="comment">// iota = 3, -(3) = 3</span></span><br><span class="line">	_    <span class="comment">// iota = 4, -(4) = 4</span></span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span> <span class="comment">// iota = 5</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(mutexLocked, mutexWoken, mutexStarving, xyz, mutexWaiterShift)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> <span class="number">-1</span> <span class="number">-2</span> <span class="number">-3</span> <span class="number">5</span></span><br><span class="line"><span class="comment">// 只需要记住, iota表达式，先计算iota的值，然后把表达工直接做替换即可.</span></span><br></pre></td></tr></table></figure>



<h3 id="例子4"><a href="#例子4" class="headerlink" title="例子4"></a>例子4</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	mutexLocked = -(<span class="literal">iota</span>)</span><br><span class="line">	mutexWoken</span><br><span class="line">	mutexStarving  <span class="comment">//没有表达式则一层一层向上查找</span></span><br><span class="line">	xyz</span><br><span class="line">	abc = <span class="number">1</span> &lt;&lt; <span class="literal">iota</span>  <span class="comment">// iota=4, 1 &lt;&lt; 4 = 16</span></span><br><span class="line">	def</span><br><span class="line">	_</span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(mutexLocked, mutexWoken, mutexStarving, xyz, abc, def, mutexWaiterShift)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line"><span class="number">0</span> <span class="number">-1</span> <span class="number">-2</span> <span class="number">-3</span> <span class="number">16</span> <span class="number">32</span> <span class="number">7</span></span><br></pre></td></tr></table></figure>



<p>使用iota能简化定义，在定义枚举时很有用。当代码需要改动的时候，也比较易于拓展或者修改。另外看起来也是有些逼格在里边的。</p>
<p>但是itoa 令代码相对的不那么的明了易懂。会加大理解代码的负担, 因此还是适当地用吧.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://blog.wolfogre.com/posts/golang-iota/">https://blog.wolfogre.com/posts/golang-iota/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(数组及slice常用操作)</title>
    <url>/2019/10/18/golang-array-slice-ops/</url>
    <content><![CDATA[<p>这篇主要学习下golang中的数组及slice(切片)及常规操作, 主要是slice, 但是并不会涉及slice的底层原理, 这个范围比较大, 有必要单独拎出来.</p>
<span id="more"></span>



<h3 id="数组-array"><a href="#数组-array" class="headerlink" title="数组(array)"></a>数组(array)</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>数组是内置(build-in)类型,是一组同类型数据的集合，它是值类型，通过从0开始的下标索引访问元素值。在初始化后长度是固定的，无法修改其长度, <strong>数组的大小是类型的一部分。因此[5]int和[25]int是不同的类型</strong></p>
<p>数组的大小必须是常量表达式, 也就是说长度需要在编译阶段能够确认.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 数组有如下几种声明方式:</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ArrayDemo</span><span class="params">()</span></span> &#123;</span><br><span class="line">	aRray := [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>&#125; <span class="comment">//如果不填写个数则会由编译器自动算出</span></span><br><span class="line">	bRray := [<span class="number">2</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>&#125; <span class="comment">//如果指定数据个数，则元素个数不能超过该值</span></span><br><span class="line">  cRray := [<span class="number">3</span>]<span class="type">int</span>&#123;<span class="number">1</span>:<span class="number">10</span>, <span class="number">2</span>:<span class="number">20</span>&#125; <span class="comment">//这里通过索引直接指定值</span></span><br><span class="line">  dRray := [<span class="number">3</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>&#125; <span class="comment">//在初始化时没有指定初值的元素将会赋值为其元素类型的默认值</span></span><br><span class="line">  eRray := [<span class="number">2</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125; <span class="comment">//报错, 元素长度大于2</span></span><br><span class="line">	<span class="comment">// var fRray []int 初始值为[], 注意，这个是slice</span></span><br><span class="line"></span><br><span class="line">	fmt.Println(aRray, bRray, cRray, dRray)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">[<span class="number">1</span> <span class="number">3</span> <span class="number">5</span> <span class="number">7</span>] [<span class="number">1</span> <span class="number">2</span>] [<span class="number">0</span> <span class="number">10</span> <span class="number">20</span>] [<span class="number">1</span> <span class="number">2</span> <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>简单来说就是, 在定义数组时需要为数组指定长度, 或者使用[…], 如果未指定的即为slice</p>
<h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">aRray := [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>&#125;</span><br><span class="line">aRray[<span class="number">0</span>] <span class="comment">// 1</span></span><br><span class="line">aRray[<span class="number">5</span>] <span class="comment">// out of range</span></span><br></pre></td></tr></table></figure>

<h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><p> <strong>数组的大小是类型的一部分。因此[5]int和[25]int是不同的类型</strong>，只有在当数据的长度及类型完全一样时，该数据组才能比较</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">aRray := [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>&#125;</span><br><span class="line">bRray := [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line">cRray := [<span class="number">4</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>&#125;</span><br><span class="line">dRray := [<span class="number">4</span>]<span class="type">string</span>&#123;<span class="string">&quot;1&quot;</span>, <span class="string">&quot;3&quot;</span>, <span class="string">&quot;5&quot;</span>, <span class="string">&quot;7&quot;</span>&#125;</span><br><span class="line">fmt.Println(aRray == bRray) <span class="comment">// false</span></span><br><span class="line">fmt.Println(aRray == cRray) <span class="comment">//true</span></span><br><span class="line">fmt.Println(aRray == dRray) <span class="comment">//invalid operation: eRray == fRray (mismatched types [4]int and [4]string)</span></span><br><span class="line">fmt.Println(aRray == bRray) <span class="comment">//invalid operation: aRray == bRray (mismatched types [4]int and [2]int)</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%p, %p&quot;</span>, &amp;aRray, &amp;cRray) <span class="comment">// 虽然两个数组相等, 但是数组的地址是不一样的</span></span><br></pre></td></tr></table></figure>

<p><strong>只有在当数组的长度及类型及元素值完全一样时，该数组才相等, 注意，相等并不相同.</strong></p>
<h4 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">aRray := [...]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>&#125;</span><br><span class="line"><span class="comment">// for</span></span><br><span class="line">	l := <span class="built_in">len</span>(array)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; l; i++ &#123;</span><br><span class="line">		fmt.Println(array[i])</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// for range</span></span><br><span class="line"><span class="keyword">for</span> k, v := <span class="keyword">range</span> aRray &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;k=%d, v=%d\n&quot;</span>, k, v)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里需要注意的是, 如果把数组做为参数传递给函数, 如果不使用slice, 则在传递时需要明确数组的长度及类型处理.</span></span><br><span class="line"><span class="comment">// 当然一般都不会直接传递数组</span></span><br></pre></td></tr></table></figure>



<h3 id="slice"><a href="#slice" class="headerlink" title="slice"></a>slice</h3><p>slice表示一个拥有相同类型元素的可变长度的序列，slice是一种轻量的数据结构, 可以用来访问数组的部分或者全部元素,而这个元素称为slice的底层数组, 这里先只学习slice的常用操作,slice 底层结构原理会单独记录.</p>
<p>现在可以简单这样理解: <strong>slice是底层数组的一层view</strong>.</p>
<p>**slice是由三部分组成: 第1个参数是指向底层数组的指针(ptr),这个指针指向真正存储数据的块, 第2个参数是长度(len), 第三个参数是容量(cap)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200313151939.png"></p>
<h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 通过字面量创建</span></span><br><span class="line"><span class="keyword">var</span> aSlice = []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125; <span class="comment">//[1 2 3]</span></span><br><span class="line">bSlice := []<span class="type">string</span>&#123;<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>&#125; <span class="comment">// [&quot;a&quot; &quot;b&quot;]</span></span><br><span class="line"><span class="comment">// 通过make函数创建, 第一个参数是类型, 第2个参数是长度, 第三个长度是容量</span></span><br><span class="line">cSlice := <span class="built_in">make</span>([]<span class="type">int</span>, <span class="number">2</span>, <span class="number">10</span>) <span class="comment">// [0 0]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="访问-1"><a href="#访问-1" class="headerlink" title="访问"></a>访问</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 下标是从0开始 ，按下标访问</span></span><br><span class="line">aSlice = []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line">aSlice[<span class="number">0</span>] <span class="comment">// 1</span></span><br><span class="line">aSlice[<span class="number">10</span>] <span class="comment">// 运行时错误: panic: runtime error: index out of range [2] with length </span></span><br></pre></td></tr></table></figure>

<h4 id="遍历-1"><a href="#遍历-1" class="headerlink" title="遍历"></a>遍历</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// slice的遍历方法写数组是一样的</span></span><br><span class="line"></span><br><span class="line">aSlice := []<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">	l := <span class="built_in">len</span>(aSlice)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; l; i++ &#123;</span><br><span class="line">		fmt.Println(aSlice[i])</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// for range</span></span><br><span class="line"><span class="keyword">for</span> k, v := <span class="keyword">range</span> aSlice &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;k=%d, v=%d\n&quot;</span>, k, v)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">eSlice := [<span class="number">10</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125; <span class="comment">// [1 2 3 4 5 0 0 0 0 0]</span></span><br><span class="line">hSlice := eSlice[:]</span><br><span class="line"><span class="built_in">len</span>(hSlice) <span class="comment">// 10</span></span><br><span class="line"><span class="built_in">cap</span>(hSlice) <span class="comment">// 10</span></span><br></pre></td></tr></table></figure>

<h5 id="Reslice"><a href="#Reslice" class="headerlink" title="Reslice"></a>Reslice</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义一个数组</span></span><br><span class="line">eSlice := [<span class="number">10</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125; <span class="comment">// [1 2 3 4 5 0 0 0 0 0]</span></span><br><span class="line"><span class="comment">// slice</span></span><br><span class="line">fSlice := eSlice[<span class="number">2</span>:<span class="number">4</span>] <span class="comment">// [3 4]</span></span><br><span class="line"><span class="comment">// 越界访问</span></span><br><span class="line">gSlice := fSlice[<span class="number">3</span>] <span class="comment">// index out of range [3] with length 2</span></span><br><span class="line"><span class="comment">// reslice</span></span><br><span class="line">gSlice := fSlice[<span class="number">1</span>:<span class="number">5</span>] <span class="comment">// [4 5 0 0]</span></span><br><span class="line"><span class="comment">// gSlice := fSlice[1:10] // slice bounds out of range [:10] with capacity 8</span></span><br></pre></td></tr></table></figure>

<p><code>gSlice := fSlice[1:5]</code>为何会输出<code>[4 5 0 0 ]</code>呢, gSlice的值为[3 4 ], 压根就没有[1: 5]，那它为何没有提示越界访问呢？这个其实就是对<strong>slice是底层数组的一层view的说明</strong></p>
<p><strong>slice是支持向后扩展的, eSlice其实就是fSlice跟gSlice的底层数组, 只要不超过底层数组的cap,就可以反应在slice上</strong>，这也是fSlice[1:5]不报错的原因</p>
<p>fSlice[1:10]是由于gSlice本身有了2个元素, 整个只有8个元素,因此越界.</p>
<h5 id="append"><a href="#append" class="headerlink" title="append"></a>append</h5><p>向slice追加元素的时候需要注意存在修改底层数组及扩容的问题.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改底层数组	</span></span><br><span class="line">eSlice := [<span class="number">10</span>]<span class="type">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;</span><br><span class="line">hSlice := eSlice[<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line">fmt.Println(eSlice, hSlice, <span class="built_in">len</span>(hSlice), <span class="built_in">cap</span>(hSlice)) <span class="comment">//[1 2 3 4 5 0 0 0 0 0] [4 5] 2 7</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%p\n&quot;</span>, &amp;hSlice) <span class="comment">//0xc0000c0020</span></span><br><span class="line"></span><br><span class="line">iSlice := <span class="built_in">append</span>(hSlice, <span class="number">100</span>) <span class="comment">// iSlice修改了底层数组</span></span><br><span class="line">fmt.Println(eSlice, iSlice) <span class="comment">//[1 2 3 4 5 100 0 0 0 0] [4 5 100]</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%p\n&quot;</span>, &amp;iSlice)	<span class="comment">//0xc0000cc000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从这里可以看到eSlice的第5个元素被修改成了100, 上面都没有直接操作eSlice，为何值会变呢?</span></span><br><span class="line"><span class="comment">// 原因就在于hSlice与iSlice都是以eSlice为底层数组, 在对hSlice做追加操作时没有超过eSlice的cap大小, 因	此修改都会反应在eSlice上.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 扩容</span></span><br><span class="line">hSlice = <span class="built_in">append</span>(hSlice, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>)</span><br><span class="line">fmt.Println(eSlice, hSlice)  <span class="comment">//[1 2 3 4 5 100 0 0 0 0] [4 5 11 12 13 14 15 16 17 18]</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%p\n&quot;</span>, &amp;hSlice)  <span class="comment">//0xc0000c0020</span></span><br><span class="line"><span class="comment">// 如果追加的元素超过底层数组的cap大小，就会创建一个更大的底层数组，因此在追加那么多元素之后会发现eSlice没	 有变化, 因为此是hSlice的底层数组已不再是eSlice, 对hSlice的修改不会影响eSlice.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从slice头追加</span></span><br><span class="line"><span class="comment">// 由于在夈追加会引起后面的元素向后移动，因此效率较差.</span></span><br><span class="line">hSlice = <span class="built_in">append</span>([]<span class="type">int</span>&#123;<span class="number">0</span>&#125;, hSlice...)</span><br></pre></td></tr></table></figure>

<h5 id="delete-x2F-insert-x2F-replace"><a href="#delete-x2F-insert-x2F-replace" class="headerlink" title="delete&#x2F;insert&#x2F;replace"></a>delete&#x2F;insert&#x2F;replace</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 删除hSlice第二个元素5</span></span><br><span class="line">hSlice := []<span class="type">int</span>&#123;<span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>&#125;</span><br><span class="line">hSlice = <span class="built_in">append</span>(hSlice[:<span class="number">2</span>], hSlice[<span class="number">2</span>+<span class="number">1</span>:]...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在hSlice的第3个元素后插入10</span></span><br><span class="line">hSlice = <span class="built_in">append</span>(hSlice[:<span class="number">3</span>], <span class="built_in">append</span>([]<span class="type">int</span>&#123;<span class="number">10</span>&#125;, hSlice[<span class="number">3</span>:]...)...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把hSlice的第3个元素替换为10</span></span><br><span class="line">hSlice = <span class="built_in">append</span>(hSlice[:<span class="number">3</span>], <span class="built_in">append</span>([]<span class="type">int</span>&#123;<span class="number">10</span>&#125;, hSlice[<span class="number">4</span>:]...)...)</span><br></pre></td></tr></table></figure>

<h5 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h5><p>内置函数 copy() 可以将一个数组切片复制到另一个数组切片中，如果加入的两个数组切片不一样大，就会按照其中较小的那个数组切片的元素个数进行复制</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 常规的赋值</span></span><br><span class="line">hSlice := []<span class="type">int</span>&#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>&#125;</span><br><span class="line">jSlice := hSlice</span><br><span class="line">hSlice[<span class="number">5</span>] = <span class="number">1000</span></span><br><span class="line">fmt.Println(hSlice, jSlice) <span class="comment">// [4 5 11 12 13 1000 15 16 17 18] [4 5 11 12 13 1000 15 16 17 18]</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%p, %p\n&quot;</span>, &amp;hSlice, &amp;jSlice) <span class="comment">//0xc0000ba020, 0xc00000c0c0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以看到, jSlice是hSlice的一个copy, 两者虽然地址是不一样, 修改其中一个的元素，都会影响另一个</span></span><br><span class="line"><span class="comment">//这也是因为slice的数据结构的第一个参数是个数据块指针, jSlice与hSlice共享这个数据块指针, 因此会相互影响</span></span><br><span class="line"></span><br><span class="line">jSlice = <span class="built_in">append</span>(jSlice, <span class="number">20000</span>)</span><br><span class="line">fmt.Println(hSlice, jSlice) <span class="comment">// [4 5 11 12 13 1000 15 16 17 18] [4 5 11 12 13 1000 15 16 17 18 20000]</span></span><br><span class="line">fmt.Println(hSlice[<span class="number">10</span>], jSlice[<span class="number">10</span>]) <span class="comment">// panic: runtime error: index out of range [10] with length 10</span></span><br><span class="line"><span class="comment">// 虽然jSlice与hSlice共享这个数据块指针, 但两者的len及cap都是各自的, 在jSlice追加了一个元素之后, 		    len(jSlice）及cap(jSlice)发生了变化, 但这个变化并不会影响hSlice, 因此hSlice不存在hSlice[10]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// copy() 返回实际发生复制的元素个数, destSlice必须分配过空间, 以小的slice长度为准进行copy</span></span><br><span class="line"><span class="comment">// copy( destSlice, srcSlice []T) int</span></span><br><span class="line">hSlice := []<span class="type">int</span>&#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">1000</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">2000</span>&#125;</span><br><span class="line">jSlice := <span class="built_in">make</span>([]<span class="type">int</span>, <span class="number">1</span>)</span><br><span class="line">n := <span class="built_in">copy</span>(jSlice, hSlice) <span class="comment">// n = 1, jslice = [4]</span></span><br><span class="line">kSlice := <span class="built_in">make</span>([]<span class="type">int</span>, <span class="built_in">len</span>(hSlice))</span><br><span class="line">n := <span class="built_in">copy</span>(jSlice, hSlice)  <span class="comment">// n = 11, jslice = [4 5 11 12 13 1000 15 16 17 18 20000]</span></span><br><span class="line">lSlice := <span class="built_in">make</span>([]<span class="type">int</span>, <span class="number">15</span>)</span><br><span class="line"> n := <span class="built_in">copy</span>(jSlice, hSlice) <span class="comment">// n = 11, jslice = [4 5 11 12 13 1000 15 16 17 18 20000 0 0 0 0 0]</span></span><br></pre></td></tr></table></figure>



<p>上面所有的例子都跟slice的底层数据结构有很大的关系, 后续会对slice底层原理写一个详文.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.cnblogs.com/sparkdev/p/10704614.html">https://www.cnblogs.com/sparkdev/p/10704614.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(package与module)</title>
    <url>/2019/11/18/golang-imports/</url>
    <content><![CDATA[<p>module其实是在go1.12才有的产物，在这之前都是由GOPATH来做包管理, GOPATH注定是要退出历史,这里来说一说module跟package的关系.</p>
<span id="more"></span>



<p>通常一个项目(Project)会根据功能拆分很多模块(module), golang 的所有文件都需要指定其所在的包（package）</p>
<h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p>一个包可以分成多个文件, 在编译的时候会被组合成一个文件.</p>
<p>包名跟所在的目录的名字可以不一样(跟文件本身的名字更没关系),  但同一个目录下的所有文件的包名字必须相同</p>
<p>比如这样, second目录下有两个go文件, others.go, second.go, 同时有一个third目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">second</span><br><span class="line">		third</span><br><span class="line">				third.go</span><br><span class="line">		others.go</span><br><span class="line">		second.go</span><br></pre></td></tr></table></figure>

<p><code>cat others.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> others <span class="comment">// 正确的为package api</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mainxx public</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Mainxx</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;Mainxx&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">mainxx</span><span class="params">()</span></span> &#123;</span><br><span class="line">  fmt.Println(<span class="string">&quot;mainxx&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>cat second.go</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> api</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mapi public</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Mapi</span><span class="params">()</span></span> &#123;</span><br><span class="line">  Mainxx()  <span class="comment">//组成包的不同文件相互之间可以直接引用变量和函数，不论是否导出</span></span><br><span class="line">	fmt.Println(<span class="string">&quot;Mapi&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在second.go中声明的包名为api, 这个时候会提示如下错误, 说两个文件的包名不一致. </p>
<p><code>can&#39;t load package: package govars/second: found packages others (others.go) and api (second.go) in /Users/zhoushuke/git_uni/zhoushuke.github.com/learn-golang/golang-vars/second</code></p>
<p>因此可以把两个文件都改成<code>package api</code> 保持一致即可, 这里也说明了<code>包名跟目录名可以不同</code></p>
<p>在main函数中可以这样调用</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;govars/second&quot;</span> <span class="comment">//govars是模块名, 由go mod init时指定</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	api.Mainxx()</span><br><span class="line">	api.Mapi()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>因此: <strong>可以这样理解，import 的是 path（路径），那么go就去那个路径下搜索，搜索当然是查找包名。只不过通常习惯是 文件名和 包名一致</strong></p>
<p>假如现在third.go的文件也被声明为package api</p>
<p>cat third&#x2F;third.go</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> api</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mapi public</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Third</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;third&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>second.go也是package api, 这两者是不会冲突的, 因为third.go是在third目录下, second.go在second目录下,</p>
<p>但是在main函数中调用就会有问题</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;govars/second&quot;</span> <span class="comment">//govars是模块名, 由go mod init时指定</span></span><br><span class="line">  <span class="string">&quot;govars/second/third&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	api.Mapi()</span><br><span class="line">  api.Third() <span class="comment">// 这里本想调用third.go中的third, 但是都是引用包api, 这时编译器就无法区分是调用哪个了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>解决方法就是在import时，为导入的包添加别名</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	second <span class="string">&quot;govars/second&quot;</span>     <span class="comment">//在这里添加别名</span></span><br><span class="line">  third <span class="string">&quot;govars/second/third&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	second.Mapi()</span><br><span class="line">  third.Third() <span class="comment">// 使用别名引用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>在go1.11之后开始支持module, 在1.13后全面铺开，当然也是兼容GOPATH方式</p>
<p>在module的方式之下的代码组织结构会有些不同.</p>
<p>这里主要理使用go mod引用本地包的问题.</p>
<p>最终目录结构:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">learn-golang/golang-mod</span><br><span class="line">		first</span><br><span class="line">				first.go</span><br><span class="line">				go.mod</span><br><span class="line">		second</span><br><span class="line">				second.go</span><br><span class="line">				go.mod</span><br><span class="line">				go.sum</span><br><span class="line">		cmd</span><br><span class="line">				main.go</span><br></pre></td></tr></table></figure>

<p>生成以上目录的命令如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> learn-golang/golang-mod</span><br><span class="line">go mod init github.com/learn-golang/golang-mod</span><br><span class="line"><span class="built_in">mkdir</span> -p learn-golang/golang-mod/&#123;first,second&#125;</span><br><span class="line"><span class="built_in">cd</span> learn-golang/golang-mod/first</span><br><span class="line">go mod init github.com/learn-golang/golang-mod/first</span><br></pre></td></tr></table></figure>

<p>这里go.sum文件暂时不管, <strong>go.sum是一个构建状态跟踪文件。它会记录当前module所有的顶层和间接依赖，以及这些依赖的校验和，从而提供一个可以100%复现的构建过程并对构建对象提供安全性的保证</strong></p>
<p>代码都是非常简单, first.go代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> first</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">First</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;First&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>second.go代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> second</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="string">&quot;github.com/sirupsen/logrus&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Second</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;second&quot;</span>)</span><br><span class="line">	logrus.Info(<span class="string">&quot;logrus.info&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>main.go代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="string">&quot;github.com/learn-golang/golang-mod/first&quot;</span></span><br><span class="line">	<span class="string">&quot;github.com/learn-golang/golang-mod/second&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;start&quot;</span>)</span><br><span class="line">	first.First()</span><br><span class="line">	second.Second()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>生成的first目录下的go.mod文件如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311210142.png"></p>
<p>由于second.go中引用了logrus, 因此在run的时候会进行下载</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311171718.png"></p>
<p>最终生成的second目录下的go.mod文件如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311171812.png"></p>
<p>而在main.go, 引用了first及second, 如果不在最外层的go.mod中使用replace来指定到本地的话, 那go 会直接通过这个地址进行下载, 那当然会报错</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311211142.png"></p>
<p>因此需要在最外层的go.mod中使用replace, 指定到本地的first及second目录</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311171843.png"></p>
<p>从上面可以看出, 虽然second.go中使用了logrus, 但在最外层的go.mod中并没有require logrus，因为main.go中并没有用到, 如果在main.go中使用了logrus, 则在go.mod中也会出现.</p>
<p>运行结果如下: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200311212202.png"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://segmentfault.com/a/1190000018398763">https://segmentfault.com/a/1190000018398763</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(go mod入门)</title>
    <url>/2019/07/09/golang-mod/</url>
    <content><![CDATA[<p>在go初期都是使用GOPATH的方法管理依赖,说实话确认难用, 大部分的golang项目都是使用第三包的包管理工具, 如vendor，也只用过vendor，好在官方推出了go module，在go v1.12后基本stable了，也支持vendor, 赶紧来学习一下.</p>
<span id="more"></span>



<h3 id="GO111MODULE"><a href="#GO111MODULE" class="headerlink" title="GO111MODULE"></a>GO111MODULE</h3><p><code>GO111MODULE</code> 有三个值：<code>off</code>, <code>on</code>和<code>auto（默认值）</code>。</p>
<ul>
<li><p><code>GO111MODULE=off</code>，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。</p>
</li>
<li><p><code>GO111MODULE=on</code>，go命令行会使用modules，而一点也不会去GOPATH目录下查找。</p>
</li>
<li><p>&#96;&#96;&#96;<br>GO111MODULE&#x3D;auto</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  默认值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形：</span><br><span class="line"></span><br><span class="line">  - 当前目录在GOPATH/src之外且该目录包含go.mod文件</span><br><span class="line">  - 当前文件在包含go.mod文件的目录下面。</span><br><span class="line"></span><br><span class="line">修改golang环境变量:</span><br><span class="line"></span><br><span class="line">`go env -w GO111MODULE=&quot;on&quot;`</span><br><span class="line"></span><br><span class="line">当然也可以使用</span><br><span class="line"></span><br><span class="line">`export GO111MODULE=&quot;on&quot;`</span><br><span class="line"></span><br><span class="line">这两种方式是临时的,只对改终端有效,**在go1.12版本之后会自动识别，如果在目录下存在go.mod就会使用module方式**, 这个还是比较人性化的.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 使用</span><br><span class="line"></span><br><span class="line">go modules使用起来也是非常方便</span><br><span class="line"></span><br><span class="line">#### go mod init</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">mkdir govars &amp;&amp; cd govars</span><br><span class="line">go mod init govars   #govars就是模块名, 当然官方推荐使用 aaa.bbb/xxx/yyy的形式</span><br><span class="line">#执行完成之后会在当前目录下生成go.mod及go.sum文件, go.sum是一个构建状态跟踪文件。它会记录当前module所有的顶层和间接依赖，以及这些依赖的校验和，从而提供一个可以100%复现的构建过程并对构建对象提供安全性的保证</span><br></pre></td></tr></table></figure></li>
</ul>
<p><code>cat go.mod</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">module govars </span><br><span class="line">go 1.13</span><br></pre></td></tr></table></figure>

<p>最开始只有这两行，一个指定模块名，另一个就是go的版本, 一般情况下go.mod这个文件也不需要管理.</p>
<p>go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。go toolchain会在各类命令执行时，比如go get、go build、go mod等修改和维护go.mod文件.</p>
<p>比如, 我在main.go中<code>import github.com/MatrixAI/Golang-Demo</code>,那么这个文件就会自动地添加依赖:</p>
<p><code>cat go.mod</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">module govars</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="number">1.13</span></span><br><span class="line"></span><br><span class="line">require (</span><br><span class="line">	github.com/MatrixAI/Golang-Demo v0<span class="number">.0</span><span class="number">.0</span><span class="number">-20200103072352</span><span class="number">-8</span>d075351fdbd</span><br><span class="line">	github.com/pelletier/<span class="keyword">go</span>-toml v1<span class="number">.6</span><span class="number">.0</span> <span class="comment">// indirect indirect表示这个包是间接引用</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>可以通过 <code>go get ./...</code>让它查找依赖，并记录在<code>go.mod</code>文件中</p>
<h4 id="go-mod-replace"><a href="#go-mod-replace" class="headerlink" title="go mod replace"></a>go mod replace</h4><p>当需要使用本地包时, 可以直接通过go mod edit或者直接编译go.mod文件, 来添加&#x2F;替换本地的包</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">module github.com/learn-golang/golang-mod</span><br><span class="line"></span><br><span class="line">require (</span><br><span class="line">	github.com/learn-golang/golang-mod/first v0<span class="number">.0</span><span class="number">.0</span></span><br><span class="line">	github.com/learn-golang/golang-mod/second v0<span class="number">.0</span><span class="number">.0</span></span><br><span class="line">	github.com/sirupsen/logrus v1<span class="number">.4</span><span class="number">.2</span> <span class="comment">// indirect</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用replace进行替换</span></span><br><span class="line">replace github.com/learn-golang/golang-mod/first =&gt; ./first</span><br><span class="line">replace github.com/learn-golang/golang-mod/second =&gt; ./second</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="number">1.13</span></span><br></pre></td></tr></table></figure>

<p><code>indirect</code>表示是间接依赖， 指在当前module中没有直接import，而被当前module使用的第三方module引入的包，相对的顶层依赖就是在当前module中被直接import的包。如果二者规则发生冲突，那么顶层依赖的规则覆盖间接依赖。</p>
<p>在这里<code>github.com/sirupsen/logrus</code>被引入，但当前项目未直接import，所以是一个<strong>间接依赖</strong>，而<code>github.com/learn-golang/golang-mod/first</code>被直接引入和使用，所以它是一个<strong>顶层依赖</strong>。</p>
<p>而我们的replace命令只能管理顶层依赖，对于间接引用是无法使用replace的，因此有时会出现在go.mod中使用了replace到本地包, 但还出现go build时仍然去网上下载的原因。</p>
<p>那么如果我把<code>// indirect</code>去掉了，那么不就变成顶层依赖了吗？答案当然是不行。不管是直接编辑还是<code>go mod edit</code>修改，我们为go.mod添加的信息都只是对<code>go mod</code>的一种提示而已，当运行<code>go build</code>或是<code>go mod tidy</code>时golang会自动更新go.mod导致某些修改无效，简单来说一个包是顶层依赖还是间接依赖，取决于它在本module中是否被直接import，而不是在go.mod文件中是否包含<code>// indirect</code>注释</p>
<h4 id="go-mod-tidy"><a href="#go-mod-tidy" class="headerlink" title="go mod tidy"></a>go mod tidy</h4><p><code>go mod tidy</code>主要是用于为<code>go.mod</code>增加丢失的依赖，删除不需要的依赖等.</p>
<h4 id="go-list"><a href="#go-list" class="headerlink" title="go list"></a>go list</h4><p>显示所有Import库信息</p>
<p><code>go list -m all </code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">govars</span><br><span class="line">github.com/BurntSushi/toml v0.3.1</span><br><span class="line">github.com/MatrixAI/Golang-Demo v0.0.0-20200103072352-8d075351fdbd</span><br><span class="line">github.com/davecgh/go-spew v1.1.1</span><br><span class="line">github.com/pelletier/go-toml v1.6.0</span><br><span class="line">gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405</span><br><span class="line">gopkg.in/yaml.v2 v2.2.4</span><br></pre></td></tr></table></figure>

<p>也可查看json详细格式 <code>go list -m all -json</code></p>
<p>当然还有一些其它的操作, 以后用到了再深入, 从上手的效果来看，确实比GOPATH、vendor省事, 很多事要可以交给go module去做了.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://tonybai.com/2018/07/15/hello-go-module/">https://tonybai.com/2018/07/15/hello-go-module/</a></li>
<li><a href="https://www.cnblogs.com/apocelipes/p/10295096.html">https://www.cnblogs.com/apocelipes/p/10295096.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(使用new跟make)</title>
    <url>/2019/08/11/golang-new-make/</url>
    <content><![CDATA[<p>golang中经常会用到make来初始化变量, 另外还有new也可以用来定义及初始化变量,这两者有啥区别呢?</p>
<span id="more"></span>



<h3 id="new-T"><a href="#new-T" class="headerlink" title="new(T)"></a>new(T)</h3><p><code>func new(Type) *Type</code></p>
<p>new(T)会为T类型的新项目，分配被<strong>置零</strong>的存储，并且返回它的地址，一个类型为*T的值</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">p1 := <span class="built_in">new</span>(<span class="type">int</span>)</span><br><span class="line">fmt.Printf(<span class="string">&quot;p1 --&gt; %#v \n &quot;</span>, p1) <span class="comment">//(*int)(0xc42000e250) </span></span><br><span class="line">fmt.Printf(<span class="string">&quot;p1 point to --&gt; %#v \n &quot;</span>, *p1) <span class="comment">//0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> p2 *<span class="type">int</span></span><br><span class="line">i := <span class="number">0</span></span><br><span class="line">p2 = &amp;i</span><br><span class="line">fmt.Printf(<span class="string">&quot;p2 --&gt; %#v \n &quot;</span>, p2) <span class="comment">//(*int)(0xc42000e278) </span></span><br><span class="line">fmt.Printf(<span class="string">&quot;p2 point to --&gt; %#v \n &quot;</span>, *p2) <span class="comment">//0</span></span><br></pre></td></tr></table></figure>

<p>一般情况下, 能够使用new的方式也可以使用下面这种方式，效果是一样的.</p>
<p><strong>new不常用</strong></p>
<h3 id="make-T"><a href="#make-T" class="headerlink" title="make(T)"></a>make(T)</h3><p>make只能用于初始化slice, map, channel这三种数据类型.</p>
<p>slice 的零值是 nil，使用 make 之后 slice 是一个初始化的 slice，即 slice 的长度、容量、底层指向的 array 都被 make 完成初始化，此时 slice 内容被类型 int 的零值填充，形式是 [0 0 0]，map 和 channel 也是类似的</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Slice</span></span><br><span class="line"><span class="keyword">var</span> aSlice []<span class="type">int</span></span><br><span class="line">fmt.Println(aSlice == <span class="literal">nil</span>, aSlice) <span class="comment">// true, nil</span></span><br><span class="line">bSlice := <span class="built_in">make</span>([]<span class="type">int</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment">// 定义一个len=1, cap=2的slice</span></span><br><span class="line">fmt.Println(bSlice == <span class="literal">nil</span>, bSlice) <span class="comment">// false, [0, 0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// map</span></span><br><span class="line">bMap := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">int</span>]<span class="type">int</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// channel </span></span><br><span class="line">aChan := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>, <span class="number">100</span>) <span class="comment">// 长度为100的channel</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h3><p>主要的区别分为两点:</p>
<ol>
<li>两者都是内建函数</li>
<li>make的作用是初始化内置的数据结构，只能用于slice, map, channel这3种结构, 返回对应的<strong>数据类型</strong></li>
<li>new只接收一种类型, 根据传入的类型在堆上分配一片内存空间并返回指向这片内存空间的<strong>指针</strong></li>
<li>在内存上分配上很难说使用两者golang编译器会如何进行分配, 关于内存分配可再深入研究一番.</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-make-and-new/">https://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-make-and-new/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(string)</title>
    <url>/2019/09/13/golang-string/</url>
    <content><![CDATA[<p>字符串在所有语言中可以说是用的最频繁的一种数据结构, 在golang中当然也存在.虽然字符串往往被看做一个整体，但是实际上字符串是一片连续的内存空间，我们也可以将它理解成一个由字符组成的数组</p>
<span id="more"></span>



<p>golang中的字符串是个只读的, 它实际上是由字符组成的数组，会占用一片连续的内存空间， 这里的只读是说无法直接改变字符串, 在运行时我们其实还是可以将这段内存拷贝到堆或者栈上，将变量的类型转换成 <code>[]byte</code> 之后就可以进行，修改后通过类型转换就可以变回 <code>string</code>，Go 语言只是不支持直接修改 <code>string</code> 类型变量的内存空间</p>
<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> StringHeader <span class="keyword">struct</span> &#123;</span><br><span class="line">	Data <span class="type">uintptr</span></span><br><span class="line">	Len  <span class="type">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="字符串定义"><a href="#字符串定义" class="headerlink" title="字符串定义"></a>字符串定义</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用&quot;&quot;, 如果字符串内部出现双引号，需要使用 \ 符号转义</span></span><br><span class="line">aString := <span class="string">&quot;我\&quot;是\&quot;中国人&quot;</span></span><br><span class="line">cString := <span class="string">&quot;我\n是中国人&quot;</span> <span class="comment">// 使用显式的换行也是可以的</span></span><br><span class="line">dString := <span class="string">&quot;我</span></span><br><span class="line"><span class="string">是中国人&quot;</span> <span class="comment">// 这样是不行的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用``, 可以由多行组成，但不支持转义，并且可以包含除了反引号外其他所有字符</span></span><br><span class="line">bString := <span class="string">`我</span></span><br><span class="line"><span class="string">是</span></span><br><span class="line"><span class="string">中国人</span></span><br><span class="line"><span class="string">！@</span></span><br><span class="line"><span class="string">`</span></span><br><span class="line">fmt.Println(aString, <span class="string">&quot;\n&quot;</span>, bString)</span><br><span class="line">fmt.Println(bString[<span class="number">1</span>]) <span class="comment">// 136 这里输出的是unicode码</span></span><br></pre></td></tr></table></figure>



<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 访问</span></span><br><span class="line"><span class="comment">// 由于字符串的底层是个字节数组, 因此也是通过下标进行访问, 如果下标越界会提示index out of range</span></span><br><span class="line">aString[<span class="number">1</span>]</span><br><span class="line"><span class="comment">// 如果强制去修改字符串的元素会提示 cannot assign to xxx</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 求长度</span></span><br><span class="line"><span class="comment">// 直接使用len(bString)即可求得长度</span></span><br><span class="line"><span class="comment">// 字符串是 UTF-8 字符的一个序列（当字符为 ASCII 码时则占用 1 个字节，其它字符根据需要占用 2-4 个字节）</span></span><br><span class="line"></span><br><span class="line">bString := <span class="string">&quot;我是中国人&quot;</span>  <span class="comment">// 长度15, 一个汉字占3个字节</span></span><br><span class="line">cString := <span class="string">&quot;我是\n中国人&quot;</span> <span class="comment">// 长度16, \n是可以用ascii表示, 因此只占用一个字节</span></span><br><span class="line">fString := <span class="string">&quot;youareme&quot;</span>    <span class="comment">// 长度8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 拼接</span></span><br><span class="line">dString := aString + bString</span><br><span class="line">dString += aString <span class="comment">// == dString = dString + aString</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 拷贝</span></span><br><span class="line">eString := aString</span><br><span class="line"><span class="comment">// 但是在正常情况下，运行时会调用 copy 将输入的多个字符串拷贝到目标字符串所在的内存空间中，新的字符串是一片新的内存空间，与原来的字符串也没有任何关联，一旦需要拼接的字符串非常大，拷贝带来的性能损失就是无法忽略的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 切片</span></span><br><span class="line"><span class="comment">// 由于字符串底组是字节数组, 因此也支持切片操作</span></span><br><span class="line"><span class="comment">// gString跟eString指向的底层数组为同一个</span></span><br><span class="line">gString := eString[<span class="number">2</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>



<h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">bString := <span class="string">&quot;abc我是中国人&quot;</span></span><br><span class="line"><span class="keyword">for</span> i, v := <span class="keyword">range</span> bString &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;(%d, %x) &quot;</span>, i, v)</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println()</span><br><span class="line"><span class="comment">// (0, 61) (1, 62) (2, 63) (3, 6211) (6, 662f) (9, 4e2d) (12, 56fd) (15, 4eba) </span></span><br><span class="line"><span class="comment">// 遍历bString,取出的是Unicode码，可以看到“我”这个汉字的Unicode是 6211，它是从第3个开始的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, v := <span class="keyword">range</span> []<span class="type">byte</span>(bString) &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;(%d, %x) &quot;</span>, i, v)</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println()</span><br><span class="line"><span class="comment">// (0, 61) (1, 62) (2, 63) (3, e6) (4, 88) (5, 91) (6, e6) (7, 98) (8, af) (9, e4) (10, b8) (11, ad) (12, e5) (13, 9b) (14, bd) (15, e4) (16, ba) (17, ba) </span></span><br><span class="line"><span class="comment">// 把字符串转为[]bype数组，打印出utf-8的字节，可以看到“我”这个汉字占了e6 88 91三个16进制字节。看到一个汉字占3个字节，一个英文字母占1个字节，utf-8是可变宽度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, v := <span class="keyword">range</span> []<span class="type">rune</span>(bString) &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;(%d, %c) &quot;</span>, i, v)</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println()</span><br><span class="line"><span class="comment">// (0, a) (1, b) (2, c) (3, 我) (4, 是) (5, 中) (6, 国) (7, 人)</span></span><br><span class="line"><span class="comment">// 将字符串转为[]rune 也就是int32的别名，占4个字节。打印出了abc好好学习。</span></span><br></pre></td></tr></table></figure>



<h3 id="strings包的使用"><a href="#strings包的使用" class="headerlink" title="strings包的使用"></a>strings包的使用</h3><p>strings包中包含非常多的实用函数, 这里挑几个比较常用的记录一下</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 判断给定字符串s中是否包含子串substr, 找到返回true, 找不到返回false</span></span><br><span class="line">fmt.Println(<span class="string">&quot;包含子串返回：&quot;</span>, strings.Contains(<span class="string">&quot;oldboy&quot;</span>, <span class="string">&quot;boy&quot;</span>)) <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 在字符串s中查找sep所在的位置, 返回位置值, 找不到返回-1, 如果存在多个, 只返回第一个匹对的位置</span></span><br><span class="line">fmt.Println(<span class="string">&quot;存在返回第一个匹配字符的位置：&quot;</span>, strings.Index(<span class="string">&quot;heloboyboy&quot;</span>, <span class="string">&quot;boy&quot;</span>))  <span class="comment">// 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计给定子串sep的出现次数, sep为空时, 返回字符串的长度 + 1</span></span><br><span class="line">fmt.Println(<span class="string">&quot;子字符串出现次数：&quot;</span>, strings.Count(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;o&quot;</span>)) <span class="comment">// 2</span></span><br><span class="line">fmt.Println(<span class="string">&quot;子字符串为空时, 返回：&quot;</span>, strings.Count(<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;&quot;</span>)) <span class="comment">// 6</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重复s字符串count次, 最后返回新生成的重复的字符串</span></span><br><span class="line">fmt.Println(strings.Repeat(<span class="string">&quot;嘀嗒&quot;</span>, <span class="number">4</span>), <span class="string">&quot;时针它不停在转动&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在s字符串中, 把old字符串替换为new字符串，n表示替换的次数，如果n&lt;0会替换所有old子串</span></span><br><span class="line">fmt.Println(strings.Replace(<span class="string">&quot;hel hel hel&quot;</span>, <span class="string">&quot;l&quot;</span>, <span class="string">&quot;llo&quot;</span>, <span class="number">2</span>)) <span class="comment">// hello hello hel</span></span><br><span class="line">fmt.Println(strings.Replace(<span class="string">&quot;hel hel hel&quot;</span>, <span class="string">&quot;l&quot;</span>, <span class="string">&quot;llo&quot;</span>, <span class="number">-1</span>)) <span class="comment">// hello hello hello</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除在s字符串的头部和尾部中由cutset指定的字符, 并返回删除后的字符串</span></span><br><span class="line">fmt.Println(strings.Trim(<span class="string">&quot;   hello   &quot;</span>, <span class="string">&quot; &quot;</span>)) <span class="comment">// hello</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 给定字符串转换为英文标题的首字母大写的格式(不能正确处理unicode标点)</span></span><br><span class="line">fmt.Println(strings.Title(<span class="string">&quot;It is never too late to learn.&quot;</span>)) </span><br><span class="line"><span class="comment">// 返回将所有字母都转为对应的小写版本的拷贝</span></span><br><span class="line">fmt.Println(strings.ToLower(<span class="string">&quot;It Is Never Too Late To Learn.&quot;</span>)) </span><br><span class="line"><span class="comment">// 返回将所有字母都转为对应的大写版本的拷贝</span></span><br><span class="line">fmt.Println(strings.ToUpper(<span class="string">&quot;It is never too late to learn.&quot;</span>)) </span><br><span class="line"><span class="comment">// It Is Never Too Late To Learn.</span></span><br><span class="line"><span class="comment">// it is never too late to learn.</span></span><br><span class="line"><span class="comment">// IT IS NEVER TOO LATE TO LEARN.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断字符串是否包含前缀prefix，大小写敏感</span></span><br><span class="line">fmt.Println(<span class="string">&quot;前缀是以hello开头的：&quot;</span>, strings.HasPrefix(<span class="string">&quot;helloworld&quot;</span>, <span class="string">&quot;hello&quot;</span>)) <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断s是否有后缀字符串suffix，大小写敏感</span></span><br><span class="line">fmt.Println(<span class="string">&quot;后缀是以world开头的：&quot;</span>, strings.HasSuffix(<span class="string">&quot;helloworld&quot;</span>, <span class="string">&quot;world&quot;</span>)) <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用去掉s中出现的sep的方式进行分割，会分割到结尾，并返回生成的所有片段组成的切片（每一个sep都会进行一次切割，即使两个sep相邻，也会进行两次切割）。如果sep为空字符，Split会将s切分成每一个unicode码值一个字符串。</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;%q\n&quot;</span>, strings.Split(<span class="string">&quot;a mountain a temple&quot;</span>, <span class="string">&quot;a &quot;</span>)) <span class="comment">// [&quot;&quot; &quot;mountain &quot; &quot;temple&quot;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回将字符串按照空白（unicode.IsSpace确定，可以是一到多个连续的空白字符）分割的多个字符串。如果字符串全部是空白或者是空字符串的话，会返回空切片。</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;Fields are: %q\n&quot;</span>, strings.Fields(<span class="string">&quot; Linux Python Golang  Java &quot;</span>))</span><br><span class="line"><span class="comment">// [&quot;Linux&quot; &quot;Python&quot; &quot;Golang&quot; &quot;Java&quot;]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><ul>
<li>字符串是不可变值类型，内部⽤指针指向 UTF-8 字节数组。</li>
<li>默认值是空字符串 “”。</li>
<li>⽤索引号访问某字节，如 s[i], 取出的是字节，不是字符</li>
<li><strong>由于字符串是只读的, 因此不能⽤序号获取字节元素指针， &amp;s[i] ⾮法, 提示cannot take the address of xxx,原因在于如果支持取地址操作,则就可以使用指针对改地址指向的值进行修改, 这就违背了字符串只读的前提</strong>。</li>
<li>不可变类型，⽆法修改字节数组。使用[]rune进行修改是重新分配内存，并复制字节数组</li>
<li>字符串可以用&#x3D;&#x3D;和&lt;进行比较</li>
</ul>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://draveness.me/golang/docs/part2-foundation/ch03-datastructure/golang-string/">https://draveness.me/golang/docs/part2-foundation/ch03-datastructure/golang-string/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(golang搭建邮件系统解决Unrecognized authentication type)</title>
    <url>/2020/04/20/golang-restfulAPI-mail-server/</url>
    <content><![CDATA[<p>最近在搞一个邮件RestfulAPI的邮件系统, 通过接口可直接发送邮件, 后端使用的是outlook,在测试过程发现会提示 Unrecognized authentication type错误, 折腾一下终于解决了, 记录一下.</p>
<span id="more"></span>



<p>刚开始的时候使用的是PlainAuth类型的认证方式,  因为之前的一个账号一直使用，没出过问题, 到这里就提示以下错误</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">504 5.7.4 Unrecognized authentication type [xxx.prod.partner.outlook.cn]</span><br></pre></td></tr></table></figure>

<p>这个意思很明确, 不能识别的认证方式，PlainAuth这里简单说一下，PlainAuth的是大部分的邮件系统都支持的认证, 但由于是明文传输, 安全性不太好, 因此很多公司默认都关闭了.</p>
<p>可以通过telnet命令来查看一下邮件服务器支持的验证方式:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">telnet yourmailserver 25</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200421141715.png"></p>
<p>可以看到在这里是不支持<code>PlainAuth</code>这种认证方式的, 因此可以使用<code>STARTTLS</code>这种支持的方式</p>
<p>好在大部分情况下都支持我们使用自定义的认证.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> loginAuth <span class="keyword">struct</span> &#123;</span><br><span class="line">	username, password <span class="type">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LoginAuth</span><span class="params">(username, password <span class="type">string</span>)</span></span> smtp.Auth &#123;</span><br><span class="line">	<span class="keyword">return</span> &amp;loginAuth&#123;username, password&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a *loginAuth)</span></span> Start(server *smtp.ServerInfo) (<span class="type">string</span>, []<span class="type">byte</span>, <span class="type">error</span>) &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;LOGIN&quot;</span>, []<span class="type">byte</span>(a.username), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a *loginAuth)</span></span> Next(fromServer []<span class="type">byte</span>, more <span class="type">bool</span>) ([]<span class="type">byte</span>, <span class="type">error</span>) &#123;</span><br><span class="line">	<span class="keyword">if</span> more &#123;</span><br><span class="line">		<span class="keyword">switch</span> <span class="type">string</span>(fromServer) &#123;</span><br><span class="line">		<span class="keyword">case</span> <span class="string">&quot;Username:&quot;</span>:</span><br><span class="line">			<span class="keyword">return</span> []<span class="type">byte</span>(a.username), <span class="literal">nil</span></span><br><span class="line">		<span class="keyword">case</span> <span class="string">&quot;Password:&quot;</span>:</span><br><span class="line">			<span class="keyword">return</span> []<span class="type">byte</span>(a.password), <span class="literal">nil</span></span><br><span class="line">		<span class="keyword">default</span>:</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, errors.New(<span class="string">&quot;Unkown fromServer&quot;</span>)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送邮件</span></span><br><span class="line"><span class="comment">// auth := smtp.PlainAuth(&quot;&quot;, user, passwd, host)</span></span><br><span class="line">auth := LoginAuth(config.GlobalConfig.User, config.GlobalConfig.Password)</span><br><span class="line"></span><br><span class="line">err := smtp.SendMail(mailServer, auth, config.GlobalConfig.User, to, []<span class="type">byte</span>(msg)</span><br></pre></td></tr></table></figure>

<p>将auth 从PlainAuth切换至自定义的auth即可.</p>
<p>目前该小工具已经开源, <a href="https://github.com/zhoushuke/sendxmail">github</a>在这里, 感兴趣的可以查看<code>README.md</code></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://being23.github.io/2015/09/17/%E4%BD%BF%E7%94%A8golang%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/]">https://being23.github.io/2015/09/17/%E4%BD%BF%E7%94%A8golang%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/]</a>(<a href="https://being23.github.io/2015/09/17/%E4%BD%BF%E7%94%A8golang%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/">https://being23.github.io/2015/09/17/使用golang发送邮件/</a></li>
<li><a href="https://github.com/zhoushuke/sendxmail">https://github.com/zhoushuke/sendxmail</a></li>
<li><a href="http://blog.fatedier.com/2015/08/20/use-smtp-to-sendmail-in-go-and-some-problems-with-smtp/">http://blog.fatedier.com/2015/08/20/use-smtp-to-sendmail-in-go-and-some-problems-with-smtp/</a></li>
<li><a href="https://blog.51cto.com/linxucn/837365">https://blog.51cto.com/linxucn/837365</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(变量)</title>
    <url>/2019/08/26/golang-vars/</url>
    <content><![CDATA[<span id="more"></span>



<h3 id="变量引用"><a href="#变量引用" class="headerlink" title="变量引用"></a>变量引用</h3><p>组成一个package的多个文件，编译后实际上和一个文件类似，组成包的不同文件相互之间可以直接引用变量和函数，不论是否导出</p>
<p><strong>go中对于变量(函数也是如此)使用首字母大小写来区别是否可引用. 首字母大写则能被其它包引用</strong>(不完全正常, 目前先这么理解).</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cat main.go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Aa public var</span></span><br><span class="line"><span class="keyword">var</span> Aa <span class="type">int</span> <span class="comment">//包变量, 同时对外是public的，初始值跟变量初始值相同， int为0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	a = <span class="number">1</span> &lt;&lt; <span class="literal">iota</span></span><br><span class="line">	b</span><br><span class="line">	_</span><br><span class="line">	d</span><br><span class="line">	e</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  fmt.Println(Aa) <span class="comment">// 0</span></span><br><span class="line">  Aa := <span class="number">789</span>  <span class="comment">//main中的局部变量，注意这里使用了 :=</span></span><br><span class="line">	fmt.Println(Aa) <span class="comment">// 789</span></span><br><span class="line">	fmt.Println(a, b, d, e)</span><br><span class="line">	mainxx()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cat other.go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">mainxx</span><span class="params">()</span></span> &#123;</span><br><span class="line">	aa := <span class="number">123</span></span><br><span class="line">	fmt.Println(aa, Aa)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在命令行下运行<code>go run . </code></p>
<p>输出很正常 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">789</span><br><span class="line">1 2 8 16</span><br><span class="line">123 0</span><br></pre></td></tr></table></figure>



<h3 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h3><h4 id="var"><a href="#var" class="headerlink" title="var"></a>var</h4><p>var比较直接, 使用没有限制</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">  <span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> aa = <span class="number">100</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">fu</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> aa = <span class="number">200</span></span><br><span class="line">  fmt.Println(aa)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  aa := <span class="number">300</span></span><br><span class="line">  fmt.Println(aa)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>####:&#x3D;</p>
<p>:&#x3D;个人觉得是个比较好的设计, 干练简短, 虽然在开开始学的时候常常会用错</p>
<p>:&#x3D;用于<strong>声明变量并初始化, 且只能用在函数中</strong>, 如果你在函数外使用:&#x3D;会提示</p>
<p><code>non-declaration statement outside function body</code></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> xx <span class="type">int</span></span><br><span class="line">	fmt.Println(xx)</span><br><span class="line">  xx := <span class="number">100</span>  <span class="comment">// 会提示错误: no new variables on left side of :</span></span><br><span class="line">  <span class="comment">// xx = 100</span></span><br><span class="line">	fmt.Println(xx)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的错误提示，说没有一个新的变量, 原因就在于xx之前已经被定义了, 不能重复定义. 所以这里只能使用赋值</p>
<p><code>a := 100</code>这句话是声明了变量a, 且给a赋值100</p>
<h4 id="make"><a href="#make" class="headerlink" title="make"></a>make</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// make slice、map和channel，并且返回一个有初始值(非零)的类型</span></span><br><span class="line"><span class="comment">// 声明一个slice</span></span><br><span class="line">aSlice := <span class="built_in">make</span>([]<span class="type">string</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line"><span class="comment">// 声明一个map</span></span><br><span class="line">aMap := <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>)</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure>

<h4 id="new"><a href="#new" class="headerlink" title="new"></a>new</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// new 用于分配了零值填充的T类型的内存空间, 它返回了一个指针，指向新分配的类型T的零值</span></span><br><span class="line"><span class="comment">// 申明一个整体,并返回一个指向整型的指针</span></span><br><span class="line">aInt := <span class="built_in">new</span>(<span class="type">int</span>)</span><br><span class="line">fmt.Println(*aInt) <span class="comment">// 0</span></span><br><span class="line">*aInt = <span class="number">2</span></span><br><span class="line">fmt.Println(*aInt) <span class="comment">// 2</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure>

<p>make跟new是两个用的比较多, 在这里只是简单介绍下, 后面会单独学习它们的区别</p>
<h3 id="常量的定义"><a href="#常量的定义" class="headerlink" title="常量的定义"></a>常量的定义</h3><h4 id="const"><a href="#const" class="headerlink" title="const"></a>const</h4><p>const跟其它语言的用法一样, 都是声明常量, 常量就是不能被改变的变量(当然通过常量指针还是可以修改的)</p>
<p>const 可以在任何地方使用</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  aa = <span class="number">100</span>  <span class="comment">//这里不能使用:=</span></span><br><span class="line">  bb = <span class="number">200</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	a = <span class="number">1</span> &lt;&lt; <span class="literal">iota</span></span><br><span class="line">	b</span><br><span class="line">	_</span><br><span class="line">	d</span><br><span class="line">	e</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">const</span> main_a = <span class="number">100</span></span><br><span class="line">  main_a = <span class="number">200</span> <span class="comment">// 如果试图修改const常量会提示: cannot assign to main_a</span></span><br><span class="line">  fmt.Println(aa, bb, a, b, d, e, main_a)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="iota"><a href="#iota" class="headerlink" title="iota"></a>iota</h4><p>iota在const中比较特别, 因此特意提出来说一下</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	mutexLocked = <span class="number">1</span> &lt;&lt; <span class="literal">iota</span></span><br><span class="line">	mutexWoken</span><br><span class="line">	mutexStarving</span><br><span class="line">  _</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 这是一行注释</span></span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span></span><br><span class="line">  xzy</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">fmt.Println(mutexLocked, mutexWoken, mutexStarving, mutexWaiterShift, xyz)</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">4</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>

<p>首先，iota是可以参与运算的， iota带表达式的都将应用到下面的常量</p>
<p>比如上面的1&lt;&lt; iota，此时等于0, 整个表达式的值为1</p>
<p>每二行, iota的值等于1了, 1 &lt;&lt; 1, 整个表达式的值2</p>
<p>以此类推.</p>
<p><strong>iota在const关键字出现时将被重置为0(const内部的第一行之前)，从第一行开始, 每新增一行常量声明(不包括空行及注释)将使iota计数一次(iota可理解为const语句块中的行索引)。使用iota能简化定义，主要用于定义枚举，而且很有用</strong></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> ( <span class="comment">// iota在const内部的第一行之前就被初始化为0</span></span><br><span class="line">	mutexLocked = <span class="number">0</span> <span class="comment">// iota=0</span></span><br><span class="line">	mutexWoken = <span class="number">2</span> <span class="comment">// iota=1</span></span><br><span class="line">	mutexStarving = <span class="number">3</span> <span class="comment">// iota=2</span></span><br><span class="line">	_  <span class="comment">// iota=3</span></span><br><span class="line">	</span><br><span class="line">  <span class="comment">// 这是一行注释</span></span><br><span class="line">	mutexWaiterShift = <span class="literal">iota</span>   <span class="comment">// 所以当到这一行时，共增加了5行，索引从0开始，所以iota为4</span></span><br><span class="line">	xyz</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="literal">iota</span> := <span class="number">33</span>  <span class="comment">//iota不是关键字， 因此也可以被当成一个普通的变量对待，跟其它变量没有区别</span></span><br><span class="line">	fmt.Println(mutexLocked, mutexWoken, mutexStarving, mutexWaiterShift, xyz)</span><br><span class="line">  fmt.Println(<span class="literal">iota</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line"><span class="number">33</span></span><br></pre></td></tr></table></figure>

<p>const及iota可参考<a href="https://izsk.me/2019/08/11/golang-const-iota/">这里</a></p>
<h3 id="常见基本类型零值"><a href="#常见基本类型零值" class="headerlink" title="常见基本类型零值"></a>常见基本类型零值</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 常见的基本类型零值</span></span><br><span class="line"><span class="type">int</span>     <span class="number">0</span></span><br><span class="line"><span class="type">int8</span>    <span class="number">0</span></span><br><span class="line"><span class="type">int32</span>   <span class="number">0</span></span><br><span class="line"><span class="type">int64</span>   <span class="number">0</span></span><br><span class="line"><span class="type">uint</span>    <span class="number">0x0</span></span><br><span class="line"><span class="type">rune</span>    <span class="number">0</span> <span class="comment">//rune的实际类型是 int32</span></span><br><span class="line"><span class="type">byte</span>    <span class="number">0x0</span> <span class="comment">// byte的实际类型是 uint8</span></span><br><span class="line"><span class="type">float32</span> <span class="number">0</span> <span class="comment">//长度为 4 byte</span></span><br><span class="line"><span class="type">float64</span> <span class="number">0</span> <span class="comment">//长度为 8 byte</span></span><br><span class="line"><span class="type">bool</span>    <span class="literal">false</span></span><br><span class="line"><span class="type">string</span>  <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>有个比较特殊的零值<code>nil</code>, 这个必须要单独说.</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>如果把一个包分成多个文件，在IDE如vscode中run时，可提示如下错误</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200310124200.png"></p>
<p>最开始的时候一脸茫然, 很明显是因为在ide运行的时候，执行的命令是<code>go run main.go</code>, 只传入了一个main.go,而没有传递others.go, 因此找不到mainxx(), 在命令行下执行也会报同样的错误. 需要运行<code>go run .</code>或者<code>go run *.go</code>,另外, 最好不要形成循环引用, 如果两个文件需要相互引用的话, 引用第三方是个推荐的办法.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://blog.wolfogre.com/posts/golang-iota/">https://blog.wolfogre.com/posts/golang-iota/</a></li>
<li><a href="https://izsk.me/2019/08/11/golang-const-iota/">https://izsk.me/2019/08/11/golang-const-iota/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(grafana资源备份工具)</title>
    <url>/2019/11/15/grafana-source-backup-tools/</url>
    <content><![CDATA[<p>目前，Grafana是使用statefulset的形式部署的, 单实例，底层存储也没有使用分布式容灾, 里面有很配置好的模板，整天提醒吊胆地怕这个实例磁盘出问题,在不给分配更多资源的情况下，必须要找个合适的备份方案, 当然，可以直接备份整个目录, 这里介绍下使用kubernetes的cronjob进行定时备份</p>
<span id="more"></span>

<h3 id="Grafana-backup-tool"><a href="#Grafana-backup-tool" class="headerlink" title="Grafana-backup-tool"></a><strong>Grafana-backup-tool</strong></h3><p>在github上找到一个比较好用的grafana数据的备份工具, 用python写的, 地址在<a href="https://github.com/ysde/grafana-backup-tool">这里</a>, ReadMe写的挺详细，一看就明白, 这个工具能备份 datasource, dashboard及配置的alert. 需要备份的也就这些数据了,其它的数据用处不大,</p>
<p>也有直接的docker 镜像可用，我们发布到kubernetes中,在发布之前需要一个Grafana的token来访问数据</p>
<h3 id="Grafana-Token"><a href="#Grafana-Token" class="headerlink" title="Grafana Token"></a><strong>Grafana Token</strong></h3><p>新建一个grafana token，这个token需要配置到备份工具脚本中，使用这个token通过api访问grafana的数据</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119164352.png"></p>
<p>在新建的时候可以限定角色, 需要注意的是这个token只有在第一次新建的时候会显示，所以最好保存在其它地方，要不然后续都无法再显示.</p>
<h3 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a><strong>CronJob</strong></h3><p>将该容器在Kubernetes集群中发布成CronJob定时执行, 详细的CronJob，大家可参考<a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">这里</a></p>
<p>yaml文件中需要指定两个环境变量</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119165403.png"></p>
<p>同时，最好也指定successfulJobsHistoryLimit 及failedJobsHistoryLimit, 不然会遗留很多的pod.</p>
<p>指定备份目录,因为我这里使用了NodeName直接让该pod定向调度到指定机器，每次执行完之后都会在该机器的目录下生成备份数据，之后通过rsync工具同步到其它机器上，实现灾备.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119165704.png"></p>
<p>确认部署成功后:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119172338.png"></p>
<p>至此, 每天的22:33即会备份一次数据.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/ysde/grafana-backup-tool">grafana-backup-tool</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>golang学习(zap二次封装, 支持动态修改日志级别)</title>
    <url>/2019/08/14/golang-zap/</url>
    <content><![CDATA[<p>golang到现在还没出现一款类似log4j这么强大的日志打印工具, 在调研了一番之后发现，现在用的最多的是logrus跟zap, github上的star也比较多, logrus使用起来比较简单, 不需要做过多的设置, 导入就能使用, 非常方便, 而zap则侧重在高性能方面, 基本所有的参数都可配置,不过需要的代码比logrus多. </p>
<span id="more"></span>



<p>对于日志打印工具的需求其实非常简单, <strong>日志轮转，多点打印,  json格式</strong>就可以了, logrus跟zap这些都支持, 如果是从易用性上来, 优先会选择logrus, 因为基本不需要写代码.</p>
<p>但是当看到<strong>zap支持在运行时动态修改日志级别</strong>这个功能时, 我眼前一亮, 而且在看完zap的性能比logrus高一个数量级时, 果断地选择了zap.</p>
<p>因为zap是高度可配置的, 文档也不是很完善, 做个简单的二次封装是个不错的选择</p>
<p>这样在不需要重启应用的情况下就能修改日志级别, 还是很爽的.</p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>Demo在<a href="https://github.com/zhoushuke/change-zap-LogLevel-by-restfulAPI-in-Runtime">这里</a></p>
<p>当然封装的也比较简单, 有些参数是可以抽离出来的</p>
<p>下载下来后直接运行<code>go run main.go</code></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>启动时默认的日志级别被设置为<strong>info</strong>，因此不会打印<code>debug</code>日志</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304172512160.png"></p>
<p>可以通过以下接口查看: <code>curl localhost:9090/handle/level</code> 代码中已改成<code>curl localhost:9090/change/level</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304172527923.png"></p>
<p>把日志级别修改为<code>debug</code>(向下修改)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304173749638.png"></p>
<p>查看日志, 发现<code>debug</code>日志打印出来了, 说明动态修改生效</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304173732489.png"></p>
<p>再把日志级别修改为<code>error</code>(向上修改)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304173914389.png"></p>
<p>查看日志, 只有<code>error</code>的日志打印出来</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304173830840.png"></p>
<p>最后把日志级别修改为<code>panic</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304174008960.png"></p>
<p>发现不打印日志了，因为main中最高级别为<code>warn</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200304173952284.png"></p>
<p><strong>总结</strong>: 现在很多成熟的日志工具都支持在运行时通过http接口修改日志级别，比如Log4j, 不过log4j一般都会使用在java的项目上，golang下zap也原生提示了这个能力, 还是挺有用的, 生产环境下可以设置一个比较高的打印级别, 比如warn，这样可以节省大量磁盘空间同时提高效率, 如果发现问题，只需要通过http接口把日志级别调低, 很方便的可以不停应用排查问题，非常方便. </p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>在测试时发现出现有以下的打印格式, 发现msg字段的格式有点诡异, 这样不太利于做日志分析.</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;level&quot;</span><span class="punctuation">:</span><span class="string">&quot;info&quot;</span><span class="punctuation">,</span><span class="attr">&quot;time&quot;</span><span class="punctuation">:</span><span class="string">&quot;2020-03-04T16:41:12.828+0800&quot;</span><span class="punctuation">,</span><span class="attr">&quot;caller&quot;</span><span class="punctuation">:</span><span class="string">&quot;github/demo.go:29&quot;</span><span class="punctuation">,</span><span class="attr">&quot;msg&quot;</span><span class="punctuation">:</span><span class="string">&quot;无法获取网址&#123;url 15 0 http://www.baidu.com &lt;nil&gt;&#125; &#123;attempt 11 3  &lt;nil&gt;&#125; &#123;backoff 8 1000000000  &lt;nil&gt;&#125;&quot;</span><span class="punctuation">,</span><span class="attr">&quot;serviceName&quot;</span><span class="punctuation">:</span><span class="string">&quot;main&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>在main.go中对应的代码</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">logger.Info(<span class="string">&quot;无法获取网址&quot;</span><span class="punctuation">,</span></span><br><span class="line">    		zap.String(<span class="string">&quot;url&quot;</span><span class="punctuation">,</span> <span class="string">&quot;http://www.baidu.com&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">    		zap.Int(<span class="string">&quot;attempt&quot;</span><span class="punctuation">,</span> <span class="number">3</span>)<span class="punctuation">,</span></span><br><span class="line">    		zap.Duration(<span class="string">&quot;backoff&quot;</span><span class="punctuation">,</span> time.Second)<span class="punctuation">,</span></span><br><span class="line">		)</span><br></pre></td></tr></table></figure>

<p>原因在于, 现在的logger对象是一个zap.SugaredLogger, 而不是logger对象(最开始测试时用的是Logger对象), 因此在这里不能使用zap.String这种格式, 当然, zap.SugaredLogger跟zap.Logger是可以很方便地进行转换的, 可参考<a href="https://godoc.org/go.uber.org/zap">这里</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://godoc.org/go.uber.org/zap">https://godoc.org/go.uber.org/zap</a></li>
<li><a href="https://github.com/zhoushuke/change-zap-LogLevel-by-restfulAPI-in-Runtime">https://github.com/zhoushuke/change-zap-LogLevel-by-restfulAPI-in-Runtime</a></li>
<li><a href="https://juejin.im/post/5bffa2f15188256693607d7c">https://juejin.im/post/5bffa2f15188256693607d7c</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(grpc-gateway在istio(kubernetes)中的负载均衡)</title>
    <url>/2020/02/18/grpc-gateway-loadbalance-on-kubernetes-and-istio/</url>
    <content><![CDATA[<p>业务中要在kubernetes中接入grpc， 之前写过一篇<a href="https://izsk.me/2020/01/17/grpc-service-on-kubernetes/">文章</a>测试过grpc服务在kubernetes中由于http&#x2F;2长连接的特性无法达到负载均衡效果, 上次使用的是直接在业务中引用kubernetes的list&#x2F;watch机制，这次使用istio，同样可以实现grpc的负载均衡.</p>
<span id="more"></span>



<p>如上所说, 直接在业务代码中引用<a href="https://github.com/sercand/kuberesolver">第三方库</a>来实时地获取服务的ep变化情况，从而实现grpc的负载均衡, 测试情况可参考<a href="https://izsk.me/2020/01/17/grpc-service-on-kubernetes/">这里</a></p>
<p>近期在调研istio, 学习一段时间之内, 发现istio中的eds对象是实时地获取服务的ep情况, 这个特性是不是可以直接实现grpc的负载均衡功能. </p>
<p><strong>需要指出的是，这次grpc服务端的svc使用的是clusterip类型,而不必一定是headless svc.</strong></p>
<h3 id="Kubernetes-amp-Istio"><a href="#Kubernetes-amp-Istio" class="headerlink" title="Kubernetes &amp; Istio"></a>Kubernetes &amp; Istio</h3><p>首先有个kubernetes跟istio环境, 在这里就不细说安装流程了，各自的官网都有详细说明</p>
<h3 id="grpc"><a href="#grpc" class="headerlink" title="grpc"></a>grpc</h3><p>准备一个grpc服务的demo，代码很简单，<strong>服务端跟客户端各自打印对方的容器地址</strong>，来反映负载均衡效果，详细的代码及使用方法在<a href="https://github.com/zhoushuke/grpc-loadbalance-on-kubernetes-and-istio">这里</a>, 大家可参考</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>先启动<strong>一个服务端，一个客户端</strong>，很平常,</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/grpc-on-kubernetes-loadbalance00.png"></p>
<p><strong>服务端个数一个保持不变，客户端进行扩容</strong>，从一个扩到三个，这时在服务端显示有3个客户端的请求</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/grpc-on-kubernetes-loadbalance01.png"></p>
<p><strong>客户端个数三个保持不变, 服务端进行扩容，从一个扩容至五个</strong>，这下图可以从客户端发现，请求被均匀地分配给了服务端的5个地址, 说明，通过istio是可以实现grpc服务的负载均衡效果的</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/grpc-on-kubernetes-loadbalance03.png"></p>
<p>最后，测试了服务端的缩容操作,<strong>从五个降至一个</strong>, 客户端也能实时地发现服务的变动, 如上图下半部分所示。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>从测试的结论可以看出, <strong>在istio的流量管理的作用下, grpc服务不需要做额外的操作即可实现负载均衡效果, 原因就是因为istio的xds是动态模型, 得到cluster后，根据cluster 实时地查询endpoint列表，然后再根据istio中配置的负载均衡配置(默认是roundbalance)直接路由到endpoint，而不需要经过kubernetes中的service(kube-proxy)机制</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2020/01/17/grpc-service-on-kubernetes/">https://izsk.me/2020/01/17/grpc-service-on-kubernetes/</a></li>
<li><a href="https://github.com/zhoushuke/grpc-loadbalance-on-kubernetes-and-istio">https://github.com/zhoushuke/grpc-loadbalance-on-kubernetes-and-istio</a></li>
<li><a href="https://github.com/sercand/kuberesolver">https://github.com/sercand/kuberesolver</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana学习(在Grafana中统计物理机上容器状态分类汇总)</title>
    <url>/2019/09/07/grafana-statisty-containers-status-on-hosts/</url>
    <content><![CDATA[<p>Kubernetes集群中的容器监控数据已经通过Prometheus采集上来，在Grafana上配置可视化的时候，发现了个很有意思的需求，即：<strong>按物理机展示各种状态下的容器占比情况</strong></p>
<span id="more"></span>

<p>当然，是可以在Prometheus采集的时候对原始数据进行relabel，因为不好再回去做调整, 目前采集上来的数据不能直接实现上面的需求，所以只能在Prometheus上用提供的函数对数据做各种操作，最终达到效果,因为grafana直接支持prom语法，所以可以直接对prometheus的数据做sql.</p>
<h3 id="展示需求"><a href="#展示需求" class="headerlink" title="展示需求"></a><strong>展示需求</strong></h3><p><strong>按物理机展示各种状态下的容器占比</strong></p>
<p>最终要实现的效果如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119152857.png"></p>
<p>说明: 这里运行的容器数跟pod数不不一致是因为运行的容器数中包含静态pod, 所以会比pod数多.</p>
<h3 id="Grafana完整语句"><a href="#Grafana完整语句" class="headerlink" title="Grafana完整语句"></a><strong>Grafana完整语句</strong></h3><p>这里的语句跟Prometheus采集到的原始数据有很大的关系, 我只是想记录下grafana的几个比较有用的语法, 所以可能这个语句没有很大的参考性.</p>
<p>grafana中实现以上的效果，完整的语句如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(<span class="built_in">sum</span> by(node)(avg by(node,pod)(kube_pod_info&#123;node=~<span class="string">&quot;hd1g-senserealty-k8s-1&quot;</span>&#125;) * on(pod) group_right(node) kube_pod_status_phase&#123;phase=<span class="string">&quot;Running&quot;</span>&#125;) ) </span><br><span class="line">/ </span><br><span class="line">on(node) label_replace(kubelet_running_pod_count,<span class="string">&quot;node&quot;</span>, <span class="string">&quot;<span class="variable">$1</span>&quot;</span>, <span class="string">&quot;kubernetes_io_hostname&quot;</span>,<span class="string">&quot;(.*)&quot;</span>) *100</span><br></pre></td></tr></table></figure>

<p>下面进行这个语句的拆解, 主要使用到了3个promql语法,所以分为3部分来看</p>
<blockquote>
<ul>
<li>avg by(node,pod)(kube_pod_info{node&#x3D;~”hd1g-senserealty-k8s-1”})</li>
<li>* on(pod) group_right</li>
<li>Sum by</li>
</ul>
</blockquote>
<h3 id="语句解析"><a href="#语句解析" class="headerlink" title="语句解析"></a><strong>语句解析</strong></h3><h4 id="avg-by"><a href="#avg-by" class="headerlink" title="avg by"></a><strong>avg by</strong></h4><p>avg by 这个语法跟常用的数据库里的语法一样，以node,pod为纬度统计出现的次数</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119154209.png"></p>
<h4 id="on-xx-group-right-group-left"><a href="#on-xx-group-right-group-left" class="headerlink" title="on(xx) group_right|group_left"></a><strong>on(xx) group_right|group_left</strong></h4><p>group_left(node)&#x2F;group_right(node) 一般使用在<strong>一对多或者多对一</strong>的情况，指的是一侧的结果中的每一个向量元素可以与另一侧的多个元素匹配的情况，哪一边的向量具有更高的基数，说是如果右侧的对应多的一方，则使用group_right, 左边多则对应 group_left,Node对应label, 这个label是两边都必须具有的。</p>
<p>上面这个例子 <strong>kube_pod_status_phase</strong> 中并没有node这个label，所有无法使用group语句进行match, 因此需要使用label_replace添加一个node这样的label</p>
<p>on(pod)则表示当表1以node label映射到表2时，从映射结果中再以pod为key label,从下面的映射结果已经把 node label加到结果了.</p>
<p>(除了on， 还可以使用ignoring(xxx)， 即除了这些lable,用剩下的labal为key)</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119154552.png"></p>
<h4 id="sum-by"><a href="#sum-by" class="headerlink" title="sum by"></a><strong>sum by</strong></h4><p>最后再以node为纬度进行sum</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119154650.png"></p>
<p>最终即能实现想要的效果了, 这里再说下label_replace这个函数，还是挺有用的</p>
<h4 id="label-replace"><a href="#label-replace" class="headerlink" title="label_replace"></a><strong>label_replace</strong></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#label_replace用于添加/覆盖label</span></span><br><span class="line">label_replace(table, dst_label, replace_string, src_label, regex)</span><br><span class="line"></span><br><span class="line">table表示需要操作的表</span><br><span class="line">dst_label： 添加/被覆盖的label, 如果指定的label不存在，则添加</span><br><span class="line">replace_string:  匹配到的string ,使用<span class="variable">$1</span>,<span class="variable">$2</span>...引用</span><br><span class="line">src_label: regex表达式匹配的源label </span><br><span class="line">regex: 正则表达式，可使用<span class="variable">$1</span>,<span class="variable">$2</span>…引用</span><br></pre></td></tr></table></figure>

<p>比如，table的原始数据是这样</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119155014.png"></p>
<p>如果我想添加一个上面没有的的叫node的label，这个时候就需要使用label_replace</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">label_replace(kubelet_running_pod_count,<span class="string">&quot;node&quot;</span>, <span class="string">&quot;<span class="variable">$1</span>&quot;</span>, <span class="string">&quot;kubernetes_io_hostname&quot;</span>,<span class="string">&quot;(.*)&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>添加一个node label</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119155259.png"></p>
<p>覆盖label</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200119155433.png"></p>
<p>这里把instance给覆盖了，做打码处理了</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://prometheus.io/docs/prometheus/latest/querying/functions/">Prometheus functions</a></li>
<li><a href="https://grafana.com/docs/grafana/latest/features/datasources/prometheus/">Grafana</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio学习(使用jaeger实现grpc-gateway全链路追踪)</title>
    <url>/2020/02/21/grpc-gateway-with-opentracing/</url>
    <content><![CDATA[<p>一直想把istio引入到业务中, 前期调研实操中发现istio完美解决了grpc在kubernetes中的负载均衡<a href="https://izsk.me/2020/02/18/grpc-gateway-loadbalance-on-kubernetes-and-istio/">问题</a>, 这次学习一下opentracing在istio中的应用, 这里主要是想看grpc-gateway的调用，目前业务中用的最多, 当然tcp就更没问题.</p>
<span id="more"></span>

<p>fork了一个大神的<a href="https://github.com/garystafford/k8s-istio-observe-backend">demo</a>, 精简了部分流程, 把在google cloud上环境的搭建去除了，同时将grpc-gateway部分的代码改了,fork的源码中把grpc-gateway单独写成一个模块提供服务, 这里改成了跟真正的服务结合在一起，少一层中转, github在<a href="https://github.com/zhoushuke/grpc-gateway-with-opentracing.git">这里</a></p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>这个demo里涉及三个服务:</p>
<p>​             —————————————————————-</p>
<p>curl &#x3D;&#x3D; |<strong>(grpc-gateway) http</strong> &#x3D;&#x3D;&gt; service-a &#x3D;&#x3D; grpc| &#x3D;&#x3D;&gt; service-c &#x3D;&#x3D; tcp &#x3D;&#x3D;&gt; mongodb</p>
<p>​              —————————————————————-</p>
<p><code>service-a服务中整合了grpc-gateway服务, 暴露出8088 http接口, 通过8088端口转到 grpc 50051端口调用 service-c</code></p>
<p><code>service-c 通过tcp调用mongodb写入数据</code></p>
<p><code>这里直接使用命令行curl service-a的8088端口</code></p>
<p>grpc-gateway的实现过程可参考<a href="https://blog.csdn.net/dapangzi88/article/details/63686334">这里</a></p>
<h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>需要一个kubernetes集群,部署了istio及开启了jaeger和kiali, 这部分不在这里展开. 可参考<a href="https://izsk.me/2020/01/03/Istio-Install-Upon-Kubernetes/">这里</a></p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="编译镜像"><a href="#编译镜像" class="headerlink" title="编译镜像"></a>编译镜像</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> grpc-gateway-with-opentracing/services</span><br><span class="line">bash build_images.sh</span><br></pre></td></tr></table></figure>

<h4 id="发布"><a href="#发布" class="headerlink" title="发布"></a>发布</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> grpc-gateway-with-opentracing/deployments</span><br><span class="line">kubectl apply -f namespaces/middle.yaml</span><br><span class="line">kubectl apply -f secrets/go-srv-demo.yaml</span><br><span class="line">kubectl apply -f yaml/*</span><br><span class="line"><span class="comment">#其中, mongodb使用到的用户名及密码为forguest:VnximZWwED， 端口27017</span></span><br></pre></td></tr></table></figure>

<p>这里为了方便， mongodb数据库没有使用持久卷, 数据库启动之后连接mongodb进行用户授权, </p>
<p>安装mongo shell 命令行工具, ubuntu下进行以下操作</p>
<p><strong>注意: 使用的mongodb server为4.2的版本, 请安装对应版本的客户端,不然问题一大堆</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add -</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/4.2 multiverse&quot;</span> | sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/mongodb-org-4.2.list</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">apt-get install -y mongodb-org-shell</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">use admin;</span><br><span class="line">db.createUser(&#123;user: <span class="string">&quot;forguest&quot;</span> , <span class="built_in">pwd</span>: <span class="string">&quot;VnximZWwED&quot;</span>, roles: [  <span class="string">&quot;userAdminAnyDatabase&quot;</span>,<span class="string">&quot;readWriteAnyDatabase&quot;</span> ]&#125;)</span><br></pre></td></tr></table></figure>



<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>使用以下命令进行测试, 请对应替换service-a的集群ip.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 1 100);<span class="keyword">do</span> <span class="built_in">sleep</span> 5;curl -H<span class="string">&#x27;Content-Type: application/json&#x27;</span> <span class="string">&#x27;your-service-a-svc-clusterIP:8088/api/v1/greeting&#x27;</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>Jaeger效果如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200221174553.png"></p>
<p>kialiu效果如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200221174801.png"></p>
<h3 id="Opentracing分析"><a href="#Opentracing分析" class="headerlink" title="Opentracing分析"></a>Opentracing分析</h3><p>在grpc-agteway-with-opentracing.go代码中, 定义了如下对象, </p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">        otHeaders = []<span class="type">string</span>&#123;</span><br><span class="line">                <span class="string">&quot;x-request-id&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-b3-traceid&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-b3-spanid&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-b3-parentspanid&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-b3-sampled&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-b3-flags&quot;</span>,</span><br><span class="line">                <span class="string">&quot;x-ot-span-context&quot;</span>&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>在grpc调用时， istio会做如下的操作:</p>
<ul>
<li>Inbound 流量：对于经过 Sidecar 流入应用程序的流量，如果经过 Sidecar 时 Header 中没有任何跟踪相关的信息，则会在创建一个根 Span，TraceId 就是这个 SpanId，然后再将请求传递给业务容器的服务；如果请求中包含 Trace 相关的信息，则 Sidecar 从中提取 Trace 的上下文信息并发给应用程序。</li>
<li>Outbound 流量：对于经过 Sidecar 流出的流量，如果经过 Sidecar 时 Header 中没有任何跟踪相关的信息，则会创建根 Span，并将该跟 Span 相关上下文信息放在请求头中传递给下一个调用的服务；当存在 Trace 信息时，Sidecar 从 Header 中提取 Span 相关信息，并基于这个 Span 创建子 Span，并将新的 Span 信息加在请求头中传递</li>
</ul>
<p>总结就是: <strong>每一次的调用, istio都会将请求中的header进行解析, 看header中是否包含上面声明的变量,如果不存在, 则创建一个root span, 如果存在, 则将spanid做为自己的parentid,如果还有调用, 则依次这样传递下去</strong></p>
<p>可以来看一下jaeger上的header信息:</p>
<p>首先，每一次请求istio都会生成一个唯一的<strong>traceID(x-b3-traceid)及requestID(x-request-id)</strong>, 这两个值在这次请求中不管经过几个服务都不会改变, 这样就能够通过这者查询出这次请求所经过的服务有哪一些，但是这两者无法确定经过的服务间的顺序是怎样的, 所以必须看<strong>x-b3-spanid及x-b3-parentspanid</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200221180136.png"></p>
<p>从上图可以看出service-a(根请求)的spanID为traceID的后16位, 再通过这个spanID，可以看到</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200221180419.png"></p>
<p><code>8932e8b49bbddf3a</code> 为<code>c78400f5fce93775</code>的 <code>child_of</code>即孩子span</p>
<p>通过这种父子关系就可以很容易确定谁调用谁，再结合traceID或者RequestID(通常这两者一样)的唯一性就能够定位所有调用过的服务了. </p>
<p><strong>从上面也可以看出, 要使用jaeger实现全链路监控,还是需要业务代码做集成,, 尽管istio帮我们在业务上做了很多的事情, 但目前像上面的例子需要加入header的部分istio无法实现，因为它不知道service-a会去调用哪个服务. 当然 ，以后这部分的coding会越来越少</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://blog.csdn.net/dapangzi88/article/details/63686334">https://blog.csdn.net/dapangzi88/article/details/63686334</a></li>
<li><a href="https://github.com/garystafford/k8s-istio-observe-backend">https://github.com/garystafford/k8s-istio-observe-backend</a></li>
<li><a href="https://izsk.me/2020/02/18/grpc-gateway-loadbalance-on-kubernetes-and-istio/">https://izsk.me/2020/02/18/grpc-gateway-loadbalance-on-kubernetes-and-istio/</a></li>
<li><a href="https://github.com/zhoushuke/grpc-gateway-with-opentracing">https://github.com/zhoushuke/grpc-gateway-with-opentracing</a></li>
<li><a href="https://izsk.me/2020/01/03/Istio-Install-Upon-Kubernetes/">https://izsk.me/2020/01/03/Istio-Install-Upon-Kubernetes/</a></li>
<li><a href="https://www.infoq.cn/article/pqy*PFPhox9OQQ9iCRTt">https://www.infoq.cn/article/pqy*PFPhox9OQQ9iCRTt</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>grpc gateway loadbalance on kubernetes</title>
    <url>/2020/01/17/grpc-service-on-kubernetes/</url>
    <content><![CDATA[<p>由于grpc天生的长连接特性, 如果要在kubernetes中使用grpc, 想要实现负载均衡问题需要考虑长连接问题.</p>
<p>由于本人对grpc了解的不是很深入，没法说的太多，看官们勿喷.</p>
<span id="more"></span>

<h3 id="http-x2F-2长连接"><a href="#http-x2F-2长连接" class="headerlink" title="http&#x2F;2长连接"></a><strong>http&#x2F;2长连接</strong></h3><p>详情大家可参考这篇文章<a href="https://cloud.tencent.com/developer/news/394310">在K8S上使用gRPC做负载均衡</a></p>
<p>上述文章中也提出了可有多种方式处理这种场景.</p>
<p>今天要跟大家说一说，使用kubernetes的headless 给grpc做负载均衡引出的问题.因为这种方式最简单且最直接.</p>
<p><strong>需要很简单: 客户端通过grpc调用服务端, 请求可以负载均衡</strong></p>
<p>很简单的demo, 一个服务端, 一个客户端</p>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a><strong>客户端</strong></h3><p>客户端逻辑很简单, 主要代码如下:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">lbconn</span><span class="params">()</span></span> *grpc.ClientConn &#123;</span><br><span class="line">	<span class="comment">//progress-service-svc.just-4-test.svc.cluster.local:50051  服务端服务名</span></span><br><span class="line">  <span class="comment">//使用dns进行resolv</span></span><br><span class="line">	conn, err := grpc.Dial(<span class="string">&quot;dns:///progress-service-svc.just-4-test.svc.cluster.local:50051&quot;</span>,</span><br><span class="line">		grpc.WithInsecure(),</span><br><span class="line">		grpc.WithBalancerName(roundrobin.Name),</span><br><span class="line">	)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;did not connect: %v \n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> conn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;begin client&quot;</span>)</span><br><span class="line">	conn := lbconn()</span><br><span class="line">	c := pb.NewHelloClient(conn)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++ &#123;</span><br><span class="line">		call(c, i)</span><br><span class="line">		time.Sleep(time.Second * <span class="number">5</span>)</span><br><span class="line">	&#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">func</span> <span class="title">call</span><span class="params">()</span></span>&#123;</span><br><span class="line">      <span class="comment">// 略</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>开始只有一个实例，启动日志: </p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/githubimage-20200117161415769.png"></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/githubimage-20200117160438043.png"></p>
<p>从这里也可以看到，grpc启动的时候，会直接通过服务名拿到服务端pod的ip列表</p>
<p>下面是多个服务端实例的情况</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/githubimage-20200117160526405.png"></p>
<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a><strong>服务端</strong></h3><p>服务端主要代码:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *grpcServer)</span></span> Hi(ctx context.Context, in *pb.HiRequest) (*pb.HiResponse, <span class="type">error</span>) &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;service got request!&quot;</span>, in)</span><br><span class="line">	<span class="keyword">return</span> &amp;pb.HiResponse&#123;FromWho: in.ToWho, Message: fmt.Sprintf(<span class="string">&quot; %v receive message : &quot;</span>, id) + in.Message&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	rand.Seed(time.Now().UnixNano())</span><br><span class="line">	id = rand.Intn(<span class="number">10000</span>)</span><br><span class="line">	lis, err := net.Listen(<span class="string">&quot;tcp&quot;</span>, <span class="string">&quot;:50051&quot;</span>)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		log.Fatal(<span class="string">&quot;failed to listen: %v&quot;</span>, err)</span><br><span class="line">	&#125;</span><br><span class="line">	s := grpc.NewServer()</span><br><span class="line">	pb.RegisterHelloServer(s, &amp;grpcServer&#123;&#125;)</span><br><span class="line">	<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">		cf := &amp;gg.GatewayConfig&#123;</span><br><span class="line">			Fn:       pb.RegisterHelloHandlerFromEndpoint,</span><br><span class="line">			GrpcPort: <span class="string">&quot;:50051&quot;</span>,</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> err := gg.RegisterGRPCGateway(cf); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			fmt.Println(<span class="string">&quot;grpc gateway closed:&quot;</span>, err)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	fmt.Println(<span class="string">&quot;start...&quot;</span>)</span><br><span class="line">	s.Serve(lis)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>服务端启动日志:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/githubimage-20200117160438043.png"></p>
<h3 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a><strong>ClusterIP</strong></h3><p>服务端使用常规的ClusterIP的svc代理出来，这种情况我们发现, 客户端所有的请求都打在一个实例上, 服务端新增或者减少对客户端都没有影响(当然, 如果减少的刚好是连接的那个实例,那所有的实例都会切换到另一实例上), 至始至终都是一个服务端实例在响应.没有达到效果.</p>
<p><strong>因为ClusterIP暴露出来的是一个ip, grpc通过这个ip只能转到某一个实例上进行响应, 所以客户端一起与这个特定的实例建立长连接.</strong></p>
<p>可参考<a href="https://medium.com/google-cloud/loadbalancing-grpc-for-kubernetes-cluster-services-3ba9a8d8fc03">这里</a></p>
<h3 id="Headless"><a href="#Headless" class="headerlink" title="Headless"></a><strong>Headless</strong></h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">progress-service-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">just-4-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">50051tcp50051</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">50051</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">50051</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">workload.user.cattle.io/workloadselector:</span> <span class="string">deployment-just-4-test-process-service</span></span><br><span class="line">  <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h3><p>从测试的情况来看:</p>
<p>如果服务实例从2个scale up到5个, 所有的请求还是直接路由到之前的2个, 新增的3个没有请求进来. </p>
<p>如果是从2个scale down到1个，那客户端可以感应到，请求会自动地切换剩余的实例,这个很好理解, grpc维持的长连接具有探活能力.</p>
<p>Scale up这种情况grpc客户端无法更新服务端列表，原因估计<strong>是grpc启动的时候通过服务名 –&gt; dns –&gt; headless 直接就拿到了服务端pod的实例ip列表，维持在类似连接池中, 后面所有的请求都直接从连接池中做路由，而不是每次都通过服务名实时地查询dns记录, (实时查询的话那一定是会得到新增实例的.), 这也让grpc更高效</strong></p>
<p>所以,在kubernetes中对grpc的服务，需要做特殊的处理, 如果说服务上线就能确定服务端实例个数, 后续实例数都不会变化,那可以直接使用headless这种方式，但谁也不能保证实例数永远不变或者不会发生重启等情况.</p>
<p>因此, 可以在客户端侧实现<strong>watch endpoints</strong>机制来保证，服务端实例发生变化时，客户端会即时地拿到最新的实例列表,从而确保了请求的负载均衡, github上有直接的解决方案, 大家可以参考这里<a href="https://github.com/sercand/kuberesolver">kuberesolve</a></p>
<p>客户端实现创建k8s client即可.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> conns []*grpc.ClientConn</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">resolverconn</span><span class="params">()</span></span> *grpc.ClientConn &#123;</span><br><span class="line">	client, err := kuberesolver.NewInClusterK8sClient()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Println(<span class="string">&quot;errrr&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line">	resolver.Register(kuberesolver.NewBuilder(client, <span class="string">&quot;kubernetes&quot;</span>))</span><br><span class="line">	<span class="comment">// USAGE:</span></span><br><span class="line">	<span class="comment">// if schema is &#x27;kubernetes&#x27; then grpc will use kuberesolver to resolve addresses</span></span><br><span class="line">	cc, err := grpc.Dial(<span class="string">&quot;kubernetes:///progress-service-svc:50051&quot;</span>, grpc.WithInsecure())</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;did not connect: %v \n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> cc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>使用这种watch方式，我们同时测试了，服务端用headless或者clusterip的svc效果都是一样的.</strong></p>
<p>至此，虽然grpc也是基于http协议，但是因为其长连接的特性, 在kubernetes中确实与http会有所不同.</p>
<p>当然, 把watch ep逻辑集成在client侧还是有点侵入性的,应该有更好的办法能实现, 目前没有找到, 如果有了解的同学欢迎告知.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://medium.com/google-cloud/loadbalancing-grpc-for-kubernetes-cluster-services-3ba9a8d8fc03">loadbalancing-grpc-for-kubernetes-cluster-services</a></li>
<li><a href="https://cloud.tencent.com/developer/news/394310">在K8S上使用gRPC做负载均衡</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>做黑盒监控引发的思考</title>
    <url>/2019/10/06/how-to-do-blackbox-monitor/</url>
    <content><![CDATA[<p>经常听到运维前辈说到方法论这个词, 之前觉得很高深, 后来想想, 其实也就是经验多了, 把对某类事务的处理沉淀或者抽象为一种统一的方法. 就像下面我要说一个例子来讲一讲</p>
<span id="more"></span>



<p>线上有个基于gitlab二次开发的使用的模块要正式应用到公有云环境, 做为sre，如何对gitlab的<code>clone --&gt; commit --&gt; push</code>进行黑盒监控?</p>
<p>当然, 可以通过gitlab本身的一些性能指标进行采集分析, 但这些性能指标可能是分散的, 无法准确地反应clone –&gt; commit –&gt;push这条操作是否存在异常, 那么更好的办法就是模拟这3个操作, 不间断地对gitlab进行改操作, 如果gitlab在某个环节出现问题导致命令不成功, 则必然会产生报警.</p>
<h3 id="监控目的"><a href="#监控目的" class="headerlink" title="监控目的"></a>监控目的</h3><p>实现对gitlab的黑盒监控</p>
<h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><ol>
<li>在gitlab上新建一个监控使用git仓库, 用于不断操作</li>
<li>git clone该仓库, 验证是否正常</li>
<li>git add 一个时间戳文件, 然后使用git commit进行提示, 验证是否正常</li>
<li>git push到仓库, 验证是否正常</li>
</ol>
<p>如果在预设的超时时间(当然，这个时间由运维根据情况而定, 跟gitlab性能有一定关系)内这几个步骤都没问题的话,那基本上可以说gitla服务是正常 </p>
<h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="string">&quot;<span class="subst">$(dirname $0)</span>&quot;</span> || <span class="built_in">exit</span> 1</span><br><span class="line">WORKSPACE=<span class="string">&quot;<span class="subst">$(pwd)</span>&quot;</span></span><br><span class="line">GIT_URL=<span class="string">&quot;git@code-commit-tpmonitor-auto.git&quot;</span></span><br><span class="line">TIME_OUT=5</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_gitstatus</span></span>()&#123;</span><br><span class="line">    start_time=$(<span class="built_in">date</span> +%s)</span><br><span class="line">    <span class="built_in">timeout</span> <span class="variable">$TIME_OUT</span> \</span><br><span class="line">    git <span class="built_in">clone</span> <span class="variable">$GIT_URL</span> &gt; <span class="variable">$WORKSPACE</span>/git_clone.log 2&gt;&amp;1 &amp;&amp; \</span><br><span class="line">    <span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/code-commit-tpmonitor-auto &amp;&amp; \</span><br><span class="line">    [ -f <span class="variable">$WORKSPACE</span>/code-commit-tpmonitor-auto/timestamps.time ] &amp;&amp; \</span><br><span class="line">    gap=$(<span class="built_in">cat</span> timestamps.time) &amp;&amp; \</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$&#123;start_time&#125;</span> &gt; timestamps.time &amp;&amp; \</span><br><span class="line">    git add timestamps.time &amp;&amp; \</span><br><span class="line">    git commit -m <span class="string">&quot;<span class="variable">$&#123;start_time&#125;</span>&quot;</span> &gt; <span class="variable">$WORKSPACE</span>/git_commit.log 2&gt;&amp;1 &amp;&amp; \</span><br><span class="line">    git push &gt; <span class="variable">$WORKSPACE</span>/git_push.log 2&gt;&amp;1</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;$?&quot;</span> -eq <span class="string">&quot;0&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;gap_time:$[<span class="variable">$&#123;start_time&#125;</span>-<span class="variable">$&#123;gap&#125;</span>]&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;run_time:$[<span class="subst">$(date +%s)</span>-<span class="variable">$&#123;start_time&#125;</span>]&quot;</span></span><br><span class="line">        <span class="built_in">rm</span> -rf <span class="variable">$WORKSPACE</span>/code-commit-tpmonitor-auto </span><br><span class="line">        <span class="built_in">echo</span> rnt:0</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> rnt:1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>()&#123;</span><br><span class="line">    <span class="built_in">rm</span> -rf <span class="variable">$WORKSPACE</span>/code-commit-tpmonitor-auto</span><br><span class="line">    check_gitstatus</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure>



<p>有人可能会好奇为何需要计算一个<code>gap_time</code>,这其实也是在监控gitlab的一个性能</p>
<p>想像这么一种情况, 使用git clone下来的文件如果不是我上一次push的, 那么其实也是有问题, 有可能是push虽然成功了, 但接着使用clone时发现clone下来的文件居然没有包含上次push的文件, 那么就有可能gitlab来不及处理海量的小文件导致出现性能问题.</p>
<p>通过计算一个gap_time是非常有必要了, 每一次都push一个文件, 这个文件非常简单, 只包含一行时间戳, 在下一次clone下来时用当前时间跟这个文件里的时间戳进行比较, 假如这个脚本每隔5分钟运行一次, 那么这个这个时间差值一定会在5分钟左右, 那么就可以添加一个报警规则: 这个时间差如果超过6分钟, 则说明有问题.</p>
<p>通过添加一个这样的监控脚本, 稳定运行一年多的时间内成功地提前发现了几次gitlab故障, 及时处理后并没有影响客户.</p>
<p>这里要注意一点的是, 开始这个脚本是每次都是新生成的时间戳文件, 一直都在push, 会造成这个仓库的文件越来越多, 因此时间久了之后, git clone的时间会变长, 出现过几次命令执行时间超过timeout的值而造成的误报, 而gitlab本身是没有问题的， 所以后来不提交新的文件, 从始至终仓库里只有一个文件, 文件内的时间戳不一样，就是现在的版本</p>
<p>另外还有一个使用的例子是线上的<code>实时日志搜索</code>模块的监控, 同样是采用了这个策略, 添加一个额外的时间戳字段来反应日志搜索是否达到实时.</p>
<p>另外, 黑盒监控还会使用到Prometheus的blackbox来做, 但如果是比较复杂的涉及多个业务流程的话, blackbox的支持还是比较难适配的, 所以blackbox一般都用在单一接口的监控上.</p>
<p>所以有时候是需要使用一种不常规的方式来配合做到业务稳定性, 方法论里面的思想还是很有帮助的.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>方法论</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Influxdb-relay实现Influxdb高可用</title>
    <url>/2020/08/04/influx-single-transfer-to-relay/</url>
    <content><![CDATA[<p>目前平台有些遗留应用还是使用的influxdb保存监控数据, influxdb为单实例, 随时可能出现单机故障, 考虑到influxdb还将运行很长一段时间, 因此需要扩展成HA机制, 这里选择influxdb-relay方案</p>
<span id="more"></span>

<p>这里需要说明的是, relay不会同步两个influxdb实例之间的数据，<strong>它只提供双写的能力</strong>，即在某个实例出现问题后还能将数据正常写入另一个正常的实例, 保证数据不丢失.如果想在问题实例上同步这部分数据话需要人工介入.</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a><a href="https://github.com/influxdata/influxdb-relay">架构</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">				┌─────────────────┐                 </span><br><span class="line">        │writes &amp; queries │                 </span><br><span class="line">        └─────────────────┘                 </span><br><span class="line">                 │                          </span><br><span class="line">                 ▼                          </span><br><span class="line">         ┌───────────────┐                  </span><br><span class="line">         │               │                  </span><br><span class="line">┌────────│ Load Balancer │─────────┐        </span><br><span class="line">│        │               │         │        </span><br><span class="line">│        └──────┬─┬──────┘         │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│        ┌──────┘ └────────┐       │        </span><br><span class="line">│        │ ┌─────────────┐ │       │┌──────┐</span><br><span class="line">│        │ │/write or UDP│ │       ││/query│</span><br><span class="line">│        ▼ └─────────────┘ ▼       │└──────┘</span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │ InfluxDB │      │ InfluxDB │  │        </span><br><span class="line">│  │ Relay    │      │ Relay    │  │        </span><br><span class="line">│  └──┬────┬──┘      └────┬──┬──┘  │        </span><br><span class="line">│     │    |              |  │     │        </span><br><span class="line">│     |  ┌─┼──────────────┘  |     │        </span><br><span class="line">│     │  │ └──────────────┐  │     │        </span><br><span class="line">│     ▼  ▼                ▼  ▼     │        </span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │          │      │          │  │        </span><br><span class="line">└─▶│ InfluxDB │      │ InfluxDB │◀─┘        </span><br><span class="line">   │          │      │          │           </span><br><span class="line">   └──────────┘      └──────────┘ </span><br></pre></td></tr></table></figure>

<p>共涉及到3个应用</p>
<ol>
<li>Influxdb: influxdb实例</li>
<li>Influxdb-relay: 代理influxdb的写流量, 通过双写机制保证数据写入到2个influxdb数据库中,读流量还是从lb直接转到influxdb实例，不会经过relay</li>
<li>loadBalancer: 对influxdb的读写流量都通过该服务进行代理到influxdb relay, 选择nginx即可,其它需要访问influxdb服务的配置参数都需要指定该应用的地址</li>
</ol>
<h3 id="新增实例"><a href="#新增实例" class="headerlink" title="新增实例"></a>新增实例</h3><p>线上influxdb的版本为:</p>
<p><code>InfluxDB v1.7.4 (git: 1.7 ef77e72f435b71b1ad6da7d6a6a4c4a262439379)</code></p>
<p>部署机器: <code>192.168.1.5</code></p>
<p>需要在一台机器上部署一个influxdb新实例: <code>192.168.1.6</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://dl.influxdata.com/influxdb/releases/influxdb_1.7.4_amd64.deb</span><br><span class="line">dpkg -i influxdb_1.7.4_amd64.deb</span><br></pre></td></tr></table></figure>

<p>两个实例使用的配置文件如下, 具体的参数配置可以根据情况定:</p>
<p><code>cat /etc/influxdb/influxdb.conf</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reporting-disabled = <span class="literal">false</span></span><br><span class="line">bind-address = <span class="string">&quot;:8088&quot;</span></span><br><span class="line">[meta]</span><br><span class="line">  <span class="built_in">dir</span> = <span class="string">&quot;/data/influxdb/meta&quot;</span></span><br><span class="line">  retention-autocreate = <span class="literal">true</span></span><br><span class="line">[data]</span><br><span class="line">  <span class="built_in">dir</span> = <span class="string">&quot;/data/influxdb/data&quot;</span></span><br><span class="line">  wal-dir = <span class="string">&quot;/data/influxdb/wal&quot;</span></span><br><span class="line">  wal-fsync-delay = <span class="string">&quot;50ms&quot;</span></span><br><span class="line">  index-version = <span class="string">&quot;inmem&quot;</span></span><br><span class="line">  trace-logging-enabled = <span class="literal">false</span></span><br><span class="line">  query-log-enabled = <span class="literal">false</span></span><br><span class="line">  validate-keys = <span class="literal">true</span></span><br><span class="line">  cache-max-memory-size = <span class="string">&quot;4g&quot;</span></span><br><span class="line">  compact-full-write-cold-duration = <span class="string">&quot;6h&quot;</span></span><br><span class="line">  max-concurrent-compactions = 0</span><br><span class="line">  compact-throughput = <span class="string">&quot;64m&quot;</span></span><br><span class="line">  compact-throughput-burst = <span class="string">&quot;128m&quot;</span></span><br><span class="line">  tsm-use-madv-willneed = <span class="literal">false</span></span><br><span class="line">  max-series-per-database = 0</span><br><span class="line">  max-values-per-tag = 0</span><br><span class="line">[coordinator]</span><br><span class="line">  write-timeout = <span class="string">&quot;600s&quot;</span></span><br><span class="line">  max-concurrent-queries = 0</span><br><span class="line">  query-timeout = <span class="string">&quot;0s&quot;</span></span><br><span class="line">  log-queries-after = <span class="string">&quot;60s&quot;</span></span><br><span class="line">  max-select-point = 0</span><br><span class="line">  max-select-series = 0</span><br><span class="line">  max-select-buckets = 0</span><br><span class="line">[retention]</span><br><span class="line">  enabled = <span class="literal">true</span></span><br><span class="line">  check-interval = <span class="string">&quot;1h&quot;</span></span><br><span class="line">[shard-precreation]</span><br><span class="line">  enabled = <span class="literal">true</span></span><br><span class="line">  check-interval = <span class="string">&quot;30m&quot;</span></span><br><span class="line">  advance-period = <span class="string">&quot;30m&quot;</span></span><br><span class="line">[monitor]</span><br><span class="line">  store-enabled = <span class="literal">true</span></span><br><span class="line">  store-database = <span class="string">&quot;_internal&quot;</span></span><br><span class="line">  store-interval = <span class="string">&quot;60s&quot;</span></span><br><span class="line">[http]</span><br><span class="line">  enabled = <span class="literal">true</span></span><br><span class="line">  flux-enabled = <span class="literal">true</span></span><br><span class="line">  bind-address = <span class="string">&quot;:8086&quot;</span></span><br><span class="line">  log-enabled = <span class="literal">false</span></span><br><span class="line">  write-tracing = <span class="literal">false</span></span><br><span class="line">  pprof-enabled = <span class="literal">false</span></span><br><span class="line">  debug-pprof-enabled = <span class="literal">false</span></span><br><span class="line">  https-enabled = <span class="literal">false</span></span><br><span class="line">  max-row-limit = 0</span><br><span class="line">  max-connection-limit = 0</span><br><span class="line">  unix-socket-enabled = <span class="literal">false</span></span><br><span class="line">  max-body-size = 0</span><br><span class="line">  max-concurrent-write-limit = 0</span><br><span class="line">  max-enqueued-write-limit = 0</span><br><span class="line">  enqueued-write-timeout = 0</span><br><span class="line">[logging]</span><br><span class="line">  format = <span class="string">&quot;json&quot;</span></span><br><span class="line">  level = <span class="string">&quot;info&quot;</span></span><br><span class="line">  suppress-logo = <span class="literal">true</span></span><br><span class="line">[subscriber]</span><br><span class="line">  enabled = <span class="literal">false</span></span><br><span class="line">[[graphite]]</span><br><span class="line"> enabled = <span class="literal">true</span></span><br><span class="line"> database = <span class="string">&quot;graphite&quot;</span></span><br><span class="line"> retention-policy = <span class="string">&quot;day_hour&quot;</span></span><br><span class="line"> bind-address = <span class="string">&quot;:2003&quot;</span></span><br><span class="line"> protocol = <span class="string">&quot;tcp&quot;</span></span><br><span class="line"> consistency-level = <span class="string">&quot;one&quot;</span></span><br><span class="line"> batch-size = 1000</span><br><span class="line"> batch-pending = 50</span><br><span class="line"> batch-timeout = <span class="string">&quot;1s&quot;</span></span><br><span class="line">[[udp]]</span><br><span class="line">  enabled = <span class="literal">false</span></span><br><span class="line">[continuous_queries]</span><br><span class="line">  enabled = <span class="literal">true</span></span><br><span class="line">  log-enabled = <span class="literal">true</span></span><br><span class="line">  query-stats-enabled = <span class="literal">false</span></span><br><span class="line">  run-interval = <span class="string">&quot;10s&quot;</span></span><br><span class="line">[tls]</span><br></pre></td></tr></table></figure>

<p>实例启停方式:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start influxd.service</span><br><span class="line">systemctl stop influxd.service</span><br></pre></td></tr></table></figure>

<p><strong>启动新的influxdb实例后，需要将线上数据导入到该实例.</strong></p>
<h3 id="relay"><a href="#relay" class="headerlink" title="relay"></a>relay</h3><p>relay服务会对统一接入的流程进行转发， 可直接docker部署，目前只有一个实例, 可扩容成2个,配置相同.</p>
<p>配置文件做为configmap的形式挂载到容器中, 内容如下: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[[http]]</span><br><span class="line">name = <span class="string">&quot;relay-http&quot;</span></span><br><span class="line">bind-addr = <span class="string">&quot;:9096&quot;</span></span><br><span class="line">output = [</span><br><span class="line">    &#123; name=<span class="string">&quot;db1&quot;</span>, location = <span class="string">&quot;http://192.168.1.5:8086/write&quot;</span> &#125;,</span><br><span class="line">    &#123; name=<span class="string">&quot;db2&quot;</span>, location = <span class="string">&quot;http://192.168.1.6:8086/write&quot;</span> &#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>同时，生成一个relay的svc，名为influxdb-relay-headless.sensego，端口号9096</p>
<h3 id="nginx"><a href="#nginx" class="headerlink" title="nginx"></a>nginx</h3><p>从架构图中可以看出, 需要部署一个proxy层来代理influxdb的读写流量, 这里选择nginx</p>
<p>nginx做为容器部署, 一个实例即可</p>
<p>配置文件如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">worker_processes 8;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 10240;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    client_max_body_size 0;</span><br><span class="line">    </span><br><span class="line">upstream relay &#123;</span><br><span class="line">		<span class="comment"># relay实例svc</span></span><br><span class="line">    server influxdb-relay-headless.sensego:9096;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">upstream db &#123;</span><br><span class="line">		<span class="comment"># 后端influx实例地址，这里最好使用健康检查，当influxdb有一个节点宕机时,会被nginx踢除</span></span><br><span class="line">		ip_hash;</span><br><span class="line">    server 192.168.1.5:8086 max_fails=1 fail_timeout=10s;</span><br><span class="line">    server 192.168.1.6:8086 max_fails=1 fail_timeout=10s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 9096;</span><br><span class="line">    location /ping &#123;</span><br><span class="line">        proxy_pass http://db;</span><br><span class="line">    &#125;</span><br><span class="line">    location /write &#123;</span><br><span class="line">        limit_except POST &#123;</span><br><span class="line">            deny all;</span><br><span class="line">        &#125;</span><br><span class="line">        proxy_pass http://relay;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /query &#123;</span><br><span class="line">        proxy_pass http://db;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h3><p>由于influxdb只是保存metrics数据, <code>数据量大概在300G左右</code></p>
<p>不是特别敏感, 因此可以不停机进行备份, </p>
<p>这里采用的是在某个时间点进行全量备份，之后再通过增量备份来导入在操作期间写入的数据</p>
<p>虽然influxdb支持远程备份，建议在192.168.1.5本地进行备份,然后复制到新节点上</p>
<h4 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先全量备份</span></span><br><span class="line">influxd backup -host 192.168.1.5:8088 -portable /tmp/backup-all/influxdb</span><br><span class="line"><span class="comment"># 全库备份必须为一个全新的influxdb实例</span></span><br><span class="line"><span class="comment"># 全库备份包含retention policy.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增量备份</span></span><br><span class="line">influxd backup  -portable -database mytsd -start 2017-04-28T06:49:00Z -end 2017-04-28T06:50:00Z /tmp/backup-ins/influxdb</span><br></pre></td></tr></table></figure>

<h4 id="恢复"><a href="#恢复" class="headerlink" title="恢复"></a>恢复</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在172.16.104.203上做本地恢复</span></span><br><span class="line">influxd restore -portable /tmp/backup-all/influxdb</span><br><span class="line"><span class="comment"># 再导入增量备份数据</span></span><br><span class="line">influxd restore -portable /tmp/backup-ins/influxdb</span><br></pre></td></tr></table></figure>



<p>到此，influxdb由单机点扩容到双节点, 避免了单机故障</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/influxdata/influxdb-relay">https://github.com/influxdata/influxdb-relay</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>iptables学习一(iptables介绍)</title>
    <url>/2017/10/12/iptables%E5%AD%A6%E4%B9%A0%E4%B8%80(iptables%E4%BB%8B%E7%BB%8D)/</url>
    <content><![CDATA[<p>我们说linux系统下的大都指的是iptables,但事实是iptables只是一个用户层命令行工具,真正实现防火墙功能的是内核层的netfilter,我们可以不必区分这两者的区别,大多数情况下我们只对iptables进行操作,由iptables操作netfilter.</p>
<span id="more"></span>

<p>netfilet&#x2F;iptables可做为主机防火墙,也可做为网络防火墙.</p>
<p>作用于主机防火墙则是针对单个主机的,网络防火墙则是针对该网络后的所有服务器集体,例如负载均衡、网关设备上</p>
<p>首先,盗用<a href="http://www.zsythink.net/archives/1199">一张图</a>,我们先看看数据包在主机里的流向(假设已开启iptables).</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/iptables2.JPG" alt="iptables2"></p>
<h3 id="数据包流向"><a href="#数据包流向" class="headerlink" title="数据包流向"></a><strong>数据包流向</strong></h3><blockquote>
<ol>
<li>一个数据包进入网卡时,它首先进入PREROUTING链,内核根据数据包目的IP判断是否需要转发出去.</li>
<li>如果数据包就是进入本机的,它就会沿着图向下移动,到达INPUT链.数据包到了INPUT链后,任何进程都会收到它.根据数据包中的目的端口号来决定具体由哪个进程处理,本机上运行的程序可以发送数据包,这些数据包会经 过OUTPUT链,然后到达POSTROUTING链输出.</li>
<li>如果数据包是要转发出去的,且内核允许转发,数据包就会如图所示向右移动,经过 FORWARD链,然后到达POSTROUTING链输出转发至其它主机</li>
<li>由本机出去的数据包经OUTPUT转到POSTROUTING</li>
</ol>
</blockquote>
<p>简言之:</p>
<p>**经过本机的数据包: PREROUTING —-&gt; IPNUT —-&gt; OUTPUT —-&gt; POSTROUTING **</p>
<p>**经本机转发的数据包: PREROUTING —-&gt; FORWARD —-&gt; POSTROUTING **</p>
<p>**由本机出去的数据包: OUTPUT —-&gt; POSTROUTING **</p>
<h3 id="四表五链"><a href="#四表五链" class="headerlink" title="四表五链"></a><strong>四表五链</strong></h3><p>那上面的那些红色标注的是什么呢,还有大家常说的四表五链又到底什么意思?</p>
<h4 id="规则"><a href="#规则" class="headerlink" title="规则"></a><strong>规则</strong></h4><p><strong>规则</strong>(rules)其实就是网络管理员预定义的条件,规则一般的定义为“如果数据包头符合这样的条件,就这样处理这个数据包”.规则存储在内核空间的信息包过滤表中,这些规则分别指定了源地址、目的地址、传输协议(如TCP、UDP、ICMP)和服务类型(如HTTP、FTP和SMTP)等.当数据包与规则匹配时,iptables就根据规则所定义的方法来处理这些数据包,如放行(accept)、拒绝(reject)和丢弃(drop)等.配置防火墙的主要工作就是添加、修改和删除这些规则.</p>
<p><strong>总结之: 规则就是规定哪些数据可以通过&#x2F;直接丢弃,哪些数据包需要转发,哪些数据包需要由本机处理,所有的规则一条一条按顺序去匹配直到匹配上,如果到最后一条规则执行完之后都匹配不上则使用默认规则.</strong></p>
<p>要说清楚四表五链,再盗<a href="https://www.liuliqiang.info/post/dive-in-iptables/">一张图</a></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/iptables1.JPG" alt="iptables1"></p>
<p>上图中黄色方框框住的为四表</p>
<p>红色框住的为五链</p>
<h4 id="四表"><a href="#四表" class="headerlink" title="四表"></a><strong>四表</strong></h4><blockquote>
<ol>
<li>filter(默认表)<br>主要用于过滤数据包,该表根据系统管理员预定义的一组规则过滤符合条件的数据包.对于防火墙而言,主要利用在filter表中指定的规则来实现对数据包的过滤.<strong>Filter表是默认的表</strong>,如果没有指定哪个表,iptables 就默认使用filter表来执行所有命令,filter表包含了INPUT链(处理进入的数据包),RORWARD链(处理转发的数据包),OUTPUT链(处理本地生成的数据包)在filter表中只能允许对数据包进行接受,丢弃的操作,而无法对数据包进行更改.</li>
<li>nat<br>主要用于网络地址转换NAT,该表可以实现一对一,一对多,多对多等NAT工作,iptables就是使用该表实现共享上网的,NAT表包含了PREROUTING链(修改即将到来的数据包),POSTROUTING链(修改即将出去的数据包),OUTPUT链(修改路由之前本地生成的数据包).</li>
<li>mangle<br>主要用于对指定数据包进行更改,在内核版本2.4.18后的linux版本中该表包含的链为：INPUT链(处理进入的数据包),RORWARD链(处理转发的数据包),OUTPUT链(处理本地生成的数据包)POSTROUTING链(修改即将出去的数据包),PREROUTING链(修改即将到来的数据包).</li>
<li>raw<br>只使用在PREROUTING链和OUTPUT链上,因为优先级最高,从而可以对收到的数据包在连接跟踪前进行处理.一但用户使用了RAW表,在 某个链上,RAW表处理完后,将跳过NAT表和 ip_conntrack处理,即不再做地址转换和数据包的链接跟踪处理了.</li>
</ol>
</blockquote>
<h4 id="五链"><a href="#五链" class="headerlink" title="五链"></a><strong>五链</strong></h4><blockquote>
<ol>
<li>PREROUTING: 在对数据包作路由选择之前,应用此链中的规则.</li>
<li>FORWARD: 当接收到需要通过防火墙发送给其他地址的数据包(转发)时,应用此链中的规则.</li>
<li>INPUT: 当接收到防火墙本机地址的数据包(入站)时,应用此链中的规则.</li>
<li>OUTPUT: 当防火墙本机向外发送数据包(出站)时,应用此链中的规则.</li>
<li>POSTROUTING: 在对数据包作路由选择之后,应用此链中的规则.</li>
</ol>
</blockquote>
<p>从上面两张图可以看出,四表五链当中,有些链只能存在于特定的表或者说有些表只能包含特定的链</p>
<p><strong>一般情况 下,写iptables规则的时候都是以表为入口,这点很重要</strong></p>
<p>PREROUTING 规则可以存在于: raw、mangle、nat</p>
<p>INPUT规则可以存在于: mangle、filter</p>
<p>FORWARD规则可以存在于: mangle、filter</p>
<p>OUTPUT规则可以存在于: raw、filter、mangle、nat</p>
<p>POSTROUTING规则可以存在于:  mangle、nat</p>
<p>表中的规则可以被五链使用:</p>
<p>raw: PREROUTING、OUTPUT</p>
<p>mangle: PREROUTING 、INPUT、FORWARD、OUTPUT、POSTROUTING</p>
<p>nat: PREROUTING、OUTPUT、POSTROUTING</p>
<p>filter: INPUT、FORWARD、OUTPUT</p>
<p>再次盗用<a href="http://www.zsythink.net/archives/1199">一张图</a></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/iptables3.JPG" alt="iptables1"></p>
<h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a><strong>动作</strong></h3><p>前面我们说了,规则是按顺序一条一条进行匹配直到匹配上,那数据包匹配上了之后如何操作呢？这就是动作.常用的动作有如下几个:</p>
<blockquote>
<ol>
<li>DROP: 直接丢弃该数据包,且对端不会有任何提示,对端可能在超时时才会知道</li>
<li>ACCEPT: 接受数据包</li>
<li>REJECT:  拒绝该数据包,会在对端进行提示</li>
<li>LOG: 记录该数据包然后把该数据包传递到下一规则链中</li>
</ol>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ol>
<li>iptables是处于用户空间的命令行工具,真正实现功能的位于内核空间的netfilter.</li>
<li>iptables有四表五链来实现数据包的流转.</li>
<li>iptables中的数据包是按规则顺序一条一条匹配的,如果匹配上的话则执行相应的动作,如果到最后一条规则执行完之后都匹配不上,则使用默认规则.</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://wiki.archlinux.org/index.php/Iptables">iptables-wiki</a></li>
<li><a href="https://www.liuliqiang.info/post/dive-in-iptables/">iptables 深度详解</a></li>
<li><a href="http://www.zsythink.net/archives/1199">iptables概念</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>iptables学习二(iptables常用命令)</title>
    <url>/2017/11/04/iptables%E5%AD%A6%E4%B9%A0%E4%BA%8C(iptables%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4)/</url>
    <content><![CDATA[<p>上一篇介绍了iptables是如何工作,现在记录下iptables一些实用的命令</p>
<span id="more"></span>

<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a><strong>语法</strong></h3><p><code>iptables [-t 表名] -命令 匹配规则 动作</code></p>
<p>语法说明:</p>
<blockquote>
<ul>
<li>表名: 即那四表: raw、mangle、nat、filter,该参数可以忽略,忽略时默认为filter表</li>
<li>命令: 用来增删规则,常用的命令如下:<ol>
<li>-P  –policy        &lt;链名&gt;  定义默认策略</li>
<li>-L  –list          &lt;链名&gt;  查看iptables规则列表</li>
<li>-A  –append        &lt;链名&gt;  在规则列表的最后增加1条规则</li>
<li>-I  –insert        &lt;链名&gt;  在指定的位置插入1条规则</li>
<li>-D  –delete        &lt;链名&gt;  从规则列表中删除1条规则</li>
<li>-R  –replace       &lt;链名&gt;  替换规则列表中的某条规则</li>
<li>-F  –flush         &lt;链名&gt;  删除表中所有规则</li>
<li>-Z  –zero          &lt;链名&gt;  将表中数据包计数器和流量计数器归零</li>
<li>-X  –delete-chain  &lt;链名&gt;  删除自定义链</li>
<li>-v  –verbose       &lt;链名&gt;  与-L他命令一起使用显示更多更详细的信息</li>
</ol>
</li>
<li>匹配规则: 用来对数据包进行匹配,常用如下:<ol>
<li>-i –in-interface    网络接口名&gt;     指定数据包从哪个网络接口进入</li>
<li>-o –out-interface   网络接口名&gt;     指定数据包从哪个网络接口输出</li>
<li>-p —proto          协议类型        指定数据包匹配的协议，如TCP、UDP和ICMP等</li>
<li>-s –source          源地址或子网&gt;   指定数据包匹配的源地址</li>
<li>–sport           源端口号&gt;       指定数据包匹配的源端口号</li>
<li>–dport           目的端口号&gt;     指定数据包匹配的目的端口号</li>
<li>-m –match           匹配的模块      指定数据包规则所使用的过滤模块</li>
</ol>
</li>
<li>动作,在上一篇中已经列出来了,常用的如下:<ol>
<li>DROP: 直接丢弃该数据包,且对端不会有任何提示,对端可能在超时时才会知道</li>
<li>ACCEPT: 接受数据包</li>
<li>REJECT:  拒绝该数据包,会在对端进行提示</li>
<li>LOG: 记录该数据包然后把该数据包传递到下一规则链中</li>
</ol>
</li>
</ul>
</blockquote>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a><strong>常用命令</strong></h3><ol>
<li><p>查看当前iptables设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -L -n -v --line-numbers</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-L: 查看iptables规则列表</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-n: 直接以ip的形式显示ip地址,不进行名称转换</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-v: 显示更多更详细的信息</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--line-numbers: 显示每条规则所在行号</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">当然也可以指定查看具体的表</span></span><br><span class="line">iptables -t filter -L -nv --line-numbers</span><br></pre></td></tr></table></figure>

<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/iptables4.JPG" alt="iptables4"></p>
<p>说明:</p>
<p>chain INPUT: 指明正在查看的链为INPUT</p>
<p>policy ACCEPT: 指明INPUT默认的策略为ACCEPT</p>
<p>0 packets: 指明当前链使用默认策略匹配到的所有包的数量</p>
<p>0 bytes: 明当前链使用默认策略匹配到的所有包的字节大小</p>
<p><strong>参数-Z就是用来清空这两个值</strong></p>
</li>
<li><p>插入一条规则:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -I INPUT -s 192.168.1.10 -j DROP</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-t: 指定规则应用于表filter</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-I: 表明这条规则需要插入到INPUT链中,默认在开头位置插入</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-s: 匹配数据包中源ip地址为192.168.1.10</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-j: 指定动作为丢弃</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上面这条命令用来丢弃从192.168.1.10发送过来的数据包</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">也可指定插入的位置</span></span><br><span class="line">iptables -t filter -I INPUT 3 -s 192.168.1.10 -j DROP</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在表filter中的INPUT链中的第3条规则之前插入该规则</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>增加一条规则(在规则列表的最后增加):</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -A INPUT -s 192.168.1.10 -j DROP</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-t: 指定规则应用于表filter</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-A: 表明这条规则需要增加到INPUT链中,在最后位置增加</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-s: 匹配数据包中源ip地址为192.168.1.10</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-j: 指定动作为丢弃</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上面这条命令用来丢弃从192.168.1.10发送过来的数据包</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>删除一条规则:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -D INPUT -s 192.168.1.10 -j DROP</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除使用-D命令,其它命令参数跟3相同</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">这条命令用来删除在表filter的INPUT链中,源地址为192.168.1.10且动作为DROP的规则</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">我们也可以直接使用行号来删除规则</span></span><br><span class="line">iptables -t filter -D INPUT 3</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除在表filter的INPUT链中的第3行规则</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">那如果我们要删除指定表中的所有规则呢</span></span><br><span class="line">iptables -t filter -F</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-F: (flush)清空指定表上的所有规则</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">清空所有表的规则</span></span><br><span class="line">iptables -F</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改规则(用来修改动作)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -R INPUT -s 192.168.1.10 -j ACCEPT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-R:(replace):如果使用-R来修改规则,则需要指定原本规则的匹配条件</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-R一般用来修改规则的动作</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果要修改规则,则更靠谱的办法是删除原来的规则,新增一条新规则</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>保存规则</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">把规则保存为文件中</span></span><br><span class="line">iptables-save &gt; /etc/sysconfig/iptables</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">直接保存</span></span><br><span class="line">service iptables save</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">加载规则</span></span><br><span class="line">iptables-restore &lt; /etc/sysconfig/iptables</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用的清空所有规则</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -F</span><br><span class="line">iptables -X</span><br><span class="line">iptables -Z</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">下面增加其它规则</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="常用模块"><a href="#常用模块" class="headerlink" title="常用模块"></a><strong>常用模块</strong></h3><p>我们使用-m 参数来指定使用到的iptables模块,常用的模块如下:</p>
<p>最开头匹配条件中,有些我们可以直接使用,能直接使用的属于基本条件匹配,还有些是不能直接使用的,不能直接使用的就需要借助模块来使用了,比如–sport&#x2F;–dport,比较常用的模块如下:</p>
<ol>
<li><p>tcp模块</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -I INPUT -s 192.168.1.10 -p tcp -m tcp --dport 80 -j ACCEPT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-p: 指明协议为tcp</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">-m: 指明智使用tcp模块</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--dport: 这个参数是属于tcp模块的,所以必须使用-m tcp指定,且--dport 必须指定-p参数</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果使用的协议跟模块是一个名字的话,则可以省略-m,如上面的命令等价于</span></span><br><span class="line">iptables -t filter -I INPUT -s 192.168.1.10 -p tcp --dport 80 -j ACCEPT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">但是建议大家还是尽量不要省略,因为那么多模块,有时你不能确定协议名跟模块名是否一致</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>multiport模块</p>
</li>
</ol>
   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -I INPUT -s 192.168.1.10 -p tcp -m multiport --dport 22,80 -j ACCEPT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">multiport: 用来指定多端口</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>string模块</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -I INPUT -p tcp --sport 80 -m string --algo bm --string &quot;helloworld&quot; -j REJECT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">algo: 指定对应的匹配算法,有bm,kpm,为必需项</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">string: 用来匹配数据包中包含有hellowworld</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>state模块</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">state模块用来追踪连接的状态</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">常用的状态有如下几种:</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">1. NEW： 建立连接中的第一个包</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">2. ESTABLISHED: TCP三次连接成功后为ESTABLISHED</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">3. RELATED: 相关边的数据连接为RELATED,如FTP的22跟23端口之间的控制连接</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">4. INVALID: 无法识别状态的数据包为INVALID</span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">5. UNTRACKED: 未被追踪的包为UNTRACKED</span></span><br><span class="line">  </span><br><span class="line">iptables -t filter -I INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">只有状态是RELATED,ESTABLISHED的才通过</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>iprange模块</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iptables -t filter -I INPUT -m iprange --src-range 192.168.1.1-192.168.1.10 -j DROP</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">iprange: 指定一个ip范围</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="白名单-x2F-黑名单机制"><a href="#白名单-x2F-黑名单机制" class="headerlink" title="白名单&#x2F;黑名单机制"></a><strong>白名单&#x2F;黑名单机制</strong></h3><p>这两种说法其实是一个意思,关键在于默认的策略是什么?</p>
<p>如果默认的策略为ACCEPT,如果在匹配完所有的规则之后还是无法匹配,该数据包会被ACCEPT</p>
<p>如果默认的策略为DROP,如果在匹配完所有的规则之后还是无法匹配,该数据包会被DROP</p>
<p>所以,正常情况下,</p>
<p><strong>如果默认的策略为ACCEPT,则链中的规则匹配上的数据包动作应该指定为DROP&#x2F;REJECT,这是黑名单机制</strong></p>
<p><strong>如果默认的策略为DROP,则链中的规则匹配上的数据包动作应该指定为ACCEPT,这是白名单机制</strong></p>
<p>考虑如下这种情况:</p>
<p>假如使用了默认规则为DROP</p>
<p>突然有一天,管理员使用了<code>iptables -F</code>清空了所有规则,这个时候你会发现管理员自己也登录不上去了,因为默认使用的是DROP,其它能够匹配的规则都被删除了,这个时候所有的数据包都会被DROP掉,这就是黑名单的缺点</p>
<p> 那如果我们使用了默认规则为ACCEPT</p>
<p>就算我们清空了其它规则,那管理员还能能够登录上去,因为默认接受所有数据包.</p>
<p><strong>所以我们一般会这样设置:</strong></p>
<ol>
<li><p>把默认规则设置为ACCEPT,然后在规则链的最后加一条规则:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">我们希望数据包能够匹配的规则</span></span><br><span class="line">...</span><br><span class="line">iptables -t filter -j REJECT</span><br></pre></td></tr></table></figure>

<p>这样的话,如果我们想要的数据包能够匹配前面已有的规则,不匹配的数据包执行到最后这条的时候会被拒绝.</p>
<p>就算是我们使用了<code>iptables -F</code>清空了规则,也能够使用默认的ACCEPT</p>
</li>
<li><p>我们可以复用crontab的定时执行机制写一条规则(允许通过指定ip登录的规则),让它每隔几份钟执行一次,这样不管我现在是什么规则导致我登录不了,过了几份钟后新规则生效,管理员就可以登录了, 不至于永远登录不了.</p>
</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://wiki.archlinux.org/index.php/Iptables">iptables-wiki</a></li>
<li><a href="https://www.liuliqiang.info/post/dive-in-iptables/">iptables 深度详解</a></li>
<li><a href="http://www.zsythink.net/archives/1199">iptables概念</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>highcharts中的svg在服务器端自动转化成图片</title>
    <url>/2017/09/17/highcharts%E4%B8%AD%E7%9A%84svg%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E8%87%AA%E5%8A%A8%E8%BD%AC%E5%8C%96%E6%88%90%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p><a href="https://highcharts.com/">Highcharts</a> 是一个用纯JavaScript编写的一个图表库， 能够很简单便捷的在web网站或是web应用程序添加有交互性的图表，并且免费提供给个人学习、个人网站和非商业用途使用。HighCharts支持的图表类型有曲线图、区域图、柱状图、饼状图、散状点图和综合图表等各种图形,效果非常美观,被广泛的应用于图表生成,类似图表库还有百度的Echart等.</p>
<span id="more"></span>

<h3 id="highcharts"><a href="#highcharts" class="headerlink" title="highcharts"></a><strong>highcharts</strong></h3><p>在html上生成highcharts图表是很容易的,<strong>我们只需要根据需求选择图形,然后定义样式,最后把数据塞进highcharts的series中即可渲染</strong>,而且highcharts还提供了页面(客户端)导出功能,可以很方便的导出为jpg、png、pdf、svg等格式,但是如何在服务器端(这里使用flask)自动生成图形然后保存为图片呢?</p>
<p>其实,我们在网面上看到的highcharts图形,其实不是一张图片,而是svg格式,具体的名词解释请自行搜索,说白了就是一种xml语言,格式如下:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">svg</span> <span class="attr">xmlns:xlink</span>=<span class="string">&quot;http://www.w3.org/1999/xlink&quot;</span> <span class="attr">version</span>=<span class="string">&quot;1.1&quot;</span> <span class="attr">class</span>=<span class="string">&quot;highcharts-root&quot;</span> <span class="attr">...</span>&gt;</span><span class="tag">&lt;/<span class="name">svg</span>&gt;</span> </span><br></pre></td></tr></table></figure>

<p>那么自动生成并保存为图片思路就很清晰了:</p>
<blockquote>
<ol>
<li>获取数据填充模板html</li>
<li>从模板html中获取所有svg标签,使用Highcharts自带function-getSVG()</li>
<li>使用cairosvg将svg转换成png或者使用phantomjs</li>
</ol>
</blockquote>
<p>第1点渲染模板html没有什么好说的,无非就是按照需求组织数据的问题,我们直接从第2点开始</p>
<h3 id="getSVG"><a href="#getSVG" class="headerlink" title="getSVG()"></a><strong>getSVG()</strong></h3><p>主要思路为<strong>一个按钮点击后通过getSVG获取所有图表的svg信息,再使用Highcharts.post构造post表单传入后端</strong>,当然这里也可以使用request库的xpath获取svg元素,Highcharts既然的现成的办法所以就直接用了,这里有个需要注意的地方就是如果想要保留图表原本的长宽的话,在chart.getSVG()时像下面代码所示那样使用长宽,不然得到的svg的长宽会变成默认值</p>
<p>模板html中getSVG()代码如下:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="title class_">Highcharts</span>.<span class="property">getSVG</span> = <span class="keyword">function</span> (<span class="params">charts</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">var</span> svgArr = [];</span></span><br><span class="line"><span class="language-javascript">    <span class="title class_">Highcharts</span>.<span class="title function_">each</span>(charts, <span class="keyword">function</span> (<span class="params">chart</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">    	  <span class="comment">//需要指定长宽,要不然导出svg时会长宽会变成默认值</span></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> svg = chart.<span class="title function_">getSVG</span>(&#123;<span class="attr">chart</span>:&#123;<span class="attr">width</span>:chart.<span class="property">chartWidth</span>,<span class="attr">height</span>:chart.<span class="property">chartHeight</span>&#125;&#125;);</span></span><br><span class="line"><span class="language-javascript">        svgArr.<span class="title function_">push</span>(svg);</span></span><br><span class="line"><span class="language-javascript">    &#125;);</span></span><br><span class="line"><span class="language-javascript">    <span class="keyword">return</span> svgArr;</span></span><br><span class="line"><span class="language-javascript">&#125;;</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"><span class="title class_">Highcharts</span>.<span class="property">exportCharts</span> = <span class="keyword">function</span> (<span class="params">charts, options</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Merge the options</span></span></span><br><span class="line"><span class="language-javascript">    options = <span class="title class_">Highcharts</span>.<span class="title function_">merge</span>(<span class="title class_">Highcharts</span>.<span class="title function_">getOptions</span>().<span class="property">exporting</span>, options);</span></span><br><span class="line"><span class="language-javascript">    <span class="comment">// Post to export server</span></span></span><br><span class="line"><span class="language-javascript">    <span class="title class_">Highcharts</span>.<span class="title function_">post</span>(options.<span class="property">url</span>, &#123;</span></span><br><span class="line"><span class="language-javascript">        <span class="comment">//filename: options.filename || &#x27;chart&#x27;,</span></span></span><br><span class="line"><span class="language-javascript">        <span class="attr">type</span>: options.<span class="property">type</span>,</span></span><br><span class="line"><span class="language-javascript">        <span class="comment">//width: options.width,</span></span></span><br><span class="line"><span class="language-javascript">        <span class="attr">svg</span>: <span class="title class_">Highcharts</span>.<span class="title function_">getSVG</span>(charts),</span></span><br><span class="line"><span class="language-javascript">        <span class="attr">_search</span>: <span class="variable language_">window</span>.<span class="property">location</span>.<span class="property">search</span></span></span><br><span class="line"><span class="language-javascript">    &#125;);</span></span><br><span class="line"><span class="language-javascript">&#125;;</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">$(<span class="string">&#x27;#exportPDF&#x27;</span>).<span class="title function_">click</span>(<span class="keyword">function</span> (<span class="params"></span>)&#123;</span></span><br><span class="line"><span class="language-javascript">     <span class="title class_">Highcharts</span>.<span class="title function_">exportCharts</span>([chart1,chart2],</span></span><br><span class="line"><span class="language-javascript">     &#123;</span></span><br><span class="line"><span class="language-javascript">        <span class="attr">type</span>: <span class="string">&#x27;image/png&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">        <span class="comment">//width: &#x27;600&#x27;,</span></span></span><br><span class="line"><span class="language-javascript">        <span class="attr">url</span>: <span class="string">&#x27;/pdfdownload/&#x27;</span></span></span><br><span class="line"><span class="language-javascript">    &#125;);</span></span><br><span class="line"><span class="language-javascript">&#125;);</span></span><br><span class="line"><span class="language-javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这样post到后端之后再request.form.to_dict()得到所有参数即可拿到svg信息,当然这里得到的是所有图表的svg串,需要进行分割处理,这里以<code>&lt;svg</code>为分割符,然后使用开源库**<a href="http://cairosvg.org/documentation/">cairosvg</a>**中的svg2png将svg转换成png图片.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cairosvg2png</span>(<span class="params">svg,chartlst</span>):</span><br><span class="line">    field_svg = re.split(<span class="string">r&quot;,&lt;svg &quot;</span>,svg)</span><br><span class="line">    <span class="comment">#第一个svg是完整的&lt;svg&gt;&lt;/svg&gt;,从第二个开始就需要在开头补&lt;svg ,</span></span><br><span class="line">    svg2png(bytestring=field_svg[<span class="number">0</span>], write_to=<span class="string">&quot;.\\static\\img\\pdf\\&quot;</span> + chartlst[<span class="number">0</span>] + <span class="string">&quot;.png&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(field_svg)-<span class="number">1</span>):</span><br><span class="line">        svg2png(bytestring=<span class="string">&quot;&lt;svg &quot;</span> + field_svg[x+<span class="number">1</span>], write_to=<span class="string">&quot;.\\static\\img\\pdf\\&quot;</span> + chartlst[x+<span class="number">1</span>] + <span class="string">&quot;.png&quot;</span>)  </span><br></pre></td></tr></table></figure>

<h3 id="phantomjs"><a href="#phantomjs" class="headerlink" title="phantomjs"></a><strong>phantomjs</strong></h3><p>除了上面的方法外,还可以使用phantomjs来自动在服务器端把svg转换成png&#x2F;jpeg&#x2F;pdf,这也是highcharts推荐的方式,phantomjs是一个很强大的基于WebKit的服务器端JavaScript API,也是开源库,使用场景也非常广泛.特别在自动化测试及爬虫方面,有兴趣的可以<a href="https://github.com/ariya/phantomjs">看这里</a></p>
<p>而且phantomjs支持以服务的方式一直运行,只需要把信息传递到它指定的端口即可,非常方便,这次是因为不能安装在服务器上,所以没有采纳,这里使用子进程运行phantomjs.exe,直接使用时需用-resources指定js文件:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exportForm</span>():</span><br><span class="line">    command = <span class="string">&quot;phantomjs.exe highcharts-convert.js -infile test.json -outfile chart2.png -scale 2.5 width=1000 -constr Chart -resources highcharts.js,jquery.js&quot;</span></span><br><span class="line">    child = subprocess.Popen(command,shell=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/ariya/phantomjs">phantomjs</a></li>
<li><a href="http://cairosvg.org/documentation/">cairosvg</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes学习(再谈kubernetes中的各种内存OOM)</title>
    <url>/2024/11/30/kubernetes-memory-talk/</url>
    <content><![CDATA[<p>本篇内容主要围绕kubernetes中的各种内存，对以下<code>灵魂话题</code>进行深入剖析:</p>
<ol>
<li>为什么容器没有到达limit后却被OOM了?</li>
<li>容器中产生的PageCache如何统计?</li>
<li>容器中emptydir类型为medium&#x3D;Memory是否会引起OOM?</li>
<li>如何更好地监控容器内存?</li>
<li>PageCache相关参数</li>
</ol>
<p>说明: 为了方便，这里不对容器与pod进行区分.</p>
<span id="more"></span>

<p>之前也写有几篇关于kubernetes中OOM的文章, 感兴趣的以先睹为快, </p>
<blockquote>
<ul>
<li><a href="https://izsk.me/2022/12/30/Kubernetes-emptyDir/">Kubernetes学习(深入理解emptyDir)</a></li>
<li><a href="https://izsk.me/2023/02/15/Kubernetes-Out-Of-Memory-1/">Kubernetes学习(kubernetes中的OOM-killer和应用程序运行时含义)</a></li>
<li><a href="https://izsk.me/2023/02/15/Kubernetes-Out-Of-Memory-2/">Kubernetes学习(pod驱逐机制及OOM流程)</a></li>
</ul>
</blockquote>
<p>这一篇与之前几篇的不同之处主要在于讨论了PageCache这类内存对OOM的影响.</p>
<h3 id="OOM日志怎么看"><a href="#OOM日志怎么看" class="headerlink" title="OOM日志怎么看?"></a>OOM日志怎么看?</h3><p>首先还是有必要对Linux OOM日志做一个细致的说明,以便更好地带入下面的话题.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Apr 14 14:13:32 sha2uvp-gert01 kernel: telegraf invoked OOM-killer: gfp_mask=0x201da, order=0, oom_score_adj=0</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: Free swap  = 0kB</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: Total swap = 8191996kB</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: 9437070 pages RAM</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: 0 pages HighMem/MovableOnly</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: [ 5205]   996  5205  1285299    28846     522   180556             0 mysqld</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: [ 6033]  1005  6033  7482510  2656268    8492  1473287             0 java</span><br><span class="line">      0             0 sh</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: [10985]     0 10985   128215    13778     169        0             0 python</span><br><span class="line">.....</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: Out of memory: Kill process 6033 (java) score 367 or sacrifice child</span><br><span class="line">Apr 14 14:13:36 sha2uvp-gert01 kernel: Killed process 6033 (java) total-vm:29930040kB, anon-rss:10625048kB, file-rss:0kB, shmem-rss:24kB</span><br></pre></td></tr></table></figure>

<p>以上log是发生了oom后内核打印出来的信息, 有几个重要的参数需要关注:<br>前几行说明系统在发生OOM时当前内存的一个snapshot, 第一行直接说明了是telegraf触发了OOM-killer<br>然后中间几行是发生OOM时各个进程内存的相关情况,需要说明的是:<br>最后二行是被OOM-killer选出来的进程(这里是telegraf)的内存占用情况<br>中间那几行中的total_vm那列所表示的值的单位<code>页</code>，在转换为内存kB时需要<code>*4</code>,<br>比如上面pid: 6033在中间的total_vm的值为<code>7482510*4=29930040</code>，刚好等于最后一行中的total-vm的kB值。</p>
<p>好。这里再解释一下total_vm, anon-rss, file-rss, shmem-rss这几个名词的含义:</p>
<blockquote>
<ul>
<li>total-vm：<br>含义：总虚拟内存（Total Virtual Memory）。<br>解释：表示进程使用的总虚拟内存量，包括所有已分配的内存（无论是否实际使用），即进程请求的虚拟地址空间的大小。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>anon-rss：<br>含义：匿名驻留集（Anonymous Resident Set Size）。<br>解释：表示进程当前使用的匿名内存部分，这部分内存不与任何文件关联，通常由动态分配（如通过 malloc()）生成。它反映了实际驻留在物理内存中的匿名页的数量</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>file-rss：<br>含义：文件驻留集（File Resident Set Size）。<br>解释：表示进程使用的与文件映射相关的内存部分。这部分内存是通过文件映射（如 mmap）分配给进程的，反映了实际驻留在物理内存中的文件页的数量。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>shmem-rss：<br>含义：共享内存驻留集（Shared Memory Resident Set Size）。<br>解释：表示进程使用的共享内存部分。这种内存可以被多个进程访问，通常用于进程间通信, 最常见的有&#x2F;dev&#x2F;shm。</li>
</ul>
</blockquote>
<p>使用一个简单的例子来说明:<br>假设一个进程通过 malloc() 请求了 1GB 的内存，但只使用了其中的一部分，比如256MB，其中&#x2F;dev&#x2F;shm中使用了100MB, 那么：<br>total-vm 将显示为 1GB<br>anon-rss 将显示为 256MB（实际使用的匿名内存）<br>没有映射到任何文件，file-rss 将为 0<br>因为使用了&#x2F;dev&#x2F;shm, 则shmem-rss将为100MB</p>
<p>total-vm与shmem-rss都比较好理解，这里再重点说一下anon-rss与file-rss.</p>
<p>上面提到, anon-rss是进程当前使用的<code>匿名内存, 它不与任何文件相关联</code>, 这到底是什么意思呢?<br>打个比方:</p>
<p>我在C程序中通过malloc()方法申请了并使用了一段内存,之所以叫匿名内存指的是不需要手动指定对应的文件(linux中万物皆文件，内存也是如此)<br>最常见的是在<code>堆和栈</code>上分配的内存都属于 anon-rss, 这部分内存中的数据即使<code>被销毁了也不会被写回到任何文件中</code>,这就是它不与任何文件相关联的特点</p>
<p>而file-rss内存则是通过文件映射（如 mmap）分配给进程的，这又是什么意思呢?<br>打个比方,假设你有一个进程打开了一个大文件并且该文件被映射到内存中。这个过程可能涉及以下步骤：</p>
<ol>
<li><p>打开文件：进程使用系统调用打开了这个文件。</p>
</li>
<li><p>内存映射：进程通过 mmap() 系统调用将该文件映射到其虚拟地址空间。这使得文件内容可以直接在内存中访问，而不需要每次都从磁盘读取。</p>
</li>
<li><p>使用文件：当进程读取或修改该文件的内容时，相关的内存页会被加载到物理内存中。</p>
</li>
</ol>
<p>这就是最常见的场景: <code>在处理大文件或进行大量I/O操作时，file-rss的值可能会显著增加.</code></p>
<p>ok, 了解了这些概念之后,再来分析内核中出现如上的OOM日志后就可以进一步分析。</p>
<h3 id="为什么容器没有到达limit后却被OOM了"><a href="#为什么容器没有到达limit后却被OOM了" class="headerlink" title="为什么容器没有到达limit后却被OOM了?"></a>为什么容器没有到达limit后却被OOM了?</h3><p>正常的超过limit被OOM的场景这里就不分析了，如果监控比较完善就应该很容易发现。<br>那有没有可能一个node上的所有容器都没有达到limit，但产生了OOM事件，其中一个容器被OOM了?</p>
<p>答案是肯定存在的, 有如下的几种常见场景都可能造成这个局面</p>
<p>一、节点上有不受kubernetes管控的程序</p>
<p>从<a href="https://izsk.me/2023/02/15/Kubernetes-Out-Of-Memory-2/">这里</a>可以知道, 节点上显示的可分配资源(allocatable)无法控制运行在集群节点上、但在集群之外的应用使用的资源。<br>也就是说，如果我直接在节点上运行一个进程，这个进程所占用的内存是不被kubelet监控的，因此当所有容器使用的内存加上节点上直接运行的进程的内存达到系统的安全内存阈值之下(见最后一节)时，系统就会通过OOM-killer来杀死某些进程来让节点可用内存维持在安全阈值以下,<br>这个被杀的进程可能就是某个容器。至于系统怎么选择出被杀进程的过程将不在本文中展开。</p>
<p>二、容器产生了大量的pageCache<br>详情见下文</p>
<h3 id="容器中产生的PageCache如何统计"><a href="#容器中产生的PageCache如何统计" class="headerlink" title="容器中产生的PageCache如何统计?"></a>容器中产生的PageCache如何统计?</h3><p>考虑这样一种场景: 节点A上的容器B在运行过程中产生大量的page cache, 这部分内存该如何统计呢?</p>
<p>这个例子相对来说比较复杂, 同样有几个概率需要先进行说明:</p>
<p>首先，解释下page cache是什么?</p>
<p>每当读取一个文件，数据会被缓存在 page cache 中以避免后续访问时重复读取磁盘带来的昂贵开销。也就是说我们平时访问文件（read 或 mmap 系统调用）都会创建对应的 page cache，<code>这部分内存由操作系统管理，并不记录在用户程序的内存开销中</code></p>
<p><code>Page Cache 是由内核管理的内存</code>，位于虚拟文件系统（VFS）层与具体文件系统层之间。</p>
<p>常见的查看page cache的命令如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.7G        239M        3.2G        580K        251M        3.2G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>

<p>其中:<br>shared对应的是shared memory共享内存。</p>
<p>buff&#x2F;cache对应的就是page cache(这里没有必要去区别buff跟cache到底是什么区别，你看它们都放在了同一列)</p>
<p>因此回到上面那个场景,如果容器B进行了大量的文件读写,将会导致free中的buff&#x2F;cache值增大,当大到一定程序时就有可能产生OOM事件, 这又是为什么呢?</p>
<p><code>Page Cache 是由内核管理的内存</code>,请务必牢记这句话,下面的分析无时不刻地在印证这句话。<br>这句话有两层含义: 一是<code>page cache是由内核管理</code>, 二是<code>Page Cache是一种内存</code></p>
<p>既然它是由内核直接管理的，因此将不会计算在容器的limit内，所以就会有这样的情况:<br>容器A(假如limit设置为1Gi)中的进程进行了大量的文件读写,可能产生了2Gi的pageCache, 但容器A本身的内存使用量（包括 <code>anon-rss 和 file-rss</code>）一直保持在1GiB, 所以不会OOM.</p>
<p>但对于节点A来说就不一定，考虑一种极端情况, 如果节点A上存在很多容器B这样的问题，<br>虽然内核对pagecache有清理机制(LRU（Least Recently Used）算法)，当page cache增涨超出了内核安全内存的阈值之下，那么OOM-killer将会被激活,然后杀死进程以让系统可用内存保持在安全线之下.</p>
<p>从这个例子可以看出，虽然<code>所有的容器使用的内存都在limit之内，但由于pagecache的原因还是会产生OOM事件</code></p>
<p>还有以下结论我这里直接给出答案:</p>
<p>Q1: 一个节点上产生的pagecache是否可以在这个节点上的所有容器中进行共享?<br>A: 由于pagecache是由内核管理的，所以当容器读取宿主机上相同的文件时，产生的PageCache是可以共享的，不过存在一些潜在的安全和一致性问题不在此讨论。</p>
<p>Q2: 如果可以共享,这部分cache内存是算在哪个容器上的？<br>A: 有一个<code>metrics: container_memory_cache</code>是来统计容器使用了多少cache的,<br>产生的PageCache是算在第一个读取文件的容器对应的container_memory_cache, 后续读取相同文件的容器的container_memory_cache则不会被重复统计，<code>即使第一个容器被删除了，后面的容器的container_memory_cache也不统计</code>。</p>
<p>Q3: 在容器中是否可以管理pagecache?<br>A: 由于pagecache是由内核统一管理的，正常情况下是不能在容器中进行管理的,注意这里说的是默认情况下你无法在容器中执行清理pagecache的操作, 但在<code>privileged模式</code>下可以</p>
<p>Q4: 容器单是否可以隔离pagecache?<br>A: 不行, pagecache 是由宿主机内核统一管理</p>
<p>Q5: 如果在宿主机上清理了pagecache, 是否会立即反映到容器里?<br>A: 是的, 这是因为容器共享宿主机的内核和物理内存，虽然容器通过cgroups限制了内存的使用上限，但还是因为pagecache 是由宿主机内核统一管理, 如果在宿主机上清理缓存，容器内也会感受到内存的释放效果。</p>
<h3 id="容器中emptydir类型为medium-x3D-Memory是否会引起OOM"><a href="#容器中emptydir类型为medium-x3D-Memory是否会引起OOM" class="headerlink" title="容器中emptydir类型为medium&#x3D;Memory是否会引起OOM?"></a>容器中emptydir类型为medium&#x3D;Memory是否会引起OOM?</h3><p>对于在pod中使用emptydir类型为medium&#x3D;Memory，一般是直接将这种volume直接挂载到容器中的&#x2F;dev&#x2F;shm。</p>
<p>这个话题在我的另一篇文章: <a href="https://izsk.me/2022/12/30/Kubernetes-emptyDir/">Kubernetes学习(深入理解emptyDir)</a>中详细进行了说明, 感兴趣的可以移步,这里也直接说结论了:</p>
<p><code>emptydir类型为medium=Memory中指定的内存不能超过pod中所有容器limit.memory资源之和, 所以会引起OOM</code></p>
<h3 id="如何更好地监控容器内存"><a href="#如何更好地监控容器内存" class="headerlink" title="如何更好地监控容器内存?"></a>如何更好地监控容器内存?</h3><p>最后我想再说一下, 容器中存在多个与内存相关的metrics, 在生产上这几个metrics如何更好地反应容器的真实使用情况呢?<br>先来看看有哪些metrics</p>
<table>
<thead>
<tr>
<th><strong>指标名称</strong></th>
<th><strong>含义</strong></th>
<th><strong>是否包括缓存</strong></th>
<th><strong>实际内存反映</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>container_memory_usage_bytes</code></td>
<td>容器使用的总内存，包括匿名内存、Page Cache 和 Slab Cache。</td>
<td>是</td>
<td>部分反映实际使用，但包括可回收内存。</td>
</tr>
<tr>
<td><code>container_memory_working_set_bytes</code></td>
<td>实际使用的内存，不包括可回收的缓存（Page Cache 和交换空间）。</td>
<td>否</td>
<td>最能反映实际需求的内存。</td>
</tr>
<tr>
<td><code>container_memory_rss</code></td>
<td>容器分配的匿名内存和文件映射内存，实际占用的物理内存部分。</td>
<td>否</td>
<td>反映了物理内存使用。</td>
</tr>
<tr>
<td><code>container_memory_cache</code></td>
<td>文件系统缓存（Page Cache），用于加速磁盘访问。</td>
<td>是</td>
<td>只包含缓存，不反映应用需求。</td>
</tr>
</tbody></table>
<p>总结起来的结论如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">container_memory_cache = active_cache + inactive_cache</span><br><span class="line">container_memory_usage_bytes = container_memory_rss + container_memory_cache</span><br><span class="line">container_memory_working_set_bytes = container_memory_usage_bytes - inactive_cache = container_memory_rss + active_cache</span><br></pre></td></tr></table></figure>

<ol>
<li><code>container_memory_working_set_bytes是否OOM的唯一指标，当这个值到达limit后, OOM-killer将会生效</code></li>
<li>容器中的共享内存(如&#x2F;dev&#x2F;shm)包含在除container_memory_cache外的其它3个metrics中。</li>
<li>container_memory_cache无法直接区分出active和inactive的值, 如果需要采集，只能通过解析容器的<code>/sys/fs/cgroups/memory/memory.stat</code>文件获取</li>
<li>文件映射内存是指通过 <code>mmap()</code> 系统调用将一个文件的内容映射到进程的虚拟地址空间中。这种方式允许进程直接访问文件数据，而不需要通过标准 I&#x2F;O 操作（如 <code>read()</code> 和 <code>write()</code>）进行数据传输</li>
<li>container_memory_rss提到的实际占用的物理内存，这里的物理内存是实实在在的物理层面上的内存概念，有别于虚拟内存。</li>
<li>表格中不存在<code>container_memory_swap以及container_memory_max_usage_bytes</code>, 这两个没有歧义不用在此讨论</li>
</ol>
<p>注:<br>active_file：活跃 LRU 列表中所有 file-backed 进程使用内存。<br>inactive_file：不活跃 LRU 列表中所有 file-backed 进程使用内存。<br><code>Linux内核使用LRU（Least Recently Used）算法来管理活动和非活动缓存</code>,当内存需要被回收时，系统会首先考虑回收非活动缓存，而保留活跃缓存。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20241130195008.png"></p>
<p>注: 该图引用于<a href="https://kubeservice.cn/2024/06/24/fault-k8s-pod-container-memory-high/">这里</a>,原作者画的有识破，图中下面这个红色的应该是<code>total_inactive_file</code></p>
<h3 id="PageCache相关参数"><a href="#PageCache相关参数" class="headerlink" title="PageCache相关参数"></a>PageCache相关参数</h3><p>Linux内存水位控制与可用内存计算中存在两个水位:</p>
<ol>
<li>low watermark：当 free 内存低于 low watermark 时触发异步内存回收</li>
<li>min watermark：当内存低于 min watermark 时暂停内存分配，立即进行内存回收</li>
</ol>
<p>也就是说系统中剩余的内存不能低于min watermark，这是一个操作系统的保护机制：预留一部分内存给内存回收等关键程序使用。<br>可以通过 cat &#x2F;proc&#x2F;zoneinfo 看到 min watermark 的取值，单位是页。</p>
<p>从上面的几个例子中可以看到PageCache的使用也会引起OOM的产生, linux中有以下参数可以控制pagecache的行为:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">`vm.vfs_cache_pressure`是一个内核参数，控制 Linux 系统如何平衡 inode 和 dentry 缓存（VFS 缓存）的回收与匿名内存（进程使用的堆栈、数据段等）回收之间的优先级</span><br><span class="line"></span><br><span class="line">取值范围和含义：</span><br><span class="line">1. 默认值: 100,内核会对 inode 和 dentry 缓存与其他内存类型（如匿名内存、page cache）进行平衡回收。</span><br><span class="line">2. 值 &gt; 100, 增大回收 inode 和 dentry 缓存的倾向，意味着系统会更快地释放这些缓存，以腾出内存给其他用途（如进程内存）</span><br><span class="line">   示例：vm.vfs_cache_pressure=200 表示内核将更积极地回收 VFS 缓存。</span><br><span class="line">3. 值 &lt; 100, 降低对VFS缓存的回收倾向，保留更多的 inode 和 dentry 缓存，可能有助于加速文件系统访问。</span><br><span class="line"></span><br><span class="line">推荐按场景进行设置:</span><br><span class="line">适用于内存紧张的场景，希望减少缓存占用，优先保证进程内存, 推荐高值&gt;100</span><br><span class="line">适用于I/O密集型工作负载，希望提高文件访问性能，减少缓存被回收，推荐低值&lt;100</span><br><span class="line"></span><br><span class="line">`vm.dirty_ratio`: 定义脏页占用系统内存的最大比例。达到该比例时，会触发数据写回磁盘,调整这个参数可以降低pagecache的大小。</span><br><span class="line"></span><br><span class="line">`vm.dirty_background_ratio`: 定义后台写回进程开始写脏页的比例。比 dirty_ratio 小。</span><br></pre></td></tr></table></figure>


<p>当然，还有一些其它的内存对象如Slab Cache等, 不再此讨论。</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://izsk.me/2022/12/30/Kubernetes-emptyDir/">https://izsk.me/2022/12/30/Kubernetes-emptyDir/</a></li>
<li><a href="https://izsk.me/2023/02/09/Kubernetes-Out-Of-Memory-1/">https://izsk.me/2023/02/09/Kubernetes-Out-Of-Memory-1/</a></li>
<li><a href="https://izsk.me/2023/02/15/Kubernetes-Out-Of-Memory-2/">https://izsk.me/2023/02/15/Kubernetes-Out-Of-Memory-2/</a></li>
<li><a href="https://help.aliyun.com/zh/alinux/support/causes-of-and-solutions-to-the-issue-of-oom-killer-being-triggered">https://help.aliyun.com/zh/alinux/support/causes-of-and-solutions-to-the-issue-of-oom-killer-being-triggered</a></li>
<li><a href="https://blog.hdls.me/17255242628777.html">https://blog.hdls.me/17255242628777.html</a></li>
<li><a href="https://blog.lv5.moe/p/from-k8s-pod-memory-usage-to-linux-memory-management">https://blog.lv5.moe/p/from-k8s-pod-memory-usage-to-linux-memory-management</a></li>
<li><a href="https://faun.pub/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d">https://faun.pub/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d</a></li>
<li><a href="https://kubeservice.cn/2024/06/24/fault-k8s-pod-container-memory-high/">https://kubeservice.cn/2024/06/24/fault-k8s-pod-container-memory-high/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>使用crash查看Linux系统异常重启原因</title>
    <url>/2020/09/27/kernel-panic-debug/</url>
    <content><![CDATA[<p>最近有一台物理机间歇性的发生重启, 本以为是偶发事件就没多在意，今天又给重启了, 因此决定探探究竟</p>
<span id="more"></span>

<p>首先看一下kdump服务是否开启, kdump主要用于内核发生crash时记录相关上下文的. 用来转储运行内存的一个工具</p>
<p>简言之: 系统一旦崩溃，内核就没法正常工作了，这个时候将由kdump提供一个用于捕获当前运行信息的内核，</p>
<p>该内核会将此时内存中的所有运行状态和数据信息收集到一个dump core文件中以便之后分析崩溃原因</p>
<p>之后系统便会重启.</p>
<p>关于kdump在内核已经崩溃的前提下还能够捕获到异常信息, 原理可<a href="https://blog.csdn.net/zhangskd/article/details/38084337">参考</a></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927180148.png"></p>
<p>这里引用<a href="https://blog.csdn.net/zhangskd/article/details/38084337">一张图</a></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927185450.png"></p>
<p>要分析core文件，可以使用crash工具，首先是安装:</p>
<p>注意, 要安装与当前内核对应的版本, 要不然crash无法使用.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-693.el7.x86_64.rpm</span><br><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-693.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-693.el7.x86_64.rpm</span><br><span class="line">rpm -ivh kernel-debuginfo-3.10.0-693.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">yum install crash</span><br></pre></td></tr></table></figure>

<p>通常, 内核crash后保存的core文件保存在<code>/var/crash</code>目录下，在该目录下找到最近一次crash的时间，使用以下命令启动crash分析dump.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crash vmcore /usr/lib/debug/lib/modules/3.10.0-693.el7.x86_64/vmlinux</span><br></pre></td></tr></table></figure>

<p>启动之后就进入到了crash命令行，很明显地显示出crash的原因,如下图红框所示:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927175237.png"></p>
<p>这里简要地反应出内核crash时的一些内存信息快照，包含一些物理信息, tasks数量，最重要的就是导致crash的command与panic的原因</p>
<p>这里可直接使用ps查看进程的相关信息,跟linux 命令行下的ps功能相似</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200927175207.png"></p>
<p>还可以使用bt命令来查看trace信息, 当然都是一些汇编层的内容，需要有一定的专业知识，理解起来比较困难.</p>
<p>从上面已经知道了panic的原因后就可对症下药.</p>
<p>另外crash支持很多的子命令,大家可使用man crash查看，比较常用的为</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crash ps <span class="comment"># 查看进程</span></span><br><span class="line">crash sys <span class="comment"># 查看系统信息</span></span><br><span class="line">crash bt <span class="comment"># 查看堆栈信息</span></span><br><span class="line">crash files <span class="comment"># 查看异常时进程打开的文件</span></span><br><span class="line">crash task <span class="comment"># 查看指定task的信息</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.linuxtechi.com/how-to-enable-kdump-on-rhel-7-and-centos-7/">https://www.linuxtechi.com/how-to-enable-kdump-on-rhel-7-and-centos-7/</a></li>
<li><a href="https://www.cnblogs.com/doctormo/p/12619485.html">https://www.cnblogs.com/doctormo/p/12619485.html</a></li>
<li><a href="https://blog.csdn.net/zhangskd/article/details/38084337">https://blog.csdn.net/zhangskd/article/details/38084337</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql学习(SQL执行过程)</title>
    <url>/2017/09/23/mysql%E5%AD%A6%E4%B9%A0%E4%BA%8C(SQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B)/</url>
    <content><![CDATA[<p>数据库一般都分为服务器端跟客户端两部分,客户端可能有多样的形式,我们使用客户端执行一条sql语句到服务器返回数据这个过程中,整个流程是如何操作的呢? 这里以mysql数据库来说.</p>
<span id="more"></span>

<p>这里再借用一条图来看看mysql的基础组件</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/mysqlbasic1.jpg" alt="mysqlbasic1"></p>
<p>可以看出mysql的连接可以通过多种方式进来,都可以称之为客户端,当我们在客户端执行语句时(比如最简单的select),客户端会把这条 SQL 语句发送给服务器端,让服务器端的进程来处理这语句,这里要注意的是<strong>客户端不会对sql语句做任何的工作,只是负责把sql语句发送到服务器端</strong>,那服务器跟客户端的通信方式又是如何的呢?</p>
<h3 id="C-x2F-S通信机制"><a href="#C-x2F-S通信机制" class="headerlink" title="C&#x2F;S通信机制"></a><strong>C&#x2F;S通信机制</strong></h3><p>关于这部分不过分去深究原理了,大家可以看看这篇<a href="http://www.ijava.com/article/mysql-20822.html">文章</a>,对这部分写的不错,这里挑重点:</p>
<p>MySQL客户端和服务器之间的通信协议是“半双工”的,这意味着,在任何一个时刻,要么是由服务器向客户端发送数据,要么是由客户端向服务器发送数据,这两个动作不能同时发生,服务器会使用<strong>TCP</strong>监听一个本地socket端口或本地socket链接.当一个客户端的连接请求到达,就会执行握手和权限验证.如果验证成功,会话开始.客户端发送消息,服务器会以一个适合该发送命令的数据类型的数据集或一条消息进行回复.当客户端发送完成后,会发送一个特殊的命令,告诉服务器已发送,然后会话结束. 通信的基本单位是应用程序包.多个指令责成一个包.答复可以包含几个包.</p>
<p>所以说<strong>c&#x2F;s通信是分两个阶段:握手认证阶段和命令执行阶段</strong></p>
<p>还有一点就是,通常客户端给服务器发送的数据包很少(其它就是些sql语句),所以基本一个数据包就能够传递给服务器,相反的,一般服务器响应给用户的数据通常很多,由多个数据包组成.当服务器开始响应客户端请求时,客户端必须完整的接受整个返回结果,而不是简单的只收取前面几条结果,然后让服务器停止发送数据</p>
<p>当sql到达服务端之后,服务端会如何处理呢?</p>
<h3 id="服务器处理"><a href="#服务器处理" class="headerlink" title="服务器处理"></a><strong>服务器处理</strong></h3><p>服务器会生成一个进程来处理这个客户端的请求,服务器跟客户端是一一对应的关系</p>
<p>我们再来看一张图:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/mysqlsql1.jpg" alt="mysqlsql1"></p>
<p>从上面的图可以看出流向大概为:</p>
<p><strong>client发送SQL –&gt; [ 查询缓存 ] –&gt; 解析器 –&gt; 生成解析树 –&gt; 预处理器 –&gt; 再次生成解析器 –&gt; 优化器 –&gt; 生成执行计划 –&gt; 调用存储引擎API –&gt; 返回结果到client</strong></p>
<h4 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a><strong>查询缓存</strong></h4><p>这里要重点提下<strong>查询缓存,mysql中的查询缓存类似于Oracle中的软硬解析的机制</strong></p>
<p>在解析一个查询语句之前,<code>如果查询缓存是打开的</code>,那么mysql会优先检查这个查询是否命中查询缓存中的数据.使用查询语句、数据库名称、客户端协议的版本等因素生成一个对大小写敏感的哈希值,查询和缓存中的查询即使只有一个字节不同(或者sql语句相同但是用户权限相关的不同,总之很多的条件来判断是否为<code>同一条</code>sql),那也不会匹配缓存结果,这种情况下查询就会进入下一阶段的处理.</p>
<p>如果当前的查询恰好命中了查询缓存,那么在返回查询结果之前mysql会检查一次用户权限.这仍然是无须解析查询SQL语句的,因为在查询缓存中已经存放了当前 查询需要访问的表信息.如果权限没有问题,mysql会跳过所有其他阶段,直接从缓存中拿到结果并返回给客户端.这种情况下,查询不会被解析,不用生成执行计划,不会被执行,所以查询缓存可以在一定程度上提高查询效率,但是打开查询缓存也有一定的<strong>弊端</strong>:</p>
<p>如果是写多读少的场景下话,大量的更新&#x2F;插入操作会使查询缓存的更新频率也变得多,这对性能有很大影响</p>
<p>对于InnoDB而言,事物的一些特性还会限制查询缓存的使用.当在事物A中修改了B表时,因为在事物提交之前,对B表的修改对其他的事物而言是不可见的.为了保证缓存结果的正确性,InnoDB采取的措施让所有涉及到该B表的查询在事物A提交之前是不可缓存的.如果A事物长时间运行,会严重影响查询缓存的命中率</p>
<h4 id="解析器及预处理"><a href="#解析器及预处理" class="headerlink" title="解析器及预处理"></a><strong>解析器及预处理</strong></h4><p>其实就是检查sql语句是否正确及权限是否能够访问,首先mysql通过关键字将SQL语句进行解析,并生成一颗对应的“解析树”.mysql解析器将使用mysql语法规则验证和解析查询；预处理器则根据一些mysql规则进一步检查解析数是否合法</p>
<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a><strong>优化器</strong></h4><p>到了这步,优化器会对sql语句进行优化,包括表的连接方式,条件的先后等都会进行优化,而且现在的优化器越来越智能,优化器将语法树转换成执行计划,一条查询可以有很多种执行方式,最后都返回相同的结果.优化器的作用就是找到这其中最好的执行计划,这就是<strong>CBO(cost-based optimizer 基于成本的优化器)</strong></p>
<h4 id="执行引擎"><a href="#执行引擎" class="headerlink" title="执行引擎"></a><strong>执行引擎</strong></h4><p>mysql简单的根据执行计划给出的指令逐步执行.在根据执行计划逐步执行的过程中,有大量的操作需要通过调用存储引擎实现的接口来完成.为了执行查询,mysql只需要重复执行计划中的各个操作,直到完成所有的数据查询,当然不同的存储引擎实现的API接口是不一样的,内部的机制也有所不同,如myisam跟innodb就有所不同,innodb因为支持事务,所以有更多额外的机制。</p>
<h4 id="返回结果"><a href="#返回结果" class="headerlink" title="返回结果"></a><strong>返回结果</strong></h4><p> 查询执行的最后一个阶段是将结果返回给客户端.即使查询不需要返回结果给客户端,mysql仍然会返回这个查询的一些信息,如该查询影响到的行数.如果查询可以被缓存,那么mysql在这个阶段也会将结果放到查询缓存中.</p>
<p>mysql将结果集返回客户端是一个增量、逐步返回的过程.这样有两个好处:服务器端无须存储太多的结果,也就不会因为返回太多结果而消耗太多的内存；这样处理也让msyql客户端第一时间获得返回的结果.</p>
<p>结果集中的每一行都会以一个满足mysql客户端&#x2F;服务器通信协议的包发送,再通过tcp协议进行传输,在tcp传输的过程中,可能对mysql的封包进行缓存然后批量传输</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.ijava.com/article/mysql-20822.html">详解MySQL服务器和客户端通信协议</a></li>
<li><a href="http://blog.itpub.net/30604784/viewspace-1978430/">SQL语句的执行过程详解</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql学习(MVCC实现)</title>
    <url>/2017/12/02/mysql%E5%AD%A6%E4%B9%A0%E5%9B%9B(MVCC%E5%AE%9E%E7%8E%B0)/</url>
    <content><![CDATA[<p>MVCC(Multiversion Concurrency Control,多版本并发控制)是数据库为了提高并发的一种机制,现已普遍运用于各大数据库系统中,如Oracle,MS SQL Server 2005+, Postgresql, Firebird, Maria等等,开源数据库MYSQL中流行的INNODB引擎也采用了类似的并发控制技术.本文就来说说INNODB的MVCC实现原理.</p>
<span id="more"></span>

<h3 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a><strong>MVCC</strong></h3><p>MVCC(Multiversion Concurrency Control,多版本并发控制),即多版本并发控制技术,它使得大部分支持行锁的事务引擎,不再单纯的使用行锁来进行数据库的并发控制,取而代之的是,把数据库的行锁与行的多个版本结合起来,只需要很小的开销,就可以实现一致性非锁定读(在repeatable read隔离等级下),从而大大提高数据库系统的并发性能.</p>
<h3 id="MVCC实现"><a href="#MVCC实现" class="headerlink" title="MVCC实现"></a><strong>MVCC实现</strong></h3><p>MVCC是通过保存数据在某个时间点的快照来实现的.  不同存储引擎的MVCC实现是不同的,典型的有乐观并发控制和悲观并发控制,<strong>mysql的innodb则是使用的乐观锁机制,即在每次事务开始之前取出该行的版本号,再次取出时会比对该行数据的版本号是否是事务之前的版本号.</strong></p>
<h3 id="MVCC原理"><a href="#MVCC原理" class="headerlink" title="MVCC原理"></a><strong>MVCC原理</strong></h3><p>对于mysql来说,MVCC由于其实现原理,只支持read committed和repeatable read隔离等级.</p>
<p>MVCC可以提供基于某个时间点的快照,使得对于事务看来,总是可以提供与事务开始时刻相一致的数据,而不管这个事务执行的时间有多长.所以在不同的事务看来,同一时刻看到的相同行的数据可能是不一样的,即一个行可能有多个版本.</p>
<p>其它innodb会为每行数据自动维护三个隐藏字段,这三个字段通过查询无法看到,字段如下:</p>
<table>
<thead>
<tr>
<th>字段1</th>
<th align="left">字段2</th>
<th>DB_TRX_ID</th>
<th>DB_ROLL_PTR</th>
<th>DB_ROW_ID</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td align="left">yangming</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td align="left">zhouxingchi</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>说明如下:</p>
<blockquote>
<ul>
<li>DB_TRX_ID: 长6字节,表示插入或更新行的最后一个事务的事务标识符.另外,删除在内部被视为一个更新,其中有个特殊位可以用来标记为该行删除</li>
<li>DB_ROLL_PTR: 长7字节,该字段用于指向当前记录项中的undo log中的记录,也就是该语句执行之前该行数据的地址</li>
<li>DB_ROW_ID: 大小是6byte,该值随新行插入单调增加,当由innodb自动产生聚簇索引时,只有聚簇索引包括这个DB_ROW_ID的值,该值不会出现在其它索引中</li>
</ul>
</blockquote>
<p>更多的官方的说明请看<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html">这里</a></p>
<p>所以对于增删改查4个场景来说,mvcc流程有点不同,以当前事务的版本号为当前版本号且隔离级别为repeatable read:</p>
<ol>
<li><p>select</p>
<p>只有同时满足了下面两个条件的行,才能被返回:</p>
<blockquote>
<ol>
<li>行的被修改版本号小于或者等于该事务号</li>
<li>行的被删除版本号要么没有被定义,要么大于事务的版本号:行的删除版本号如果没有被定义,说明该行没有被删除过;如果删除版本号大于当前事务的事务号,说明该行是被该事务后面启动的事务删除的,由于是repeatable read隔离等级,后开始的事务对数据的影响不应该被先开始的事务看见,所以该行应该被返回</li>
</ol>
</blockquote>
</li>
<li><p>insert</p>
<p>对新插入的行,行的更新版本被修改为该事务的事务号</p>
</li>
<li><p>delete(delete被视为更新操作)</p>
<p>对于删除,innodb直接把该行的被删除版本号设置为当前的事务号,相当于标记为删除,而不是实际删除(commit时删除)</p>
</li>
<li><p>update</p>
<p>在更新行的时候,innodb会把原来的行复制一份到回滚段中,然后新插入了一行记录,并保存其创建时间为当前事务的ID,同时保存当前事务ID到要行的删除位</p>
</li>
</ol>
<p>从上面的上述策略可以看出,在读取数据的时候,innodb几乎不用获得任何锁, 每个查询都通过版本检查,只获得自己需要的数据版本,从而大大提高了系统的并发度.</p>
<p>但是为了实现多版本,innodb必须对每行增加相应的字段来存储版本信息,同时需要维护每一行的版本信息,而且在检索行的时候,需要进行版本的比较,因而降低了查询效率;innodb还必须定期清理不再需要的行版本,及时回收空间,这也增加了一些开销</p>
<h3 id="一致性非锁定读-快照读"><a href="#一致性非锁定读-快照读" class="headerlink" title="一致性非锁定读(快照读)"></a><strong>一致性非锁定读(快照读)</strong></h3><p>在REPEATABLE READ事务隔离级别下,同一事务内的一致性读均会读取到该事务中第一个读创建的快照,其他事务在之后提交或未提交的更新对当前事务的读均不可见,除非提交了该事务并开启新事务发起新查询.</p>
<p>事务A查询某些数据,而事务B刚好在对它些数据进行修改,正常情况下事务B会对这些数据加x锁, 其它事务便不能获取任何锁了,事务A只能等待,直到事务B释放,但是此时事务A会从undo log中读取这些数据的’’前一次备份’’,大大提高的查询的并发,不至于大量的读请求饿死.</p>
<p>所以,mvcc跟undo log结合,给数据库的并发提高了很大一步.</p>
<p>undo跟redo相关信息,可参考<a href="https://izsk.me/2017/11/30/mysql%E4%B9%8Bredo,undo/">这里</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html">mysql官方</a></li>
<li><a href="http://lib.csdn.net/article/mysql/32694">innodb 多版本并发控制原理详解</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql学习(mysql基础架构)</title>
    <url>/2017/09/02/mysql%E5%AD%A6%E4%B9%A0%E4%B8%80(mysql%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84)/</url>
    <content><![CDATA[<h3 id="Mysql与MariaDB"><a href="#Mysql与MariaDB" class="headerlink" title="Mysql与MariaDB"></a><strong>Mysql与MariaDB</strong></h3><p>引用<a href="https://zh.wikipedia.org/wiki/MySQL">wikipedia</a>上对mysql的定义: 由于性能高、成本低、可靠性好,已经成为最流行的开源数据库,因此被广泛地应用在Internet上的中小型网站中,MySQL有点与众不同,它的架构可以在多种不同场景中应用并发挥良好作用.主要体现在存储引擎的架构上,插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离.这种架构可以根据业务的需求和实际需要选择合适的存储引擎.</p>
<span id="more"></span>

<p>mysql最新版为mysql-5.7,<strong>MariaDB</strong>数据库管理系统是<a href="https://zh.wikipedia.org/wiki/MySQL">MySQL</a>的一个分支,主要由开源社区在维护,采用<a href="https://zh.wikipedia.org/wiki/GPL">GPL</a>授权许可.开发这个分支的原因之一是：<a href="https://zh.wikipedia.org/wiki/%E7%94%B2%E9%AA%A8%E6%96%87%E5%85%AC%E5%8F%B8">甲骨文公司</a>收购了MySQL后,有将MySQL<a href="https://zh.wikipedia.org/wiki/%E9%97%AD%E6%BA%90">闭源</a>的潜在风险,因此社区采用分支的方式来避开这个风险</p>
<h3 id="Mysql的底层架构"><a href="#Mysql的底层架构" class="headerlink" title="Mysql的底层架构"></a><strong>Mysql的底层架构</strong></h3><p>网上翻到一张图,非常直接的展示了mysql内部的基础架构</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/mysqlbasic1.jpg" alt="mysqlbasic1"></p>
<p>从这张图可以看出mysql从上到下分为以下几个大块:</p>
<ol>
<li><p><strong>Connectors</strong></p>
<p>与其他编程语言中的sql 语句进行交互,如php、java等.包含本地sock通信和大多数基于客户端&#x2F;服务端工具实现的类似于tcp&#x2F;ip的通信.主要完成一些类似于连接处理、授权认证、及相关的安全方案.在该层上引入了线程池的概念,为通过认证安全接入的客户端提供线程.同样在该层上可以实现基于SSL的安全链接.服务器也会为安全接入的每个客户端验证它所具有的操作权限</p>
</li>
<li><p><strong>Management Services &amp;Utilities</strong></p>
<p>系统管理和控制工具,主要负责备份、恢复、主从复制等工作</p>
</li>
<li><p><strong>Connection Pool</strong></p>
<p>管理缓冲用户连接,线程处理等需要缓存的需求,连接控制等</p>
</li>
<li><p><strong>SQL Interface</strong></p>
<p>接受用户的SQL命令,并且返回用户需要查询的结果.比如select from就是调用SQL Interface</p>
</li>
<li><p><strong>Parser</strong></p>
<p>SQL命令传递到解析器的时候会被解析器验证和解析.<br>主要功能：<br>a. 将SQL语句分解成数据结构,并将这个结构传递到后续步骤,后面SQL语句的传递和处理就是基于这个结构的<br>b. 如果在分解构成中遇到错误,那么就说明这个sql语句是不合理的,语句将不会继续执行下去</p>
</li>
<li><p><strong>Optimizer</strong></p>
<p>服务器会解析查询并创建相应的内部解析树,并对其完成相应的优化如确定查询表的顺序,是否利用索引等,最后生成相应的执行操作</p>
</li>
<li><p><strong>Cache &amp; Buffer</strong></p>
<p>如果查询缓存有命中的查询结果,查询语句就可以直接去查询缓存中取数据.这个缓存机制是由一系列小缓存组成的.比如表缓存,记录缓存,key缓存,权限缓存等,这样在解决大量读操作的环境中能够很好的提升系统的性能</p>
</li>
<li><p><strong>Storage Engines</strong></p>
<p>存储引擎真正的负责了MySQL中数据的存储和提取,服务器通过API与存储引擎进行通信.不同的存储引擎具有的功能不同,这样我们可以根据自己的实际需要进行选取</p>
</li>
<li><p><strong>File System(属于操作系统级别)</strong></p>
<p>要是将数据存储在运行于裸设备的文件系统之上,并完成与存储引擎的交互</p>
</li>
</ol>
<h3 id="Mysql存储引擎"><a href="#Mysql存储引擎" class="headerlink" title="Mysql存储引擎"></a><strong>Mysql存储引擎</strong></h3><p>存储引擎说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法.因为在关系数据库中数据的存储是以表的形式存储的,所以存储引擎也可以称为表类型（即存储和操作此表的类型）.</p>
<p>在Oracle 和SQL Server等数据库中只有一种存储引擎,所有数据存储管理机制都是一样的.而MySql数据库提供了多种存储引擎.用户可以根据不同的需求为数据表选择不同的存储引擎,用户也可以根据自己的需要编写自己的存储引擎</p>
<p>从上图中可看到mysql有着非常多的存储引擎,而且还支持自定义,常用的mysql的存储引擎有如下几种:</p>
<blockquote>
<ol>
<li>MyISAM</li>
<li>InnoDB</li>
<li>Archive</li>
<li>Memory</li>
<li>NDB</li>
</ol>
</blockquote>
<p>关于存储引擎的区别及场景,下次再更.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/MySQL">https://zh.wikipedia.org/wiki/MySQL</a></li>
<li><a href="https://zh.wikipedia.org/wiki/MariaDB">https://zh.wikipedia.org/wiki/MariaDB</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Opentelemetry调研实践四(k8s中golang应用接入opentelemetry实现可观测性)</title>
    <url>/2021/11/07/opentelemetry-Code-Deploy/</url>
    <content><![CDATA[<h3 id="历史文章"><a href="#历史文章" class="headerlink" title="历史文章:"></a>历史文章:</h3><p><a href="https://izsk.me/2021/10/19/OpenTelemetry-what-is-observability/">可观测性到底在说什么</a></p>
<p><a href="https://izsk.me/2021/10/27/OpenTelemetry-Introduct/">opentelemetry架构及名词介绍</a></p>
<p><a href="https://izsk.me/2021/10/31/OpenTelemetry-Trace/">全链路追踪的TraceID与SpanID</a></p>
<p>整个opentelemetry体系还是相当复杂的，这里没办法将所有opentelemetry的东西讲清楚，直接通过case顺带opentelemetry里的概念来拆解会比较直观</p>
<p>这里通过一个简单的golang demo来介绍怎么接入opentelemetry以实现Metrics跟Trace的传递</p>
<span id="more"></span>



<p>由于篇幅有限，整个demo会成两部分进行，第一部分为部署篇，第二部分会来拆分主要代码实现。</p>
<h3 id="链路"><a href="#链路" class="headerlink" title="链路"></a>链路</h3><p>要说明的是，由于prometheus同时支持pull&#x2F;push模式,这里主要会以pull的方式进行说明，这也是普遍采用的方式</p>
<p>整个请求非常简单，如下:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">web -- &gt; kong API -- &gt; Golang-client -- &gt; Golang-Server </span><br></pre></td></tr></table></figure>

<p>要实现的效果为:</p>
<p>当在浏览器中访问URL时，经过kong API网关转发到 Client端， Client端通过http调用server端， server端正确响应后按路径返回给web</p>
<p>在这期间，借助opentelemetry暴露方法级的metrics，如请求时间、请求次数等度量数据以及整个链路的Trace.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20211101231146.png"></p>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>由于Collector的Receivers支持多种数据模型，在这里，Metrics采用的是Prometheus的pull模式进行采集，而Trace则采用的是OTLP格式进行采集，当采集数据到达Receivers后，会再经过Collector的Processors进行处理，这里使用了Batch及k8s_tagger(在较新版本中重命名为k8sprocessor)，batch是基于性能考虑可对采集数据进行批量操作，k8s_tagger则是在采集原始的数据基础之上新增一些跟k8s相关的<code>label</code>，这部分留在后面进行详解，等processor处理完后，最终会到存储后端，显然，Metrics会落在Prometheus中，Trace的数据则存储在jeager中</p>
<p>这里要重点说明的是: Receivers中的prometheus(也就是左边的)，不是一个真的prometheus实例，而是一个Prometheus的client,Collector从Instrumenttation获取到的数据按prometheus支持的数据模型进行转换，而右边的Prometheus才是具时存储序数据的实例，也就是最终真正存储时序metrics数据的实例.</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>上述kong、client、server都部署在k8s中，prometheus、jaeger这里为了方便，直接使用的<code>docker-compose</code> all-in-one的方式在k8s集群外部署.</p>
<h4 id="prometheus-x2F-jaeger"><a href="#prometheus-x2F-jaeger" class="headerlink" title="prometheus&#x2F;jaeger"></a>prometheus&#x2F;jaeger</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">jaeger-all-in-one:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">jaegertracing/all-in-one:latest</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;16686:16686&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;14268&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;14250&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">prometheus:</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">prometheus</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">prom/prometheus:latest</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./prometheus.yaml:/etc/prometheus/prometheus.yml</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;9090:9090&quot;</span></span><br><span class="line">      </span><br><span class="line"><span class="comment"># 其中prometheus.yml的内容是定义抓取的collector中的数据</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;otel-collector&#x27;</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">10s</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;10.198.x.x:55715&#x27;</span>] <span class="comment"># 55715是collector本身暴露的metrics指标</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span> [<span class="string">&#x27;10.198.x.x:62034&#x27;</span>] <span class="comment"># 72034是collector采集到的client pull到的指标</span></span><br></pre></td></tr></table></figure>



<h4 id="Collector"><a href="#Collector" class="headerlink" title="Collector"></a>Collector</h4><p>collector这里是以daemonset部署在集群中的</p>
<p>这里主要说明下collector的配置文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># otel-ds-agent-conf.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">otel-agent-config:</span></span><br><span class="line">    <span class="attr">receivers:</span></span><br><span class="line">      <span class="attr">otlp:</span></span><br><span class="line">        <span class="attr">protocols:</span></span><br><span class="line">          <span class="attr">grpc:</span></span><br><span class="line">          <span class="attr">http:</span></span><br><span class="line">      <span class="attr">prometheus:</span> <span class="comment"># 采用的是pull方式，因此需要配置ds.</span></span><br><span class="line">        <span class="attr">config:</span></span><br><span class="line">          <span class="attr">scrape_configs:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">k8s-apps</span></span><br><span class="line">            <span class="attr">scrape_interval:</span> <span class="string">30s</span></span><br><span class="line">            <span class="attr">scrape_timeout:</span> <span class="string">30s</span></span><br><span class="line">            <span class="attr">honor_timestamps:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">metrics_path:</span> <span class="string">/metrics</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">            <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br><span class="line">              <span class="attr">selectors:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br><span class="line">                <span class="comment"># only scrape data from pods running on the same node as collector</span></span><br><span class="line">                <span class="attr">field:</span> <span class="string">&quot;spec.nodeName=$KUBE_NODE_NAME&quot;</span></span><br><span class="line">            <span class="attr">relabel_configs:</span></span><br><span class="line">            <span class="comment"># scrape pods annotated with &quot;prometheus.io/scrape: true&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_scrape</span>]</span><br><span class="line">              <span class="attr">regex:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">              <span class="attr">action:</span> <span class="string">keep</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>, <span class="string">__meta_kubernetes_pod_annotation_prometheus_io_port</span>]</span><br><span class="line">              <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">              <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">              <span class="attr">regex:</span> <span class="string">([^:]+)(?::\d+)?;(\d+)</span></span><br><span class="line">              <span class="comment"># escaped $1:$2</span></span><br><span class="line">              <span class="attr">replacement:</span> <span class="string">$$1:$$2</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">exporters:</span></span><br><span class="line">      <span class="attr">prometheus:</span></span><br><span class="line">        <span class="attr">endpoint:</span> <span class="string">&quot;0.0.0.0:8889&quot;</span>  <span class="comment"># 8889端口用于collector以prometheus的格式保存metrics数据</span></span><br><span class="line">        <span class="attr">namespace:</span> <span class="string">otel</span></span><br><span class="line">        <span class="attr">const_labels:</span></span><br><span class="line">          <span class="attr">app_env:</span> <span class="string">test</span>   <span class="comment"># 给所有的metrics数据添加静态labels.</span></span><br><span class="line">        <span class="attr">resource_to_telemetry_conversion:</span> <span class="comment"># 开启将collector中所有的resource labels转换成metrics中的labels</span></span><br><span class="line">          <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    </span><br><span class="line">      <span class="attr">jaeger:</span></span><br><span class="line">        <span class="attr">endpoint:</span> <span class="number">10.4</span><span class="string">.xx.xx:14250</span> <span class="comment"># 指定jaeger的IP+Port</span></span><br><span class="line">        <span class="attr">tls:</span></span><br><span class="line">          <span class="attr">insecure:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">logging:</span></span><br><span class="line">        <span class="attr">loglevel:</span> <span class="string">debug</span>  <span class="comment"># 开启 collector 的debug日志</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">processors:</span></span><br><span class="line">      <span class="attr">k8s_tagger:</span></span><br><span class="line">      <span class="attr">k8s_tagger/2:</span></span><br><span class="line">        <span class="attr">passthrough:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">extract:</span></span><br><span class="line">          <span class="attr">metadata:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.pod.name</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.deployment.name</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.namespace.name</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.node.name</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.pod.start_time</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">k8s.pod.uid</span></span><br><span class="line">        <span class="attr">pod_association:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">resource_attribute</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ip</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">resource_attribute</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">k8s.pod.ip</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">resource_attribute</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">host.name</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">connection</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ip</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">from:</span> <span class="string">resource_attribute</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">k8s.pod.uid</span></span><br><span class="line">        <span class="attr">filter:</span></span><br><span class="line">          <span class="attr">node_from_env_var:</span> <span class="string">KUBE_NODE_NAME</span> <span class="comment"># 限制collector agent只收集本node的数据,与上述field保持一致</span></span><br><span class="line">    </span><br><span class="line">      <span class="attr">batch:</span></span><br><span class="line">    <span class="attr">extensions:</span></span><br><span class="line">      <span class="attr">zpages:</span> &#123;&#125;</span><br><span class="line">    <span class="attr">service:</span> <span class="comment"># 指定要开启的Pipeline</span></span><br><span class="line">      <span class="attr">extensions:</span> [<span class="string">zpages</span>]</span><br><span class="line">      <span class="attr">pipelines:</span></span><br><span class="line">        <span class="attr">metrics:</span></span><br><span class="line">          <span class="attr">receivers:</span> [<span class="string">otlp</span>, <span class="string">prometheus</span>]</span><br><span class="line">          <span class="attr">processors:</span> [<span class="string">k8s_tagger</span>, <span class="string">batch</span>]</span><br><span class="line">          <span class="attr">exporters:</span> [<span class="string">prometheus</span>, <span class="string">logging</span>]</span><br><span class="line">        <span class="attr">traces:</span></span><br><span class="line">          <span class="attr">receivers:</span> [<span class="string">otlp</span>]</span><br><span class="line">          <span class="attr">processors:</span> [<span class="string">k8s_tagger</span>, <span class="string">batch</span>]</span><br><span class="line">          <span class="attr">exporters:</span> [<span class="string">jaeger</span>, <span class="string">logging</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">opentelemetry</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">otel-agent-conf</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">otel-agent-conf</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">opentelemetry</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>当然， 这个配置文件有点长，同时涉及的配置项也非常多，同时也需要阅读者对prometheus的ds配置有了解，这里不对所有的配置进行展开，感兴趣的可自行参考官方<a href="https://opentelemetry.io/docs/collector/configuration/">说明</a>及<a href="https://github.com/open-telemetry/opentelemetry-specification">opentelemetry-specification</a></p>
<p>有几点个重要的参数说明如下:</p>
<blockquote>
<ul>
<li><code>service</code>: service部分是真正的指定使用哪些pipeline, 如果没有在service里配置的，则不会起作用，相当于只有一个定义.</li>
<li><code>node_from_env_var</code>: 由于collector是以ds的方式运行，每个实例只需要负责采集所在本机的数据即可，因此通过node_from_env_var来控制只采集本机的数据</li>
<li><code>k8s_tagger</code>: <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md">k8s_tagger</a>中的配置是给原始的metrics信息添加相关的k8s集群信息</li>
</ul>
</blockquote>
<p>Collector其它文件的部署由于篇幅有限，不再贴出来，可以参考官网的<a href="https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector/main/examples/k8s/otel-config.yaml">YAML文件</a></p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>然后Golang-client及Golang-server的部署，同样，可以用官方的<a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/demo/docker-compose.yaml">example</a>,是用docker-compose启动的，可通过<code>kompose</code>工具转换成k8s的yaml文件，这里篇幅有限就不贴出来</p>
<p>这里简单要说明的是，官网的example中client端的代码使用的是push的方式推送metrics到prometheus，而我们将把push方式改成pull的方式来实现collector从client中拉取metrics, 然后由prometheus server从collector中拉取。</p>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>最终，在k8s集群的opentelemetry ns下会存在collector-ds、一个前端，一个后端、及一个kong api服务.</p>
<p>接下来会从代码的角度来阐述go应用如何通过opentelemetry暴露出来的metrics及链路信息</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h3><blockquote>
<ul>
<li><a href="https://codingnote.cc/p/246692">https://codingnote.cc/p/246692</a></li>
<li><a href="https://opentelemetry.io/docs/collector/configuration/">https://opentelemetry.io/docs/collector/configuration/</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification">https://github.com/open-telemetry/opentelemetry-specification</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md">https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md</a></li>
<li><a href="https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector/main/examples/k8s/otel-config.yaml">https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector/main/examples/k8s/otel-config.yaml</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/demo/docker-compose.yaml">https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/demo/docker-compose.yaml</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql学习(redo,undo)</title>
    <url>/2017/11/30/mysql%E5%AD%A6%E4%B9%A0%E4%B8%89(redo,undo)/</url>
    <content><![CDATA[<p>redo跟undo是很多数据库都支持的特性,不同的数据库redo跟undo可能有点细节上的不同,整个方向几乎都起着同样的效果.</p>
<p>undo: 撤销,也就是取消之前的操作.</p>
<p>redo: 重做,重新执行一遍之前的操作.</p>
<span id="more"></span>

<p>这里主要以mysql常用的且支持事务特性的innodb存储引擎为例,myisam不在这之列.</p>
<h3 id="redo"><a href="#redo" class="headerlink" title="redo"></a><strong>redo</strong></h3><p>redo,从字面上来理解,就是<strong>重新执行一次之前做的操作,我们一般叫做前滚(rollforward)</strong>,是一种事务日志(transaction logs),可分为online和archived,以<strong>恢复</strong>为目的,保证事务的<strong>持久性</strong>.</p>
<p>Redo Log记录的是<strong>新数据</strong>的备份.在事务提交前,只要将Redo Log持久化即可,不需要将数据持久化(当然这需要根据配置而定具体什么时候数据文件持久化).当系统崩溃时,虽然数据没有持久化,但是Redo Log已经持久化.系统可以根据Redo Log的内容,将所有数据恢复到最新的状态.</p>
<p>一般的,redo log 都是会以日志组的形式出现,以顺序的方式写入文件文件,这就是 online redo log(在线重做日志),写满时则回溯到第一个文件,在进行覆盖写之前,有些数据库如Oracle则会把之前的redo log文件打包到特定目录下存档,这种文件就叫做archived redo log(归档重做日志).</p>
<p>redo log 其实由两部分组成,:<strong>redo log buffer 跟redo log file</strong>.redo log buffer 跟redo log file.buffer pool中把数据修改情况记录到redo log buffer,出现以下情况,再把redo log刷下到redo log file:</p>
<blockquote>
<ul>
<li>Redo log buffer空间不足</li>
<li>事务提交（依赖innodb_flush_log_at_trx_commit参数设置）</li>
<li>后台线程</li>
<li>做checkpoint</li>
<li>实例shutdown</li>
<li>binlog切换</li>
</ul>
</blockquote>
<p>这里有个重要的参数需要解释一下</p>
<h4 id="innodb-flush-log-at-trx-commit"><a href="#innodb-flush-log-at-trx-commit" class="headerlink" title="innodb_flush_log_at_trx_commit"></a><strong>innodb_flush_log_at_trx_commit</strong></h4><p>官方的解释请看<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit">这里</a>:</p>
<blockquote>
<ul>
<li>innodb_flush_log_at_trx_commit&#x3D;1,每次事务提交时,log buffer 会被写入到日志文件并刷写到磁盘.这也是默认值.这是最安全的配置,但由于每次事务都需要进行磁盘I&#x2F;O,所以也最慢.</li>
<li>innodb_flush_log_at_trx_commit&#x3D;2,每次事务提交会写入日志文件,但并不会立即刷写到磁盘,日志文件会每秒刷写一次到磁盘.这时如果 mysqld 进程崩溃,由于日志已经写入到系统缓存,所以并不会丢失数据；在操作系统崩溃的情况下,通常会导致最后 1s 的日志丢失</li>
<li>innodb_flush_log_at_trx_commit&#x3D;0,log buffer 会 每秒写入到日志文件并刷写（flush）到磁盘.但每次事务提交不会有任何影响,也就是 log buffer 的刷写操作和事务提交操作没有关系.在这种情况下,MySQL性能最好,但如果 mysqld 进程崩溃,通常会导致最后 1s 的日志丢失.</li>
<li><strong>注意:由于进程调度策略问题,这个“每秒执行一次 flush(刷到磁盘)操作”并不是保证100%的“每秒”.</strong></li>
</ul>
</blockquote>
<p>在<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit">官网</a>上还有这么一句话:</p>
<p><code>InnoDB log flushing frequency is controlled by innodb_flush_log_at_timeout, which allows you to set log flushing frequency to N seconds (where N is 1 ... 2700, with a default value of 1). However, any mysqld process crash can erase up to N seconds of transactions</code>.</p>
<p>innodb log 的从buffer刷新到log的频率还受<code>innodb_flush_log_at_timeout</code>这个参数的控制,这个参数允许你设置1-2700秒中的任何一个秒数,但是当mysql宕机的时候,会损失这N秒内的数据</p>
<p><code>DDL changes and other internal InnoDB activities flush the InnoDB log independent of the innodb_flush_log_at_trx_commit setting.</code></p>
<p>对于DDL的改变和一些innodb内部的一些刷新机制不依赖于<code>innodb_flush_log_at_trx_commit </code>参数</p>
<p><code>InnoDB crash recovery works regardless of the innodb_flush_log_at_trx_commit setting. Transactions are either applied entirely or erased entirely.</code></p>
<p>innodb的宕机恢复不受<code>innodb_flush_log_at_trx_commit</code>参数影响,事务要么全部应用要么全部擦除.</p>
<p>借用网上<a href="http://www.cnblogs.com/xinysu/p/6555082.html">一张图</a>来解释这个日志流向过程:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/mysqlredoundo.png" alt="mysqlredoundo"></p>
<h4 id="redo恢复"><a href="#redo恢复" class="headerlink" title="redo恢复"></a><strong>redo恢复</strong></h4><p>从上面我们知道的redo 日志的数据流向, 那么redo是如何恢复数据的呢?</p>
<ol>
<li>在设置innodb_flush_log_at_trx_commit&#x3D;1的情况下,如果只在redo log buffer中写入了日志,还没来的及写到redo log file中,此时mysql数据库宕机,再启动mysql时 redo log buffer内的记录肯定都不复存在,没有关系,也无需恢复,就相当于该事务还没发生一样,因为数据库的datafile并没有改变.</li>
<li>在设置innodb_flush_log_at_trx_commit&#x3D;1的情况下,如果日志已经写进了redo log file中(或者说从redo log file中刷数据到datafile刷到一半),此时mysql数据库宕机,再启动mysql时,mysql会自动的把redo log file中的记录执行一次到宕机的失败点,这样就能保证事务完整执行,这就是所谓的<strong>前滚</strong>.</li>
</ol>
<h3 id="undo"><a href="#undo" class="headerlink" title="undo"></a><strong>undo</strong></h3><p>undo,从字面上来理解就是<strong>撤销这次操作,我们一般叫做回滚(rollback)</strong>,它也是一种事务日志(transaction logs).它主要是保证事务的<strong>原子性</strong>及提供<strong>读一致性</strong>.</p>
<p>Undo Log的原理很简单,为了满足事务的原子性,在操作任何数据之前,首先将数据备份到一个地方(这个存储数据备份的地方称为Undo Log).然后进行数据的修改.如果出现了错误或者用户执行了<strong>ROLLBACK</strong>语句,系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态.</p>
<h4 id="回滚"><a href="#回滚" class="headerlink" title="回滚"></a><strong>回滚</strong></h4><p>这个就不用说了,就是撤销到事务开始之前的状态.</p>
<h4 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a><strong>MVCC</strong></h4><p>mvcc(Multiversion Concurrency Control,多版本并发控制),mvcc+undo一同为mysql提供<strong>一致性非锁定读</strong>的机制,提供更高的并发.</p>
<p>我们知道MySQL中的InnoDB存储引擎的默认隔离级别REPEATABLE READ(RR), 行级锁(在不能确定范围的情况下使用表级锁),当有一个事务正在更新某些数据时(写操作),同时另一个事务还能读取这些数据(读操作),确切的说是历史数据据,这就是mvcc与undo的作用,这里涉及到mysql的隔离级别,先不在这篇讨论,会另起一篇.</p>
<h3 id="redo-undo"><a href="#redo-undo" class="headerlink" title="redo+undo"></a><strong>redo+undo</strong></h3><p>假设有A、B两个数据,值分别为1,2,开始一个事务,事务的操作内容为:把1修改为3,2修改为4,那么实际的记录如下(简化):</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">A.事务开始.</span><br><span class="line">B.记录A=1到undo log.</span><br><span class="line">C.修改A=3.</span><br><span class="line">D.记录A=3到redo log.</span><br><span class="line">E.记录B=2到undo log.</span><br><span class="line">F.修改B=4.</span><br><span class="line">G.记录B=4到redo log.</span><br><span class="line">H.将redo log写入磁盘.</span><br><span class="line">I.事务提交</span><br></pre></td></tr></table></figure>

<p>Undo + Redo的设计主要考虑的是提升IO性能,增大数据库吞吐量.可以看出,B D E G H,均是新增操作,但是B D E G 是缓冲到buffer区,<strong>只有H是真正的增加了IO操作</strong>,为了保证Redo Log能够有比较好的IO性能,InnoDB 的 Redo Log的设计有以下几个特点:</p>
<blockquote>
<ul>
<li>A.尽量保持Redo Log存储在一段连续的空间上.因此在系统第一次启动时就会将日志文件的空间完全分配. 以顺序追加的方式记录Redo Log,通过顺序IO来改善性能.</li>
<li>B. 批量写入日志.日志并不是直接写入文件,而是先写入redo log buffer.当需要将日志刷新到磁盘时 (如事务提交),将许多日志一起写入磁盘.</li>
<li>C. 并发的事务共享Redo Log的存储空间,它们的Redo Log按语句的执行顺序,依次交替的记录在一起,</li>
<li>D. 因为C的原因,当一个事务将Redo Log写入磁盘时,也会将其他未提交的事务的日志写入磁盘.</li>
<li>E. Redo Log上只进行顺序追加的操作,当一个事务需要回滚时,它的Redo Log记录也不会从Redo Log中删除掉,对于 回滚事务先执行redo,再从undo中回滚.</li>
</ul>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">以减少日志占用的空间.例如,Redo Log中的记录内容可能是这样的：</span><br><span class="line">记录1: &lt;trx1, insert …&gt;</span><br><span class="line">记录2: &lt;trx2, update …&gt;</span><br><span class="line">记录3: &lt;trx1, delete …&gt;</span><br><span class="line">记录4: &lt;trx3, update …&gt;</span><br><span class="line">记录5: &lt;trx2, insert …&gt;</span><br></pre></td></tr></table></figure>
<p>  一次提交IO,会将其它日志一同提交,这样便提高了IO性能.</p>
<h3 id="恢复"><a href="#恢复" class="headerlink" title="恢复"></a><strong>恢复</strong></h3><p>有2种不同的恢复策略:</p>
<blockquote>
<ul>
<li>A.进行恢复时,只重做已经提交了的事务.</li>
<li>B.进行恢复时,重做所有事务包括未提交的事务和回滚了的事务.然后通过Undo Log回滚那些</li>
</ul>
</blockquote>
<p>MySQL数据库InnoDB存储引擎使用了B策略, InnoDB存储引擎中的恢复机制有几个特点:</p>
<blockquote>
<ul>
<li>在重做Redo Log时,并不关心事务性. 恢复时,没有BEGIN,也没有COMMIT,ROLLBACK的行为.也不关心每个日志是哪个事务的.尽管事务ID等事务相关的内容会记入Redo Log,这些内容只是被当作要操作的数据的一部分</li>
<li>使用B策略就必须要将Undo Log持久化,而且必须要在写Redo Log之前将对应的Undo Log写入磁盘.Undo和Redo Log的这种关联,使得持久化变得复杂起来.为了降低复杂度,InnoDB将Undo Log看作数据,因此记录Undo Log的操作也会记录到redo log中.这样undo log就可以象数据一样缓存起来,而不用在redo log之前写入磁盘了</li>
<li>Innodb也会将事务回滚时的操作记录到redo log中.回滚操作本质上也是对数据进行修改,因此回滚时对数据的操作也会记录到Redo Log中</li>
<li>一个被回滚了的事务在恢复时的操作就是先redo再undo,因此不会破坏数据的一致性</li>
</ul>
</blockquote>
<p><strong>简言之就是:mysql恢复时会执行所有的redo(包含了回滚的事务), 对于那些被回滚的事务再应用undo操作.</strong></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><table>
<thead>
<tr>
<th></th>
<th>UNDO</th>
<th>REDO</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Record of</strong></td>
<td><strong>How to undo a change</strong></td>
<td><strong>How to reproduce a change</strong></td>
</tr>
<tr>
<td><strong>Used for</strong></td>
<td><strong>Rollback, Read-Consistency</strong></td>
<td><strong>Rolling forward DB Changes</strong></td>
</tr>
<tr>
<td><strong>Stored in</strong></td>
<td><strong>Undo segments</strong></td>
<td><strong>Redo log files</strong></td>
</tr>
<tr>
<td><strong>Protect Against</strong></td>
<td><strong>Inconsistent reads in multiuser systems</strong></td>
<td><strong>Data loss</strong></td>
</tr>
</tbody></table>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.cnblogs.com/xinysu/p/6555082.html">说说MySQL中的Redo log Undo log都在干啥</a></li>
<li><a href="https://oraclenz.wordpress.com/2008/06/22/differences-between-undo-and-redo/">Differences between UNDO and REDO</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>oracle数据导入导出工具sqluldr2/sqlldr</title>
    <url>/2017/01/15/oracle%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E5%B7%A5%E5%85%B7sqluldr2-sqlldr/</url>
    <content><![CDATA[<h3 id="场景概述"><a href="#场景概述" class="headerlink" title="场景概述:"></a><strong>场景概述:</strong></h3><p><strong>场景:</strong></p>
<p>oracle中有一条大表(物品编码表wpbm),该表中存放了很多种物品信息,大约有4亿条数据,其中最重要的两个字段为wpmc(物品名称),spbm(商品编码),其中某一物品可能对应多个商品编码,如何根据这两个字段对某一物品的各个商品编码计数?</p>
<span id="more"></span>

<p>我们都知道,数据库最不擅长的事就是做运算,因为只要涉及到运算,就必然会涉及到在语句中使用某些数学函数,使用函数对于亿级且使用频繁的大表来说必然会影响数据库的效率,执行时间长不说,还可能会出现把数据库拖跨.所以对于这么大的数据量,直接使用count,groupby明显是不可取的,那么就只能使用一些歪方法了,无法在linux环境下用sed,awk等脚本语言对文本进行操作,这又得想办法解决windows下处理文本的效率问题,又一次想感叹Linux的伟大.</p>
<p><strong>思路:</strong></p>
<p>使用离线统计的办法,4亿条数据已经按照某种partition关系建成分区表,按各分区表导出csv文件(最大的一个分区表数据7000万行,2G,需要在使用sqluldr2导出的时候使用size参数分隔成多个小文件,结合第二步需要统计来看size&#x3D;200M时比较合理),然后结合python做统计,最后把csv再导入库中做分析.</p>
<p>Oracle对于大表的导出使用sqluldr2,数据分析使用python的dataframe库,Oracle导入使用sqlldr</p>
<h4 id="sqluldr导出"><a href="#sqluldr导出" class="headerlink" title="sqluldr导出:"></a><strong>sqluldr导出:</strong></h4><p>oracle数据的导出大概如下常用方式:</p>
<blockquote>
<ul>
<li>界面工具plsql等: 图形界面,傻瓜式,小表的首选</li>
<li>oracle自带的包ult_file:  需要自己编写业务逻辑</li>
<li>oracle spool:  没怎么用过</li>
<li>oracle exp&#x2F;imp: 导入&#x2F;导出为dmp包</li>
<li>sqluldr2: 神器,对于千万级别的大表简直不能更爽</li>
</ul>
</blockquote>
<p>对于亿级的大表,很明显会使用sqluldr2,4亿条数据导出花费了几十分钟,特别酸爽.</p>
<p>最开始的时候想用python的多线程实现了数据的导出,但是由于wpmc字段大部分为中文,且有的记录还包含了各式各样的毫无规律的字符,导致GBK也无法解码,换了各种字符编码都无法解决之后放弃从而转向sqluldr2.</p>
<p>sqluldr2对于大表的优势还是比较明显的,100万条数据也只用了几秒钟的时间,而且还有很多非常有用的参数.</p>
<p>下载地址请移步<a href="http://www.onexsoft.com/zh/download">官网</a></p>
<p>详细的说明请移步<a href="http://www.onexsoft.com/software/sqluldr2.pdf">这里</a></p>
<p>使用方法:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqluldr264.exe USER=admin/123@192.0.1.1:1521/test query=&quot;SELECT WP_MC, SPBM FROM TEST PARTITION(SYS_P24)&quot; field=0x-07 file=E:\SYS_P24.csv</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">说明 上面语句中field=0x-07,由于其它字符转换的原因在0x跟07之间加了横杠,实际使用时没有中间的黄杠,以下涉及到的加了的都是这个原因</span></span><br></pre></td></tr></table></figure>

<p>这里只列举几个有用的参数:</p>
<blockquote>
<ul>
<li>query: 指定查询的语句</li>
<li>sql: 如果查询语句太长,可写成sql文本里,然后提定sql&#x3D;test.sql</li>
<li>field: 可指定字段间的分隔符,默认是,这里最好指定一个字段中不会出现的字符,要不然的话字段分隔就会出现错误,这里也支持用字符的ASCII代码,我一般指0x-07,这个字符基本不会出现在日常打字中,主要的分隔符如下:<br><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-16.JPG" alt="oracle-3y-16"></li>
<li>record: 记录分隔符,默认为回车换行,Windows下的换行</li>
<li>file: 生成的文件,支持csv, txt等常用格式</li>
<li>size: 如果一个表太大,可按指定大小分隔成多个文件,此时file需使用%b参数,如file&#x3D;E:\SYS_%b.csv</li>
<li>batch: 如果指定batch&#x3D;yes,则100万条记录生成一个文件,此时file需使用%b参数,如file&#x3D;E:\SYS_%b.csv</li>
</ul>
</blockquote>
<p>4亿条数据用了20来分钟,7000万数据用了5分钟,这个速度已经是比较理想了</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-3.JPG" alt="oracle-3y-3"></p>
<p>当然sqluldr2还有很多其它好用的参数,比如通过管道连接sqlldr入库、直接指定字符集导出等,感兴趣的可移步<a href="http://www.onexsoft.com/software/sqluldr2.pdf">这里</a></p>
<p><strong>数据预处理:</strong></p>
<p>数据已经导出来了,在统计的时候发现有些数据包含空格,这会对统计靠成影响,所以需要先预处理数据,而且通过sqluldr2导出的数据没有经过任何处理,直接统计的时候还是会报gbk无法解码的问题</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-2.JPG" alt="oracle-3y-2"></p>
<p>因此在用python写去除空格的脚本时同时也把不能解码的记录一并处理,因为不需要精确统计,这里选择直接跳过无法解码的记录,毕竟是少数,对于4亿条数据统计来说影响可以忽略不计,因为大的分区表已在sqluldr2中用size参数分隔成了多个文件,python脚本里就直接逐行处理,之前是想用re正则来处理的,但想到每个文件的行数都是千万级,只能一行一行的读取,这种情况下正则的效率是比较低下的,如果能把整个内容放到一个变量中,使用re.sub()的效率还是会比string的快很多.</p>
<p>虽然可以不做这一步,但是后来会发现,这么做是非常有必要的.</p>
<p>统计完之后,4亿条数据无法解码的行数大致在20000多行.</p>
<p>因为是在自己的用了快5年的笔记本上执行的,CPU,内存,IO等性能有限,预处理比较耗磁盘,下图是预处理所花时间及资源使用率:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-5.JPG" alt="oracle-3y-5"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-6.JPG" alt="oracle-3y-6"></p>
<p>最后生成的CSV文件格式如下:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-4.JPG" alt="oracle-3y-4"></p>
<h4 id="python之dataframe"><a href="#python之dataframe" class="headerlink" title="python之dataframe:"></a><strong>python之dataframe:</strong></h4><p>DataFrame是Pandas中的一个表结构的数据结构,包括三部分信息,表头(列的名称),表的内容(二维矩阵),索引(每行一个唯一的标记).</p>
<p>官网文档请移步<a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html">这里</a></p>
<p>之前看过一些dataframe的资料,但是没有实际工作用到过它,这次刚好借助这次机会学习下这个python库用于数据统计.</p>
<p>由上图得到了比较纯净的数据之后,开始以(物品[]编码)做为一个整体来统计:</p>
<p>说明:上述代码中to_csv中的sep的分隔符实际为一个[],对应的ASCII代码为0_07(请将_换成x).</p>
<p>通过read_csv指定空格为分隔符,这样就能够保证(物品[]编码)做为一个整体来统计,通过df[col].value_counts()来统计该列出现的次数,使用type可以知道这个方法返回的是一个series,只保存了次数,没有了(商品[]编码),但是我希望得到类似以下结果:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-11.JPG" alt="oracle-3y-11"></p>
<p>所以必须通过reset_index()重新定义索引,然后指定索引名,这样才能出现如上结果.</p>
<p>应该有更好的办法可以得到如上的结果,通过指定sep&#x3D;’0x-07’,再使用df.groupby([‘wp’,’bm’].size()直接把两列做为一行进行统计,只不过这里wp为中文,不是数值型,size()方法对非数值型统计直接不显示,所以无法直接用groupby().size()统计.</p>
<p>这个阶段因为要做大量运算,比较消耗内存和CPU,资源使用率及所花时间为:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-7.JPG" alt="oracle-3y-7"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-8.JPG" alt="oracle-3y-8"></p>
<p>最后得到的是一个CSV文件,再通过sqlldr导入数据库即可.</p>
<h4 id="sqlldr导入"><a href="#sqlldr导入" class="headerlink" title="sqlldr导入:"></a><strong>sqlldr导入:</strong></h4><p>sqlldr跟sqluldr2是一对利器,速度非常快,使用也很简单.</p>
<p>sqlldr的官方文档请移步<a href="https://docs.oracle.com/cd/B19306_01/server.102/b14215/ldr_params.htm">这里</a></p>
<p>启动方法:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sqlldr.exe test/123@192.0.1.1:1521/test control=E:\data_to_oracle.ctl log=E:\data_to_oracle.log</span><br></pre></td></tr></table></figure>

<p>其中控制文件:data_to_oracle.ctl内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Load DATA</span><br><span class="line">INFILE &#x27;E:\data\fpcgl\SYS_P21_1_SYS_P22_1_char.CSV&#x27;</span><br><span class="line">INFILE &#x27;E:\data\fpcgl\SYS_P23_1_after.CSV&#x27;</span><br><span class="line">BADFILE &#x27;E:\data\fpcgl\data_to_oracle_bad.bad&#x27;</span><br><span class="line">DISCARDFILE &#x27;E:\data\fpcgl\data_to_oracle.dsc&#x27;</span><br><span class="line">APPEND</span><br><span class="line">INTO TABLE WPMC_SPBM</span><br><span class="line">FIELDS TERMINATED BY &#x27;[]&#x27;</span><br><span class="line">TRAILING NULLCOLS</span><br><span class="line">(WPMC,</span><br><span class="line">SPBM,</span><br><span class="line">TOTAL)</span><br></pre></td></tr></table></figure>

<p>infile: 导入文件所在路径</p>
<p>badfile: 文件中不能插入数据库的不合法记录</p>
<p>discardfile: 丢弃的数据文件,默认情况不产生,必须指定</p>
<p>append: 插入数据库的模式,主要由以下几种:</p>
<blockquote>
<ul>
<li>insert  –为缺省方式,在数据装载开始时要求表为空</li>
<li>append  –在表中追加新记录</li>
<li>replace –删除旧记录(用 delete from table 语句),替换成新装载的记录</li>
<li>truncate –删除旧记录(用 truncate table 语句),替换成新装载的记录</li>
</ul>
</blockquote>
<p>fields: 字段分隔符,默认为,</p>
<p>trailing nullcols: 表的字段没有对应的值时允许为空</p>
<p>使用率及时间为:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-10.JPG" alt="oracle-3y-10"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-9.JPG" alt="oracle-3y-9"></p>
<p>从产生的log可以看到,插入的记录数(这只是数据其中的一部分),时间等信息,这里没有产生错误数据,看来前期数据预处理还是有效果的,虽然多花了点时间,但是很值得.</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-12.JPG" alt="oracle-3y-12"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-14.JPG" alt="oracle-3y-14"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-13.JPG" alt="oracle-3y-13"></p>
<p>最后到库的数据再做统计就是秒秒钟的事了,Oracle11g添加了几个非常实用的行列互转函数,非常有用<br>这里按商品编码分类统计物品名称且条数大于1000,sql语句如下:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> decode(<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> SPBM <span class="keyword">order</span> <span class="keyword">by</span> zongshu <span class="keyword">desc</span>),</span><br><span class="line">              <span class="number">1</span>,</span><br><span class="line">              SPBM) SPBM,</span><br><span class="line">       WPMC,zongshu</span><br><span class="line">  <span class="keyword">from</span> SPBMISNULL t <span class="keyword">where</span>  zongshu<span class="operator">&gt;</span><span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-15.JPG" alt="oracle-3y-15"></p>
<h3 id="番外篇"><a href="#番外篇" class="headerlink" title="番外篇:"></a><strong>番外篇:</strong></h3><h4 id="Series"><a href="#Series" class="headerlink" title="Series:"></a><strong>Series:</strong></h4><p>series 可以看做一个定长的有序字典。基本任意的一维数据都可以用来构造 Series 对象:</p>
<p>s &#x3D; pd.Series(data&#x3D;[1,2,3,4], index&#x3D;[‘a’,’b’,’c’,’d’])</p>
<p>如果没有指定index,则会默认生成从0开始递增的索引列</p>
<p><strong>查看信息:</strong></p>
<p>s.values: 查看data信息</p>
<p>s.index: 查看索引的信息</p>
<p>s.sum(): 求data值的和,如果是非数值型,则会把整个data列表组成一个字符串</p>
<p>s.count(): 求data的个数</p>
<h4 id="dataframe"><a href="#dataframe" class="headerlink" title="dataframe"></a><strong>dataframe</strong></h4><p>DataFrame 是一个表格型的数据结构，它含有一组有序的列(类似于 index),每列可以是不同的值类型(不像 ndarray 只能有一个 dtype).基本上可以把 DataFrame 看成是共享同一个 index 的 Series 的集合.</p>
<p>DataFrame 的构造方法与 Series 类似,只不过可以同时接受多条一维数据源,每一条都会成为单独的一列:</p>
<p>df &#x3D; pd.DataFrame([[1,2,3],[4,5,6]],columns&#x3D;[‘a’,’b’,’c’],index&#x3D;[0,1])</p>
<p>较完整的 DataFrame 构造器参数为:DataFrame(data&#x3D;None,index&#x3D;None,columns&#x3D;None), columns 即为列名:</p>
<p><strong>查看信息:</strong></p>
<p>df.values:  查看data信息</p>
<p>df.index: 查看索引的信息</p>
<p>df.columns: 查看列名信息</p>
<p>len(df)或len(df.index): 获取数据行数</p>
<p>df.head(5): 显示前5行数据</p>
<p>df.tail(5): 显示后5行数据</p>
<p><strong>选择数据:</strong></p>
<ol>
<li><p>取特定的列:</p>
<p>df[‘x’]或得df.x: 那么将会返回columns为x的列,返回的是一个列</p>
</li>
<li><p>取特定的行则通过切片[]来选择:</p>
<p>df[0:3]: 选择的是前3行数</p>
<p>不过须要注意,因为 pandas 对象的 index 不限于整数,所以当使用非整数作为切片索引时,它是末端包含的</p>
</li>
<li><p>通过标签来选择:</p>
<p>df.loc[‘one’]: 则会默认表示选取index为’one’的行,返回一个series</p>
<p>df.loc[:,[‘a’,’b’] ]: 表示选取所有的行以及columns为a,b的列,返回一个dataframe</p>
<p>df.loc[1:3,:]: 对行进行切片,选择的是行</p>
<p>df.loc[:,1:3]: 对列进行切片,选取的是列</p>
<p>df.loc[[‘one’,’two’],[‘a’,’b’]]: 表示选取’one’和’two’这两行以及columns为a,b的列,返回一个dataframe</p>
<p>df.loc[‘one’,’a’]与a.loc[[‘one’],[‘a’]]: 作用是一样的,不过前者只显示对应的值,而后者会显示对应的行和列标签</p>
</li>
<li><p>通过位置来选择数据:<br>df.iloc[1:2,1:2]: 则会显示第一行第一列的数据(切片后面的值取不到)</p>
<p>df.iloc[1:2]: 即后面表示列的值没有时,默认选取行位置为1的数据</p>
<p>df.iloc[[0,2],[1,2]]: 即可以自由选取行位置,和列位置对应的数据</p>
</li>
<li><p>使用条件来选择:<br>a[a.c&gt;0]:  表示选择c列中大于0的行</p>
<p>a[a&gt;0]:  表直接选择a中所有大于0的行</p>
<p>a1[a1[‘one’].isin([‘2’,’3’])]:  表显示满足条件,列one中的值包含’2’,’3’的所有行</p>
</li>
</ol>
<p><strong>缺失值处理:</strong></p>
<ol>
<li>df.dropna(axis&#x3D;0, how&#x3D;): 去除所有数据中包含空值的行<br>​       其中:axis的取值可为0&#x2F;1来表示不同的维度,0表示按行,1表示按列<br>​       how的取值可为any&#x2F;all: all表示所有的列为空时才会去除,any则表示只要有一列为空即去除<br>​       还有一种用法df.dropna(thresh&#x3D;3): 会在一行中至少有 3 个非 NA 值时将其保留不去除</li>
<li>df.fillna(value&#x3D;100): 对缺失的值进行填充</li>
<li>df.isnull()&#x2F;df.notnull: 判断是否为空&#x2F;不为空, 返回一个布尔型数组</li>
<li>df.drop_duplicates(): 去除重复行</li>
<li>inplace(): 凡是会对数组作出修改并返回一个新数组的,往往都有一个 replace&#x3D;False 的可选参数.如果手动设定为 True,那么原数组就可以被替换.</li>
</ol>
<p><strong>常用方法:</strong></p>
<ol>
<li><p>Apply(): 对数据应用函数,如:</p>
<p>a.apply(lambda x:x.max()-x.min()): 表示返回所有列中最大值-最小值的差</p>
</li>
<li><p>sort_index(ascending&#x3D;False):  按照索引列排序,默认是以升序排序</p>
</li>
<li><p>groupby(‘columns’): 这个方法不会返回数据,必须连同如sum(),mean(),count()等函数一直使用</p>
<p>如:df.groupby([‘a’,’b’]).sum(): 对列a,b进行分组然后再进行求和</p>
<p>​    df.groupby([‘a’]).size(): 对各个a下的数目进行计数,如果是字符串的话则不显示</p>
</li>
<li><p>is_unique(): 判断值是否唯一</p>
</li>
<li><p>其它常用方法有:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/oracle-3y-1.JPG" alt="oracle-3y-1"></p>
</li>
<li><p>dataframe还有一些其它高级的应用,比如Concat(),join(),append()等,以后有时间再研究下!</p>
</li>
</ol>
<h4 id="read-csv-x2F-to-csv"><a href="#read-csv-x2F-to-csv" class="headerlink" title="read_csv()&#x2F;to_csv():"></a><strong>read_csv()&#x2F;to_csv():</strong></h4><p>read_csv的参数详解请移步<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv">这里</a></p>
<p>中文说明请移步<a href="http://www.cnblogs.com/datablog/p/6127000.html">这里</a></p>
<p>这两个方法比较简单,这里就不展开写了.</p>
<p>收工!</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><p><a href="http://blog.sina.com.cn/s/blog_5ceb51480102vppw.html">利用sqluldr2导出数据和sqlldr导入数据的方法</a></p>
<p><a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html">pandas官方文档</a></p>
<p><a href="https://docs.oracle.com/cd/B19306_01/server.102/b14215/ldr_params.htm">sqlldr的官方文档</a></p>
<p><a href="http://www.cnblogs.com/flish/archive/2010/05/31/1748221.html">关于 Oracle 的数据导入导出及 Sql Loader (sqlldr) 的用法</a></p>
<p><a href="http://www.webtag123.com/python/44619.html">Python 数据分析包:pandas 基础</a></p>
<p><a href="http://www.cnblogs.com/datablog/p/6127000.html">pandas.read_csv参数详解</a></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(prometheus基本原理)</title>
    <url>/2019/08/21/prometheus-basic-theory/</url>
    <content><![CDATA[<p>Prometheus做为CNCF的第二个项目，被给予厚望,  现已然成为监控系统设计的标杆. 特别是对于Kubernetes的支持,可谓天衣无缝. 使用kubernetes，很难跳过prometheus不说.</p>
<span id="more"></span>

<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/prometheus-architecture.png"></p>
<p>Prometheus由多个组件组成，但是其中许多组件是可选的:</p>
<blockquote>
<ul>
<li>Prometheus Server: 用于抓取指标、存储时间序列数据</li>
<li>Service discovery: 通过服务发现机制获取需要scrape目标</li>
<li>Pushgateway: push的方式将指标数据推送到网关</li>
<li>alertmanager: 处理报警的报警组件</li>
<li>DataVisualization: 支持多种数据可视化UI</li>
<li>PromQL: 数据查询语言</li>
</ul>
</blockquote>
<h3 id="数据抓取"><a href="#数据抓取" class="headerlink" title="数据抓取"></a>数据抓取</h3><p><strong>PULL</strong>: prometheus<strong>主动</strong>地按照配置的时间周期去<strong>需要抓取的</strong>目标对象获取metrics</p>
<p><strong>PUSH</strong>: 程序按照配置的时间周期将metrics<strong>推送</strong>到pushgateway, pushgateway本地存储数据, prometheus<strong>主动</strong>去pushgateway<strong>抓取</strong>.</p>
<p><strong>规则:</strong> </p>
<blockquote>
<ul>
<li>按照promethus定义的<strong>metric数据类型</strong>及规则组织数据<strong>格式</strong>. </li>
<li>暴露指定的端口供prometheus scrape且表明自己需要被scrape</li>
</ul>
</blockquote>
<p><strong>metrics</strong>: key-value对</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121135817702.png"></p>
<p>一条metrics中<code>&#123;&#125;</code>引起来的部分为label, 可用于聚合&#x2F;查询条件等.</p>
<p><strong>数据类型:</strong></p>
<blockquote>
<ul>
<li>counter: 只增不减,允许重置为0</li>
<li>gauge: 没有规律的数值</li>
<li>histogram: 用于统计一些数据分布的情况，用于计算在一定范围内的分布情况，同时还提供了度量指标值的总和</li>
<li>Summary: 主要用于计算在一定时间窗口范围内度量指标对象的总数以及所有对量指标值的总和，计算过程是在client端完成,计算结果存在server。因为没有最初的metric数据，所以summary不支持数据聚合</li>
</ul>
</blockquote>
<p><strong>scrape</strong></p>
<p>对于scrape, 一个比较重要的指标是scrape timeout, 这个要根据具体场景来设置,过大或者过小都可能造成问题</p>
<p>关于如何发现scrape目标, 见<a href="#ServiceDiscovery">ServiceDiscovery</a></p>
<p><strong>PromSQL</strong></p>
<p>prometheus中查询中使用了自有的查询语言,PromSQL,时间关系, 这节也不展开讲了,大家可参考<a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">官网</a></p>
<p><strong>exporter</strong></p>
<p>exporter是prometheus一类数据采集组件的总称, 随着prometheus逐渐流行, 并不是所有的第三方软件都支持prometheus的metric格式, 如果本身不改造的话,那无法使用prometheus进行scrapek,因此需要额外的一个东西从目标处收集数据, 将其转化为prometheus支持的格式.</p>
<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>所有的监控对象(基础设施、应用、服务)都在动态的变化, 显然无法静态的定义监控目标。而对于Prometheus而言其解决方案就是引入一个中间的代理人（服务注册中心），这个代理人掌握着当前所有监控目标的访问信息，Prometheus只需要向这个代理人询问有哪些监控目标控即可， 这种模式被称为<strong>服务发现</strong></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191121135214878.png"></p>
<p>摘取prometheus配置文件部分, 完整的配置见<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">官网</a>:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="comment"># How frequently to scrape targets by default.</span></span><br><span class="line">  [ <span class="attr">scrape_interval:</span> <span class="string">&lt;duration&gt;</span> <span class="string">|</span> <span class="string">default</span> <span class="string">=</span> <span class="string">1m</span> ]</span><br><span class="line">  <span class="comment"># How long until a scrape request times out.</span></span><br><span class="line">  [ <span class="attr">scrape_timeout:</span> <span class="string">&lt;duration&gt;</span> <span class="string">|</span> <span class="string">default</span> <span class="string">=</span> <span class="string">10s</span> ]</span><br><span class="line">  <span class="comment"># How frequently to evaluate rules.</span></span><br><span class="line">  [ <span class="attr">evaluation_interval:</span> <span class="string">&lt;duration&gt;</span> <span class="string">|</span> <span class="string">default</span> <span class="string">=</span> <span class="string">1m</span> ]</span><br><span class="line">  <span class="comment"># The labels to add to any time series or alerts when communicating with</span></span><br><span class="line">  <span class="comment"># external systems (federation, remote storage, Alertmanager).</span></span><br><span class="line">  <span class="attr">external_labels:</span></span><br><span class="line">    [ <span class="string">&lt;labelname&gt;:</span> <span class="string">&lt;labelvalue&gt;</span> <span class="string">...</span> ]</span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">prometheus</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">20s</span></span><br><span class="line">  <span class="attr">scrape_timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">/metrics</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">static_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">localhost:9090</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-service-endpoints</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">20s</span></span><br><span class="line">  <span class="attr">scrape_timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">/metrics</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">api_server:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">role:</span> <span class="string">endpoints</span></span><br><span class="line">    <span class="attr">namespaces:</span></span><br><span class="line">      <span class="attr">names:</span> []</span><br><span class="line">  <span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_scrape</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">keep</span>   <span class="comment">#keep, drop</span></span><br><span class="line">  <span class="comment">#其它relabel规则省略...</span></span><br><span class="line">  <span class="comment">#...</span></span><br></pre></td></tr></table></figure>

<p>对应的service主动expose metrics:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">prometheus-logging</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">logging</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9090&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">9090</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9090</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">prometheus-logging</span></span><br></pre></td></tr></table></figure>

<p><code>一般情况下, 为避免抓取数据量太大对prometheus造成压力, 最佳实践是只抓取对监控有益的对象, </code></p>
<p><code>prometheus配置文件中对指定某组目标使用了relabel_configs</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">source_labels:</span> <span class="string">__meta_kubernetes_service_annotation_prometheus_io_scrape</span></span><br><span class="line"><span class="attr">action:</span> <span class="string">keep</span></span><br></pre></td></tr></table></figure>

<p><code>该场景下: 只有当prometheus能通过服务发现机制找到抓取对象且抓取对象定义了prometheus.io/scrape: &quot;true&quot;时, prometheus才会进行抓取.</code></p>
<p>抓取的数据格式如下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20191125104649435.png"></p>
<h3 id="报警规则"><a href="#报警规则" class="headerlink" title="报警规则"></a>报警规则</h3><p>这里举两个例子:</p>
<p>连续5分钟之内, 如果存在容器的启动时间一直小于180s的,则认为容器发生重启,则报警</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alert: container_restart</span><br><span class="line">expr: (time()</span><br><span class="line">  - (avg by(name) (container_start_time_seconds))) &lt; 180</span><br><span class="line">for: 5m</span><br><span class="line">labels:</span><br><span class="line">  severity: warning</span><br><span class="line">annotations:</span><br><span class="line">  description: container &#123;&#123;$labels.name&#125;&#125; restart in 5 minutes.</span><br></pre></td></tr></table></figure>

<p>连续5分钟之内,如果node的可用使用内存大于90%且可用内存小于2G的, 则报警</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">alert:</span> <span class="string">node_memory_usage_higher_0.9</span></span><br><span class="line"><span class="attr">expr:</span> <span class="string">(((node_memory_MemTotal</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">node_memory_MemFree</span> <span class="bullet">-</span> <span class="string">node_memory_Cached)</span> <span class="string">/</span> <span class="string">(node_memory_MemTotal))</span> <span class="string">*</span> <span class="number">100</span><span class="string">)</span> <span class="string">&gt;</span></span><br><span class="line"><span class="string">  90 and (node_memory_MemFree + node_memory_Cached) &lt; 2 * 1024 * 1024 * 1024</span></span><br><span class="line"><span class="string"></span><span class="attr">for:</span> <span class="string">5m</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">  <span class="attr">severity:</span> <span class="string">warning</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line">  <span class="attr">description:</span> <span class="string">Host</span> <span class="string">avg</span> <span class="string">memory</span> <span class="string">usage</span> <span class="string">in</span> <span class="string">5m</span> <span class="string">is</span> &#123;&#123; <span class="string">$value</span> &#125;&#125;<span class="string">%</span> <span class="string">(&gt;90%).</span> <span class="string">Reported</span> <span class="string">by</span></span><br><span class="line">    <span class="string">instance</span> &#123;&#123; <span class="string">$labels.instance</span> &#125;&#125; <span class="string">of</span> <span class="string">job</span> &#123;&#123; <span class="string">$labels.job</span> &#125;&#125;<span class="string">.</span></span><br></pre></td></tr></table></figure>

<h3 id="报警管理"><a href="#报警管理" class="headerlink" title="报警管理"></a>报警管理</h3><p>prometheus中处理metric是否符合定义的报警规则,然后通过alertmanager组件实现了, alertmanager充当中间人,后端对接多种报警介质</p>
<p>prometheus将<code>异常事件</code>发送给alertmanager, alertmanager中我们可以设置各种报警规则等</p>
<blockquote>
<ul>
<li>报警静默</li>
<li>报警抑制</li>
<li>报警分类</li>
<li>报警聚合</li>
</ul>
</blockquote>
<p>alertmanager的使用也相对容器, 不过Prometheus原生不支持某些实用的功能, 比如最大报警次数,因此, 如果产生报警, 如果不处理的话, prometheus会一直产生报警, prometheus开发者认为既然产生报警，那必然要尽快解决,因此觉得没必要支持设置最大报警次数，不过，可以直接通过修改prometheus的源码定制该功能.</p>
<p>###可视化</p>
<p>prometheus&#x2F;alertmanager自带的webui实在是<code>太过于丑陋</code>, 一般只用于<code>debug数据</code>时使用, 数据可视化都会结合grafana进行展示</p>
<p>grafana支持alertmanager.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://prometheus.io/">https://prometheus.io/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(MetricTypes)</title>
    <url>/2019/12/28/prometheus-4-types/</url>
    <content><![CDATA[<p>在学习prometheus的时候，相信大家也会对prometheus里的各种数据类型及内置函数有很多疑惑, 为了加深印象来记录下工作中常用到的函数，在这之前, 先来温故下prometheus中的4种metric types.</p>
<span id="more"></span>



<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>prometheus中所有的数据都是时序流, 我们在prometheus的ui中看到的数据虽然没有看到时间戳, 但在prometheus底层存储中是存在时间戳与之对应, 简单来说, prometheus的数据模型就是<code>Metrics+labels+timestamp</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200408212749.png"></p>
<p><code>alertmanager_alerts</code>为<code>metrics key</code></p>
<p><code>instance</code>、<code>job</code>等这些都是<code>labels</code>，可以理解为查询时的条件</p>
<p><code>value</code>为这条metric的值</p>
<p>注意: <strong>在prometheus中value只能是float类型的数值, 不可以是其它值</strong></p>
<h3 id="监控项类型"><a href="#监控项类型" class="headerlink" title="监控项类型"></a>监控项类型</h3><p>虽然prometheus的value只能是float类型的数值, 但是这个value是有一种数据类型与之对应的, prometheus中存在4种监控项类型, 官方文档在<a href="https://prometheus.io/docs/concepts/metric_types/">这里</a></p>
<h4 id="gauge"><a href="#gauge" class="headerlink" title="gauge"></a>gauge</h4><p>gauge表示<strong>可以任意波动的单一值, 没有规律</strong>, 直观的例子就是可以表示机器的网卡流量, 因为你无法预料在下一秒的有多少入口流量 或多或少.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200408213836.png"></p>
<h4 id="counter"><a href="#counter" class="headerlink" title="counter"></a>counter</h4><p>counter为<strong>计数器,只增不减</strong> 用于只增不减的场合,直观的例子机器的开机时t、长nginx的请求数等, 在某个时刻之后, 这个数只会一直增长而不会减少, 但是可以允许被重置为0</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200408214317.png"></p>
<p>上面两个类型比较好理解, 难理解的histogram与summary.</p>
<h4 id="histogram"><a href="#histogram" class="headerlink" title="histogram"></a>histogram</h4><p>histogram表示<strong>累积直方图</strong>， 主要用于表示一段时间范围内对数据进行采样（通常是请求持续时间或响应大小),并将其计入可配置的存储桶（bucket）中，并能够对其指定区间以及总数进行统计</p>
<p>所以, 在很多的metric key中如果看到xxx_bucket时, 大部分表示这条记录的类型为histogram.</p>
<p>很直观的一个例子, 统计ingress_nginx中请求的响应时间(单位:秒)</p>
<p><code>nginx_ingress_controller_response_duration_seconds_bucket</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200408215430.png"></p>
<p>看到这可能会有人问, 这个结果是不是有问题, 这个<code>le</code>是什么意思?</p>
<p>首先说一下这个<code>le</code>,它并不是一个单位,而是表示bucket的一个区间, 这个le是在ingress_nginx的代码中定义的.</p>
<p>另一个要明白的是, histogram是<code>累积直方图</code></p>
<p>因此上图就很容易理解了</p>
<p>Ingress_nginx代码中定义了所有bucket的区间值(当然，作者可以随便定义), 所有请求产生的响应时间都会落在这些区间之内。</p>
<ol>
<li>响应时间&lt;0.005的请求个数为24</li>
<li>响应时间&lt;0.01的请求个数为24, 包括&lt;0.005,也就是说没有请求的响应时间在 0.005与0.01之间</li>
<li>响应时间&lt;0.025的请求个数为24,包含&lt;0.01的区间, 也就是说没有请求的响应时间在0.01与0.025之间</li>
<li>响应时间&lt;0.05的请求个数为30, 包含&lt;0.025的区间, 也就是说请求的响应时间有6个落在0.025与0.05之间</li>
<li>…</li>
<li>响应时间&lt;0.25的请求个数为27034, 包含&lt;0.1的区间</li>
<li>…</li>
</ol>
<p>从这个过程就可以知道为什么叫<code>累积直方图了</code>, 从上面的例子来总结,<strong>大部分的请求都落在0.1s到0.25s之间</strong></p>
<p>这里要注意一下<code>le=+lnf</code>这条记录, 这个其实表示所有的记录, <code>lnf</code>表示最大的bucket的往上, 这里是10s以上的记录,从上图可以看到, 不存在响应时间大于10s, 正常也不应该有</p>
<p>同时histogram也提供&lt;basename&gt;_sum，&lt;basename&gt;_count这两个指标</p>
<p>总结来就: </p>
<p><strong>&lt;basename&gt;_count就是一个计数器, 它只会增加,表示次数</strong></p>
<p><strong>&lt;basename&gt;_sum也可以是一个计数器,它表示监控项对应的值的总和</strong></p>
<p>先来看一下<code>nginx_ingress_controller_response_duration_seconds_count</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/image-20200409111003966.png"></p>
<p> 会发现这3条记录value之和与<code>le=&#123;+lnf&#125;</code>的值是相等的, 因此count表示<code>对采样点的次数累计和</code>,这里也就是请求总数</p>
<p>而&#96;&#96;nginx_ingress_controller_response_duration_seconds_sum&#96;</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409112322.png"></p>
<p>sum表示的是<code>所有value的总和</code>,这个很好理解</p>
<p>因此，如果想计算最近5分钟内的平均请求持续时间</p>
<p>可以使用以下表达式:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rate(nginx_ingress_controller_response_duration_seconds_sum[5m]) / rate(nginx_ingress_controller_response_duration_seconds_count[5m])</span><br></pre></td></tr></table></figure>



<p>同时 histogram也是可以计算分位数, 使用<code>histogram_quantile</code></p>
<p>Prometheus 通过 <code>histogram_quantile</code> 函数来计算分位数（quantile），而且是一个预估值，并不完全准确，因为这个函数是假定每个区间内的样本分布是线性分布来计算结果值的。预估的准确度取决于 bucket 区间划分的粒度，粒度越大，准确度越低</p>
<p><code>histogram_quantile(φ float, b instant-vector)</code></p>
<p>假如，要计算95%的响应时间落在哪个区间可以使用以下命令</p>
<p><code>histogram_quantile(0.95, sum(rate(nginx_ingress_controller_response_duration_seconds_bucket&#123;path=~&quot;/sensego/v2.0/mingyuan&quot;,status=&quot;200&quot;&#125;[1800m])) by (le))</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409115919.png"></p>
<p>会发现大约在0.23s</p>
<p>注意: 在数据量大的情况下，<code>histogram_quantile</code>计算可能会消耗大量CPU, 因为它是在服务端实时计算的.</p>
<p>这个函数的源码在<a href="https://github.com/prometheus/prometheus/blob/master/promql/quantile.go">这里</a></p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>summary表示摘要数据, 这个跟histogram比较想像, 反映的都是统计类数据</p>
<p>summary主要用于表示一段时间内数据采样结果（通常是请求持续时间或响应大小），<code>它直接存储了 quantile 数据，而不是根据统计区间计算出来的</code>。</p>
<p><code>prometheus_target_interval_length_seconds</code>表示目标抓取所用时间</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409112817.png"></p>
<p>很大的一个区别是<code>quantile</code>, 翻译过来就是分位数, 这个区间也是在源码里定义</p>
<p>上面的图表示的含义是:</p>
<ol>
<li>有1%的目标抓取时间为19.936946996</li>
<li>有5%的目标抓取时间为19.999956628</li>
<li>有50%的目标抓取时间为20.000021361</li>
<li>有90%的目标抓取时间为20.00006137</li>
<li>有99%的目标抓取时间为20.071892568</li>
</ol>
<p>从这里可以看出,99%的时间都是20s内, 跟设置的interval&#x3D;20s基本符合.</p>
<p>同时summary也提供&lt;basename&gt;_sum，&lt;basename&gt;_count这两个指标,这个跟histogram差不多</p>
<p><code>prometheus_target_interval_length_seconds_sum</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409114922.png"></p>
<p><code>prometheus_target_interval_length_seconds_count</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409114941.png"></p>
<p>需要注意的是: <strong>不能对Summary产生的quantile值进行aggregation运算（例如sum, avg等）</strong></p>
<h4 id="summary与histogram区别"><a href="#summary与histogram区别" class="headerlink" title="summary与histogram区别"></a>summary与histogram区别</h4><p>从上面可以发现Summary和histogram是比较类似，在使用上要如何区分呢？prometheus上有篇post专门解释了这两个的区别, <a href="https://prometheus.io/docs/practices/histograms/">详情</a></p>
<p>首先要明白是Summary的quantile计算是在数据上报的时候(简单来说就是在客户端就计算好的)就已经计算好的，需要在定义数据指标的时候就指定quantile的值，因为是数据上报计算的quantile，所以不支持包含数据过滤和聚合的quantile计算</p>
<p>因此对于分位数的计算而言，Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。反之对于客户端而言Histogram消耗的资源更少。在选择这两种方式时用户应该按照自己的实际场景进行选择.  Summary 结构有频繁的全局锁操作，对高并发程序性能存在一定影响。histogram仅仅是给每个桶做一个原子变量的计数就可以了，而summary要每次执行算法计算出最新的X分位value是多少，算法需要并发保护。会占用客户端的cpu和内存</p>
<p>histogram不能得到精确的分为数，设置的bucket不合理的话，误差会非常大</p>
<p>两条经验法则：</p>
<ol>
<li>如果需要汇总，请选择直方图。</li>
<li>否则，如果您对将要观察的值的范围和分布有所了解，请选择直方图。无论值的范围和分布如何，如果需要准确的分位数，请选择摘要</li>
</ol>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://prometheus.io/docs/concepts/metric_types/">https://prometheus.io/docs/concepts/metric_types/</a></li>
<li><a href="https://prometheus.io/docs/practices/histograms/">https://prometheus.io/docs/practices/histograms/</a></li>
<li><a href="https://www.yangcs.net/posts/prometheus-histograms/">https://www.yangcs.net/posts/prometheus-histograms/</a></li>
<li><a href="https://cloud.tencent.com/developer/news/319419">https://cloud.tencent.com/developer/news/319419</a></li>
<li><a href="https://github.com/prometheus/prometheus/blob/master/promql/quantile.go">https://github.com/prometheus/prometheus/blob/master/promql/quantile.go</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(PromSQL常用函数)</title>
    <url>/2020/01/05/prometheus-functions/</url>
    <content><![CDATA[<p>prometheus从某种程序上也算是一种数据库, 使用的是promSQL语言, 既然做为一种数据库查询语言, 自然也跟其它数据库一样内置各种查询函数, prometheus中内置的函数比较多, 完整的函数列表在<a href="https://prometheus.io/docs/prometheus/latest/querying/functions/">这里</a>, 这里会记录在工作中常用到的一些函数.</p>
<span id="more"></span>



<h3 id="时间区间"><a href="#时间区间" class="headerlink" title="时间区间"></a>时间区间</h3><p>首先要了解的是, 在prometheus中，不能直接指定时间区间，不过可以使用如<code>[180m]、offset 5m</code>等方式来表示距当前时间的一段时间内, 这种方式在prometheus中叫做<code>范围向量选择器(range-vector)</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回当前五分钟之前http_requests_total的数据</span></span><br><span class="line">http_requests_total&#123;job=<span class="string">&quot;prometheus&quot;</span>&#125;[5m]</span><br><span class="line"><span class="comment"># offset 允许在单个即时向量或范围向量查询中设置相对于当前时间的时间偏移</span></span><br><span class="line"><span class="comment"># 返回http_requests_total5分钟前的值</span></span><br><span class="line"><span class="built_in">sum</span>(http_requests_total&#123;method=<span class="string">&quot;GET&quot;</span>&#125; offset 5m)</span><br></pre></td></tr></table></figure>



<h3 id="速率函数"><a href="#速率函数" class="headerlink" title="速率函数"></a>速率函数</h3><h4 id="increase-增长量"><a href="#increase-增长量" class="headerlink" title="increase(增长量)"></a>increase(增长量)</h4><p><code>increase(v range-vector)</code></p>
<p>获取时间区间内的第一个样本与最后一个样本，返回之间的增长量</p>
<p>这个比较容易理解，就是在时间区间内的最后一个值减去第一个值, 获得差值</p>
<p>比如: 计算http get类型的请求在5m内增长数</p>
<p><code>increase(http_requests_total&#123;instance=&quot;10.42.6.52:9100&quot;, method=&quot;get&quot;&#125;[5m])</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409144657.png"></p>
<h4 id="rate-增长率"><a href="#rate-增长率" class="headerlink" title="rate(增长率)"></a>rate(增长率)</h4><p><code>rate(v range-vector)</code></p>
<p>计算范围向量中时间序列的每秒, 第一个点及最后一个点来平均增长率，然后求平均增长率</p>
<p>比如： 计算http get类型的请求在5m内增长率</p>
<p><code>rate(http_requests_total&#123;instance=&quot;10.42.6.52:9100&quot;, method=&quot;get&quot;&#125;[5m])</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409144842.png"></p>
<p>可以发现value&#x3D;0.05, 恰恰是在5m内增的15除于300所得到的</p>
<p>因此<strong>rate所得到的增长率就是rate&#x3D;increase&#x2F;range</strong></p>
<h4 id="irate-增长率"><a href="#irate-增长率" class="headerlink" title="irate(增长率)"></a>irate(增长率)</h4><p><code>irate(v range-vector)</code></p>
<p>计算范围向量中时间序列的每秒, 最后两个点来计算瞬时增加率，用增长量&#x2F;时间区间来计算增长率</p>
<p>同样，比如： 计算http get类型的请求在5m内增长率</p>
<p><code>irate(http_requests_total&#123;instance=&quot;10.42.6.52:9100&quot;, method=&quot;get&quot;&#125;[5m])</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409145217.png"></p>
<p>我们能够发现, irate得到的数据跟rate得到的数据是一样, 那为何同时存在这两个函数呢?</p>
<p>先回答下为何这两个值是一样的, 因为这个测试环境没多少请求, 请求数增长才几十个, 在量小的情况下对于使用rate与irate的并别不会很明显</p>
<p>如果这里换个例子, 用rate&#x2F;irate来计算某机器在1m内网卡的增长率，就能看出点区别了</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409145907.png"></p>
<p>所以变化越快的场合这两者的差别越大, 那两者到底有啥区别呢</p>
<h4 id="rate与irate区别"><a href="#rate与irate区别" class="headerlink" title="rate与irate区别"></a>rate与irate区别</h4><p>irate和rate都会用于计算某个指标在一定时间间隔内的变化速率。但是它们的计算方法有所不同：irate取的是在指定时间范围内的最近两个数据点来算速率，而rate会取指定时间范围内所有数据点，算出一组速率，然后取平均值作为结果。</p>
<p>所以官网文档说：<strong>irate适合快速变化的计数器（counter），而rate适合缓慢变化的计数器（counter）</strong>。</p>
<p>根据以上算法我们也可以理解，对于快速变化的计数器，如果使用rate，因为使用了平均值，很容易把峰值削平.而irate则是在范围向量中每个时间序列的两个最近数据点的增长率.</p>
<h3 id="计算函数"><a href="#计算函数" class="headerlink" title="计算函数:"></a>计算函数:</h3><h4 id="sum"><a href="#sum" class="headerlink" title="sum"></a>sum</h4><p>很简单, 直接求和</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409151117.png"></p>
<h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>很简单, 直接请记录数</p>
<p>![image-20200409151152568](&#x2F;Users&#x2F;zhoushuke&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200409151152568.png)</p>
<h4 id="ceil"><a href="#ceil" class="headerlink" title="ceil"></a>ceil</h4><p>四舍五入，将所有元素的样本值四舍五入<code>v</code>到最接近的整数</p>
<h4 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h4><p>返回绝对值</p>
<h4 id="round"><a href="#round" class="headerlink" title="round"></a>round</h4><p>保留小数点位数</p>
<p>后面这几个的意思都比较明确, 在这就不贴图了, 这些计算函数一般都会跟<code>by</code>结合使用.</p>
<h3 id="排序函数"><a href="#排序函数" class="headerlink" title="排序函数"></a>排序函数</h3><h4 id="sort-x2F-sort-desc"><a href="#sort-x2F-sort-desc" class="headerlink" title="sort&#x2F;sort_desc"></a>sort&#x2F;sort_desc</h4><p><code>sort(v instant-vector)/sort_desc(v instant-vector)</code></p>
<p>将结果按升序&#x2F;降序排列</p>
<p><code>sort(sum(nginx_ingress_controller_response_duration_seconds_bucket&#123;path=~&quot;/sensego/v2.0/mingyuan&quot;,status=&quot;200&quot;&#125;) by (le))</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409151642.png"></p>
<p>####topk</p>
<p>k表示按照某种排列只保留K条记录</p>
<p><code>topk(3, sort(sum(nginx_ingress_controller_response_duration_seconds_bucket&#123;path=~&quot;/sensego/v2.0/mingyuan&quot;,status=&quot;200&quot;&#125;) by (le)))</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409151827.png"></p>
<h3 id="标签函数"><a href="#标签函数" class="headerlink" title="标签函数"></a>标签函数</h3><h4 id="label-join"><a href="#label-join" class="headerlink" title="label_join"></a>label_join</h4><p>将多个 src_label 拼接成一个新的 dst_label，用分隔符 separator 连接</p>
<p><code>label_join(v instant-vector, dst_label string, separator string, src_label_1 string, src_label_2 string, ...)</code></p>
<blockquote>
<ul>
<li>v: 表示需要操作记录</li>
<li>dst_label： 添加&#x2F;覆盖的label, 如果指定的label不存在，则添加</li>
<li>separator:  分隔符</li>
<li>src_label_1: 源label </li>
<li>可以有N个标签</li>
</ul>
</blockquote>
<p>比如， 想在以下记录中添加一个foo 标签, 这个标签的值来自instance与job</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409154244.png"></p>
<p><code>label_join(up&#123;instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;&#125;, &quot;foo&quot;, &quot;,&quot;, &quot;instance&quot;, &quot;job&quot;)</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409154700.png"></p>
<p>最终的结果会出现多了个<code>foo=&quot;localhost:9090,prometheus&quot;</code>,达到目的.</p>
<h4 id="lable-replace"><a href="#lable-replace" class="headerlink" title="lable_replace"></a>lable_replace</h4><p>用于给监控项添加&#x2F;覆盖label</p>
<p><code>label_replace(v instant-vector, dst_label string, replacement string, src_label string, regex string)</code></p>
<blockquote>
<ul>
<li>v: 表示需要操作记录</li>
<li>dst_label： 添加&#x2F;被覆盖的label, 如果指定的label不存在，则添加</li>
<li>replacement:  匹配到的string ,使用$1,$2…引用</li>
<li>src_label: regex表达式匹配的源label </li>
<li>regex: 正则表达式，可使用$1,$2…引用</li>
</ul>
</blockquote>
<p>比如: </p>
<p><code>label_replace(kubelet_running_pod_count,&quot;node&quot;, &quot;$1&quot;, &quot;kubernetes_io_hostname&quot;,&quot;(.*)&quot;)</code></p>
<p>原kubelet_running_pod_count 结果集如下，可以看出集中没有node 标签</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/410B432A-94C5-4EBB-9BFA-FFB0DDFA71C1.png"></p>
<p>通过以上语句添加一个node label：</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/99BC5562-4B2F-4136-860E-637FF147BF29.png"></p>
<p>覆盖label:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/EA4D8213-3E13-41B4-B2E2-AD651B577D43.png"></p>
<h3 id="分位函数"><a href="#分位函数" class="headerlink" title="分位函数"></a>分位函数</h3><h4 id="histogram-quantile"><a href="#histogram-quantile" class="headerlink" title="histogram_quantile"></a>histogram_quantile</h4><p>用于计算通过histogram获取数据的分位数</p>
<p>假如，要计算nginx_ingress中95%的响应时间落在哪个区间可以使用以下命令</p>
<p><code>histogram_quantile(0.95, sum(rate(nginx_ingress_controller_response_duration_seconds_bucket&#123;path=~&quot;/sensego/v2.0/mingyuan&quot;,status=&quot;200&quot;&#125;[1800m])) by (le))</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200409115919.png"></p>
<p>当然正常情况下都是在grafana中对Prometheus的数据进行查询, grafana本身也有一些常用的函数, 有机会也学习记录下.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/prometheus/prometheus/blob/master/promql/quantile.go">https://github.com/prometheus/prometheus/blob/master/promql/quantile.go</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/querying/functions/">https://prometheus.io/docs/prometheus/latest/querying/functions/</a></li>
<li><a href="https://my.oschina.net/54188zz/blog/3070582">https://my.oschina.net/54188zz/blog/3070582</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(Node-Exporter采集自定义Metrics)</title>
    <url>/2019/08/28/prometheus-nodeexporter-customized-metrics/</url>
    <content><![CDATA[<p>如果使用Prometheus做监控系统的话, node-exporter一定是不能绕过的采集工具, 开箱即用的实用性让运维同学节省很多时间, 最近也是发现了一个比较有意思的功能, NodeExporter可以采集额外的Metrics到Prometheus中</p>
<span id="more"></span>

<p>假如有时我们有自定义的Metrics需要上传到Promethues中，一般情况下, 我们会通过PushGateway来做采集，假如这个时候恰恰没有部署PushGateway, 有没有更快捷的办法, 这个时候NodeExporter就派上用场了.</p>
<h3 id="启动参数"><a href="#启动参数" class="headerlink" title="启动参数"></a><strong>启动参数</strong></h3><p>查看node-exporter的github描述,node-exporter启动的时候指定–collector.textfile.directory&#x3D;&#x2F;opt&#x2F;exporter&#x2F;node_exporter&#x2F;key 即可, 这个参数会解析指定目录下的所有以prom结尾的文件.</p>
<p>即：</p>
<p>我们只需要把自定义脚本的输出Metrics数据写入到指定目录下的文件,并以Prom结尾即可, Prometheus在定时采集node exporter获取数据的时候，也会把这份数据一同上传, 非常方便</p>
<p><strong>需要注意的是，要保证在Prometheus在一个周期内，自定义脚本将最新的数据写入文件中即可, 每次写入都需要覆盖之前的Metrics.如果文件内的Metrics没有更新，则会一直拿到之前的旧数据, 所以最好在执行之前执行旧数据的清理</strong></p>
<h3 id="自定义脚本"><a href="#自定义脚本" class="headerlink" title="自定义脚本"></a><strong>自定义脚本</strong></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">prom_file=/opt/exporter/node_exporter/key/key.prom</span><br><span class="line"> </span><br><span class="line">IFS=<span class="string">&quot;;&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">export</span> TERM=vt100</span><br><span class="line"> </span><br><span class="line">key_value=<span class="string">&quot;</span></span><br><span class="line"><span class="string">Logical_CPU_core_total  `cat /proc/cpuinfo| grep &quot;</span>processor<span class="string">&quot;| wc -l`;</span></span><br><span class="line"><span class="string">logined_users_total     `who | wc -l`;</span></span><br><span class="line"><span class="string">procs_total             `/bin/top -b -n 1|grep Tasks|sed &#x27;s/,/\n/g&#x27;|grep total|awk &#x27;&#123; print <span class="subst">$(NF-1)</span> &#125;&#x27;`;</span></span><br><span class="line"><span class="string">procs_zombie            `/bin/top -b -n 1|grep Tasks|sed &#x27;s/,/\n/g&#x27;|grep zombie|awk &#x27;&#123; print <span class="subst">$(NF-1)</span> &#125;&#x27;`&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$key_value</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    IFS=<span class="string">&quot; &quot;</span></span><br><span class="line">    j=(`<span class="built_in">echo</span> <span class="variable">$i</span>`)</span><br><span class="line">    key=<span class="variable">$&#123;j[0]&#125;</span></span><br><span class="line">    value=<span class="variable">$&#123;j[1]&#125;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$key</span> <span class="variable">$value</span> &gt;&gt; <span class="string">&quot;<span class="variable">$prom_file</span>&quot;</span>.tmp</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">cat</span> <span class="string">&quot;<span class="variable">$prom_file</span>&quot;</span>.tmp &gt; <span class="variable">$prom_file</span></span><br><span class="line"><span class="built_in">rm</span> -rf <span class="string">&quot;<span class="variable">$prom_file</span>&quot;</span>.tmp</span><br><span class="line">IFS=<span class="variable">$OLD_IFS</span></span><br></pre></td></tr></table></figure>

<p>加上可执行权限.</p>
<h3 id="Crontab"><a href="#Crontab" class="headerlink" title="**Crontab **"></a>**Crontab **</h3><p>在系统的crontab上加上定时执行脚本</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/prometheus/node_exporter">node_exporter</a></li>
<li><a href="https://www.cnblogs.com/momoyan/p/11520676.html">node-exporter的安装与配置</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus学习(Prometheus基于kubernetes做服务发现)</title>
    <url>/2019/09/20/prometheus-service-discovery-on-k8s/</url>
    <content><![CDATA[<p>之前学习了prometheus的架构及一些基本术语, 今天学习一下Prometheus的服务发现机制，这块的内容占据比较大的份额， 主要学习下基于kubernetes做服务发现.</p>
<span id="more"></span>



<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>由于是opnuPrometheus的服务发现的, 但<code>服务发现</code>这个词不应该放到里详细地展开说它是个什么东西，</p>
<p>大家可以想像下DNS的机制,它就是一种<code>服务发现</code>方法. 详解地可参考<a href="https://www.jianshu.com/p/1bf9a46efe7a">这里</a></p>
<p>随着kubernetes被光放地用于在微服务架构下, 错综复杂的服务间调用使得对<code>服务发现</code>机制越来越重要，</p>
<p>当然, 存在<code>服务发现</code>那必然就有<code>服务注册</code>，好在kubernetes本身做了这些事情, 使得我们可以更加专注于业务.</p>
<p>可以有很多种方式实现<code>服务发现</code>, 最早使用的DNS其实就是一种， </p>
<p>在prometheus的<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">官网</a>上, 列出了目前支持的<code>服务发现</code>方式,图中的sd表示的<code> service discovery</code>, 这里了解下最kubernetes跟file这种方式</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200317182657.png"></p>
<h4 id="static-config"><a href="#static-config" class="headerlink" title="static_config"></a>static_config</h4><p>最简单的是静态配置, 这种也权当是一种<code>服务发现</code>吧, 就是直接在prometheus配置中直接指定需要抓取的目标</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200317184413.png"></p>
<p>像上图那样直接在target中指定目标地址即可, 非常简单</p>
<h4 id="file-sd-config"><a href="#file-sd-config" class="headerlink" title="file_sd_config"></a>file_sd_config</h4><p>基于文件的服务发现, 可以理解通过解析文件的方式获取目标, 这种方式跟static_config差不太多</p>
<p>这种方式的有点是可以根据业务进行分类, 而不用全都塞到一个配置文件中，当这个yaml或者json文件内容有改变的时候，prometheus 会通过watch file的形式感知到target内容的变动</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;test_server&#x27;</span></span><br><span class="line">  <span class="attr">file_sd_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">files:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/app/hananmin/prometheus/file_sd/test_server.json</span></span><br><span class="line">      <span class="attr">refresh_interval:</span> <span class="string">10s</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> test_server.json</span><br><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">&quot;targets&quot;</span>:  [<span class="string">&quot;10.161.4.63:9091&quot;</span>,<span class="string">&quot;10.161.4.61:9100&quot;</span>]</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>文件支持通配规则.</p>
<h4 id="kubernetes-sd-config"><a href="#kubernetes-sd-config" class="headerlink" title="kubernetes_sd_config"></a>kubernetes_sd_config</h4><p>要理解prometheus如何基于kubernetes做服务发现这个问题, 那么就需要理解kubernetes是如何做服务发现的.</p>
<p>这里一句话总结就是,<code>kubernetes的服务发现机制是基于dns的,所有的service都注册到api-server最终存储到etcd中,任何组件想要访问其它服务的话都需要从api-server中获取, 因此etcd中存储有所有元数据</code></p>
<p>因此, prometheus要发现kubernetes中的服务, 只需要通过list-watch机制从api-server中获取即可, 当然获取之后prometheus是会缓存在自己的内存中.</p>
<p>从上面能看到prometheus知道从kube-api中获取服务, 但怎么知道具体哪些服务是真正需要获取的, 一个kubernetes集群中可能存在上千个服务, <code>难道所有的服务都需要吗</code>？显然不是.</p>
<p>在学习使用prometheus的时候，有同学看到prometheus的配置文件时肯定会疑惑, 而且整个配置文件中对kubernetes的部分占了大半部分,这里挑监控pod的部分简单说下, 先来看下配置文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-pods</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">20s</span></span><br><span class="line">  <span class="attr">scrape_timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">/metrics</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">kubernetes_sd_configs:</span>	<span class="comment"># 指定kubernetes做为sd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">api_server:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">role:</span> <span class="string">pod</span>    <span class="comment">#指定kubernetes角色.</span></span><br><span class="line">    <span class="attr">namespaces:</span></span><br><span class="line">      <span class="attr">names:</span> []</span><br><span class="line">  <span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_scrape</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">keep</span>  <span class="comment">#对于metric的label中存在__meta_kubernetes_pod_annotation_prometheus_io_scrape时才会保留</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_path</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">(.+)</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>, <span class="string">__meta_kubernetes_pod_annotation_prometheus_io_port</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">([^:]+)(?::\d+)?;(\d+)</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1:$2</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">__meta_kubernetes_pod_label_(.+)</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">(.*)</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_name</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">(.*)</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">kubernetes_pod_name</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_node_name</span>]</span><br><span class="line">    <span class="attr">separator:</span> <span class="string">;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">(.*)</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">node</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1</span></span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br></pre></td></tr></table></figure>

<p>这个配置文件可在<a href="https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml">这里</a>看到，至于上面的配置文件的含义, 网上已经有人解释了，这里就不多说，感兴趣的可以看看<a href="%5Bhttps://jeremyxu2010.github.io/2018/11/%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7%E5%A4%9Ak8s%E9%9B%86%E7%BE%A4/#prometheus%E9%87%87%E9%9B%86%E5%85%B6%E5%AE%83k8s%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE%5D(https://jeremyxu2010.github.io/2018/11/%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7%E5%A4%9Ak8s%E9%9B%86%E7%BE%A4/#prometheus%E9%87%87%E9%9B%86%E5%85%B6%E5%AE%83k8s%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE)">这里</a></p>
<p>这里要说的是, 按照上面的理解, 我一定是需要指定target才行, 但是上面这段配置并没有target字段, 那又是为何呢?</p>
<p>秘密就在于<code>role: pod</code>,上面已经标注出来, 在prometheus的<a href="https://github.com/prometheus/prometheus/blob/master/discovery/kubernetes/pod.go">源码</a>中, 可以看到对pod的具体处理</p>
<p>从prometheus的<a href="https://github.com/prometheus/prometheus/blob/master/discovery/kubernetes/kubernetes.go">源码</a>中也可以看到对kubernetes支持的role共有<code>endpoints,ingress,node,pod,service</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200317213527.png"></p>
<p>所以，如果role 等于其它的值是没有用的, 因此在整个prometheus的配置文件中对于Kubernetes的配置只会有上面那5种role, 可以少, 比如我集群中没有ingress, 那么是可以没有ingress的配置的</p>
<p>所以总结一下, <strong>prometheus通过list-watch机制来更新上面指定role资源</strong></p>
<p>那另一个问题是, 对于上面pod的配置，<code>难道所有的pod对象都会prometheus抓取吗</code>？显然也不是的</p>
<p>如果kubernetes集群到达一个数据级, 全部的pod都scrape的话,那对prometheus跟kubernetes的性能都是个巨大考验, 而且一般情况下，我们只会关心想关心的应用pod，另一此不是很重要的pod其实是可以不监控的。</p>
<p>因此prometheus通过annotations的方式，指定哪些需要需要scrape, 哪些可以忽略.</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200318112248.png"></p>
<p>prometheus.io&#x2F;scrape: 如果加上这个annotations，表示当前pod是允许prometheus抓取数据的，如不加这个annotations，prometheus就不会自动把这个pod纳入到监控中,  scrape&#x3D;true：允许抓取；scrape&#x3D;flase：不允许抓取</p>
<p>prometheus.io&#x2F;port: 抓取数据时使用的套接字端口, 这个端口用于暴露metric</p>
<p>prometheus.io&#x2F;path: 当scrape&#x3D;true时，再加此annotation，表示当前Pod能够输出指标数据的Url,默认为&#x2F;metrics</p>
<p>因此, 整个annotations的完整url为 <a href="http://alertmanager-logging:9093/metrics">http://alertmanager-logging:9093/metrics</a></p>
<p>prometheus通过这种方式来决定哪些pod才是真正需要监控target, 同理其它role也是如此.</p>
<p>peometheus定时抓取target后将metrics数据存在在storge中</p>
<p>到此, 整个prometheus的服务发现target的流程就完了, 这里没有过多的介绍源码层面上的东西,网上有大神写的是感觉好, 大家有兴趣的话可以参考<a href="https://xumc.github.io/blog/2018/09/12/promethues-scrape-source-lookup">这篇</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://xumc.github.io/blog/2018/09/02/promethues-discover">https://xumc.github.io/blog/2018/09/02/promethues-discover</a></li>
<li><a href="https://xumc.github.io/blog/2018/09/12/promethues-scrape-source-lookup">https://xumc.github.io/blog/2018/09/12/promethues-scrape-source-lookup</a></li>
<li><a href="https://www.jianshu.com/p/1bf9a46efe7a">https://www.jianshu.com/p/1bf9a46efe7a</a></li>
<li><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">https://prometheus.io/docs/prometheus/latest/configuration/configuration/</a></li>
<li><a href="https://blog.csdn.net/u010278923/article/details/70943506">https://blog.csdn.net/u010278923/article/details/70943506</a></li>
<li><a href="https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml">https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>CloudNavi</category>
      </categories>
      <tags>
        <tag>CloudNavi</tag>
      </tags>
  </entry>
  <entry>
    <title>pyinstaller打包Flask(-)</title>
    <url>/2017/08/20/pyinstaller%E6%89%93%E5%8C%85Flask(-)/</url>
    <content><![CDATA[<p>每当写完一个脚本工具之后,我们常常被要求需要对客户友好一键式,能有多傻瓜式就多傻瓜式,更别去说给客户机器装些乱七八糟的运行环境,python的打包工具也很好几种,pyinstaller做为python程序打包成windows下的exe,还是非常给力的,当然,还有其它的一些比较好的作用于python打包工具,本人我只用过pyinstaller,对其它的不好评价,这里主要记录下pyinstaller在windows下对python程序打包,总之一句话:<strong>pyinstaller会把代码中所有涉及到有包或者库都抽离出来,exe运行的前再全部解压到一个目录下</strong></p>
<span id="more"></span>

<h3 id="pyinstaller的安装"><a href="#pyinstaller的安装" class="headerlink" title="pyinstaller的安装"></a><strong>pyinstaller的安装</strong></h3><p>在windows下,pyinstaller需要PyWin32的支持,当用pip安装pyinstaller时未找到PyWin32,会自动安装pypiwin32.</p>
<p>pyinstaller的<a href="http://www.pyinstaller.org/">官网</a>其实有非常详细的步骤,最新版为pyinstaller-3.2.1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##使用pip安装</span></span><br><span class="line">pip install pyinstaller</span><br></pre></td></tr></table></figure>

<h3 id="pyinstaller常用选项"><a href="#pyinstaller常用选项" class="headerlink" title="pyinstaller常用选项"></a><strong>pyinstaller常用选项</strong></h3><p>pyinstaller安装好之后,便可以用来打包了,最常用的打包命令:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将所有文件打包成一个exe</span></span><br><span class="line">pyinstaller -F name.exe</span><br><span class="line"><span class="comment">#将所有文件打包在一个文件夹里,这是默认的打包方式</span></span><br><span class="line">pyinstaller -D 	/path/to/go</span><br><span class="line"><span class="comment">#指定icon</span></span><br><span class="line">pyinstaller -F name.exe -i name.icon</span><br><span class="line"><span class="comment">#指定python程序中依赖的包文件</span></span><br><span class="line">pyinstaller -p /path/to/go</span><br><span class="line"><span class="comment">#使用控制台界面,只在windows下有效,默认是不使用</span></span><br><span class="line">pyinstaller -c ...</span><br><span class="line"><span class="comment">#使用--add-data</span></span><br><span class="line">pyinstaller --add-data /src/path/:/dest/path</span><br></pre></td></tr></table></figure>

<h4 id="使用-SPEC文件"><a href="#使用-SPEC文件" class="headerlink" title="使用.SPEC文件"></a><strong>使用.SPEC文件</strong></h4><p>我们可以直接把命令写在一个叫.spec的文件中,这样就不用每次都写那么一长串命令,直接使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用.spec文件</span></span><br><span class="line">pyinstaller name.spec</span><br></pre></td></tr></table></figure>

<p>.spec文件的内容格式如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- mode: python -*-</span></span><br><span class="line"></span><br><span class="line">block_cipher = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">added_files = [</span><br><span class="line">         (<span class="string">&#x27;.\\templates&#x27;</span>, <span class="string">&#x27;templates&#x27;</span>),</span><br><span class="line">         (<span class="string">&#x27;.\\static&#x27;</span>, <span class="string">&#x27;static&#x27;</span>)</span><br><span class="line">         ]</span><br><span class="line"></span><br><span class="line">a = Analysis([<span class="string">&#x27;chartson.py&#x27;</span>],</span><br><span class="line">             pathex=[<span class="string">&#x27;D:\\chartson&#x27;</span>],</span><br><span class="line">             hiddenimports=[],</span><br><span class="line">             binaries=[],</span><br><span class="line">             datas=added_files,</span><br><span class="line">             hookspath=[],</span><br><span class="line">             runtime_hooks=[],</span><br><span class="line">             excludes=[],</span><br><span class="line">             win_no_prefer_redirects=<span class="literal">False</span>,</span><br><span class="line">             win_private_assemblies=<span class="literal">False</span>,</span><br><span class="line">             cipher=block_cipher)</span><br><span class="line">pyz = PYZ(a.pure, a.zipped_data,</span><br><span class="line">             cipher=block_cipher)</span><br><span class="line">exe = EXE(pyz,</span><br><span class="line">          a.scripts,</span><br><span class="line">          a.binaries,</span><br><span class="line">          a.zipfiles,</span><br><span class="line">          a.datas,</span><br><span class="line">          name=<span class="string">&#x27;chartson&#x27;</span>,</span><br><span class="line">          debug=<span class="literal">False</span>,</span><br><span class="line">          strip=<span class="literal">False</span>,</span><br><span class="line">          upx=<span class="literal">True</span>,</span><br><span class="line">          console=<span class="literal">True</span> , icon=<span class="string">&#x27;chartson.ico&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="spec中的datas选项"><a href="#spec中的datas选项" class="headerlink" title="spec中的datas选项"></a><strong>spec中的datas选项</strong></h4><p>上面的命令中有个很重要的选项: <strong>datas</strong>,我们知道,在flask架构中,我们常用的项目结构一般都包含有<strong>static</strong>,用来存放一些引用的js,css文件等,<strong>template</strong>中则用来存放模板文件,这些文件夹都是在flask代码之外,flask在运行的时候是需要去找这些文件夹里的文件,那么在用pyinstaller打包时如何把这些额外文件夹一同打包进去呢,换一句话说:<strong>如何把我们在项目中引用的其它非二进制文件跟python程序打包在一个exe中</strong>,这就是datas需要做的</p>
<p>而在Flask的app.py中,app的声明应该如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">getattr</span>(sys, <span class="string">&#x27;frozen&#x27;</span>, <span class="literal">False</span>):</span><br><span class="line">    template_folder = os.path.join(sys._MEIPASS, <span class="string">&#x27;templates&#x27;</span>)</span><br><span class="line">    static_folder = os.path.join(sys._MEIPASS, <span class="string">&#x27;static&#x27;</span>)</span><br><span class="line">    app = Flask(__name__, template_folder=template_folder,static_folder=static_folder)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    app = Flask(__name__)</span><br></pre></td></tr></table></figure>

<p>上面的added_files(这个名字可随意,只要跟Analysis中的datas配对即可)中指定了templates指向的当前目录下的templates,而且if&#x2F;else保证了即使是单独使用python app.py运行项目也能跑起来</p>
<p>exe解压路径如果没有指定的话默认是在C:\Users***\AppData\Roaming\pyinstaller\下,在这目录下,你能看到很多pyd文件,当然这些都是被编译过的了,还有很多项目中import的库跟windows下的dll.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://pyinstaller.readthedocs.io/en/stable/usage.html">pyinstaller官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>pyinstaller打包flask(二)</title>
    <url>/2017/10/08/pyinstaller%E6%89%93%E5%8C%85Flask(%E4%BA%8C)/</url>
    <content><![CDATA[<p>在使用pyinstaller库的过程中遇到几个问题,看了下源码,发现造成这几个问题的原因类似,感觉挺有意思,之前说过,pyinstaller其实就是分析python文件中的import 语句,然后打包成pyd或都是dll等库文件,但是对于一些python中使用动态导入或者是使用指定路径等形式,或者换句话说就是:<strong>在运行中才能确定导入了哪些库或者是使用了哪些dll</strong>,这种情况下pyinstaller是无法自动辨别的,下面遇到的三个问题都是由于动态导入的问题,这个时候就需要使用pyinstaller更高级的用法了.</p>
<span id="more"></span>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h3><h4 id="错误一-No-executor-by-the-name-“threadpool”-was-found"><a href="#错误一-No-executor-by-the-name-“threadpool”-was-found" class="headerlink" title="错误一: No executor by the name “threadpool” was found"></a><strong>错误一: No executor by the name “threadpool” was found</strong></h4><p>在<a href="https://izsk.me/2017/09/27/python%E5%AE%9A%E6%97%B6%E6%A1%86%E6%9E%B6APScheduler(%E4%B8%80)">上一篇</a>介绍jobstore时我们知道apscheduler中默认的执行器为threadpool,而且有3种初始化写法,具体请参考这篇<a href="http://apscheduler.readthedocs.io/en/latest/userguide.html#configuring-the-scheduler">官网</a>,而那对于flask-apscheduler,默认的初始化方式也很json风格</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    JOBS = [</span><br><span class="line">        <span class="comment">#每月15号23:30删除32天之前采集的awr.db的历史数据,一键生成报表时需要当月数据,历史数据需要保留一个月,这里设置为32天</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;truncatedb&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;func&#x27;</span>: <span class="string">&#x27;dbfunc:truncatedb&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;args&#x27;</span>: (<span class="number">32</span>,),</span><br><span class="line">            <span class="string">&#x27;trigger&#x27;</span>: CronTrigger(day=<span class="number">15</span>,hour=<span class="number">23</span>,minute=<span class="number">30</span>),</span><br><span class="line">            <span class="string">&#x27;replace_existing&#x27;</span>: <span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">    SCHEDULER_JOBSTORES = &#123;</span><br><span class="line">        <span class="string">&#x27;default&#x27;</span>: SQLAlchemyJobStore(url=<span class="string">&#x27;sqlite:///&#x27;</span> + SQLITE_DB)</span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_EXECUTORS = &#123;</span><br><span class="line">        <span class="string">&#x27;default&#x27;</span>: &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;threadpool&#x27;</span>, <span class="string">&#x27;max_workers&#x27;</span>: <span class="number">20</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_JOB_DEFAULTS = &#123;</span><br><span class="line">        <span class="comment">#如果某一任务错过执行多次,设定为True时,只会执行一次</span></span><br><span class="line">        <span class="string">&#x27;coalesce&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="comment">#应该调度执行的时间跟当前时间差值泛围,小于则会被再次调度</span></span><br><span class="line">        <span class="string">&#x27;misfire_grace_time&#x27;</span>: <span class="number">20</span>,</span><br><span class="line">        <span class="comment">#每个job在同一时刻能够运行的最大实例数</span></span><br><span class="line">        <span class="string">&#x27;max_instances&#x27;</span>: <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_API_ENABLED = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>上面指定了默认的执行器为threadpool,按照flask-apscheduler用户手册来说这样写没有问题,事实上单独写成脚本执行也没有问题,但是一整合到flask web中,就会报如下错误,提示找不到threadpool执行器:</p>
<p>查看flask-apscheduler源码,有如下函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_load_config</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Load the configuration from the Flask configuration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        options = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">        job_stores = self.app.config.get(<span class="string">&#x27;SCHEDULER_JOBSTORES&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> job_stores:</span><br><span class="line">            options[<span class="string">&#x27;jobstores&#x27;</span>] = job_stores</span><br><span class="line"></span><br><span class="line">        executors = self.app.config.get(<span class="string">&#x27;SCHEDULER_EXECUTORS&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> executors:</span><br><span class="line">            options[<span class="string">&#x27;executors&#x27;</span>] = executors</span><br><span class="line"></span><br><span class="line">        job_defaults = self.app.config.get(<span class="string">&#x27;SCHEDULER_JOB_DEFAULTS&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> job_defaults:</span><br><span class="line">            options[<span class="string">&#x27;job_defaults&#x27;</span>] = job_defaults</span><br><span class="line"></span><br><span class="line">        timezone = self.app.config.get(<span class="string">&#x27;SCHEDULER_TIMEZONE&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> timezone:</span><br><span class="line">            options[<span class="string">&#x27;timezone&#x27;</span>] = timezone</span><br><span class="line">        self._scheduler.configure(**options)</span><br><span class="line">            ...</span><br><span class="line">            </span><br></pre></td></tr></table></figure>

<p>该函数从app中get到配置,然后通过字典形式传入_scheduler中,_scheduler是一个BackgroundScheduler()对象,而这个对象又是继承于BaseScheduler(),看BaseScheduler类这完全没问题,那为何找不到threadpool呢?</p>
<h4 id="问题二-No-modules-named-‘reportlab-graphics-barcode-common’"><a href="#问题二-No-modules-named-‘reportlab-graphics-barcode-common’" class="headerlink" title="问题二:No modules named ‘reportlab.graphics.barcode.common’"></a><strong>问题二:No modules named ‘reportlab.graphics.barcode.common’</strong></h4><p>同样的问题,单独脚本执行可以,使用pyinstaller打包flask成all-in-one就提示找不到库了,当时还以为是reportlab库需要适配,也是追了reportlab的源码,没发现什么问题,reportlab出问题的源码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_BCW</span>(<span class="params">doc,codeName,attrMap,mod,value,**kwds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;factory for Barcode Widgets&quot;&quot;&quot;</span></span><br><span class="line">    _pre_init = kwds.pop(<span class="string">&#x27;_pre_init&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    _methods = kwds.pop(<span class="string">&#x27;_methods&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    name = <span class="string">&#x27;Barcode&#x27;</span>+codeName</span><br><span class="line">    ns = <span class="built_in">vars</span>().copy()</span><br><span class="line">    code = <span class="string">&#x27;from %s import %s&#x27;</span> % (mod,codeName)</span><br><span class="line">    rl_exec(code,ns)</span><br><span class="line">    ns[<span class="string">&#x27;_BarcodeWidget&#x27;</span>] = _BarcodeWidget</span><br><span class="line">    code = <span class="string">&#x27;&#x27;&#x27;class %(name)s(_BarcodeWidget,%(codeName)s):</span></span><br><span class="line"><span class="string">\t_BCC = %(codeName)s</span></span><br><span class="line"><span class="string">\tcodeName = %(codeName)r</span></span><br><span class="line"><span class="string">\tdef __init__(self,**kw):%(_pre_init)s</span></span><br><span class="line"><span class="string">\t\t_BarcodeWidget.__init__(self,%(value)r,**kw)%(_methods)s&#x27;&#x27;&#x27;</span> % ns</span><br><span class="line">    rl_exec(code,ns)</span><br><span class="line">    Klass = ns[name]</span><br><span class="line">    <span class="keyword">if</span> attrMap: Klass._attrMap = attrMap</span><br><span class="line">    <span class="keyword">if</span> doc: Klass.__doc__ = doc</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwds.items():</span><br><span class="line">        <span class="built_in">setattr</span>(Klass,k,v)</span><br><span class="line">    <span class="keyword">return</span> Klass</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>很明显上面的代码中有<code>code = &#39;from %s import %s&#39; % (mod,codeName)</code>,动态导入方式.</p>
<h4 id="问题三-dlopen-failed-to-load-a-library-cairo-x2F-cairo-2"><a href="#问题三-dlopen-failed-to-load-a-library-cairo-x2F-cairo-2" class="headerlink" title="问题三:dlopen() failed to load a library: cairo &#x2F; cairo-2"></a><strong>问题三:dlopen() failed to load a library: cairo &#x2F; cairo-2</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cairo = ffi.dlopen(os.path.join(os.path.dirname(__file__), <span class="string">&#x27;cairo.dll&#x27;</span>)) <span class="comment"># case1</span></span><br><span class="line">    <span class="comment">#cairo = ffi.dlopen(os.path.join(os.path.dirname(os.path.abspath(__file__)), &#x27;cairo.dll&#x27;)) # case2</span></span><br><span class="line">    <span class="comment">#cairo = ffi.dlopen(&#x27;cairo.dll&#x27;) # case3</span></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    cairo = dlopen(ffi, <span class="string">&#x27;cairo&#x27;</span>, <span class="string">&#x27;cairo-2&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>  上面最开始那句是cairocffi的源码,可以看出这里使用了os.path.join(os.path.dirname(_<em>file_</em>,’cairo.dll’),其实就是使用该目录下的cairo.dll文件,这也是只能在脚本运行时才能确定路径,所以pyinstaller运行时会产生异常.</p>
<p>那么最重要的问题来了,同样也是解决上面3个问题的方法</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a><strong>解决</strong></h3><h4 id="pyinstaller之hiddenimports"><a href="#pyinstaller之hiddenimports" class="headerlink" title="pyinstaller之hiddenimports"></a><strong>pyinstaller之hiddenimports</strong></h4><p>从字面上可以理解,隐藏式导入就是可以不以代码为标准直接导入指定的模块,<strong>问题二</strong>:可以直接使用hiddenimports导入pyinstaller不能自动导入的模块:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hiddenimports = [</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.common&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.code128&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.code93&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.code39&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.usps&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.usps4s&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;reportlab.graphics.barcode.ecc200datamatrix&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>这样pyinstaller在打包的时候会把上面指定的模块也打包进去,是不是很方便</p>
<h4 id="pyinstaller之打包二进制文件"><a href="#pyinstaller之打包二进制文件" class="headerlink" title="pyinstaller之打包二进制文件"></a><strong>pyinstaller之打包二进制文件</strong></h4><p><strong>问题三</strong>则是pyinstaller无法打到dll文件,这个时候可以使用binaries指定,</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = Analysis([<span class="string">&#x27;xxx.py&#x27;</span>],</span><br><span class="line">             pathex=[<span class="string">&#x27;D:\\xxx&#x27;</span>],</span><br><span class="line">             hiddenimports=hiddenimports,</span><br><span class="line">             binaries=[(<span class="string">&#x27;.\\cairo.dll&#x27;</span>,<span class="string">&#x27;.&#x27;</span>)],</span><br><span class="line">             datas=added_files,</span><br><span class="line">             hookspath=[],</span><br><span class="line">             runtime_hooks=[],</span><br><span class="line">             excludes=[],</span><br><span class="line">             win_no_prefer_redirects=<span class="literal">False</span>,</span><br><span class="line">             win_private_assemblies=<span class="literal">False</span>,</span><br><span class="line">             cipher=block_cipher)</span><br></pre></td></tr></table></figure>

<p>pyinstaller则会在当前目录下查找cairo.dll打包进exe,exe执行的时候会把cairo.dll解压到pyinstaller的临时生成的解压路径,而且也把问题三的源码修改成了case3,不需要再使用os.path就能找到了</p>
<p>而<strong>问题一</strong>则需要改成如下声明:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> apscheduler.jobstores.sqlalchemy <span class="keyword">import</span> SQLAlchemyJobStore</span><br><span class="line"><span class="keyword">from</span> apscheduler.executors.pool <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"><span class="keyword">from</span> apscheduler.triggers.cron <span class="keyword">import</span> CronTrigger</span><br><span class="line"><span class="keyword">from</span> apscheduler.triggers.interval <span class="keyword">import</span> IntervalTrigger</span><br><span class="line"><span class="keyword">from</span> apscheduler.events <span class="keyword">import</span> EVENT_JOB_EXECUTED,EVENT_JOB_ERROR</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    JOBS = [</span><br><span class="line">        <span class="comment">#每月15号23:30删除32天之前采集的awr.db的历史数据,一键生成报表时需要当月数据,历史数据需要保留一个月,这里设置为32天</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;truncatedb&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;func&#x27;</span>: <span class="string">&#x27;dbfunc:truncatedb&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;args&#x27;</span>: (<span class="number">32</span>,),</span><br><span class="line">            <span class="string">&#x27;trigger&#x27;</span>: CronTrigger(day=<span class="number">15</span>,hour=<span class="number">23</span>,minute=<span class="number">30</span>),</span><br><span class="line">            <span class="string">&#x27;replace_existing&#x27;</span>: <span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">    SCHEDULER_JOBSTORES = &#123;</span><br><span class="line">        <span class="string">&#x27;default&#x27;</span>: SQLAlchemyJobStore(url=<span class="string">&#x27;sqlite:///&#x27;</span> + SQLITE_DB)</span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_EXECUTORS = &#123;</span><br><span class="line">        <span class="comment">#使用pyinstaller打包时只能采用第二种写法,第一种写法会提示No executor by the name &quot;threadpool&quot; was found</span></span><br><span class="line">        <span class="comment">#&#x27;default&#x27;: &#123;&#x27;type&#x27;: &#x27;threadpool&#x27;, &#x27;max_workers&#x27;: 20&#125;</span></span><br><span class="line">        <span class="string">&#x27;default&#x27;</span>:ThreadPoolExecutor(<span class="number">20</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_JOB_DEFAULTS = &#123;</span><br><span class="line">        <span class="comment">#如果某一任务错过执行多次,设定为True时,只会执行一次</span></span><br><span class="line">        <span class="string">&#x27;coalesce&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="comment">#应该调度执行的时间跟当前时间差值泛围,小于则会被再次调度</span></span><br><span class="line">        <span class="string">&#x27;misfire_grace_time&#x27;</span>: <span class="number">20</span>,</span><br><span class="line">        <span class="comment">#每个job在同一时刻能够运行的最大实例数</span></span><br><span class="line">        <span class="string">&#x27;max_instances&#x27;</span>: <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line">    SCHEDULER_API_ENABLED = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>总之就是一句话,在打包好的exe运行所需要的环境需要在pyinstaller解压路径下存在,对于在程序运行时才能确定的模块则需要额外处理了,可以指定路径(不是所有的机器都有环境,要不然就不需要一键打包了),可以hiddenimports,也可以hooks,hooks意为勾子,也是pyinstaller的一种查找模块机制,下次再研究.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://pyinstaller.readthedocs.io/en/stable/usage.html">pyinstaller官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python学习(flask-apispec简洁之道)</title>
    <url>/2022/10/05/python-flask-apispec/</url>
    <content><![CDATA[<p>在使用python写web框架时，经常会碰到需要对request参数进行检验或者过滤，如果将诸多的校验逻辑都堆积在业务逻辑中，会显得很臃肿，在flask中，推荐一个很棒的库，可以写法变得很清晰.</p>
<span id="more"></span>



<!-- more -->

<p>比如有这么个需求: 在flask中，有个app route需要验证requests的post中body传递过来的token字段必须在配置文件中才合法，同时phone字段需要符合规则，再同时，除了必要的字段外，不能有其它的字段，如果出现其它的字段，但返回指定的错误码.<br>如果在代码中直接写各种判断也是ok，之前作者也常是这么做，但明显不够优雅，引入flask-apispec就可以很好的解决问题，让代码看上去很简洁.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/local/bin/python </span></span><br><span class="line"><span class="comment">#-* coding: utf-8-*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json </span><br><span class="line"><span class="keyword">import</span> fire </span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> cfg <span class="keyword">as</span> CFG </span><br><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">from</span> utils.Dingtalk <span class="keyword">import</span> DingtalkChatbot </span><br><span class="line"><span class="keyword">from</span> pkg resources <span class="keyword">import</span> require</span><br><span class="line"><span class="keyword">from</span> marshmallow <span class="keyword">import</span> fields, ValidationError</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request, Flask, jsonify, make_response <span class="keyword">from</span> flask_apispec <span class="keyword">import</span> use_kwargs</span><br><span class="line"><span class="keyword">from</span> tenacity <span class="keyword">import</span> retry, stop_after_attempt, wait_fixed</span><br><span class="line"></span><br><span class="line">app.Flask(__name__)</span><br><span class="line">app.Config[<span class="string">&#x27;JSON_AS_ASCII&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理出现422及400错误，返回对应的信息，发送回403</span></span><br><span class="line"><span class="comment"># Hand http error code 422 and 400 </span></span><br><span class="line"><span class="meta">@app.Errorhandler(<span class="params"><span class="number">422</span></span>) </span></span><br><span class="line"><span class="meta">@app.Errorhandler(<span class="params"><span class="number">400</span></span>) </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handle_error</span>(<span class="params">err</span>):</span><br><span class="line">  msg = err.Data.Get(<span class="string">&quot;messages&quot;</span>,  [<span class="string">&quot;Invalid Request&quot;</span>]) </span><br><span class="line">  logger.Critical(msg)</span><br><span class="line">  <span class="keyword">return</span> make_response(jsonify(&#123;<span class="string">&quot;msg&quot;</span>: msg[<span class="string">&quot;json&quot;</span>]&#125;), <span class="number">403</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是dingding发送逻辑，不重要</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dingtalk</span>(<span class="params">**kw</span>):</span><br><span class="line">  is_at_all=<span class="literal">False</span></span><br><span class="line">  content = kw.Get(<span class="string">&quot;content&quot;</span>, <span class="string">&quot;alarm: THIS-IS-DEFAULT-CONTENT-FROM-DINGTALK&quot;</span>) </span><br><span class="line">  token = kw.Get(<span class="string">&quot;token&quot;</span>) </span><br><span class="line">  to = kw.Get(<span class="string">&quot;to&quot;</span>) </span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">or</span> <span class="keyword">not</span> to:</span><br><span class="line">    logger.Error(<span class="string">&quot;token or to can&#x27;t be null... &quot;</span>)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;@all&quot;</span> <span class="keyword">in</span> to:</span><br><span class="line">      is_at_all = <span class="literal">True</span></span><br><span class="line">      tos = []</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      tos = [x <span class="keyword">for</span> x <span class="keyword">in</span> to.Replace (<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>).Split(<span class="string">&quot;, &quot;</span>)]</span><br><span class="line">      logger.Info (<span class="string">&quot;alert [dingtalk]: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(content))</span><br><span class="line">      xiaoding.DingtalkChatbot(**&#123;<span class="string">&quot;token&quot;</span>: token&#125;)</span><br><span class="line">      xiaoding.Send_text(msg=<span class="string">&quot;alarm: &quot;</span> + content, at_mobiles=tos, is_at_all=is_at_all)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则判断手机号是否合法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">phone_check</span>(<span class="params">phone</span>):</span><br><span class="line">  phone_rule=<span class="string">&quot;^((13[0-9])|(14[5,7])|(15[0-3,5-9])|(17[0,3,5-8])|(18[0-9])166|198|199|(147))\\d&#123;8&#125;$&quot;</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">True</span> <span class="keyword">if</span> re.Match(phone_rule, phone.Strip()) <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验发送者手机号是否合法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_sender</span>(<span class="params">to</span>):</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&quot;@all&quot;</span> <span class="keyword">in</span> to:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> to.Strip().Split (<span class="string">&quot;, &quot;</span>):</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> x.Strip().Isdigit() <span class="keyword">or</span> <span class="keyword">not</span> phone_check(x):</span><br><span class="line">        <span class="keyword">raise</span> ValidationError(<span class="string">&quot;phone NotFound or number is invalid&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看token是否在配置文件中, 配置文件是个yaml格式，格式不重要.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">must_exist_in_conf</span>(<span class="params">t</span>):</span><br><span class="line">  <span class="keyword">if</span> t.Strip() <span class="keyword">not</span> <span class="keyword">in</span> CFG[<span class="string">&quot;dingtalk&quot;</span>][<span class="string">&quot;tokens&quot;</span>]:</span><br><span class="line">    <span class="keyword">raise</span> ValidationError(<span class="string">&quot;token is invalid&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重要</span></span><br><span class="line">args_dt = &#123;<span class="string">&quot;token&quot;</span>: fields.Str(required=<span class="literal">True</span>, validate=must_exist_in_conf), <span class="string">&quot;content&quot;</span>: fields.Str(required=<span class="literal">True</span>), <span class="string">&quot;to&quot;</span>: fields.Str(load_default=<span class="string">&quot;@all&quot;</span>, validate=check_sender)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重要</span></span><br><span class="line"><span class="comment"># use kwargs dict field will pass to **kw args</span></span><br><span class="line"><span class="meta">@app.Route(<span class="params"><span class="string">&quot;/webhook/dingtalk&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>) </span></span><br><span class="line"><span class="meta">@use_kwargs(<span class="params">args_dt, location=<span class="string">&quot;json&quot;</span></span>) </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">webhook_dingtalk</span>(<span class="params">**kw</span>):</span><br><span class="line">  _dingtalk (**kw)</span><br><span class="line">  <span class="keyword">return</span> make_response(jsonify(&#123;<span class="string">&quot;msg&quot;</span>: <span class="string">&quot;OK&quot;</span>&#125;), <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动flask</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">runapp</span>():</span><br><span class="line">  app.Run(host=<span class="string">&#x27;0.0.0.0&#x27;</span>, port=<span class="number">5555</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">watchdog</span>():</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> name == <span class="string">&quot; main &quot;</span>:</span><br><span class="line">  fire. Fire(&#123;<span class="string">&quot;runapp&quot;</span>: runapp, <span class="string">&quot;watchdog&quot;</span>: watchdog&#125;)</span><br></pre></td></tr></table></figure>

<p>其它的也没什么，重要的只的<code>@use_kwargs</code>装饰器,接下来详细展开</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.Route(<span class="params"><span class="string">&quot;/webhook/dingtalk&quot;</span>, methods= [<span class="string">&quot;POST&quot;</span>]</span>) </span></span><br><span class="line"><span class="meta">@use_kwargs(<span class="params">args_dt, location=<span class="string">&quot;json&quot;</span></span>) </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">webhook_dingtalk</span>(<span class="params">**kw</span>):</span><br><span class="line">  _dingtalk (**kw)</span><br><span class="line">  <span class="keyword">return</span> make_response(jsonify(&#123;<span class="string">&quot;msg&quot;</span>: <span class="string">&quot;OK&quot;</span>&#125;), <span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<p>如果按照正常的写法一般是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.Route(<span class="params"><span class="string">&quot;/webhook/dingtalk&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>) </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">webhook_dingtalk</span>():</span><br><span class="line">  res = request.json()</span><br><span class="line">  <span class="comment"># 调用各种逻辑对参数进行判断</span></span><br><span class="line">  _dingtalk (**res)</span><br><span class="line">  <span class="keyword">return</span> make_response(jsonify(&#123;<span class="string">&quot;msg&quot;</span>: <span class="string">&quot;OK&quot;</span>&#125;), <span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<p>在函数中直接使用<code>request.json()</code>获取body的key-value, 然后做各种判断<br>那现在主函数<code>webhook_dingtalk(**kw)</code>非常简短，答案就在引入了<code>use_kwargs</code>,简单来讲就是<code>use_kwargs</code>将接收request的参数，然后将参数作用于第一个参数指定的字典，即<code>args_dt</code>，简单看一下<code>args_dt</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args_dt = &#123;<span class="string">&quot;token&quot;</span>: fields.Str(required=<span class="literal">True</span>, validate=must_exist_in_conf), <span class="string">&quot;content&quot;</span>: fields.Str(required=<span class="literal">True</span>), <span class="string">&quot;to&quot;</span>: fields.Str(load_default=<span class="string">&quot;@all&quot;</span>, validate=check_sender)&#125;</span><br></pre></td></tr></table></figure>

<p>其实也是非常清晰，在这个字典中其它指定了参数列表，也指定了各个参数需要进行的判断,比如token字段，required&#x3D;True,表明这个字段是必要的，validate则表示这个字段需要进行的逻辑处理, 那么这里就可以写各种业务上的判断了,其实内置了很多常用的一些规则，比如判断是不是布尔型<code>fields.Boolean()</code>等等.<br>这种写法是不是让主函数看上去简洁了许多.<br>主函数<code>webhook_dingtalk(**kw)</code>接收的参数只有kw, kw其它就是args_dt合法后传递过来，<strong>如果从requst中拿到的参数不符合args_dt中指定的任一规则，则会触发422或者400错误，将会调用代码最上面定义的handle_error自定义逻辑，这里是返回403错误</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_kwargs</span>(<span class="params">args, locations=<span class="literal">None</span>, inherit=<span class="literal">None</span>, apply=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>另外, use_kwargs的第2个参数location,表示的是从http传递过来的参数是以什么方式呈现，可以选用(‘json’, ‘querystring’, ‘form’, ‘headers’, ‘cookies’, ‘files’)作为locations的值,因为参数可以放在body中，也可以放在header或者是cookies中，这个参数主要是告诉use_kwargs需要的参数保存在哪里.<br>flask-apispec内部用了webargs用于参数解析, marshmallow库用于返回响应,也用了apispec，还有一些很实用的功能，感兴趣的可以查看<a href="https://github.com/jmcarp/flask-apispec">官网</a><br>使用flask-apispec, 代码看上去舒服多了</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/jmcarp/flask-apispec">https://github.com/jmcarp/flask-apispec</a></li>
<li><a href="https://blog.csdn.net/weixin_43845541/article/details/95106349">https://blog.csdn.net/weixin_43845541/article/details/95106349</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python学习(全局配置文件引用问题)</title>
    <url>/2019/04/17/python-global-config-object/</url>
    <content><![CDATA[<p>在python工程代码中, 配置文件的引用是一定无法躲避的问题, 最次的办法是每次调用一个读取配置文件的函数，获得需要的变量</p>
<p>但是如果配置文件的格式是ini、yaml、json等富文本的形式呢?每次还得进行解析, 显然不是很好, 有更pythonic的方法么？</p>
<span id="more"></span>



<p>python里有<a href="https://docs.python.org/3/library/configparser.html">configparse</a>库可以方便我们进行配置文件的解析, 同时也支持多种格式</p>
<p>经过一番研究之后还得觉得跟我想要的效果差点什么？</p>
<p>然后我去看了<a href="https://github.com/django/django">django</a>的配置文件是如何实现全局效果的，恍然大悟. That’s it.</p>
<p>这里以yaml格式的配置文件为例，目录结构</p>
<p>假如所有的配置文件都放在conf&#x2F;global.yml文件中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">topdir</span><br><span class="line">	- conf</span><br><span class="line">		- __init__.py</span><br><span class="line">		- global.yml <span class="comment"># yaml格式的配置文件</span></span><br><span class="line">	- bin</span><br><span class="line">	- app</span><br><span class="line">		- app.py</span><br></pre></td></tr></table></figure>

<p><code>global.yml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">default:</span></span><br><span class="line">  <span class="comment"># 基础配置</span></span><br><span class="line">  <span class="attr">uniall:</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">&#x27;https://baidu.com&#x27;</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">    <span class="comment"># 功能重试次数</span></span><br><span class="line">    <span class="attr">func_retries:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 功能重试时 延迟时间, 单位: 秒</span></span><br><span class="line">    <span class="attr">func_retry_delay_seconds:</span> <span class="number">10</span></span><br><span class="line">    <span class="comment"># http 重试次数</span></span><br><span class="line">    <span class="attr">http_max_retries:</span> <span class="number">2</span></span><br><span class="line">    <span class="comment"># http 超时时间, 单位: 秒</span></span><br><span class="line">    <span class="attr">http_timeout:</span> <span class="number">10</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dev:</span></span><br><span class="line">	<span class="attr">http:</span></span><br><span class="line">		<span class="string">...</span></span><br><span class="line">	<span class="string">...</span></span><br></pre></td></tr></table></figure>

<p><code>conf/__init__.py</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> yaml <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> yaml <span class="keyword">import</span> CLoader <span class="keyword">as</span> Loader</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">from</span> yaml <span class="keyword">import</span> Loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Get_Config</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path=<span class="string">&quot;global.yml&quot;</span>, mode=<span class="string">&quot;default&quot;</span></span>):</span><br><span class="line">        self.path = path</span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.cfg = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_conf</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(self.path):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(self.path, <span class="string">&quot;rt&quot;</span>) <span class="keyword">as</span> fh:</span><br><span class="line">                config = load(fh, Loader=Loader)</span><br><span class="line">                cfg = config[self.mode]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;CONFIG FILE NOT FOUND&quot;</span>)</span><br><span class="line">            os._exit(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> cfg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CONF_PATH = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">cfg = Get_Config(path=<span class="string">&quot;&#123;&#125;/global.yml&quot;</span>.<span class="built_in">format</span>(CONF_PATH), mode=<span class="string">&quot;default&quot;</span>).parse_conf()</span><br></pre></td></tr></table></figure>

<p>在多个环境下同一份配置文件时, <code>mode</code>则变得非常有用, 只会解析<code>mode</code>对应的配置</p>
<p>这里用到了<code>__init__.py</code>这个文件的magic, 在导入时会自动地执行.</p>
<p>这样, 在app&#x2F;app.py等其它文件中如果需要使用配置文件,则只需要导入cfg即可</p>
<p>然后像使用字典一样引用配置就行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> conf <span class="keyword">import</span> cfg</span><br><span class="line"></span><br><span class="line">cfg[<span class="string">&quot;http&quot;</span>][<span class="string">&quot;func_retry_delay_seconds&quot;</span>]</span><br></pre></td></tr></table></figure>





<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://hackernoon.com/4-ways-to-manage-the-configuration-in-python-4623049e841b">https://hackernoon.com/4-ways-to-manage-the-configuration-in-python-4623049e841b</a></li>
<li><a href="https://www.cnblogs.com/wang-yc/p/5620944.html">https://www.cnblogs.com/wang-yc/p/5620944.html</a></li>
<li><a href="https://docs.python.org/3/library/configparser.html">https://docs.python.org/3/library/configparser.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python中文分词器-结巴分词</title>
    <url>/2017/03/17/python%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8-%E7%BB%93%E5%B7%B4%E5%88%86%E8%AF%8D/</url>
    <content><![CDATA[<p>由于工作需要用到对中文进行分词,但碍于中文不同英文,英文只有26个字母组成,中文汉字常见的就有几千个,各种不同形式的组合,而且还有些生僻字,所以中文分词这块一起比较复杂,需要大量的语料库做训练.中科院的ICTCLAS,哈工大的ltp,东北大学的NIU Parser是学术界著名的中文分词器,但由于不开源,想要更好的二次开始很困难,jieba分词是python写成的一个比较有名的中文分词开源库,比较强大,其github地址**<a href="https://github.com/fxsjy/jieba">在这里</a>**</p>
<span id="more"></span>

<h3 id="Jieba"><a href="#Jieba" class="headerlink" title="Jieba"></a><strong>Jieba</strong></h3><p>该项目的作者其实挺有意思,对结巴这个词反其道用之,想想还确实比较符合中文分词,jieba分词支持3种模式及可以自定义词典,而且还支持繁体中文的分词,还是比较强大的</p>
<h3 id="采用算法"><a href="#采用算法" class="headerlink" title="采用算法"></a><strong>采用算法</strong></h3><blockquote>
<ul>
<li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li>
<li>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</li>
<li>TopN的关键字提取采用的则是基于 TF-IDF 算法</li>
</ul>
</blockquote>
<h3 id="3种分词模式"><a href="#3种分词模式" class="headerlink" title="3种分词模式"></a><strong>3种分词模式</strong></h3><blockquote>
<ol>
<li>精确模式: 试图将句子最精确地切开，适合文本分析</li>
<li>全模式: 把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义</li>
<li>搜索引擎模式: 在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词</li>
</ol>
</blockquote>
<p>这里使用的是第一种模式,把文本分解成日常生活中常用的词是一种比较常用的适用场景,而全模式则会把文本所有的词都分开,速度快但存在歧义.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">&quot;我来到北京清华大学&quot;</span>, cut_all=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Full Mode: &quot;</span> + <span class="string">&quot;/ &quot;</span>.join(seg_list))  <span class="comment"># 全模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">&quot;我来到北京清华大学&quot;</span>, cut_all=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Default Mode: &quot;</span> + <span class="string">&quot;/ &quot;</span>.join(seg_list))  <span class="comment"># 精确模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">&quot;他来到了网易杭研大厦&quot;</span>)  <span class="comment"># 默认是精确模式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;, &quot;</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut_for_search(<span class="string">&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;</span>)  <span class="comment"># 搜索引擎模式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;, &quot;</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#精确模式: 我/ 来到/ 北京/ 清华大学</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#新词识别：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#搜索引擎模式: 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</span></span><br></pre></td></tr></table></figure>

<h3 id="自定义词典"><a href="#自定义词典" class="headerlink" title="自定义词典"></a><strong>自定义词典</strong></h3><p>在python中import jieba使用时,会引用包里自带的字典,但是允许我们添加自定义的词典,以便包含 jieba 词库里没有的词,jieba也提倡自己添加词典,以便提高具体场景下分词正确率,自带的词典文件在jieba包的dict.txt</p>
<p>词典的格式也很简单: 每一行分三部分：词语、词频(可省略)、词性(可省略),用空格隔开,顺序不可颠倒,在词频省略时使用自动计算的能保证分出该词的词频</p>
<h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a><strong>常用函数</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#载入自定义词典文件路径</span></span><br><span class="line">jieba.load_userdict(file_name)</span><br><span class="line"><span class="comment">#提取出现频率最高的TopK词及权重</span></span><br><span class="line">tags = jieba.analyse.extract_tags(content, topK=topK, withWeight=withWeight)</span><br><span class="line"><span class="comment">#提取出现频率最高的TopK词</span></span><br><span class="line">tags = jieba.analyse.extract_tags(content, topK=topK)</span><br><span class="line"><span class="comment">#对字符串进行分词,cut_all为True时采用全模式,为False时使用精确模式,该方法直接返回list对象</span></span><br><span class="line">fenci = jieba.lcut(rline,cut_all=<span class="literal">True</span>,HMM=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#同上,返回的为一个可迭代的generator,可使用for循环输出每一个词</span></span><br><span class="line">fenci = jieba.lcut(rline,cut_all=<span class="literal">True</span>,HMM=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#搜索引擎模式</span></span><br><span class="line">seg_list = jieba.cut_for_search(rline,HMM=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#程序中动态添加或删除词典</span></span><br><span class="line">jieba.add_word(word, freq=<span class="literal">None</span>, tag=<span class="literal">None</span>)</span><br><span class="line">jieba.del_word(word)</span><br><span class="line"><span class="comment">#词性标注，返回的对象中包含分词后的词及该词所属词性</span></span><br><span class="line">word = jieba.posseg.cut(rline)</span><br></pre></td></tr></table></figure>

<h3 id="商品名称替换为TopK"><a href="#商品名称替换为TopK" class="headerlink" title="商品名称替换为TopK"></a><strong>商品名称替换为TopK</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(fileone, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;GBK&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        <span class="comment">#统计文件总行数</span></span><br><span class="line">        line_total = line_total+<span class="number">1</span> </span><br><span class="line">        <span class="comment">#到文件末尾退出</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#按0_07分隔符分隔字段(请将_换成x)</span></span><br><span class="line">        rline = re.split(<span class="string">&quot;[]&quot;</span>, line, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#商品名称只保留汉字</span></span><br><span class="line">        rline[<span class="number">0</span>] = re.sub(pattern,<span class="string">&quot;&quot;</span>,rline[<span class="number">0</span>])</span><br><span class="line">        <span class="comment">#过滤商品名称为空的行</span></span><br><span class="line">        <span class="keyword">if</span> (rline[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment">#分词</span></span><br><span class="line">            fenci = jieba.lcut(rline[<span class="number">0</span>])</span><br><span class="line">            <span class="comment">#以topK为准比较分词结果,如果存在于topK,则返回第一个分词替换原有的商品名称(topK越排前频率越高)</span></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tags)):</span><br><span class="line">                <span class="keyword">if</span> tags[x] <span class="keyword">in</span> fenci:</span><br><span class="line">                    rline[<span class="number">0</span>] = tags[x]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">#如果所有的分词都不存在topk中则商品名称不变</span></span><br><span class="line">            line_fmt = rline[<span class="number">0</span>] + <span class="string">&quot;[]&quot;</span> + rline[<span class="number">1</span>]</span><br><span class="line">            f1.write(line_fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#统计处理后商品名称为空行数</span></span><br><span class="line">            blk_total = blk_total+<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> fileone,line_total,blk_total</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/fxsjy/jieba">jieba分词Github项目地址</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python中的copy及deepcopy</title>
    <url>/2017/11/20/python%E4%B8%AD%E7%9A%84copy%E5%8F%8Adeepcopy/</url>
    <content><![CDATA[<p>今天因为项目中遇到了拷贝,调试结果突然跟自己之前对拷贝相关的认知有点出路,还真跟自己想的有点不一样,看来错了很久,赶紧补下课,记录一下.</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="comment">#使用copy需要导入</span></span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">b = a</span><br><span class="line"><span class="comment">#直接赋值语句,相当于给a添加了一个引用,其中任何一个变化,另一个都会同步变化</span></span><br><span class="line"><span class="comment">#id(a)跟id(b) 输入相同,可以使用a is b 返回 True</span></span><br><span class="line"><span class="built_in">id</span>(a),<span class="built_in">id</span>(b)</span><br><span class="line"><span class="comment">#输出 (50311880, 50311880)</span></span><br><span class="line">a <span class="keyword">is</span> b</span><br><span class="line"><span class="comment">#输出 True</span></span><br><span class="line">c = copy.copy(a)</span><br><span class="line"><span class="comment">#这里对a进行浅拷贝</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment">#输出[1,2,[3,4]]</span></span><br><span class="line"><span class="built_in">id</span>(a),<span class="built_in">id</span>(c)</span><br><span class="line"><span class="comment">#输出 (50311880, 50232904)</span></span><br><span class="line">a <span class="keyword">is</span> c</span><br><span class="line"><span class="comment">#输出 False</span></span><br><span class="line"><span class="comment">#从这里可以看出,a,c指向的不是一个变量,父对象指的就是a跟c</span></span><br><span class="line"><span class="comment">#那如果看子对象呢,子对象指的就是a里面的 [4,5]</span></span><br><span class="line"><span class="comment">#先对a </span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#2004268720</span></span><br><span class="line"><span class="comment">#2004268752</span></span><br><span class="line"><span class="comment">#50311944</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#再对c</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> c:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#2004268720</span></span><br><span class="line"><span class="comment">#2004268752</span></span><br><span class="line"><span class="comment">#50311944</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从上面可以看出,a跟c虽然指向不同,也就是说父对象不同,但是两者指向的子对象却是一样的,对子对象中的[4,5]也是一样的结果</span></span><br><span class="line"></span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"><span class="comment">#对a进行深拷贝</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment">#输出[1,2,[3,4]]</span></span><br><span class="line"><span class="built_in">id</span>(a),<span class="built_in">id</span>(d)</span><br><span class="line"><span class="comment">#输出 (50311880, 50412168)</span></span><br><span class="line">a <span class="keyword">is</span> d</span><br><span class="line"><span class="comment">#输出 False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> d:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#2004268720</span></span><br><span class="line"><span class="comment">#2004268752</span></span><br><span class="line"><span class="comment">#50412040</span></span><br><span class="line"><span class="comment">#从这里可以看出 d跟a,后面的子对象[4,5]指向不一样,而c跟a的[4,5]指向是一样的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> d[<span class="number">2</span>]:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#2004268784</span></span><br><span class="line"><span class="comment">#2004268816</span></span><br><span class="line"><span class="comment">#这跟 </span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> a[<span class="number">2</span>]:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> c[<span class="number">2</span>]:</span><br><span class="line">    <span class="built_in">id</span>(x)</span><br><span class="line"><span class="comment">#这三者的结果是一样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果这时我修改a的值,再来看看c,d的变化</span></span><br><span class="line"><span class="comment">#追加一个值</span></span><br><span class="line">a.append([<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4],[5,6]]</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4],[5,6]]</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4]]</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4]]</span></span><br><span class="line"><span class="comment">#可以看出 c跟d都没有变化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果我们修改a的[4,5]这个子对象</span></span><br><span class="line">a[<span class="number">2</span>].append(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4,0]]</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4,0]]</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4,0]]</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment">#输出 [1,2,[3,4]]</span></span><br><span class="line"><span class="comment">#可以看出 c跟a变化了,d没有变化,为何c会跟着变化？这就是copy跟deepcopy的区别</span></span><br></pre></td></tr></table></figure>

<p><strong>对于不可变对象(int,string),copy跟deepcopy没有任何区别,使用id也是相同的值,但如果是复杂对象(那些对象中可包含子对象的嵌套对象,如list,tuple)等,就有区别了.</strong></p>
<p><strong>对于包含子对象的复杂对象,copy并未从原对象中真正独立出来,也就是说,如果你改变原 object 的子 list 中的一个元素,你的 copy 就会跟着一起变.这跟我们直觉上对「复制」的理解不同.</strong></p>
<p><strong>而deepcopy则是真正的跟原对象没有关系了,对原对象的任何变化都不会影响deepcopy.</strong></p>
<p><strong>代码中如果我们需要拷贝一个对象,且这个对象不受原对象的改变而改变,直接使用deepcopy.</strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://iaman.actor/blog/2016/04/17/copy-in-python">Python中 copy, deepcopy 的区别及原因</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python中调用sqlplus</title>
    <url>/2017/04/08/python%E4%B8%AD%E8%B0%83%E7%94%A8sqlplus/</url>
    <content><![CDATA[<p>在python中查询Oracle,首先当然是cx_Oracle,方便快捷,但是很无奈有时必须使用sqlplus命令的形式,比如生成oracle9i的statspack报告,cx_Oracle就无能为力了,同时,这也涉及到python中子进程调用的问题,简单的任务当然首选subprocess</p>
<span id="more"></span>

<h3 id="subprocess"><a href="#subprocess" class="headerlink" title="subprocess"></a><strong>subprocess</strong></h3><p><strong>talk is cheap,show me your code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">oracleversion</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,username,password,databaseName</span>):</span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = password</span><br><span class="line">        self.databaseName = databaseName</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            oraclever = <span class="string">&quot;sqlplus -S &quot;</span> + self.username + <span class="string">&quot;/&quot;</span> + self.password + <span class="string">&quot;@&quot;</span> + self.databaseName</span><br><span class="line">            child = subprocess.Popen(oraclever,shell=<span class="literal">True</span>,stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)</span><br><span class="line">            child.stdin.write(<span class="string">b&quot;select * from v$version where rownum&lt;2;&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> child.communicate()</span><br><span class="line">            <span class="comment">#print(&quot;&lt;&lt;%s done.&gt;&gt;&quot; %self.city)</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            err_txt = <span class="string">&quot;ERROR: 调用ORACLE SQLPLUS失败!&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(err_txt)</span><br><span class="line">            write_errtxt(err_txt)</span><br></pre></td></tr></table></figure>

<p>上面则是一个简单的判断oracle的版本,因为oracle的版本不同,涉及到后续的操作流程不一样,首先import subprocess</p>
<p>使用Popen fork一个子进程,子进程调用sqlplus,大多数的时候我们是需要在sqlplus中执行命令,如何与子进程进行交互呢？</p>
<h4 id="subprocess-Popen"><a href="#subprocess-Popen" class="headerlink" title="subprocess.Popen"></a><strong>subprocess.Popen</strong></h4><p>child&#x3D;subprocess.Popen(oraclever,shell&#x3D;True,stdout&#x3D;subprocess.PIPE,stdin&#x3D;subprocess.PIPE,stderr&#x3D;subprocess.PIPE)</p>
<p>首先我们需要在父子进程之间形成一个管道(PIPE),通过这个管道进行交互,父进程可以给子进程输入,子进程得到输入处理后把输出或者是错误信息返回给父进程,shell&#x3D;True表示使用shell执行</p>
<h4 id="child-stdin-write"><a href="#child-stdin-write" class="headerlink" title="child.stdin.write()"></a><strong>child.stdin.write()</strong></h4><p>这便是向子进程传递输入,这里有个注意的地方:</p>
<p><strong>对于python2.x,这里直接传递sql语句没问题</strong></p>
<p><strong>但是对于python3.x,必须将字符串转换成字节串,直接在字符串前加b,得到的输出则需要decode()转换成字符串</strong></p>
<h4 id="out-err-x3D-child-communicate"><a href="#out-err-x3D-child-communicate" class="headerlink" title="(out,err)&#x3D;child.communicate()"></a><strong>(out,err)&#x3D;child.communicate()</strong></h4><p>这句话其实返回的是一个元组,元组前表示输出,后者表示错误信息</p>
<h4 id="child-wait"><a href="#child-wait" class="headerlink" title="child.wait()"></a><strong>child.wait()</strong></h4><p>该函数迫使父进程等待子进程执行完之后再往下执行,返回命令执行后的返回值</p>
<h4 id="Popen-poll"><a href="#Popen-poll" class="headerlink" title="Popen.poll()"></a><strong>Popen.poll()</strong></h4><p>用于检查子进程是否已经结束</p>
<h4 id="Popen-terminate"><a href="#Popen-terminate" class="headerlink" title="Popen.terminate()"></a><strong>Popen.terminate()</strong></h4><p>停止(stop)子进程</p>
<h4 id="Popen-kill"><a href="#Popen-kill" class="headerlink" title="Popen.kill()"></a><strong>Popen.kill()</strong></h4><p>杀死子进程</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://blog.csdn.net/imzoer/article/details/8678029">Python中subprocess学习</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python之fileinput使用</title>
    <url>/2017/07/31/python%E4%B9%8Bfileinput%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>工作中我们经常需要直接修改源文件,当然,我们都被建议最好别在源文件上直接做修改,要做好时刻备份的习惯,特别是重要的配置文件,如果需要直接修改源文件的话,我们一般都是先把源文件读入脚本然后再重定向到目标文件,目标文件重命名,源文件备份,linux下的话使用awk&#x2F;sed一条命令就能搞定,python中有一个文件处理模块fileinput,比较实用</p>
<span id="more"></span>

<h3 id="fileinput"><a href="#fileinput" class="headerlink" title="fileinput"></a><strong>fileinput</strong></h3><p>fileinput模块可以对一个或多个文件中的内容进行迭代、遍历等操作,fileinput原型函数如下:</p>
<p><strong>fileinput.input (files&#x3D;None, inplace&#x3D;False, backup&#x3D;’’, bufsize&#x3D;0, mode&#x3D;’r’, openhook&#x3D;None)</strong></p>
<blockquote>
<ul>
<li>files:                  #文件的路径列表，默认是stdin方式，多文件[‘1.txt’,’2.txt’,…]</li>
<li>inplace:                #是否将标准输出的结果写回文件，默认不取代 ,这个参数设为True则直接源文件</li>
<li>backup:                 #备份文件的扩展名，只指定扩展名，如.bak。如果该文件的备份文件已存在，则会自动覆盖</li>
<li>bufsize:                #缓冲区大小，默认为0，如果文件很大，可以修改此参数，一般默认即可</li>
<li>mode:                   #读写模式，默认为只读</li>
<li>openhook:               #该钩子用于控制打开的所有文件，比如说编码方式等;</li>
</ul>
</blockquote>
<h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a><strong>常用方法</strong></h3><blockquote>
<ul>
<li>fileinput.input()       #返回能够用于for循环遍历的对象 </li>
<li>fileinput.filename()    #返回当前文件的名称</li>
<li>fileinput.lineno()      #返回当前已经读取的行的数量（或者序号）</li>
<li>fileinput.filelineno()  #返回当前读取的行的行号</li>
<li>fileinput.isfirstline() #检查当前行是否是文件的第一行</li>
<li>fileinput.isstdin()     #判断最后一行是否从stdin中读取</li>
<li>fileinput.close()       #关闭队列</li>
</ul>
</blockquote>
<p>最常用的则为fileinput()方法,该方法按行读入文件,返回一个可迭代对象,使用for循环可遍历输出,这点类似于readline()</p>
<h3 id="修改源文件"><a href="#修改源文件" class="headerlink" title="修改源文件"></a><strong>修改源文件</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encrypt_passwd</span>(<span class="params">_file</span>):</span><br><span class="line">    <span class="comment">#设置inplace=True则是通过通过下面的print重定向回源文件实现直接修改源文件</span></span><br><span class="line">    <span class="keyword">with</span> fileinput.<span class="built_in">input</span>(_file, inplace=<span class="literal">True</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment">#如果指定backup,则源文件被备份且重命名为源文件.bak</span></span><br><span class="line">    <span class="comment">#with fileinput.input(_file, backup=&#x27;.bak&#x27;,inplace=True) as f:</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&#x27;_DBPASSWD=&#x27;</span> <span class="keyword">in</span> line):</span><br><span class="line">                _passwd = line.split(<span class="string">&#x27;=&#x27;</span>,<span class="number">1</span>)[<span class="number">1</span>].strip()</span><br><span class="line">                <span class="keyword">if</span> (<span class="string">&#x27;pbkdf2:&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> _passwd):</span><br><span class="line">                    en_passwd = encrypt(<span class="number">15</span>,_passwd)</span><br><span class="line">                    line = <span class="string">&#x27;_DBPASSWD=&#x27;</span> + en_passwd</span><br><span class="line">                    <span class="built_in">print</span>(line,end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(line,end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(line,end=<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.python.org/2/library/fileinput.html">fileinput</a></li>
<li><a href="http://blog.csdn.net/jerry_1126/article/details/41926407">Python中fileinput介绍</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python中调用sqlplus生成多实例oracle-awr-html报告</title>
    <url>/2017/08/12/python%E4%B8%AD%E8%B0%83%E7%94%A8sqlplus%E7%94%9F%E6%88%90%E5%A4%9A%E5%AE%9E%E4%BE%8Boracle-awr-html%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<p>这里不多说,说是稍微改了下官方的脚本,这里使用了subprocess调用sqlplus,关于subprocess,可以参考<a href="https://izsk.me/2017/04/08/python%E4%B8%AD%E8%B0%83%E7%94%A8sqlplus/">这里</a></p>
<span id="more"></span>

<p>其中,包含了两个业务相关的参数,这里就不过多说明…</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gen_sqlfile</span>(<span class="params">_file</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(_file,<span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(</span><br><span class="line"><span class="string">&quot;&quot;&quot;set define off</span></span><br><span class="line"><span class="string">set feedback off</span></span><br><span class="line"><span class="string">set termout off</span></span><br><span class="line"><span class="string">set pages 0</span></span><br><span class="line"><span class="string">set lines 120</span></span><br><span class="line"><span class="string">set serveroutput on </span></span><br><span class="line"><span class="string">spool oracle_awr_html.sql</span></span><br><span class="line"><span class="string">declare</span></span><br><span class="line"><span class="string">   v_str varchar2(180);</span></span><br><span class="line"><span class="string">   procedure p (l_str  varchar2)</span></span><br><span class="line"><span class="string">   is</span></span><br><span class="line"><span class="string">   begin</span></span><br><span class="line"><span class="string">     dbms_output.put_line(l_str);</span></span><br><span class="line"><span class="string">   end;</span></span><br><span class="line"><span class="string">begin</span></span><br><span class="line"><span class="string">	p(&#x27;set pages 0&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable dbid number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable inst_num number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable inst_name varchar2(20)&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable bid number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable eid number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable rpt_options number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable ENABLE_ADDM number&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable cur_date varchar2(20)&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;variable city varchar2(20)&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;declare&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;begin&#x27;);</span></span><br><span class="line"><span class="string">  	p(&#x27;select distinct first_value (snap_id) over( &#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;		order by snap_id desc rows between unbounded preceding and unbounded following) max_snap_id &#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;into :eid &#x27;);</span></span><br><span class="line"><span class="string">  	p(&#x27;	    from  dba_hist_snapshot;&#x27;);</span></span><br><span class="line"><span class="string">	p(&#x27;select dbid into :dbid from dba_hist_database_instance where rownum=1;&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;end;&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;/&#x27;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">p(&#x27;exec :rpt_options :=0;&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;exec :ENABLE_ADDM :=8;&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;exec :cur_date :=to_char(sysdate,&#x27;&#x27;yyyymmdd&#x27;&#x27;);&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;exec :city :=&#x27;&#x27;&amp;1&#x27;&#x27;;&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;exec :bid :=:eid-to_number(&#x27;&#x27;&amp;2&#x27;&#x27;);&#x27;);</span></span><br><span class="line"><span class="string">p(&#x27;column rpt_name new_value rpt_name noprint;&#x27;);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">for c1 in  (select instance_number,instance_name from gv$instance order by instance_number) </span></span><br><span class="line"><span class="string">loop   </span></span><br><span class="line"><span class="string">   p(&#x27;exec :inst_num :=&#x27;||c1.instance_number||&#x27;;&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;exec :inst_name :=&#x27;&#x27;&#x27;||c1.instance_name||&#x27;&#x27;&#x27;;&#x27;);</span></span><br><span class="line"><span class="string">   v_str:= q&#x27;[select :cur_date||&#x27;_&#x27;||:city||&#x27;_&#x27;||:inst_name||&#x27;_&#x27;||:inst_num||&#x27;_&#x27;||:bid||&#x27;_&#x27;||:eid||&#x27;.html&#x27; rpt_name from dual;]&#x27;;</span></span><br><span class="line"><span class="string">   p(v_str);</span></span><br><span class="line"><span class="string">   p(&#x27;set lines 8000&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;spool &amp;rpt_name&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27; select output from table(sys.dbms_workload_repository.awr_report_html( :dbid,&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;                                                         :inst_num,&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;                                                         :bid, :eid,&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;                                                         :rpt_options ));&#x27;);</span></span><br><span class="line"><span class="string">   p(&#x27;spool off&#x27;);</span></span><br><span class="line"><span class="string">   v_str := q&#x27;[select :cur_date||&#x27;_&#x27;||:city||&#x27;_&#x27;||:inst_name||&#x27;_&#x27;||:inst_num||&#x27;_&#x27;||:bid||&#x27;_&#x27;||:eid||&#x27;.html&#x27; file_name from dual;]&#x27;;</span></span><br><span class="line"><span class="string">   p(v_str);</span></span><br><span class="line"><span class="string">   p(&#x27;set termout off&#x27;);</span></span><br><span class="line"><span class="string">end loop;</span></span><br><span class="line"><span class="string">p(&#x27;exit&#x27;);</span></span><br><span class="line"><span class="string">end;</span></span><br><span class="line"><span class="string">/</span></span><br><span class="line"><span class="string">spool off</span></span><br><span class="line"><span class="string">set define on</span></span><br><span class="line"><span class="string">set feedback off</span></span><br><span class="line"><span class="string">set termout on</span></span><br><span class="line"><span class="string">set termout off</span></span><br><span class="line"><span class="string">@oracle_awr_html.sql &amp;1 &amp;2</span></span><br><span class="line"><span class="string">set termout on</span></span><br><span class="line"><span class="string">set pagesize 24</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">callsqluldr</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,username,password,databaseName,city,beforethis</span>):</span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = password</span><br><span class="line">        self.databaseName = databaseName</span><br><span class="line">        self.city = city</span><br><span class="line">        self.beforethis = beforethis</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            command = <span class="string">&quot;sqlplus &quot;</span> + self.username + <span class="string">&quot;/&quot;</span> + self.password + <span class="string">&quot;@&quot;</span> + self.databaseName + <span class="string">&quot; @gen_oracle_awr_html.sql &quot;</span> + self.city + <span class="string">&quot; &quot;</span> + self.beforethis</span><br><span class="line">            child = subprocess.Popen(command,shell=<span class="literal">True</span>)</span><br><span class="line">            child.wait()</span><br><span class="line">            <span class="comment">#print(&quot;&lt;&lt;%s done.&gt;&gt;&quot; %self.city)</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            err_txt = <span class="string">&quot;ERROR: CALL ORACLE SQLPLUS ERROR!&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(err_txt)</span><br><span class="line">            write_errtxt(err_txt)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_errtxt</span>(<span class="params">err_txt</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>



<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python定时框架APScheduler使用(-)</title>
    <url>/2017/09/27/python%E5%AE%9A%E6%97%B6%E6%A1%86%E6%9E%B6APScheduler(%E4%B8%80)/</url>
    <content><![CDATA[<p>日常工作中经常要使用定时任务来处理一些统计,也常用time.sleep()配合多进程&#x2F;多线程来实现,但有时候tim.sleep()就有点局限了,比如在flask中引用定时工作,apscheduler就大显身手了,而且在flask环境下有集成包–flask-apscheduler,查看源码,非常简单,对apscheduler接口进行了二次封装,屡次不爽</p>
<span id="more"></span>

<h3 id="APScheduler"><a href="#APScheduler" class="headerlink" title="APScheduler"></a><strong>APScheduler</strong></h3><p>APScheduler基于<a href="http://www.quartz-scheduler.org/">Quartz</a>的一个Python定时任务框架，实现了Quartz的所有功能，使用起来十分方便。提供了基于日期、固定时间间隔以及crontab类型的任务，并且可以持久化任务。基于这些功能，我们可以很方便的实现一个python定时任务系统</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a><strong>原理</strong></h3><p>scheduler的主循环(main_loop),其实就是反复检查是不是有到时需要执行的任务，完成一次检查的函数是_process_jobs, 这个函数做这么几件事：</p>
<blockquote>
<ol>
<li>询问自己的每一个jobstore，有没有到期需要执行的任务</li>
<li>如果有,计算这些job中每个job需要运行的时间点,如果run_times有多个,会coalesce检查(下面会解释)</li>
<li>提交给executor排期运行</li>
</ol>
</blockquote>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip3 install apscheduler</span><br></pre></td></tr></table></figure>

<h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a><strong>组成</strong></h3><blockquote>
<ol>
<li>调度器(scheduler): 一些配置相关的接口都在调试器中完成,如常用的添加&#x2F;删除作业等</li>
<li>执行器(executor): 负责把具体任务提交到进程中执行,执行完之后会通知调试器</li>
<li>触发器(tigger): 某一工作到来时引发的事件,</li>
<li>作业存储(jobstore): 默认作业是在内存中被执行的,也可把作业存储到数据库中,保存的时候被序列化,执行时被反序列化,几乎支持所有的常用数据库</li>
</ol>
</blockquote>
<h4 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a><strong>scheduler</strong></h4><p>调度器分为以下几种,可根据不同的使用场景选用不同的调度器:</p>
<blockquote>
<ol>
<li>BlockingScheduler: 很明显这是种阻塞型,一般用在没有其它进程运行的场景下</li>
<li>BackGroundScheduler: 后台式,也就是单起一个进程&#x2F;线程运行该任务,不影响主程序</li>
<li>ASyncIOScheduler:</li>
<li>GeventScheduler:</li>
<li>TornadoScheduler:</li>
<li>TwistedScheduler:</li>
<li>QtScheduler:</li>
</ol>
</blockquote>
<p>本人只使用过BlockingScheduler跟BackGroundScheduler,flask-scheduler使用的即为BackGroundScheduler,其它的后续再研究研究</p>
<p>选择类型也很简单,初始化时直接实例化:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> apscheduler.schedulers.background <span class="keyword">import</span> BackgroundScheduler</span><br><span class="line">scheduler = BackgroundScheduler()</span><br><span class="line"><span class="comment">#启动</span></span><br><span class="line">scheduler.start()</span><br></pre></td></tr></table></figure>

<h4 id="trigger"><a href="#trigger" class="headerlink" title="trigger"></a><strong>trigger</strong></h4><blockquote>
<ol>
<li>cron: 类linux下的crontab格式,属于定时调度</li>
<li>interval:每隔多久调度一次</li>
<li>date:一次性调度</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1. cron风格</span></span><br><span class="line">(<span class="built_in">int</span>|<span class="built_in">str</span>) 表示参数既可以是<span class="built_in">int</span>类型，也可以是<span class="built_in">str</span>类型</span><br><span class="line">(datetime | <span class="built_in">str</span>) 表示参数既可以是datetime类型，也可以是<span class="built_in">str</span>类型</span><br><span class="line">year (<span class="built_in">int</span>|<span class="built_in">str</span>) – <span class="number">4</span>-digit year -（表示四位数的年份，如<span class="number">2008</span>年）</span><br><span class="line">month (<span class="built_in">int</span>|<span class="built_in">str</span>) – month (<span class="number">1</span>-<span class="number">12</span>) -（表示取值范围为<span class="number">1</span>-<span class="number">12</span>月）</span><br><span class="line">day (<span class="built_in">int</span>|<span class="built_in">str</span>) – day of the (<span class="number">1</span>-<span class="number">31</span>) -（表示取值范围为<span class="number">1</span>-<span class="number">31</span>日）</span><br><span class="line">week (<span class="built_in">int</span>|<span class="built_in">str</span>) – ISO week (<span class="number">1</span>-<span class="number">53</span>) -（格里历<span class="number">2006</span>年<span class="number">12</span>月<span class="number">31</span>日可以写成<span class="number">2006</span>年-W52-<span class="number">7</span>（扩展形式）或2006W527（紧凑形式））</span><br><span class="line">day_of_week (<span class="built_in">int</span>|<span class="built_in">str</span>) – number <span class="keyword">or</span> name of weekday (<span class="number">0</span>-<span class="number">6</span> <span class="keyword">or</span> mon,tue,wed,thu,fri,sat,sun) - （表示一周中的第几天，既可以用<span class="number">0</span>-<span class="number">6</span>表示也可以用其英语缩写表示）</span><br><span class="line">hour (<span class="built_in">int</span>|<span class="built_in">str</span>) – hour (<span class="number">0</span>-<span class="number">23</span>) - （表示取值范围为<span class="number">0</span>-<span class="number">23</span>时）</span><br><span class="line">minute (<span class="built_in">int</span>|<span class="built_in">str</span>) – minute (<span class="number">0</span>-<span class="number">59</span>) - （表示取值范围为<span class="number">0</span>-<span class="number">59</span>分）</span><br><span class="line">second (<span class="built_in">int</span>|<span class="built_in">str</span>) – second (<span class="number">0</span>-<span class="number">59</span>) - （表示取值范围为<span class="number">0</span>-<span class="number">59</span>秒）</span><br><span class="line">start_date (datetime|<span class="built_in">str</span>) – earliest possible date/time to trigger on (inclusive) - （表示开始时间）</span><br><span class="line">end_date (datetime|<span class="built_in">str</span>) – latest possible date/time to trigger on (inclusive) - （表示结束时间）</span><br><span class="line">timezone (datetime.tzinfo|<span class="built_in">str</span>) – time zone to use <span class="keyword">for</span> the date/time calculations (defaults to scheduler timezone) -（表示时区取值）</span><br><span class="line"><span class="comment">#如:在6,7,8,11,12月份的第三个星期五的00:00,01:00,02:00,03:00 执行该程序</span></span><br><span class="line">sched.add_job(my_job, <span class="string">&#x27;cron&#x27;</span>, month=<span class="string">&#x27;6-8,11-12&#x27;</span>, day=<span class="string">&#x27;3rd fri&#x27;</span>, hour=<span class="string">&#x27;0-3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.interval风格</span></span><br><span class="line">weeks (<span class="built_in">int</span>) – number of weeks to wait</span><br><span class="line">days (<span class="built_in">int</span>) – number of days to wait</span><br><span class="line">hours (<span class="built_in">int</span>) – number of hours to wait</span><br><span class="line">minutes (<span class="built_in">int</span>) – number of minutes to wait</span><br><span class="line">seconds (<span class="built_in">int</span>) – number of seconds to wait</span><br><span class="line">start_date (datetime|<span class="built_in">str</span>) – starting point <span class="keyword">for</span> the interval calculation</span><br><span class="line">end_date (datetime|<span class="built_in">str</span>) – latest possible date/time to trigger on</span><br><span class="line">timezone (datetime.tzinfo|<span class="built_in">str</span>) – time zone to use <span class="keyword">for</span> the date/time calculations</span><br><span class="line"><span class="comment">#如:每隔2分钟执行一次</span></span><br><span class="line">scheduler.add_job(myfunc, <span class="string">&#x27;interval&#x27;</span>, minutes=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.date风格</span></span><br><span class="line">run_date (datetime|<span class="built_in">str</span>) – the date/time to run the job at  -（任务开始的时间）</span><br><span class="line">timezone (datetime.tzinfo|<span class="built_in">str</span>) – time zone <span class="keyword">for</span> run_date <span class="keyword">if</span> it doesn’t have one already</span><br><span class="line"><span class="comment">#如:在2009年11月6号16时30分5秒时执行</span></span><br><span class="line">sched.add_job(my_job, <span class="string">&#x27;date&#x27;</span>, run_date=datetime(<span class="number">2009</span>, <span class="number">11</span>, <span class="number">6</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">5</span>), args=[<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h4 id="executor"><a href="#executor" class="headerlink" title="executor"></a><strong>executor</strong></h4><p>说白了就是指定任务是以线程池&#x2F;进程池里运行,这在初始化时可以指定,同时可以指定最大的工作池,默认的为default: ThreadPoolExecutor,max-worker为20,当然也可以指定为processpool,默认max-worker为5</p>
<h4 id="jobstore"><a href="#jobstore" class="headerlink" title="jobstore"></a><strong>jobstore</strong></h4><p>jobstore则是指的是job持久化,默认job运行在内存中,可持久化在数据库,指定为mongo的MongoDBJobStore或者是使用sqlite的SQLAlchemyJobStore,同时可指定多种jobstore</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pytz <span class="keyword">import</span> utc</span><br><span class="line"><span class="keyword">from</span> apscheduler.schedulers.background <span class="keyword">import</span> BackgroundScheduler</span><br><span class="line"><span class="keyword">from</span> apscheduler.jobstores.mongodb <span class="keyword">import</span> MongoDBJobStore</span><br><span class="line"><span class="keyword">from</span> apscheduler.jobstores.sqlalchemy <span class="keyword">import</span> SQLAlchemyJobStore</span><br><span class="line"><span class="keyword">from</span> apscheduler.executors.pool <span class="keyword">import</span> ThreadPoolExecutor, ProcessPoolExecutor</span><br><span class="line">jobstores = &#123;</span><br><span class="line">    <span class="string">&#x27;mongo&#x27;</span>: MongoDBJobStore(),</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: SQLAlchemyJobStore(url=<span class="string">&#x27;sqlite:///jobs.sqlite&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line">executors = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: ThreadPoolExecutor(<span class="number">20</span>),</span><br><span class="line">    <span class="string">&#x27;processpool&#x27;</span>: ProcessPoolExecutor(<span class="number">5</span>)</span><br><span class="line">&#125;</span><br><span class="line">job_defaults = &#123;</span><br><span class="line">    <span class="string">&#x27;coalesce&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="string">&#x27;max_instances&#x27;</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line">scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)</span><br></pre></td></tr></table></figure>

<h3 id="重要配置"><a href="#重要配置" class="headerlink" title="重要配置"></a><strong>重要配置</strong></h3><p>有几种参数需要简单说明一下:</p>
<blockquote>
<ol>
<li>max_instances: 每个job在同一时刻能够运行的最大实例数,默认情况下为1个,可以指定为更大值,这样即使上个job还没运行完同一个job又被调度的话也能够再开一个线程执行</li>
<li>coalesce:当由于某种原因导致某个job积攒了好几次没有实际运行（比如说系统挂了5分钟后恢复，有一个任务是每分钟跑一次的，按道理说这5分钟内本来是“计划”运行5次的，但实际没有执行），如果coalesce为True，下次这个job被submit给executor时，只会执行1次，也就是最后这次，如果为False，那么会执行5次（不一定，因为还有其他条件，看下面的misfire_grace_time的解释）</li>
<li>misfire_grace_time:单位为秒,假设有这么一种情况,当某一job被调度时刚好线程池都被占满,调度器会选择将该job排队不运行,misfire_grace_time参数则是在线程池有可用线程时会比对该job的应调度时间跟当前时间的差值,如果差值&lt;misfire_grace_time时,调度器会再次调度该job.反之该job的执行状态为EVENT_JOB_MISSED了,即错过运行.</li>
</ol>
</blockquote>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a><strong>API</strong></h3><p>apscheduler的API非常的简洁,官网的文档也很齐全,</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1. 添加/删除 作业</span></span><br><span class="line"><span class="comment">#参数: id,function,trigger,time</span></span><br><span class="line">job = scheduler.add_job(myfunc, <span class="string">&#x27;interval&#x27;</span>, minutes=<span class="number">2</span>)</span><br><span class="line">job.remove()</span><br><span class="line"><span class="comment">#当然也可以给job指定id</span></span><br><span class="line">job = scheduler.add_job(<span class="built_in">id</span>=<span class="string">&#x27;job1&#x27;</span>,myfunc, <span class="string">&#x27;interval&#x27;</span>, minutes=<span class="number">2</span>)</span><br><span class="line">job.remove_job(<span class="string">&#x27;job1&#x27;</span>)</span><br><span class="line"><span class="comment">#2.暂停/启用 作业 </span></span><br><span class="line">apsched.schedulers.base.BaseScheduler.pause_job(<span class="built_in">id</span>)</span><br><span class="line">apsched.schedulers.base.BaseScheduler.resume_job(<span class="built_in">id</span>)</span><br><span class="line"><span class="comment">#3.获取作业列表</span></span><br><span class="line"><span class="built_in">print</span> sched.get_job(<span class="built_in">id</span>)</span><br><span class="line">print_jobs(<span class="built_in">id</span>)</span><br><span class="line"><span class="comment">#4.修改作业</span></span><br><span class="line">scheduler.reschedule_job(<span class="string">&#x27;my_job_id&#x27;</span>, trigger=<span class="string">&#x27;cron&#x27;</span>, minute=<span class="string">&#x27;*/5&#x27;</span>)</span><br><span class="line"><span class="comment">#5.关闭调度器,默认情况下调度器会等所有任务执行完之后才关闭,指定wait=Flase可不等待直接着关闭</span></span><br><span class="line">sched.shutdown(wait=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="事件"><a href="#事件" class="headerlink" title="事件"></a><strong>事件</strong></h3><p>一个很重要的问题就是:我怎么知道任务是执行成功了还是失败还是说压根就没执行,apscheduler提供了很人性化的接口,我们可以给scheduler注册监听事件,在每次任务执行后会记录任务执行状态,这便是scheduler event功能,官方的example很简单:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_listener</span>(<span class="params">event</span>):</span><br><span class="line">    <span class="keyword">if</span> event.exception:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;The job crashed&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;The job worked&#x27;</span>)</span><br><span class="line"><span class="comment">#注册回调函数</span></span><br><span class="line">scheduler.add_listener(my_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)</span><br></pre></td></tr></table></figure>

<p>即当执行job状态变成EVENT_JOB_EXECUTED 或者EVENT_JOB_ERROR即会调用my_listener函数,当然这里的my_listener非常简单,但我们完全可以在这里通过判断event是否发生异常来实现其它一些逻辑,如job执行情况写入数据库,这样就很容易在月报表里统计定时任务执行情况了</p>
<p>其中my_listener的event类其实继承了<a href="http://apscheduler.readthedocs.io/en/latest/modules/events.html#module-apscheduler.events">class apscheduler.events.JobExecutionEvent</a>类,主要有以下属性:</p>
<blockquote>
<ol>
<li>code, 状态码,详见<a href="http://apscheduler.readthedocs.io/en/latest/modules/events.html#module-apscheduler.events">这里</a></li>
<li>job_id, </li>
<li>jobstore,</li>
<li>scheduled_run_time,计划运行时间</li>
<li>retval&#x3D;None, 执行成功时的返回值</li>
<li>exception&#x3D;None, 是否发生异常,上述代码就是判断了这个值是否为None,正常执行的话这里为None</li>
<li>traceback&#x3D;None</li>
</ol>
</blockquote>
<p>今天就记录到这吧,关于apscheduler的web应用flask-apscheduler,下次再更.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://apscheduler.readthedocs.io/en/latest/">APScheduler</a></li>
<li><a href="http://www.cnblogs.com/luxiaojun/p/6567132.html">定时任务框架APScheduler学习详解</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python并发编程二三事(二)</title>
    <url>/2017/02/11/python%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%BA%8C%E4%B8%89%E4%BA%8B(%E4%BA%8C)/</url>
    <content><![CDATA[<p>年前接到一个小活,有一阶段是需要处理某一文件夹下的CSV文本文件,起初文件量不大,用python写了个小脚本批量串行处理,后来想想估计后期还有这样的需求,如果文件量较大的话,再使用单线程时间就会比较长了,所以周末改写了个并发脚本,所用时间节省也是之前的1&#x2F;2(限于笔记本多进程切换消耗资源,理论应该还更短),还是比较给力,下面就总结下python的并发那点事.<br>有这样一句话:”<strong>Python下多线程是鸡肋,推荐使用多进程</strong>“,但是为什么这么说呢？要了解多线程多进程之前,首先要了解python的GIL.</p>
<span id="more"></span>

<h3 id="GIL"><a href="#GIL" class="headerlink" title="GIL:"></a><strong>GIL:</strong></h3><p>GIL的全称是Global Interpreter Lock(全局解释器锁),先来看看官方的解释:</p>
<blockquote>
<p>In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)</p>
</blockquote>
<p>从这段话,可以看出来几点比较重要的信息:</p>
<blockquote>
<ol>
<li>GIL是一个防止多个线程同时执行python字节码的全局的解释器(从解释器的角度)锁(从互斥的角度)</li>
<li>CPython的内存管理是非线程安全的</li>
</ol>
</blockquote>
<h4 id="CPython非线程安全"><a href="#CPython非线程安全" class="headerlink" title="CPython非线程安全:"></a><strong>CPython非线程安全:</strong></h4><p>这里不深入讨论python虚拟机(解释器主循环)jvm的运行机制,简言之,线程安不安全主要是指多线程在访问同一段代码时会不会产生不确定的结果,如果多线程运行的结果与单线程的运行结果一致而且其它变量的值和预期的一样,则可以说是线程安全的,否之为非线程安全.<br>python本身就是用C写的,所以python解释器大多数默认为CPython解释器.为了利用多核,Python开始支持多线程的时候在考虑如何解决多线程之间数据完整性和状态同步的最简单方法–自然就是加锁.于是有了GIL这把超级大锁,而当越来越多的代码库开发者接受了这种设定后,他们开始大量依赖这种特性（即默认python内部对象是thread-safe的,无需在实现时考虑额外的内存锁和同步操作）<br>python代码的执行由python虚拟机来控制,即Python先把代码（.py文件）编译成字节码（字节码在Python虚拟机程序里对应的是PyCodeObject对象,.pyc文件是字节码在磁盘上的表现形式）,交给字节码虚拟机,然后虚拟机一条一条执行字节码指令,从而完成程序的执行。同样地,虽然python解释器中可以运行多个线程,但在任意时刻,只有一个线程在解释器中运行。而对python虚拟机的访问由全局解释器锁来控制,正是<strong>由于GIL的存在</strong>,<strong>每个CPU在同一时间只能执行一个线程</strong>(在单核CPU下的多线程其实都只是并发,不是并行,并发和并行从宏观上来讲都是同时处理多路请求的概念.但并发和并行又有区别:<strong>并发通常指有多个任务需要同时进行，并行则是同一时刻有多个任务执行。用上课来举例就是，并发情况下是一个老师在同一时间段辅助不同的人功课。并行则是好几个老师分别同时辅助多个学生功课.</strong>)<br><strong>python多线程的执行流程</strong>如下:</p>
<blockquote>
<ol>
<li>设置GIL</li>
<li>切换到一个线程执行</li>
<li>执行代码直到sleep或者是python虚拟机将其挂起</li>
<li>把线程设置为睡眠状态</li>
<li>释放GIL</li>
<li>循环以上步骤</li>
</ol>
</blockquote>
<p><strong>可见,某个线程想要执行,必须先拿到GIL,我们可以把GIL看作是“通行证”,并且在一个python进程中GIL只有一个.拿不到通行证的线程就不允许进入CPU执行.</strong><br>一般我们的代码无非就是处理两种工作:</p>
<blockquote>
<ul>
<li><strong>CPU密集型代码(长时间占用CPU:各种循环处理、计数等等):</strong><br>在这种情况下,由于计算工作多,ticks计数很快就会达到阈值,然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的）,所以<strong>python下的多线程对CPU密集型代码并不友好.</strong></li>
<li><strong>IO密集型代码(长时间请求IO:文件处理、网络爬虫等):</strong><br>多线程能够有效提升效率(单线程下有IO操作会进行IO等待,造成不必要的时间浪费,而开启多线程能在线程A等待时,自动切换到线程B,可以不浪费CPU的资源,从而能提升程序执行效率)。<strong>所以python的多线程对IO密集型代码比较友好</strong>.</li>
</ul>
</blockquote>
<h4 id="线程颠簸"><a href="#线程颠簸" class="headerlink" title="线程颠簸:"></a><strong>线程颠簸:</strong></h4><p>关于多线程在多核CPU下为何执行效率不高的原因,大家可参考这篇文章<a href="http://www.dabeaz.com/python/UnderstandingGIL.pdf">UnderstandingGIL</a>,写的非常详细.</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent7.JPG" alt="python_concurrent7"></p>
<p>从上图可以看出 多核情况下,CPU1上的thread1运行完之后释放GIL,而后引起操作系统调度(按照Python社区的想法,操作系统本身的线程调度已经非常成熟稳定了,没有必要自己搞一套,所以可以粗略看成python的调度是依靠操作系统),此时唤醒CPU2上的thread2,但GIL可能会马上又被CPU1拿到,导致CPU2上被唤醒后的线程thread2醒着等待到切换时间后因为没有GIL又进入待调度状态,这就是<strong>线程颠簸(thrashing)</strong>,每次释放GIL锁,线程进行锁竞争、切换线程,上下文切换都会消耗资源,所以说CPU密集型多核CPU下thread效率很差.</p>
<h4 id="Python3-x线程改进"><a href="#Python3-x线程改进" class="headerlink" title="Python3.x线程改进:"></a><strong>Python3.x线程改进:</strong></h4><p>当然Python社区也在非常努力的不断改进GIL,甚至是尝试去除GIL,尤其很多的项目都依赖GIL,为了保证向前兼容,去除GIL还是困难的,所以出现了很多改善方案,先来比对一下python2.x与python3.x关于GIL的改变:在Python2.x里,GIL的释放逻辑是当前线程遇见IO操作或者ticks计数达到100(ticks可以看作是Python自身的一个计数器,专门做用于GIL,每次释放后归零,这个计数可以通过 sys.setcheckinterval 来调整),进行释放.<br>而在python3.x中,GIL不使用ticks计数,改为使用计时器（执行时间达到阈值后,当前线程释放GIL）,这样对CPU密集型程序更加友好,但依然没有解决GIL导致的同一时间只能执行一个线程的问题,所以效率依然不尽如人意.<br>我们可以使用以下方案解决GIL的影响</p>
<blockquote>
<ol>
<li><p>使用多进程代替多线程</p>
</li>
<li><p>使用如JPython,Pypy等其它没有GIL的解释器</p>
</li>
<li><p>使用Python3.x(同样参考这篇文章<a href="http://www.dabeaz.com/python/UnderstandingGIL.pdf">UnderstandingGIL</a>的第4、5章)</p>
<blockquote>
<ol>
<li>GIL释放从基于ticks计数改成基于时间</li>
<li>避免最近一次CPU释放GIL后又得到</li>
<li>新增线程优先级,优先级低的线程会被迫让出GIL给优先级高的线程</li>
</ol>
</blockquote>
</li>
</ol>
</blockquote>
<p><strong>而每个进程有各自独立的GIL,互不干扰,这样就可以真正意义上的并行执行,所以在python中,多进程的执行效率优于多线程(仅仅针对多核CPU而言)</strong></p>
<h3 id="线程-x2F-进程-x2F-协程"><a href="#线程-x2F-进程-x2F-协程" class="headerlink" title="线程&#x2F;进程&#x2F;协程:"></a><strong>线程&#x2F;进程&#x2F;协程:</strong></h3><p>这部分不在这里详细讨论,以后单独总结写成–<strong>python之线程&#x2F;进程&#x2F;协程及进程间通信</strong>.</p>
<h4 id="线程"><a href="#线程" class="headerlink" title="线程:"></a><strong>线程:</strong></h4><blockquote>
<ul>
<li>thread</li>
<li>threading</li>
</ul>
</blockquote>
<h4 id="进程"><a href="#进程" class="headerlink" title="进程:"></a><strong>进程:</strong></h4><blockquote>
<ul>
<li>subprocess</li>
<li>multiprocessing</li>
</ul>
</blockquote>
<h4 id="协程"><a href="#协程" class="headerlink" title="协程:"></a><strong>协程:</strong></h4><blockquote>
<ul>
<li>yield</li>
<li>async</li>
<li>greenlet</li>
<li>gevent</li>
</ul>
</blockquote>
<h3 id="线程池-x2F-进程池"><a href="#线程池-x2F-进程池" class="headerlink" title="线程池&#x2F;进程池:"></a><strong>线程池&#x2F;进程池:</strong></h3><h4 id="threadpool"><a href="#threadpool" class="headerlink" title="threadpool:"></a><strong>threadpool:</strong></h4><p>threadpool需要下载安装,关于threadpool的详细介绍可参考<a href="https://chrisarndt.de/projects/threadpool/">官网</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threadpool</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">fileone</span>):</span><br><span class="line">    <span class="comment">#do something</span></span><br><span class="line">    <span class="comment">#这里为了简单只统计文件了行数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileone,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.readlines()</span><br><span class="line">    totalline = <span class="built_in">len</span>(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;totalline length:%d&quot;</span> %totalline)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    files = os.listdir(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    pool = threadpool.threadPool(<span class="number">4</span>)  </span><br><span class="line">    results = threadpool.makeRequests(worker, files)</span><br><span class="line">    <span class="comment">#使用列表解析</span></span><br><span class="line">    [pool.putRequest(req) <span class="keyword">for</span> req <span class="keyword">in</span> results]</span><br><span class="line">    <span class="comment">#等待线程都执行完</span></span><br><span class="line">    pool.wait()</span><br></pre></td></tr></table></figure>

<p>因为脚本主要使用了进程池,所以这里重点介绍进程模块.</p>
<h4 id="multiprocessing-Pool"><a href="#multiprocessing-Pool" class="headerlink" title="multiprocessing.Pool:"></a><strong>multiprocessing.Pool:</strong></h4><p>这里使用官方提供的代码做案例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiporcessing</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">fileone</span>):</span><br><span class="line">    <span class="comment">#do something</span></span><br><span class="line">    <span class="comment">#这里为了简单只统计文件了行数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileone,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.readlines()</span><br><span class="line">    totalline = <span class="built_in">len</span>(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> fileone,totalline</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    files = os.listdir(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    <span class="comment">#windows下防止出现python运行错误</span></span><br><span class="line">    multiprocessing.freeze_support()</span><br><span class="line">    <span class="comment">#设定进程数为机器的cup核数</span></span><br><span class="line">    cpus = multiprocessing.cpu_count()</span><br><span class="line">    <span class="comment">#初始化进程池,进程池的进程个数最好等于cpu的核数,这样避免1个CPU同时运行两个任务带来任务切换,使效率减低</span></span><br><span class="line">    pool = multiprocessing.Pool(cpus)</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(files)):    </span><br><span class="line">        result = pool.apply_async(worker, (files[x], ))</span><br><span class="line">        <span class="comment">#返回的是class list 对象,最好在关闭进程池后使用get()方法获取</span></span><br><span class="line">        <span class="comment">#result.get()如果写在这里则会阻塞进程,需要进程等待运行结果返回</span></span><br><span class="line">        results.append(result)</span><br><span class="line">    pool.close()</span><br><span class="line">    <span class="comment">#join()前必须close()</span></span><br><span class="line">    pool.join()</span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">        <span class="built_in">print</span>(result.get())</span><br></pre></td></tr></table></figure>

<p>这是个最简单逻辑的进程池,我们知道,当进程池中任务队列非空时,才会触发worker进程去工作,那么如何向进程池中的任务队列中添加任务呢?进程池类有两组关键方法来创建任务:分别是<strong>apply&#x2F;apply_async和map&#x2F;map_async</strong>进程池内的apply和map方法与python内建的两个同名方法类似,apply_async和map_async分别为它们的非阻塞版.</p>
<p><code>apply与apply_async:</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_async</span>(<span class="params">self, func, args=(<span class="params"></span>), kwds=&#123;&#125;, callback=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">assert</span> self._state == RUN</span><br><span class="line">    result = ApplyResult(self._cache, callback)</span><br><span class="line">    self._taskqueue.put(([(result._job, <span class="literal">None</span>, func, args, kwds)], <span class="literal">None</span>))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"><span class="comment">#func表示执行此任务的方法</span></span><br><span class="line"><span class="comment">#args、kwds分别表func的位置参数和关键字参数</span></span><br><span class="line"><span class="comment">#callback表示一个单参数的方法,当有结果返回时,callback方法会被调用,参数即为任务执行后的结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同时ApplyResult有如下两个方法:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, timeout=<span class="literal">None</span></span>):</span><br><span class="line">    self.wait(timeout)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._ready:</span><br><span class="line">        <span class="keyword">raise</span> TimeoutError</span><br><span class="line">    <span class="keyword">if</span> self._success:</span><br><span class="line">        <span class="keyword">return</span> self._value</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> self._value</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_set</span>(<span class="params">self, i, obj</span>):</span><br><span class="line">    self._success, self._value = obj</span><br><span class="line">    <span class="keyword">if</span> self._callback <span class="keyword">and</span> self._success:</span><br><span class="line">        self._callback(self._value)</span><br><span class="line">    self._cond.acquire()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self._ready = <span class="literal">True</span></span><br><span class="line">        self._cond.notify()</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        self._cond.release()</span><br><span class="line">    <span class="keyword">del</span> self._cache[self._job]</span><br></pre></td></tr></table></figure>

<p>通过源码可以看出这两者的区别就在当把任务加入进程池中后apply_async不需要等待进程执行,直接返回创建的ApplyResult对象,set()将运行结果保存在ApplyResult._value中,唤醒阻塞在条件变量上的get()方法。客户端通过调用get()方法,返回运行结果.这就是上面所说的apply_async()需要在join()后去get()结果.<br>而apply,但是它不返回ApplyResult,而是直接调用的是apply_async的get()方法得到worker进程运行的结果:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, func, args=(<span class="params"></span>), kwds=&#123;&#125;</span>):</span><br><span class="line">   <span class="keyword">assert</span> self._state == RUN</span><br><span class="line">   <span class="keyword">return</span> self.apply_async(func, args, kwds).get()</span><br></pre></td></tr></table></figure>

<p><code>map与map_async:</code></p>
<p>以上的apply&#x2F;apply_async方法,每次只能向进程池分配一个任务,那如果想一次分配多个任务到进程池中,可以使用map&#x2F;map_async方法,<br>机制比较麻烦,以后有机会再研究,感兴趣的可参考这篇文章python进程池专题总结</p>
<h4 id="concurrent-futures"><a href="#concurrent-futures" class="headerlink" title="concurrent.futures:"></a><strong>concurrent.futures:</strong></h4><p>concurrent模块是python3.x才有的,python2.x使用的话需要先安装,详细介绍请看<a href="http://pythonhosted.org//futures/">官方文档</a><br><strong>concurrent模块是对线程池与进程池的再一次封装,且接口封装的非常好,代码几乎可任意在线程池&#x2F;进程池中切换</strong><br>concurrent.futures比较常用的方法则有submit()及map()，详细介绍请看<a href="https://docs.python.org/3/library/concurrent.futures.html">官方文档</a><br>我们可以改写上述代码:</p>
<p><strong>使用submit()版本:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">fileone</span>):</span><br><span class="line">    <span class="comment">#do something</span></span><br><span class="line">    <span class="comment">#这里为了简单只统计文件了行数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileone,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.readlines()</span><br><span class="line">        totalline = <span class="built_in">len</span>(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> totalline</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    files = os.listdir(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    <span class="comment">#with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: 则变成线程池</span></span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor(max_workers=<span class="number">4</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        files_list = &#123;executor.submit(worker,fileone):fileone <span class="keyword">for</span> fileone <span class="keyword">in</span> files&#125;</span><br><span class="line">        <span class="comment">#as_completed返回一个iterator对象</span></span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> concourrent.futures.as_completed(files_list):</span><br><span class="line">            fileone = files_list[f]</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                totalline=f.result()</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;%s generated an exception:%s&quot;</span> %(fileone,e))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;%s has totalline:%d&quot;</span> %(fileone,totalline))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>使用map()版本:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">fileone</span>):</span><br><span class="line">    <span class="comment">#do something</span></span><br><span class="line">    <span class="comment">#这里为了简单只统计文件了行数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileone,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.readlines()</span><br><span class="line">    totalline = <span class="built_in">len</span>(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> totalline</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    files = os.listdir(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    <span class="comment">#with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: 则变成线程池</span></span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor(max_workers=<span class="number">4</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        <span class="keyword">for</span> fileone, totalline <span class="keyword">in</span> <span class="built_in">zip</span>(files,executor.<span class="built_in">map</span>(worker,files)):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;%s has totalline:%d&quot;</span> %(fileone,totalline))</span><br></pre></td></tr></table></figure>

<p>可以看出使用map()更加的简洁.但是<strong>map()是以files列表元素的顺序返回的,而使用as_completed()则是乱序返回</strong>.<br>这里挑选部分文件来比对串行处理结果与用多进程处理结果时间:<br>使用串行处理所用时间:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent3.JPG" alt="python_concurrent3"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent1.JPG" alt="python_concurrent1"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent2.JPG" alt="python_concurrent2"></p>
<p>使用进程池处理结果:</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent4.JPG" alt="python_concurrent4"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent6.JPG" alt="python_concurrent6"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/python_concurrent5.JPG" alt="python_concurrent5"></p>
<p>说明:由于是在自己的已经用了多年的笔记本上做的测试,使用进程池的时间受笔记本实际配置的限制理论上多进程使用的时间应该要更短点.<br>而且从上图也可以看出,每一个图是按照文件顺序依次执行,而图2则是同时进行.<br>在使用单进程的时候,通过观察windwos资源管理器我们可以看到,串行处理的时候只有一个python进程<br>而使用进程池则可以看到有4个python进程在活动(上述代码指定开启的进程数为笔记本的CPU核数)</p>
<h3 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信:"></a><strong>进程通信:</strong></h3><blockquote>
<ul>
<li>memory shared(Value + Array)</li>
<li>Queue</li>
<li>multiprocessing.JoinableQueue</li>
<li>Pipe</li>
<li>multiprocessing.Manager()</li>
</ul>
</blockquote>
<p>更多的关于进程间通信以后也单独写到这篇文章–<strong>python之线程&#x2F;进程&#x2F;协程及进程间通信</strong>.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.codexiu.cn/python/blog/939/">python进程池专题总结</a></li>
<li><a href="http://www.dabeaz.com/python/UnderstandingGIL.pdf">UnderstandingGIL</a></li>
<li><a href="http://cenalulu.github.io/python/gil-inpython/">Python的GIL是什么鬼,多线程性能究竟如何</a><a href="http://www.ziwenxie.site/2016/12/24/python-concurrent-futures/">Python并发编程之线程池&#x2F;进程池</a></li>
<li><a href="http://www.cnblogs.com/xiaozi/p/6182990.html">python线程池threadpool模块使用笔记</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>RDMA初探(DMA/RDMA)</title>
    <url>/2025/04/04/rdma-dma/</url>
    <content><![CDATA[<p>在多机训练场景下, 如何加速多机间的数据交换是绕不开的话题, 作者之前负责的集群是以IB网络为载体, 但由于其昂贵的成本及超高的专业性往往让人退而却步,基于以太网的RoCE则具有很高的性价比。<br>作者经过一段时间的学习及落地,也算是对RoCE(英文读音类Rocky)技术有些了解,简单记录一下。</p>
<span id="more"></span>


<p>说到RoCE, 其实是个很大的范畴, 在作者学习的过程中会出现一千个疑问, 无奈作者是个细节控, 必须说服自己。<br>作者尽力从最基础的底层网卡 -&gt; DMA -&gt; RDMA -&gt; RoCE来铺开。<br>作者也不是网络工程专业出身, 整体还是非常复杂的，作者将更加聚焦于列出困惑,<br>比如，有哪些点作者花了比较长的时间去佐证、哪些点跟以往的认识有很大出入等等问题。<br>可细可简。</p>
<h3 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h3><p>网卡通过 DMA 直接将接收到的数据包写入主机内存中的 Rx Ring Buffer，无需 CPU 介入。<br>数据包存储位置：由描述符指向的 sk_buff 结构（Linux 内核中表示套接字缓冲区的数据结构)。</p>
<p>没有DMA时的NIC到Memory</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20250402104145.png"></p>
<p>有DMA时的NIC到Memory</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20250402104018.png"></p>
<p>问题:<br>为什么网卡可以直接通过DMA写入到主存中?</p>
<p>Linux内核有一个通用的DMA引擎接口，DMA engine驱动实现这个接口即可。<br>如mellanox 网卡, 包括大多数的 nvidia datacenter GPU硬件层内部集成了DMA的硬件,符合linux内核的DMA引擎接口,<br>因此可以操作主存.</p>
<p>回到话题, DMA是单机上的操作，如果想在多机也实现offload CPU, 那就是RDMA(remote DMA)</p>
<h3 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h3><p>首先要明白，<code>RDMA是一种用于多机间的高性能网络通信技术</code>, 在多机之间的数据传输中通常有TCP&#x2F;IP, 有了TCP&#x2F;IP后，为什么还需要RDMA?</p>
<p>最主要的原因还是2个: </p>
<blockquote>
<ul>
<li>一、TCP&#x2F;IP 协议栈处理带来数十微秒的时延</li>
</ul>
</blockquote>
<p>TCP 协议栈在接收&#x2F;发送报文时，内核需要做多次上下文切换，每次切换需要耗费 5~10us 左右的时延，另外还需要至少三次的数据拷贝和依赖 CPU 进行协议封装，这导致仅仅协议栈处理就带来数十微秒的固定时延，使得在 AI 数据运算和 SSD 分布式存储等微秒级系统中，协议栈时延成为最明显的瓶颈。</p>
<blockquote>
<ul>
<li>二、TCP 协议栈处理导致服务器 CPU 负载居高不下<br>除了固定时延较长问题，TCP&#x2F;IP 网络需要主机 CPU 多次参与协议栈内存拷贝。<br>网络规模越大，网络带宽越高，CPU 在收发数据时的调度负担越大，导致 CPU 持续高负载。按照业界测算数据：每传输 1bit 数据需要耗费 1Hz 的 CPU，那么当网络带宽达到 25G 以上（满载），对于绝大多数服务器来说，至少一半的 CPU<br>能力将不得不用来传输数据。</li>
</ul>
</blockquote>
<p>为了降低网络时延和 CPU 占用率，服务器端产生了RDMA功能。RDMA 是一种直接内存访问技术，他将数据直接从一台计算机的内存传输到另一台计算机，数据从一个系统快速移动到远程系统存储器中，无需双方操作系统的介入，不需要经过CPU耗时的处理，最终达到高带宽、低时延和低资源占用率的效果。</p>
<p>从这张图可以很清楚地看出使用TCP&#x2F;IP与使用RDMA两者的区别。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20250427141458.png"></p>
<p>我们需要记住两个关键词: <strong>零拷贝(Zero-Copy)、内核旁路(Kernel Bypass)</strong></p>
<p>RDMA是一种网络通信技术, 目前常见的有以下3种具体的实现:</p>
<blockquote>
<ul>
<li>iWarp: 由于其失去了最重要的 RDMA 的性能优势，已经逐渐被业界所抛弃</li>
<li>InfiniBand: 的性能最好，价格昂贵且运维难度大</li>
<li>RoCE: 可基于以太网模式，经济实用</li>
</ul>
</blockquote>
<p>这里主要介绍RoCE, RoCE也分为v1, v2两种版本,<br>v1工作在L2层, v2可以工作在L3, 网络使用范围更广, 这里以v2为主。</p>
<p>由此引出主角: RoCE v2.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/23394710069">https://zhuanlan.zhihu.com/p/23394710069</a></li>
<li><a href="https://www.cnblogs.com/h2zZhou/p/10714983.html">https://www.cnblogs.com/h2zZhou/p/10714983.html</a></li>
<li><a href="https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html">https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>RDMA初探(以太网网卡数据)</title>
    <url>/2025/04/01/rdma-nic/</url>
    <content><![CDATA[<p>在多机训练场景下, 如何加速多机间的数据交换是绕不开的话题, 作者之前负责的集群是以IB网络为载体, 但由于其昂贵的成本及超高的专业性往往让人退而却步,基于以太网的RoCE则具有很高的性价比。<br>作者经过一段时间的学习及落地,也算是对RoCE(英文读音类Rocky)技术有些了解,简单记录一下。</p>
<span id="more"></span>


<p>说到RoCE, 其实是个很大的范畴, 在作者学习的过程中会出现一千个疑问, 无奈作者是个细节控, 必须说服自己。<br>作者尽力从最基础的底层网卡 -&gt; DMA -&gt; RDMA -&gt; RoCE来铺开。<br>作者也不是网络工程专业出身, 整体还是非常复杂的，作者将更加聚焦于列出困惑,<br>比如，有哪些点作者花了比较长的时间去佐证、哪些点跟以往的认识有很大出入等等问题。<br>可细可简。</p>
<p>网络设备&#x2F;网卡&#x2F;NIC 这里都代表同一种配件设备，就是我们常见的网卡。以下可能会混合使用，不再单独区分</p>
<p>这里还是有必要对网卡的发送&#x2F;接收数据的过程进行展开。站在应用程序的角度看一个数据是如何通过网卡发送&#x2F;接收到应用中的。</p>
<p>这篇文章<a href="https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html">网卡适配器收发数据帧流程</a>非常具体的阐述了网卡是如何进行发送&#x2F;接收数据的流程, 插图也是非常形象，值得好好沉下心好好地看一看。</p>
<p>之所以说需要沉下心来，是因为确实如果不是对网络专业知识有很好掌握的话，其实是看不下去的，里面涉及到太多的专业流程:<br>协议栈、网络模块、软硬中断、NAPI等等。作者看了几遍也还是消化不了, 有些都是理论性的知识，其实在实践上并没有那么重要，因此只关注我们感兴趣的点即可。</p>
<p><code>网络设备工作在物理层和数据链路层</code>。有一些比较重要的参数可以拿出来说一说。</p>
<h3 id="Tx-x2F-Rx"><a href="#Tx-x2F-Rx" class="headerlink" title="Tx&#x2F;Rx"></a>Tx&#x2F;Rx</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 执行ifconfig 网卡名, 其中有以下几个字段:</span></span><br><span class="line">Tx errors 0 drops 0 overruns 0 frame 0</span><br><span class="line">Rx errors 0 drops 0 overruns 0 frame 0</span><br><span class="line"><span class="comment"># 这些参数分别代表什么, 从这些参数中可以得出什么结论?</span></span><br></pre></td></tr></table></figure>

<p>Tx&#x2F;Rx FIFO: Tx 表示发送（Transport）, Rx 是接收（Receive）</p>
<ul>
<li>errors: CRC错误、硬件故障、超长帧</li>
<li>drops: 内存不足、应用层延迟、过滤规则</li>
<li>overruns: 中断延迟、缓冲区过小、流量突发</li>
<li>frame: 帧格式错误、MTU不匹配、信号干扰</li>
</ul>
<p>从这个局部图可以看出, drops与overruns的发生的时机有所不同。</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20250403112845.png"></p>
<p>总结一下:<br>overruns vs dropped<br>overruns: <code>数据包未进入接收队列（硬件层）</code>, 但网卡硬件缓存（FIFO）满，通常因 CPU 处理中断不及时或队列配置过小。<br>dropped:  <code>数据包已进入接收队列（内核层）</code>, 但内核内存不足、协议栈处理能力不足，或应用程序未及时从Socket Buffer 读取数据。</p>
<h3 id="RingBuffer"><a href="#RingBuffer" class="headerlink" title="RingBuffer"></a>RingBuffer</h3><p>网卡是一种硬件, 本身也有一些<strong>片上缓存(NIC On-Chip Memory)</strong>, 这些缓存也就几M的级别, 位于网卡芯片内部，作为指针指向主机内存中的实际数据区域。记录<code>数据包在主机内存中的地址</code>、长度和状态(如ready或used)</p>
<p>另外一个比较重要的是<code>RingBuffer</code>, 可以通过以下命令查看网卡的RingBuffer设置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ethtool -g eth0: 查询ethx网口基本设置</span></span><br><span class="line"><span class="comment"># -g: 显示网卡的接收/发送环形参数(ringbuffer)。</span></span><br><span class="line"></span><br><span class="line">Ring parameters <span class="keyword">for</span> eth0:</span><br><span class="line">Pre-<span class="built_in">set</span> maximums:</span><br><span class="line">RX: 4096</span><br><span class="line">RX Mini: 0</span><br><span class="line">RX Jumbo: 0</span><br><span class="line">TX: 4096</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX: 512</span><br><span class="line">RX Mini: 0</span><br><span class="line">RX Jumbo: 0</span><br><span class="line">TX: 512</span><br></pre></td></tr></table></figure>

<p><code>Pre-set maximums</code>指的是RingBuffer的最大值。这个是由硬件的上限决定的，不能更改。<br>Current hardware settings指的是当前的设置。</p>
<p>要特别注意的是，<code>这里的512/4096指的是BD(Buffer descriptor)的数量, 单位不是字节数, BD指向的是实际存放数据的skb的地址指针</code></p>
<p>简单来说，如果内核处理得不及时导致RingBuffer满了(以当前设置的值为准,上述是RX&#x3D;512, TX&#x3D;512),那后面新来的数据包就会被丢弃,生成<code>overruns</code>,<br>因此当看到网卡在某个方面产生了<code>overruns</code>时，可以通过将RingBuffer适当调大来解决。</p>
<p>但要注意，TX&#x2F;RX也不能一直调大。<code>因为RingBuffer指向的是内存地址，位于主机内存，系统启动初始化网卡驱动时时，内核为网卡分配 Rx Ring Buffer 的主机内存(可以理解为: 网卡驱动和操作系统合作，预留（reserve）出一段内存来给网卡使用)，并与网卡硬件描述符建立映射关系, 申请的是一段连续的物理内存块，确保网卡DMA控制器可直接访问(接下来收到的包就会放到这里，进而被 操作系统取走)</code></p>
<p>当观察到RingBuffer过大时, 可能会引起排队的包过多，也会增加网络包的延时，因此如何加快内核处理网络包的速度，而不是让网络包在RingBuffer中排队是更好的选择。</p>
<p>所以通过这篇文章<a href="https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html">网卡适配器收发数据帧流程</a>中的一张图来总结一下:</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20250403131230.png"></p>
<ol>
<li>网卡中可以配置ringbuffer的数量, ringbuffer数量的多少关系到是否会产生overruns还是drops</li>
<li>网卡驱动申请Rx&#x2F;Tx descriptor并为其分配socket buffer, 本质上是在主内存中分配的一片缓冲区，用来接收数据帧。将数据缓存区的总线地址保存到 descriptor</li>
</ol>
<h3 id="sk-buff（Socket-Buffer）"><a href="#sk-buff（Socket-Buffer）" class="headerlink" title="sk_buff（Socket Buffer）"></a>sk_buff（Socket Buffer）</h3><p>当然还有一个概念会经常听到: sk_buffer(skb)<br>Linux 内核中，用sk_buff来描述一个缓存，所谓分配缓存空间，就是建立一定数量的 sk_buff.<br>sk_buff 是 <code>Linux 内核网络协议栈</code>实现中最重要的结构体，它是网络数据报文在内核中的表现形式以及在内核态和用户态间传递。</p>
<p>三者的协作流程如下:</p>
<blockquote>
<ul>
<li>接收数据:</li>
</ul>
</blockquote>
<ol>
<li>网卡根据Buffer Descriptor找到空闲内存块(可能是Rx Ring Buffer中的一个槽), 通过DMA写入数据。</li>
<li>驱动从Rx Ring Buffer中读取数据，封装为sk_buff，递交给协议栈。</li>
<li>协议栈处理完成后，sk_buff被释放或转发。</li>
</ol>
<p>网卡使用Buffer Descriptor将数据DMA到内存→驱动从Rx Ring取出数据→封装为sk_buff→协议栈处理→用户态读取</p>
<blockquote>
<ul>
<li>发送数据:</li>
</ul>
</blockquote>
<ol>
<li>应用程序数据通过系统调用进入内核，构建sk_buff。</li>
<li>协议栈处理后的sk_buff被放入Tx Ring Buffer。</li>
<li>网卡通过Buffer Descriptor找到这段Tx Ring Buffer并读取其中的数据, 然后通过DMA发送。</li>
</ol>
<p>用户态数据构建sk_buff→协议栈处理→放入Tx Ring→网卡通过Buffer Descriptor经DMA读取后发送</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.cnblogs.com/h2zZhou/p/10714983.html">https://www.cnblogs.com/h2zZhou/p/10714983.html</a></li>
<li><a href="https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html">https://www.cnblogs.com/jmilkfan-fanguiju/p/12789806.html</a></li>
<li><a href="https://devops-insider.mygraphql.com/zh-cn/latest/kernel/network/kernel-net-stack/receive-path/linux-net-stack-implementation-rx-zh.html#">https://devops-insider.mygraphql.com/zh-cn/latest/kernel/network/kernel-net-stack/receive-path/linux-net-stack-implementation-rx-zh.html#</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>python迭代器模块itertools常用方法</title>
    <url>/2017/12/07/python%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%9D%97itertools%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>itertools是python中内置的一种高效的生成各种迭代器或者是类的模块,<strong>这些函数的返回值为一个迭代器</strong>,经常被用在for循环中,当然,也可直接使用next()方法取值,今天就来说说itertools中的常用方法.</p>
<span id="more"></span>

<p>itertools按照迭代器的功能可分为三类:</p>
<blockquote>
<ul>
<li>无限迭代器: 生成一个无限序列，比如自然数序列 1, 2, 3, 4, …</li>
<li>有限迭代器: 接收一个或多个序列（sequence）作为参数，进行组合、分组和过滤等；</li>
<li>组合迭代器: 序列的排列、组合，求序列的笛卡儿积等</li>
</ul>
</blockquote>
<h3 id="无限迭代器"><a href="#无限迭代器" class="headerlink" title="无限迭代器"></a><strong>无限迭代器</strong></h3><h4 id="itertools-count-start-x3D-0-step-x3D-1"><a href="#itertools-count-start-x3D-0-step-x3D-1" class="headerlink" title="itertools.count(start&#x3D;0, step&#x3D;1)"></a><strong>itertools.count(start&#x3D;0, step&#x3D;1)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.count(start=<span class="number">0</span>, step=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#创建一个迭代器，生成从n开始的连续整数，如果忽略n，则从0开始计算（注意：此迭代器不支持长整数）,如果超出了sys.maxint，计数器将溢出并继续从-sys.maxint-1开始计算</span></span><br><span class="line"><span class="comment">#start: 起始值,默认为0，</span></span><br><span class="line"><span class="comment">#step: 步长,默认为1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">a = itertools.count()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> a:</span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"></span><br><span class="line">b = itertools.count(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> b:</span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-cycle-iterable"><a href="#itertools-cycle-iterable" class="headerlink" title="itertools.cycle(iterable)"></a><strong>itertools.cycle(iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.cycle(iterable)</span><br><span class="line"><span class="comment">#创建一个迭代器，对iterable中的元素反复执行循环操作，内部会生成iterable中的元素的一个副本，此副本用于返回循环中的重复项</span></span><br><span class="line"><span class="comment">#iterable: 可迭代对象,可以为一个列表、字符串、元组等</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">a = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> itertools.cycle(a):</span><br><span class="line">    i = i +<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(i,x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">1</span>,<span class="string">&#x27;a&#x27;</span></span><br><span class="line"><span class="number">2</span>,<span class="string">&#x27;b&#x27;</span></span><br><span class="line"><span class="number">3</span>,<span class="string">&#x27;c&#x27;</span></span><br><span class="line"><span class="number">4</span>,<span class="string">&#x27;a&#x27;</span></span><br><span class="line"><span class="number">5</span>,<span class="string">&#x27;b&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-repeat-object-times"><a href="#itertools-repeat-object-times" class="headerlink" title="itertools.repeat(object[, times])"></a><strong>itertools.repeat(object[, times])</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.repeat(<span class="built_in">object</span>[, times])</span><br><span class="line"><span class="comment">#创建一个迭代器，重复生成object，times（如果已提供）指定重复计数，如果未提供times，将无止尽返回该对象</span></span><br><span class="line"><span class="comment">#object: 需要重复的对象，对象是个整体</span></span><br><span class="line"><span class="comment">#times: 重复次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> itertools.repeat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h3 id="有限迭代器"><a href="#有限迭代器" class="headerlink" title="有限迭代器"></a><strong>有限迭代器</strong></h3><h4 id="itertools-chain-iterable1-iterable2-…"><a href="#itertools-chain-iterable1-iterable2-…" class="headerlink" title="itertools.chain(iterable1, iterable2, …)"></a><strong>itertools.chain(iterable1, iterable2, …)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.chain(iterable1, iterable2, ...)</span><br><span class="line"><span class="comment">#将多个迭代器作为参数, 但只返回单个迭代器, 它产生所有参数迭代器的内容, 就好像他们是来自于一个单一的序列</span></span><br><span class="line"><span class="comment">#参数为多个可迭代对象,就好像被链条衔接起来了一样</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> itertools.chain([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],<span class="string">&#x27;abc&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="string">&#x27;a&#x27;</span></span><br><span class="line"><span class="string">&#x27;b&#x27;</span></span><br><span class="line"><span class="string">&#x27;c&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> itertools.chain([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="string">&#x27;a&#x27;</span></span><br><span class="line"><span class="string">&#x27;b&#x27;</span></span><br><span class="line"><span class="string">&#x27;c&#x27;</span>                                  </span><br></pre></td></tr></table></figure>

<h4 id="itertools-chain-from-iterable-iterable"><a href="#itertools-chain-from-iterable-iterable" class="headerlink" title="itertools.chain.from_iterable(iterable)"></a><strong>itertools.chain.from_iterable(iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.chain.from_iterable(iterable)</span><br><span class="line"><span class="comment">#接收一个可迭代对象作为参数，返回一个迭代器</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line">a = [[<span class="string">&#x27;first&#x27;</span>,<span class="string">&#x27;second&#x27;</span>,<span class="string">&#x27;thrid&#x27;</span>],[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]]</span><br><span class="line">b = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">    <span class="built_in">list</span>(chain.from_iterable(<span class="built_in">zip</span>(a[x],b[x])))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="string">&#x27;first&#x27;</span>, <span class="number">1</span>, <span class="string">&#x27;second&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;thrid&#x27;</span>, <span class="number">3</span>]</span><br><span class="line">[<span class="string">&#x27;a&#x27;</span>, <span class="number">4</span>, <span class="string">&#x27;b&#x27;</span>, <span class="number">5</span>, <span class="string">&#x27;c&#x27;</span>, <span class="number">6</span>] </span><br></pre></td></tr></table></figure>

<h4 id="itertools-compress-data-selectors"><a href="#itertools-compress-data-selectors" class="headerlink" title="itertools.compress(data, selectors)"></a><strong>itertools.compress(data, selectors)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.compress(data, selectors)</span><br><span class="line"><span class="comment">#可用于对数据进行筛选，当 selectors 的某个元素为 true 时，则保留 data 对应位置的元素，否则去除</span></span><br><span class="line"><span class="comment">#data: 待筛选数据</span></span><br><span class="line"><span class="comment">#selectors: 当为真时,保留data对应位的数据,为假或为空时则去除</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> compress</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> compress([<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]):</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="string">&#x27;a&#x27;</span></span><br><span class="line"><span class="string">&#x27;c&#x27;</span></span><br><span class="line"><span class="comment"># 2 也为真,&#x27;d&#x27;对应值为空算假</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-dropwhile-predicate-iterable"><a href="#itertools-dropwhile-predicate-iterable" class="headerlink" title="itertools.dropwhile(predicate, iterable)"></a><strong>itertools.dropwhile(predicate, iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.dropwhile(predicate, iterable)</span><br><span class="line"><span class="comment">#创建一个迭代器，只要函数predicate(item)为True，就丢弃iterable中的项，如果predicate返回False，就会生成iterable中的项和所有后续项,即第一个不满足条件的项及它后面所有的项都返回</span></span><br><span class="line"><span class="comment">#predicate: 函数</span></span><br><span class="line"><span class="comment">#iterable: 可迭代对象</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> dropwhile</span><br><span class="line"><span class="built_in">list</span>(dropwhile(<span class="keyword">lambda</span> x: x &lt; <span class="number">5</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line">[<span class="number">6</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment">#从6开始不符合x &lt; 5 条件,所以6及6后面所有的项都需要返回</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-takewhile-predicate-iterable"><a href="#itertools-takewhile-predicate-iterable" class="headerlink" title="itertools.takewhile(predicate, iterable)"></a><strong>itertools.takewhile(predicate, iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.takewhile(predicate, iterable)</span><br><span class="line"><span class="comment">#创建一个迭代器，如果predicate返回False,立即停止迭代</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> takewhile</span><br><span class="line"><span class="built_in">list</span>(takewhile(<span class="keyword">lambda</span> x: x &lt; <span class="number">5</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-ifilter-predicate-iterable"><a href="#itertools-ifilter-predicate-iterable" class="headerlink" title="itertools.ifilter(predicate, iterable)"></a><strong>itertools.ifilter(predicate, iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.ifilter(predicate, iterable)</span><br><span class="line"><span class="comment">#创建一个迭代器，仅生成iterable中predicate(item)为True的项，如果predicate为None，将返回iterable中所有计算为True的项</span></span><br><span class="line"><span class="comment">#predicate: 函数</span></span><br><span class="line"><span class="comment">#iterable: 可迭代对象</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> ifilter</span><br><span class="line"><span class="built_in">list</span>(ifilter(<span class="keyword">lambda</span> x: x &lt; <span class="number">5</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line">[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-ifilterfalse-predicate-iterable"><a href="#itertools-ifilterfalse-predicate-iterable" class="headerlink" title="itertools.ifilterfalse(predicate, iterable)"></a><strong>itertools.ifilterfalse(predicate, iterable)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.ifilterfalse(predicate, iterable)</span><br><span class="line"><span class="comment">#创建一个迭代器，仅生成iterable中predicate(item)为False的项，如果predicate为None，将返回iterable中所有计算False的项,该函数正好跟ifilter相反</span></span><br><span class="line"><span class="comment">#predicate: 函数</span></span><br><span class="line"><span class="comment">#iterable: 可迭代对象</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> ifilterfalse</span><br><span class="line"><span class="built_in">list</span>(ifilterfalse(<span class="keyword">lambda</span> x: x &lt; <span class="number">5</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line">[<span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-groupby-iterable-key"><a href="#itertools-groupby-iterable-key" class="headerlink" title="itertools.groupby(iterable[, key])"></a><strong>itertools.groupby(iterable[, key])</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.groupby(iterable[, key])</span><br><span class="line"><span class="comment">#返回一个产生按照key进行分组后的值集合的迭代器</span></span><br><span class="line"><span class="comment">#iterable:可迭代对象</span></span><br><span class="line"><span class="comment">#key: 一个函数,该函数的返回值做为分组的标准</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> groupby</span><br><span class="line">a = [<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>, <span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;bcd&#x27;</span>, <span class="string">&#x27;abcde&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, k <span class="keyword">in</span> groupby(a, <span class="built_in">len</span>):</span><br><span class="line">     <span class="built_in">print</span> (i, <span class="built_in">list</span>(k))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">2</span>,[<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>]</span><br><span class="line"><span class="number">3</span>,[<span class="string">&#x27;abc&#x27;</span>, <span class="string">&#x27;bcd&#x27;</span>]</span><br><span class="line"><span class="number">5</span>,[<span class="string">&#x27;abcde&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-islice-iterable-stop"><a href="#itertools-islice-iterable-stop" class="headerlink" title="itertools.islice(iterable, stop)"></a><strong>itertools.islice(iterable, stop)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.islice(iterable,[start], stop,[step])</span><br><span class="line"><span class="comment">#iterable 是可迭代对象，start 是开始索引,默认为0，stop 是结束索引，step 是步长，默认为1，start 和 step 可选</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> islice,count</span><br><span class="line"><span class="built_in">list</span>(islice([<span class="number">10</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>], <span class="number">5</span>))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[[<span class="number">10</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(islice(count(), <span class="number">3</span>, <span class="number">10</span> ,<span class="number">2</span>))</span><br><span class="line"> <span class="comment">#输出</span></span><br><span class="line"> [<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"> <span class="comment">#这里的count()为文章第一个函数,用来产生无限序列</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-imap-func-iter1-iter2-iter3-…"><a href="#itertools-imap-func-iter1-iter2-iter3-…" class="headerlink" title="itertools.imap(func, iter1, iter2, iter3, …)"></a><strong><del>itertools.imap(func, iter1, iter2, iter3, …)</del></strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imap(func, iter1, iter2, iter3, ...)</span><br><span class="line"><span class="comment">#返回一个迭代器, 它是调用了一个其值在输入迭代器上的函数, 返回结果. 它类似于内置函数 map() , 只是前者在任意输入迭代器结束后就停止(而不是插入None值来补全所有的输入)</span></span><br><span class="line"><span class="comment">#注意: 该函数在python3.x中已不存在,可直接使用map</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> imap</span><br><span class="line"><span class="built_in">list</span>(imap(<span class="built_in">pow</span>, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="number">16</span>, <span class="number">9</span>, <span class="number">1000</span>]</span><br><span class="line"><span class="comment">#pow函数 求指数</span></span><br></pre></td></tr></table></figure>

<h4 id="itertools-izip-iterables"><a href="#itertools-izip-iterables" class="headerlink" title="**itertools.izip(*iterables)**"></a>**<del>itertools.izip(*iterables)</del>**</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.izip(*iterables)</span><br><span class="line"><span class="comment">#用于将多个可迭代对象对应位置的元素作为一个元组，将所有元组『组成』一个迭代器，并返回</span></span><br><span class="line"><span class="comment">#注意: 该函数在python3.x中已不存在,可直接使用zip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> izip</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> izip([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="itertools-izip-longest-iterables-fillvalue-x3D-None"><a href="#itertools-izip-longest-iterables-fillvalue-x3D-None" class="headerlink" title="**itertools.izip_longest(*iterables, [fillvalue&#x3D;None])**"></a>**<del>itertools.izip_longest(*iterables, [fillvalue&#x3D;None])</del>**</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.izip_longest(*iterables,[fillvalue=<span class="literal">None</span>])</span><br><span class="line"><span class="comment">#izip_longest 跟 izip 类似，但迭代过程会持续到所有可迭代对象的元素都被迭代完</span></span><br><span class="line"><span class="comment">#注意: 该函数在python3.x中已不存在</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> izip_longest</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> izip_longest([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>],fillvalue=<span class="string">&#x27;-&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">(<span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">(<span class="string">&#x27;-&#x27;</span>,<span class="string">&#x27;e&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="组合迭代器"><a href="#组合迭代器" class="headerlink" title="组合迭代器"></a><strong>组合迭代器</strong></h3><h4 id="itertools-product-iterables-repeat"><a href="#itertools-product-iterables-repeat" class="headerlink" title="*itertools.product(iterables[, repeat])"></a>*<em>itertools.product(<em>iterables[, repeat])</em></em></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.product(*iterables[, repeat])</span><br><span class="line"><span class="comment">#创建一个迭代器，生成表示item1，item2等中的项目的笛卡尔积的元组，repeat是一个关键字参数，指定重复生成序列的次数。</span></span><br><span class="line"><span class="comment"># 用来产生笛卡尔积</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">a = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">c = itertools.product(a,b)</span><br><span class="line"><span class="keyword">for</span> elem <span class="keyword">in</span> c:</span><br><span class="line">    <span class="built_in">print</span>(elem)</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出    </span></span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(product((<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(product(<span class="string">&#x27;ABC&#x27;</span>, repeat=<span class="number">2</span>))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;C&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-permutations-iterable-r"><a href="#itertools-permutations-iterable-r" class="headerlink" title="itertools.permutations(iterable[, r])"></a><strong>itertools.permutations(iterable[, r])</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.permutations(iterable[, r])</span><br><span class="line"><span class="comment">#创建一个迭代器，返回iterable中所有长度为r的项目序列，如果省略了r，那么序列的长度与iterable中的项目数量相同： 返回p中任意取r个元素做排列的元组的迭代器</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> permutations</span><br><span class="line"><span class="built_in">list</span>(permutations(<span class="string">&#x27;ABC&#x27;</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;B&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-combinations-iterable-r"><a href="#itertools-combinations-iterable-r" class="headerlink" title="itertools.combinations(iterable, r)"></a><strong>itertools.combinations(iterable, r)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.combinations(iterable, r)</span><br><span class="line"><span class="comment">#创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序 (不带重复)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="built_in">list</span>(combinations(<span class="string">&#x27;ABC&#x27;</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<h4 id="itertools-combinations-with-replacement-iterable-r"><a href="#itertools-combinations-with-replacement-iterable-r" class="headerlink" title="itertools.combinations_with_replacement(iterable, r)"></a><strong>itertools.combinations_with_replacement(iterable, r)</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">itertools.combinations_with_replacement(iterable, r)</span><br><span class="line"><span class="comment">#创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序 (带重复)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations_with_replacement</span><br><span class="line"><span class="built_in">list</span>(combinations_with_replacement(<span class="string">&#x27;ABC&#x27;</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;C&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.python.org/2/library/itertools.html">itertools-doc</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python闭包及装饰器</title>
    <url>/2017/10/10/python%E9%97%AD%E5%8C%85%E5%8F%8A%E8%A3%85%E9%A5%B0%E5%99%A8/</url>
    <content><![CDATA[<p>代码调试结果再一次被自己已有的认知打败,恶补基础,今天扯一扯python的闭包及装饰器.</p>
<span id="more"></span>

<h3 id="有意思的代码"><a href="#有意思的代码" class="headerlink" title="有意思的代码"></a><strong>有意思的代码</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():</span><br><span class="line">    <span class="keyword">return</span> [<span class="keyword">lambda</span> x:i*x <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>([m(<span class="number">2</span>) <span class="keyword">for</span> m <span class="keyword">in</span> f()])</span><br></pre></td></tr></table></figure>

<p>大家很快能得到答案: <code>[0,2,4,6]</code>,但是这真的对吗？</p>
<p>正确答案为<code>[6,6,6,6]</code>,<strong>我也一脸蒙蔽: But How?</strong></p>
<p>首先可以肯定的是函数f返回一个列表解析,列表里的对象是4个lambda对象</p>
<p>当使用f()调用f函数时,f返回了一个列表,列表里也确实是4个lambda对象,<strong>这个问题的关键在于i的值,对于lambda来说,变量i的值 永远都是for循环里最后一次i的值,也就是3,4个lambda都是如此</strong></p>
<p>再看这段代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = <span class="built_in">range</span>(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">funcs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    l = <span class="keyword">lambda</span> x: i * x</span><br><span class="line">    funcs.append(l)</span><br><span class="line"></span><br><span class="line">i = <span class="number">21</span></span><br><span class="line"><span class="keyword">for</span> func <span class="keyword">in</span> funcs:</span><br><span class="line">    <span class="built_in">print</span>(func(<span class="number">2</span>))</span><br><span class="line"><span class="comment">#输出:42,42,42,42</span></span><br></pre></td></tr></table></figure>

<p><code>Python的闭包(lambda和function等)会保存变量的名字(i)和scoping(global)。当你调用的时候才会去找具体的对象。所以你给i赋不同的值，在那之后的调用都会去取新的i = 21,for i in data中的i值显然不是最新值.</code></p>
<h3 id="python闭包"><a href="#python闭包" class="headerlink" title="python闭包"></a><strong>python闭包</strong></h3><p><strong>python中的闭包从表现形式上定义为:如果在一个内部函数里,对在外部作用域（但不是在全局作用域）的变量进行引用，那么内部函数就被认为是闭包(closure)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner_func</span>(<span class="params">inner_args</span>):</span><br><span class="line">        <span class="keyword">return</span> (args + inner_args) </span><br><span class="line">    <span class="keyword">return</span> inner_func</span><br><span class="line"></span><br><span class="line"><span class="comment">#在func1中嵌套了另一函数inner_func,而在inner_func中引用了args</span></span><br><span class="line">aa = func1(<span class="number">10</span>)    <span class="comment">#此时 args = 10</span></span><br><span class="line"><span class="built_in">print</span> aa</span><br><span class="line"><span class="comment">#输出 &lt;function func1.&lt;locals&gt;.inner_func at 0x00000000038DC400&gt;</span></span><br><span class="line"><span class="comment">#从这里可以看出 aa = func1(10),aa指向了函数inner_func</span></span><br><span class="line"><span class="built_in">print</span>(aa(<span class="number">20</span>))    <span class="comment">#此时 inner_args = 20</span></span><br><span class="line"><span class="comment">#输出 30</span></span><br></pre></td></tr></table></figure>

<p>inner_func中使用了args,但是inner_func中并没有定义args,此时python解析器会向上查找,在func1找到使用.</p>
<p><strong>全局作用域无法访问局部作用域，而局部作用域能够访问全局作用域就这这个原因。而当我在局部作用域创建了一个和外面同名的变量时，python在找这个变量的时候首先会在当前作用域中找,找到了,就不继续往上一级找了</strong></p>
<p><strong>当函数存在嵌套,并且子函数引用了父函数中的变量,可以访问这些变量的作用域就形成闭包,如果子函数没有访问父函数中的变量,就不存在闭包</strong></p>
<p>内部函数inner_func引用了外部func1的变量,所有inner_func是闭包.</p>
<p>那第一段代码如何才能得到结果为<code>[0,2,4,6]</code>呢,可以这样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#方法一:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():</span><br><span class="line">    <span class="keyword">return</span> [<span class="keyword">lambda</span> x,i = i:i*x <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    <span class="comment">#添加一个i = i,这样就能够直接使用for i 局部变更的 i 值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>([m(<span class="number">2</span>) <span class="keyword">for</span> m <span class="keyword">in</span> f()])</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法二:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">lambda</span> x:i*x <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>))</span><br><span class="line">    <span class="comment">#把[] 换成 (),变成一个生成器,生成器则是先生一个值,应用于lambda后再生成一个值,不会像列表解析一样一次性生成</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>([m(<span class="number">2</span>) <span class="keyword">for</span> m <span class="keyword">in</span> f()])</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<p>闭包的最常用的使用场景即是装饰器</p>
<h3 id="python装饰器"><a href="#python装饰器" class="headerlink" title="python装饰器"></a><strong>python装饰器</strong></h3><p>因为python中一切皆对象,函数也是对象,所以也可以将函数做为参数传递,我们将上面的例子改一改,将函数做为参数传递</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner_func</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始执行前&quot;</span>)</span><br><span class="line">        func()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;执行结束&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> inner_func</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func2</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我执行了func2&quot;</span>)</span><br><span class="line">    </span><br><span class="line">a = func1(func2)</span><br><span class="line"><span class="comment">#这里将func2做为参数传递给func1,此时func=func1</span></span><br><span class="line"><span class="comment">#相当于func1返回了inner_func给a</span></span><br><span class="line"><span class="comment">#时刻牢记func2 与 func2()的区别</span></span><br><span class="line">a()</span><br><span class="line"><span class="comment">#这里调用了inner_func</span></span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line"><span class="comment">#开始执行前</span></span><br><span class="line"><span class="comment">#我执行了func2</span></span><br><span class="line"><span class="comment">#执行结束</span></span><br></pre></td></tr></table></figure>

<p><strong>应该就能明白装饰器是什么了.所谓装饰器就是在闭包的基础上传递了一个函数,然后覆盖原来函数的执行入口,以后调用这个函数的时候,就可以额外实现一些功能了.装饰器的存在主要是为了不修改原函数的代码,也不修改其他调用这个函数的代码,就能实现功能的拓展.</strong></p>
<p>当然装饰器还有更简介的写法,<strong>使用@语法糖</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner_func</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始执行前&quot;</span>)</span><br><span class="line">        func()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;执行结束&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> inner_func</span><br><span class="line"></span><br><span class="line"><span class="meta">@func1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func2</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我执行了func2&quot;</span>)</span><br><span class="line"></span><br><span class="line">func2</span><br><span class="line"><span class="comment">#输出 &lt;function func1.&lt;locals&gt;.inner_func at 0x00000000038DC268&gt;</span></span><br><span class="line">func2()</span><br><span class="line"><span class="comment">#这里相当于func2 = func1(func2)</span></span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line"><span class="comment">#开始执行前</span></span><br><span class="line"><span class="comment">#我执行了func2</span></span><br><span class="line"><span class="comment">#执行结束</span></span><br></pre></td></tr></table></figure>

<p>装饰器还有很多其它特性,下次再另起一篇,这里算是开个头,困了,就到这里吧.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.wklken.me/posts/2013/07/19/python-translate-decorator.html">理解PYTHON中的装饰器</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>bash的自动化测试框架-bats</title>
    <url>/2020/08/23/shell-bats/</url>
    <content><![CDATA[<p>今天在kubernetes中部署mongodb-rs时，发现其helm模板中使用到了一个bats,搜索了一下发现这是一个对bash领域的自动化测试框架，简单学习一下.</p>
<span id="more"></span>

<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>mongodb-rs的部署不在这里详述了，直接来到使用到bats的地方,</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Source: mongodb-replicaset/templates/tests/mongodb-up-test-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">mongodb-replicaset</span></span><br><span class="line">    <span class="attr">chart:</span> <span class="string">mongodb-replicaset-3.17.0</span></span><br><span class="line">    <span class="attr">heritage:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">release:</span> <span class="string">mongodb-rs</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mongodb-rs-mongodb-replicaset-test</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">infra</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">&quot;helm.sh/hook&quot;:</span> <span class="string">test-success</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-framework</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">dduportal/bats:0.4.0</span></span><br><span class="line">      <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">bash</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -ex</span></span><br><span class="line"><span class="string">          # copy bats to tools dir</span></span><br><span class="line"><span class="string">          cp -R /usr/local/libexec/ /tools/bats/</span></span><br><span class="line"><span class="string"></span>      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tools</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/tools</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mongo</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">&quot;mongo:4.2&quot;</span></span><br><span class="line">      <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tools/bats/bats</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-t</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tests/mongodb-up-test.sh</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">FULL_NAME</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">mongodb-rs-mongodb-replicaset</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NAMESPACE</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">infra</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REPLICAS</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">AUTH</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADMIN_USER</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">secretKeyRef:</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">&quot;mongodb-rs-mongodb-replicaset-admin&quot;</span></span><br><span class="line">              <span class="attr">key:</span> <span class="string">user</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ADMIN_PASSWORD</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">secretKeyRef:</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">&quot;mongodb-rs-mongodb-replicaset-admin&quot;</span></span><br><span class="line">              <span class="attr">key:</span> <span class="string">password</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tools</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/tools</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tests</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/tests</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tools</span></span><br><span class="line">      <span class="attr">emptyDir:</span> &#123;&#125;</span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tests</span></span><br><span class="line">      <span class="attr">configMap:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">mongodb-rs-mongodb-replicaset-tests</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span> <span class="comment"># 指定pod的启动命令为Never，表示只运行一次.</span></span><br></pre></td></tr></table></figure>

<p>首先看到initContainers执行了<code>cp -R /usr/local/libexec/ /tools/bats/</code>，从<a href="https://github.com/sstephenson/bats">bats</a> github中可以看到，这个目录下是一些可执行的bash代码，这是整个核心.</p>
<p>然后在主containers中执行了<code>/tools/bats/bats -t /tests/mongodb-up-test.sh</code>, 这个脚本内容如下:</p>
<p>cat mongodb-up-test.sh</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">set</span> -ex</span><br><span class="line">    </span><br><span class="line">    CACRT_FILE=/work-dir/tls.crt</span><br><span class="line">    CAKEY_FILE=/work-dir/tls.key</span><br><span class="line">    MONGOPEM=/work-dir/mongo.pem</span><br><span class="line">    </span><br><span class="line">    MONGOARGS=<span class="string">&quot;--quiet&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> [ -e <span class="string">&quot;/tls/tls.crt&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="comment"># log &quot;Generating certificate&quot;</span></span><br><span class="line">        <span class="built_in">mkdir</span> -p /work-dir</span><br><span class="line">        <span class="built_in">cp</span> /tls/tls.crt /work-dir/tls.crt</span><br><span class="line">        <span class="built_in">cp</span> /tls/tls.key /work-dir/tls.key</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Move into /work-dir</span></span><br><span class="line">        <span class="built_in">pushd</span> /work-dir</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cat</span> &gt;openssl.cnf &lt;&lt;<span class="string">EOL</span></span><br><span class="line"><span class="string">    [req]</span></span><br><span class="line"><span class="string">    req_extensions = v3_req</span></span><br><span class="line"><span class="string">    distinguished_name = req_distinguished_name</span></span><br><span class="line"><span class="string">    [req_distinguished_name]</span></span><br><span class="line"><span class="string">    [ v3_req ]</span></span><br><span class="line"><span class="string">    basicConstraints = CA:FALSE</span></span><br><span class="line"><span class="string">    keyUsage = nonRepudiation, digitalSignature, keyEncipherment</span></span><br><span class="line"><span class="string">    subjectAltName = @alt_names</span></span><br><span class="line"><span class="string">    [alt_names]</span></span><br><span class="line"><span class="string">    DNS.1 = $(echo -n &quot;$(hostname)&quot; | sed s/-[0-9]*$//)</span></span><br><span class="line"><span class="string">    DNS.2 = $(hostname)</span></span><br><span class="line"><span class="string">    DNS.3 = localhost</span></span><br><span class="line"><span class="string">    DNS.4 = 127.0.0.1</span></span><br><span class="line"><span class="string">    EOL</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Generate the certs</span></span><br><span class="line">        openssl genrsa -out mongo.key 2048</span><br><span class="line">        openssl req -new -key mongo.key -out mongo.csr -subj <span class="string">&quot;/OU=MongoDB/CN=<span class="subst">$(hostname)</span>&quot;</span> -config openssl.cnf</span><br><span class="line">        openssl x509 -req -<span class="keyword">in</span> mongo.csr \</span><br><span class="line">            -CA <span class="string">&quot;<span class="variable">$CACRT_FILE</span>&quot;</span> -CAkey <span class="string">&quot;<span class="variable">$CAKEY_FILE</span>&quot;</span> -CAcreateserial \</span><br><span class="line">            -out mongo.crt -days 3650 -extensions v3_req -extfile openssl.cnf</span><br><span class="line">        <span class="built_in">cat</span> mongo.crt mongo.key &gt; <span class="variable">$MONGOPEM</span></span><br><span class="line">        MONGOARGS=<span class="string">&quot;<span class="variable">$MONGOARGS</span> --ssl --sslCAFile <span class="variable">$CACRT_FILE</span> --sslPEMKeyFile <span class="variable">$MONGOPEM</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$&#123;AUTH&#125;</span>&quot;</span> == <span class="string">&quot;true&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">        MONGOARGS=<span class="string">&quot;<span class="variable">$MONGOARGS</span> --username <span class="variable">$ADMIN_USER</span> --password <span class="variable">$ADMIN_PASSWORD</span> --authenticationDatabase admin&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="title">pod_name</span></span>() &#123;</span><br><span class="line">        <span class="built_in">local</span> full_name=<span class="string">&quot;<span class="variable">$&#123;FULL_NAME?Environment variable FULL_NAME not set&#125;</span>&quot;</span></span><br><span class="line">        <span class="built_in">local</span> namespace=<span class="string">&quot;<span class="variable">$&#123;NAMESPACE?Environment variable NAMESPACE not set&#125;</span>&quot;</span></span><br><span class="line">        <span class="built_in">local</span> index=<span class="string">&quot;<span class="variable">$1</span>&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$full_name</span>-<span class="variable">$index</span>.<span class="variable">$full_name</span>.<span class="variable">$namespace</span>.svc.cluster.local&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="title">replicas</span></span>() &#123;</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;REPLICAS?Environment variable REPLICAS not set&#125;</span>&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="title">master_pod</span></span>() &#123;</span><br><span class="line">        <span class="keyword">for</span> ((i = 0; i &lt; $(replicas); ++i)); <span class="keyword">do</span></span><br><span class="line">            response=$(mongo <span class="variable">$MONGOARGS</span> <span class="string">&quot;--host=<span class="subst">$(pod_name <span class="string">&quot;<span class="variable">$i</span>&quot;</span>)</span>&quot;</span> <span class="string">&quot;--eval=rs.isMaster().ismaster&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$response</span>&quot;</span> == <span class="string">&quot;true&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">                pod_name <span class="string">&quot;<span class="variable">$i</span>&quot;</span></span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="title">setup</span></span>() &#123;</span><br><span class="line">        <span class="built_in">local</span> ready=0</span><br><span class="line">        until [[ <span class="string">&quot;<span class="variable">$ready</span>&quot;</span> -eq $(replicas) ]]; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;Waiting for application to become ready&quot;</span> &gt;&amp;2</span><br><span class="line">            <span class="built_in">sleep</span> 1</span><br><span class="line">            ready=0</span><br><span class="line">            <span class="keyword">for</span> ((i = 0; i &lt; $(replicas); ++i)); <span class="keyword">do</span></span><br><span class="line">                response=$(mongo <span class="variable">$MONGOARGS</span> <span class="string">&quot;--host=<span class="subst">$(pod_name <span class="string">&quot;<span class="variable">$i</span>&quot;</span>)</span>&quot;</span> <span class="string">&quot;--eval=rs.status().ok&quot;</span> || <span class="literal">true</span>)</span><br><span class="line">                <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$response</span>&quot;</span> -eq 1 ]]; <span class="keyword">then</span></span><br><span class="line">                    ready=$((ready + <span class="number">1</span>))</span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @<span class="built_in">test</span> <span class="string">&quot;Testing mongodb client is executable&quot;</span> &#123;</span><br><span class="line">        mongo -h</span><br><span class="line">        [ <span class="string">&quot;$?&quot;</span> -eq 0 ]</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @<span class="built_in">test</span> <span class="string">&quot;Connect mongodb client to mongodb pods&quot;</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> ((i = 0; i &lt; $(replicas); ++i)); <span class="keyword">do</span></span><br><span class="line">            response=$(mongo <span class="variable">$MONGOARGS</span> <span class="string">&quot;--host=<span class="subst">$(pod_name <span class="string">&quot;<span class="variable">$i</span>&quot;</span>)</span>&quot;</span> <span class="string">&quot;--eval=rs.status().ok&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> [[ ! <span class="string">&quot;<span class="variable">$response</span>&quot;</span> -eq 1 ]]; <span class="keyword">then</span></span><br><span class="line">                <span class="built_in">exit</span> 1</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @<span class="built_in">test</span> <span class="string">&quot;Write key to primary&quot;</span> &#123;</span><br><span class="line">        response=$(mongo <span class="variable">$MONGOARGS</span> --host=$(master_pod) <span class="string">&quot;--eval=db.test.insert(&#123;\&quot;abc\&quot;: \&quot;def\&quot;&#125;).nInserted&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> [[ ! <span class="string">&quot;<span class="variable">$response</span>&quot;</span> -eq 1 ]]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">exit</span> 1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @<span class="built_in">test</span> <span class="string">&quot;Read key from slaves&quot;</span> &#123;</span><br><span class="line">        <span class="comment"># wait for slaves to catch up</span></span><br><span class="line">        <span class="built_in">sleep</span> 10</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">for</span> ((i = 0; i &lt; $(replicas); ++i)); <span class="keyword">do</span></span><br><span class="line">            response=$(mongo <span class="variable">$MONGOARGS</span> --host=$(pod_name <span class="string">&quot;<span class="variable">$i</span>&quot;</span>) <span class="string">&quot;--eval=rs.slaveOk(); db.test.find(&#123;\&quot;abc\&quot;:\&quot;def\&quot;&#125;)&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> [[ ! <span class="string">&quot;<span class="variable">$response</span>&quot;</span> =~ .*def.* ]]; <span class="keyword">then</span></span><br><span class="line">                <span class="built_in">exit</span> 1</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Clean up a document after test</span></span><br><span class="line">        mongo <span class="variable">$MONGOARGS</span> --host=$(master_pod) <span class="string">&quot;--eval=db.test.deleteMany(&#123;\&quot;abc\&quot;: \&quot;def\&quot;&#125;)&quot;</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>该脚本的上半部分可以先忽略</p>
<p>然后定义了几个函数, 其实<code>setup()</code>函数在bats中是特殊的函数,其实还有一个<code>teardown</code>，主要用于在执行测试函数前&#x2F;后执行的两个函数， <code>setup</code>用于做一些准备工作,<code>teardown</code>用于做一些清理工作</p>
<p>最重要的部分是<code>@test</code>,比如:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@<span class="built_in">test</span> <span class="string">&quot;Testing mongodb client is executable&quot;</span> &#123;</span><br><span class="line">    mongo -h</span><br><span class="line">    [ <span class="string">&quot;$?&quot;</span> -eq 0 ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>@test</code>是一个关键字，指定被包含代码块需要被testing.在上面的脚本中指定了4个<code>@test</code>函数. 它到底是如何执行的呢？</p>
<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>首先: 在<a href="https://github.com/sstephenson/bats/blob/03608115df2071fff4eaaff1605768c275e5f81f/libexec/bats-preprocess">bats-preprocess</a>中,这个代码主要是解析待测试脚本，可以发现以下代码:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pattern=<span class="string">&#x27;^ *@test  *([^ ].*)  *\&#123; *(.*)$&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> -r line; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">let</span> index+=1</span><br><span class="line">  <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$line</span>&quot;</span> =~ <span class="variable">$pattern</span> ]]; <span class="keyword">then</span></span><br><span class="line">    quoted_name=<span class="string">&quot;<span class="variable">$&#123;BASH_REMATCH[1]&#125;</span>&quot;</span></span><br><span class="line">    body=<span class="string">&quot;<span class="variable">$&#123;BASH_REMATCH[2]&#125;</span>&quot;</span></span><br><span class="line">    name=<span class="string">&quot;<span class="subst">$(eval echo <span class="string">&quot;<span class="variable">$quoted_name</span>&quot;</span>)</span>&quot;</span></span><br><span class="line">    encoded_name=<span class="string">&quot;<span class="subst">$(encode_name <span class="string">&quot;<span class="variable">$name</span>&quot;</span>)</span>&quot;</span></span><br><span class="line">    tests[<span class="string">&quot;<span class="variable">$&#123;#tests[@]&#125;</span>&quot;</span>]=<span class="string">&quot;<span class="variable">$encoded_name</span>&quot;</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;encoded_name&#125;</span>() &#123; bats_test_begin <span class="variable">$&#123;quoted_name&#125;</span> <span class="variable">$&#123;index&#125;</span>; <span class="variable">$&#123;body&#125;</span>&quot;</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;%s\n&quot;</span> <span class="string">&quot;<span class="variable">$line</span>&quot;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>通过解析<code>@test</code>来得到有多个需要testing的代码块，从pod的日志中也可以看出</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">++ bats_test_function test_Testing_mongodb_client_is_executable</span><br><span class="line">++ <span class="built_in">local</span> test_name=test_Testing_mongodb_client_is_executable</span><br><span class="line">++ BATS_TEST_NAMES[<span class="string">&quot;<span class="variable">$&#123;#BATS_TEST_NAMES[@]&#125;</span>&quot;</span>]=test_Testing_mongodb_client_is_executable</span><br><span class="line">++ bats_test_function test_Connect_mongodb_client_to_mongodb_pods</span><br><span class="line">++ <span class="built_in">local</span> test_name=test_Connect_mongodb_client_to_mongodb_pods</span><br><span class="line">++ BATS_TEST_NAMES[<span class="string">&quot;<span class="variable">$&#123;#BATS_TEST_NAMES[@]&#125;</span>&quot;</span>]=test_Connect_mongodb_client_to_mongodb_pods</span><br><span class="line">++ bats_test_function test_Write_key_to_primary</span><br><span class="line">++ <span class="built_in">local</span> test_name=test_Write_key_to_primary</span><br><span class="line">++ BATS_TEST_NAMES[<span class="string">&quot;<span class="variable">$&#123;#BATS_TEST_NAMES[@]&#125;</span>&quot;</span>]=test_Write_key_to_primary</span><br><span class="line">++ bats_test_function test_Read_key_from_slaves</span><br><span class="line">++ <span class="built_in">local</span> test_name=test_Read_key_from_slaves</span><br><span class="line">++ BATS_TEST_NAMES[<span class="string">&quot;<span class="variable">$&#123;#BATS_TEST_NAMES[@]&#125;</span>&quot;</span>]=test_Read_key_from_slaves</span><br><span class="line">+ <span class="string">&#x27;[&#x27;</span> -n <span class="string">&#x27;&#x27;</span> <span class="string">&#x27;]&#x27;</span></span><br><span class="line">+ bats_perform_tests test_Testing_mongodb_client_is_executable test_Connect_mongodb_client_to_mongodb_pods test_Write_key_to_primary test_Read_key_from_slaves</span><br><span class="line">+ <span class="built_in">echo</span> 1..4</span><br><span class="line">+ test_number=1</span><br><span class="line">+ status=0</span><br><span class="line">1..4</span><br><span class="line">+ <span class="keyword">for</span> test_name <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line">+ /tools/bats/bats-exec-test /tests/mongodb-up-test.sh test_Testing_mongodb_client_is_executable 1</span><br><span class="line">+</span><br></pre></td></tr></table></figure>

<p>从日志中可以看到，成功解析到了4个<code>@test</code>代码块, 然后最后循环调用<code>/tools/bats/bats-exec-test</code>, 来看看这个函数</p>
<p>传递了三个参数，第一个是脚本文件，第二个是获取到的<code>@test</code>代码块的名字， 第三个是序号</p>
<p>到<code>/tools/bats/bats-exec-test</code>,调用这个函数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">bats_perform_test</span></span>() &#123;</span><br><span class="line">  BATS_TEST_NAME=<span class="string">&quot;<span class="variable">$1</span>&quot;</span></span><br><span class="line">  <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$(type -t <span class="string">&quot;<span class="variable">$BATS_TEST_NAME</span>&quot;</span> || true)</span>&quot;</span> = <span class="string">&quot;function&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    BATS_TEST_NUMBER=<span class="string">&quot;<span class="variable">$2</span>&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$BATS_TEST_NUMBER</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;1..1&quot;</span></span><br><span class="line">      BATS_TEST_NUMBER=<span class="string">&quot;1&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    BATS_TEST_COMPLETED=<span class="string">&quot;&quot;</span></span><br><span class="line">    BATS_TEARDOWN_COMPLETED=<span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="built_in">trap</span> <span class="string">&quot;bats_debug_trap \&quot;\$BASH_SOURCE\&quot;&quot;</span> debug  <span class="comment"># trap是bash中用于捕捉信号函数</span></span><br><span class="line">    <span class="built_in">trap</span> <span class="string">&quot;bats_error_trap&quot;</span> err</span><br><span class="line">    <span class="built_in">trap</span> <span class="string">&quot;bats_teardown_trap&quot;</span> <span class="built_in">exit</span></span><br><span class="line">    <span class="string">&quot;<span class="variable">$BATS_TEST_NAME</span>&quot;</span> &gt;&gt;<span class="string">&quot;<span class="variable">$BATS_OUT</span>&quot;</span> 2&gt;&amp;1 <span class="comment"># 执行命令</span></span><br><span class="line">    BATS_TEST_COMPLETED=1</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;bats: unknown test name \`<span class="variable">$BATS_TEST_NAME</span>&#x27;&quot;</span> &gt;&amp;2</span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>因此，当命令在执行过程出现error时则会被trap捕获到,然后设置错误码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">bats_error_trap</span></span>() &#123;</span><br><span class="line">  BATS_ERROR_STATUS=<span class="string">&quot;$?&quot;</span></span><br><span class="line">  BATS_ERROR_STACK_TRACE=( <span class="string">&quot;<span class="variable">$&#123;BATS_PREVIOUS_STACK_TRACE[@]&#125;</span>&quot;</span> )</span><br><span class="line">  <span class="built_in">trap</span> - debug</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样，一旦生成了错误码，则pod的状态即会出现异常状态，而正常情况下指定了restart:Never的pod执行完启动命令的状态会变成complete.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n infra</span></span><br><span class="line">NAME                                 READY   STATUS      RESTARTS   AGE</span><br><span class="line">mongodb-rs-mongodb-replicaset-0      1/1     Running     0          4h36m</span><br><span class="line">mongodb-rs-mongodb-replicaset-1      1/1     Running     0          4h35m</span><br><span class="line">mongodb-rs-mongodb-replicaset-2      1/1     Running     0          4h35m</span><br><span class="line">mongodb-rs-mongodb-replicaset-test   0/1     Completed   0          4h22m</span><br></pre></td></tr></table></figure>



<p>bats还有一些其它的使用技巧, 比如可以使用skip来跳过某些<code>@test</code>代码块、可以用于<code>@test</code>网页等.可到<a href="https://github.com/sstephenson/bats/blob/03608115df/README.md">github</a>查看</p>
<p>不过batsN久没有更新了, 有一个改良版的<a href="https://github.com/bats-core/bats-core">bats-core</a>，是在bats的基本之上改良的，感兴趣的可以看看.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h3><blockquote>
<ul>
<li><a href="https://github.com/bats-core/bats-core">https://github.com/bats-core/bats-core</a></li>
<li><a href="https://github.com/sstephenson/bats">https://github.com/sstephenson/bats</a></li>
<li><a href="https://wangchujiang.com/linux-command/c/trap.html">https://wangchujiang.com/linux-command/c/trap.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>有趣的Shell Snippet</title>
    <url>/2021/03/21/shell-funny-snippet/</url>
    <content><![CDATA[<p>记录下遇到的有趣的shell代码, 可能是一些常用的snippet, 也可能是使用的时候不经意踩到的坑</p>
<p><strong>不定时更新</strong></p>
<span id="more"></span>



<h3 id="正确传递数组到函数中"><a href="#正确传递数组到函数中" class="headerlink" title="正确传递数组到函数中"></a>正确传递数组到函数中</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">update</span></span>() &#123;</span><br><span class="line">    <span class="built_in">declare</span> -a apps_version=(<span class="string">&quot;<span class="variable">$&#123;!1&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;apps_version[@]&#125;</span>&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">APPS_VERSION=(<span class="string">&quot;aaa&quot;</span> <span class="string">&quot;bbb&quot;</span> <span class="string">&quot;ccc&quot;</span>)</span><br><span class="line">update APPS_VERSION[@]</span><br></pre></td></tr></table></figure>

<p>要特别注意的是，使用不当就造成只将数组的第一个数组到函数中</p>
<p>上面是正确的使用方法</p>
<h3 id="使用sed修改Yaml文件中指定关键字的下N行"><a href="#使用sed修改Yaml文件中指定关键字的下N行" class="headerlink" title="使用sed修改Yaml文件中指定关键字的下N行"></a>使用sed修改Yaml文件中指定关键字的下N行</h3><p>文件如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">this:</span></span><br><span class="line">  <span class="attr">is:</span> <span class="string">is</span></span><br><span class="line">  <span class="attr">a:</span> <span class="string">a</span></span><br><span class="line">  <span class="attr">test:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">is:</span> <span class="string">is</span></span><br><span class="line"><span class="attr">test:</span> <span class="string">test</span></span><br></pre></td></tr></table></figure>

<p>现在要修改this所在行的下面2行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&quot;/this:/!b;n;n;c\  a: <span class="variable">$&#123;tmp_version&#125;</span>&quot;</span> <span class="variable">$&#123;file&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>!b表示中断sed命令</li>
<li>n表示读入下一行</li>
<li>c表示将当前行修改为后面的字符串</li>
</ul>
</blockquote>
<p>因为a: a这行需要缩进2个空格，空格需要使用<code>\</code>进行转义</p>
<h3 id="查看进程占用的文件句柄"><a href="#查看进程占用的文件句柄" class="headerlink" title="查看进程占用的文件句柄"></a>查看进程占用的文件句柄</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户级</span></span><br><span class="line">find /proc/*/fd/* -<span class="built_in">type</span> l -lname <span class="string">&#x27;anon_inode:inotify&#x27;</span> -<span class="built_in">print</span> 2&gt;/dev/null | <span class="built_in">cut</span> -d/ -f3 |xargs -I <span class="string">&#x27;&#123;&#125;&#x27;</span> -- ps --no-headers -o <span class="string">&#x27;%U&#x27;</span> -p <span class="string">&#x27;&#123;&#125;&#x27;</span> | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -c | <span class="built_in">sort</span> -nr</span><br><span class="line"><span class="comment"># 结果: 第一列表示打开的句柄，第二列表示用户</span></span><br><span class="line"><span class="comment">#     39 root</span></span><br><span class="line"><span class="comment">#     36 SENSETIME\zhoushuke</span></span><br><span class="line"><span class="comment">#      2 mfe</span></span><br><span class="line">      </span><br><span class="line"><span class="comment"># 进程级</span></span><br><span class="line">find /proc/*/fd/* -<span class="built_in">type</span> l -lname <span class="string">&#x27;anon_inode:inotify&#x27;</span> -<span class="built_in">print</span> 2&gt;/dev/null | <span class="built_in">cut</span> -d/ -f3 |xargs -I <span class="string">&#x27;&#123;&#125;&#x27;</span> -- ps --no-headers -o <span class="string">&#x27;%U %p %c&#x27;</span> -p <span class="string">&#x27;&#123;&#125;&#x27;</span> | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -c | <span class="built_in">sort</span> -nr</span><br><span class="line"><span class="comment"># 结果: 第一列表示打开的句柄，第二列表示用户，第三列表示用户id，第四列表示进程</span></span><br><span class="line"><span class="comment">#      6 root         1 systemd</span></span><br><span class="line"><span class="comment">#      5 SENSETI+ 11808 sogou-qimpanel</span></span><br><span class="line"><span class="comment">#      5 root      1072 kubelet</span></span><br></pre></td></tr></table></figure>



<h3 id="使用sed删除多行内容"><a href="#使用sed删除多行内容" class="headerlink" title="使用sed删除多行内容"></a>使用sed删除多行内容</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">env:</span> <span class="string">prod</span></span><br><span class="line"></span><br><span class="line"><span class="attr">sensebee3:</span></span><br><span class="line">  <span class="attr">xxx:</span></span><br><span class="line">    <span class="attr">yyy:</span> <span class="string">yyy</span></span><br><span class="line">  <span class="attr">svc_name:</span> <span class="string">ss_class.ingress_namespace.svc.cluster.local</span></span><br><span class="line">  <span class="attr">sensebee:</span> <span class="string">sensebee</span></span><br><span class="line">  <span class="attr">svc_port:</span> <span class="number">8443</span></span><br><span class="line">  <span class="attr">node_port:</span> <span class="number">30123</span></span><br><span class="line"><span class="attr">http:</span> <span class="number">1234</span></span><br><span class="line"><span class="attr">sensebee2:</span></span><br><span class="line">  <span class="attr">http:</span> <span class="number">1234</span></span><br><span class="line">  <span class="attr">svc_name:</span> <span class="string">sensebee.default</span></span><br></pre></td></tr></table></figure>

<p>对于上面的的yaml文件内容,如果想将sensebee3下的行直到http: 1234之间的内容都删除,但是<code>sensebee3及http: 1234</code>这两行不删除，使用<code>sed</code>如何操作呢?</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&quot;/^sensebee3:/I,/^[^[:space:]#]/&#123;//!d;&#125;&quot;</span> file</span><br></pre></td></tr></table></figure>

<p>从上面的格式可以看出,只需要先确定范围，然后删除即可, </p>
<p><code>/^sensebee3:/</code>用于匹配<code>sensebee3</code></p>
<p><code>/^[^[:space:]#]/</code>用于匹配到不是以空格及<code>#</code>号开头的行，自然就匹配到了<code>http: 1234</code>, 这样就选定了这两行之间的内容</p>
<p><code>&#123;//!d;&#125;</code>其中<code>//</code>表示使用前面的正则表达式, <code>!d</code>表示不删除, 这样就实现了<code>sensebee3及http: 1234</code>这两行不会删除，只删除这两行之间的内容.</p>
<h3 id="在脚本中修改crontab"><a href="#在脚本中修改crontab" class="headerlink" title="在脚本中修改crontab"></a>在脚本中修改crontab</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(crontab -l 2&gt;/dev/null; <span class="built_in">echo</span> <span class="string">&#x27;*/2 * * * * bash /usr/local/src/kestrel.openfiles.check &gt; /usr/local/src/logs/kestrel_openfiles.check.$(date &quot;+\%Y\%m\%d-\%H\%M\%S&quot;).details 2&gt;&amp;1&#x27;</span>) | crontab -</span><br></pre></td></tr></table></figure>



<h3 id="善用"><a href="#善用" class="headerlink" title="善用{}"></a>善用<code>&#123;&#125;</code></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ATEST=&quot;ISTEST&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">错误使用，打印为空, bash会把ATEST_exec当成是一个变量，也就是最大的查找_连接的字符</span></span><br><span class="line">echo $ATEST_exec</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">正确</span></span><br><span class="line">echo $&#123;ATEST&#125;_exec</span><br></pre></td></tr></table></figure>



<h3 id="使用-amp-amp"><a href="#使用-amp-amp" class="headerlink" title="使用&amp;&amp; ||"></a>使用<code>&amp;&amp;</code> <code>||</code></h3><p>有时为了shell命令能够简短, 经常会连着使用 <code>&amp;&amp;(且) ||(或)</code>, 但是如果不多想一次的话，可能就会跟结果相背.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只有当command1成功(命令返回值为0), 才会执行command2</span></span><br><span class="line">command1 &amp;&amp; command 2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只有当command1失败(命令返回值不为0), 才会执行command2</span></span><br><span class="line">command1 || command2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">command1与command2会顺序执行, 两个命令之间没有关系.</span></span><br><span class="line">command1; command2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例子</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">假如想判断从网上下载一个东西, 如果成功继续执行, 如果失败, 则打印一句话并退出.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">思考下面三个句子，哪条符合要求</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这句话相当于(wget -q someURL -O localdir || <span class="built_in">echo</span> <span class="string">&quot;DOWN FAILED!&quot;</span>); <span class="built_in">exit</span> 1</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">因此, 不管wget是否成功, <span class="built_in">exit</span> 1都将被执行.</span> </span><br><span class="line">wget -q someURL -O localdir || echo &quot;DOWN FAILED!&quot;; exit 1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当wget成功后, 忽略 || 后面的语句, 只有失败了，才会<span class="built_in">echo</span>, 由于<span class="built_in">echo</span>成功, 因此也会执行 <span class="built_in">exit</span> 1</span></span><br><span class="line">wget -q someURL -O localdir || (echo &quot;DOWN FAILED!&quot; &amp;&amp; exit 1)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这句跟上面的效果一样, 唯一的区别在于<span class="built_in">exit</span> 1必然会执行而不管<span class="built_in">echo</span>语句有没有成功</span></span><br><span class="line">wget -q someURL -O localdir || (echo &quot;DOWN FAILED!&quot;; exit 1)</span><br></pre></td></tr></table></figure>



<h3 id="换行符"><a href="#换行符" class="headerlink" title="换行符"></a>换行符</h3><p>经常会有读取文件的需要, 用的最多的是使用for循环读取文件, 使用的时候需要特别注意文件的换行符, 默认情况下，换行符为空格, 需要使用<code>IFS</code>指定为换行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测试文件</span></span><br><span class="line">cat xx.md</span><br><span class="line">this is a test</span><br><span class="line">for readline</span><br><span class="line">from bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">读取文件脚本(错误)</span></span><br><span class="line">cat readfromfile.sh</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for LINE in `cat xx.md`</span><br><span class="line">do</span><br><span class="line">    echo $LINE</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出, 没有按行输出, 第个单词都占据了一行</span></span><br><span class="line">this</span><br><span class="line">is</span><br><span class="line">a</span><br><span class="line">test</span><br><span class="line">for</span><br><span class="line">readline</span><br><span class="line">from</span><br><span class="line">bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">正确的脚本</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">IFS_old=$IFS      # 将原IFS值保存，以便用完后恢复</span><br><span class="line">IFS=$&#x27;\n&#x27;					# 指定回车为分隔符</span><br><span class="line"></span><br><span class="line">for LINE in $(cat xx.md)</span><br><span class="line">do</span><br><span class="line">        echo $&#123;LINE&#125;</span><br><span class="line">done</span><br><span class="line">IFS=$IFS_old 			# 恢复原IFS值</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出</span></span><br><span class="line">this is a test</span><br><span class="line">for readline</span><br><span class="line">from bash</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">当然以下两种方式也可以达到逐行输出效果, 大文件请考虑效率</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">second</span></span><br><span class="line">cat xx.md | while read line</span><br><span class="line">do</span><br><span class="line">    echo &quot;$&#123;line&#125;&quot;</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> third</span></span><br><span class="line">while read line</span><br><span class="line">do </span><br><span class="line">    echo &quot;$&#123;line&#125;&quot;</span><br><span class="line">done &lt; xx.md</span><br></pre></td></tr></table></figure>



<h3 id="变量默认值"><a href="#变量默认值" class="headerlink" title="变量默认值"></a>变量默认值</h3><p>有时候定义变量的时候, 经常需要默认值, shell中也有一些比较有趣的表达式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;value:=word&#125;</span>  </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果value存在且非null 则返回value 否则把word赋值给value并返回word(=value), 适合场景：如果value没有初始化 可给它赋初值</span></span><br><span class="line">echo $&#123;value:=word&#125;---&gt;文件的第一行</span><br><span class="line">echo $&#123;value1:=word&#125;---&gt;word 且会把word赋值给value</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;value:+word&#125;</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果value存在且非null 则返回word 如果value没有设置或为空 则返回value</span></span><br><span class="line">echo $&#123;value:+word&#125;---&gt;word</span><br><span class="line">echo $&#123;value1:-word&#125;---&gt; 返回空 因为value1本身就是空</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;value:?word&#125;</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果valued存在且非Null， 那就什么也不做。否则，value:word会被发送到标准错误输出，并且程序会退出；如果没有指定word 则输出  value: parameter null or not <span class="built_in">set</span></span></span><br><span class="line">echo $&#123;value:?nomessage&#125;----&gt;输出第一行</span><br><span class="line">echo $&#123;value:?nomessage&#125;----&gt;value:nomessage</span><br><span class="line">echo $&#123;value:?&#125;-----&gt;value: parameter null or not set</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="参数解析getops"><a href="#参数解析getops" class="headerlink" title="参数解析getops"></a>参数解析getops</h3><p>在写脚本的时候，经常需要对参数进行解析,shell毕竟是个脚本语言, 不可能像python等高级语言一样有很完善的参数解析库, <code>getops是bash自带的一个用于参数解析的工具, 但是它只是用于参数解析, 不能对参数进行更高级的操作，比如参数间依赖, 参数判断等</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">function usage() &#123;</span><br><span class="line">	echo &quot;$&#123;0&#125; -m [master/slave] -t [my/db] -s [slave-ip]&quot;</span><br><span class="line">    echo &#x27;&#x27;</span><br><span class="line">    # shellcheck disable=SC2016</span><br><span class="line">    echo &#x27;  -m: Specify install type: master or slave&#x27;</span><br><span class="line">    # shellcheck disable=SC2016</span><br><span class="line">    echo &#x27;  -s: If type == master, then must specify opq slave ip&#x27;</span><br><span class="line">    # shellcheck disable=SC2016</span><br><span class="line">    echo &#x27;  -t: Specify instll opq type: my or db, my is short for mingyuan&#x27;</span><br><span class="line">    echo &#x27;example:&#x27;</span><br><span class="line">    cat &lt;&lt; EOF</span><br><span class="line">    #install opq-db</span><br><span class="line"><span class="meta prompt_"> 		# </span><span class="language-bash">master</span></span><br><span class="line"><span class="meta prompt_">			# </span><span class="language-bash">./auto_install_opq_from_oss.sh -m master -s 127.0.0.1 -t db</span></span><br><span class="line"><span class="meta prompt_"> 		# </span><span class="language-bash">slave</span></span><br><span class="line"><span class="meta prompt_">			# </span><span class="language-bash">./auto_install_opq_from_oss.sh -m slave -t db</span></span><br><span class="line"><span class="meta prompt_"> 	# </span><span class="language-bash">install opq-mingyuan</span></span><br><span class="line"><span class="meta prompt_">		# </span><span class="language-bash">master</span></span><br><span class="line"><span class="meta prompt_">			# </span><span class="language-bash">./install_opq.sh -m master -s 127.0.0.1 -t my</span></span><br><span class="line"><span class="meta prompt_">		# </span><span class="language-bash">slave</span></span><br><span class="line"><span class="meta prompt_">			# </span><span class="language-bash">./install_opq.sh -m slave -t my</span></span><br><span class="line">EOF</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">while getopts &quot;m:s:t:h&quot; opt   # &quot;:m:s:t:h&quot; 如果首位的出现: 表示不打印错误信息</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">上面这句表示: -m -s -t 需要参数 -h 不需要传参</span></span><br><span class="line">do</span><br><span class="line">    case $&#123;opt&#125; in</span><br><span class="line">       m)</span><br><span class="line">         MODE=$&#123;OPTARG&#125;</span><br><span class="line">         ;;</span><br><span class="line">       s)</span><br><span class="line">         SLAVEIP=$&#123;OPTARG&#125;</span><br><span class="line">         ;;</span><br><span class="line">       t)</span><br><span class="line">         STYPE=$&#123;OPTARG&#125;</span><br><span class="line">         ;;</span><br><span class="line">       h)</span><br><span class="line">         usage</span><br><span class="line">         exit 0</span><br><span class="line">         ;;</span><br><span class="line">       \?)</span><br><span class="line">         echo &quot;Invalid option: -$OPTARG&quot; &gt;&amp;2</span><br><span class="line">         usage</span><br><span class="line">         exit 1</span><br><span class="line">         ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这里要说明一下 <span class="string">&quot;m:s:t:h&quot;</span>的含义</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果首位(m前)出现 : 表示「不打印错误信息」，也就是说如果需要带参数但没有带时不会打印错误</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">紧邻字母后面的 : 表示该选项接收一个参数, 如果没有带参数的话,则会提示错误</span></span><br></pre></td></tr></table></figure>



<h3 id="多if时不如使用case"><a href="#多if时不如使用case" class="headerlink" title="多if时不如使用case"></a>多if时不如使用case</h3><p>在需要使用法if进行业务判断时, 不防使用case，相对于层层if, 代码会更加清晰</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [[ &quot;$lmode&quot; == &quot;master&quot; ]]; then</span><br><span class="line">	case $lstype in</span><br><span class="line"> 		my)</span><br><span class="line">			OPQ_UNTAR_DIR_NAME=&#x27;multiindex_opq_master_mingyuan&#x27;</span><br><span class="line">			;;</span><br><span class="line">		db)</span><br><span class="line">			OPQ_UNTAR_DIR_NAME=&#x27;multiindex_opq_master&#x27;</span><br><span class="line">			;;</span><br><span class="line">	esac</span><br><span class="line">	PROGRAM=&#x27;opq_master&#x27;</span><br><span class="line">else</span><br><span class="line">	case $lstype in</span><br><span class="line">		my)</span><br><span class="line">			OPQ_UNTAR_DIR_NAME=&#x27;multiindex_opq_slave_mingyuan&#x27;</span><br><span class="line">			;;</span><br><span class="line">		db)</span><br><span class="line">			OPQ_UNTAR_DIR_NAME=&#x27;multiindex_opq_slave&#x27;</span><br><span class="line">			;;</span><br><span class="line">	esac</span><br><span class="line">	PROGRAM=&#x27;opq_slave&#x27;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>



<h3 id="加载key-value类的配置文件"><a href="#加载key-value类的配置文件" class="headerlink" title="加载key-value类的配置文件"></a>加载key-value类的配置文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat xx</span><br><span class="line">key1=value1</span><br><span class="line">key2=value2</span><br><span class="line"></span><br><span class="line">cat xx.sh</span><br><span class="line">. xx # 这里直接使用. xx即可把xx文件里的key-value引入,后续可直接使用k.</span><br><span class="line">echo $key1, $key2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">注意, 配置文件只能是k=v的形式(多个k=v可以在一行, 使用空格隔开), 其它形式会报错</span></span><br></pre></td></tr></table></figure>



<h3 id="一行代码-字符串是否包含子串"><a href="#一行代码-字符串是否包含子串" class="headerlink" title="一行代码: 字符串是否包含子串"></a>一行代码: 字符串是否包含子串</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 判断CONSUL_SERVER中是否包含逗号 如果包含EXPECT_LEN=3， 不包含EXPECT_LEN=1</span></span><br><span class="line">[ -z <span class="string">&quot;<span class="variable">$&#123;CONSUL_SERVER##*,*&#125;</span>&quot;</span> ] &amp;&amp; EXPECT_LEN=3 || EXPECT_LEN=1</span><br></pre></td></tr></table></figure>



<h3 id="for循环打印带空格字符串"><a href="#for循环打印带空格字符串" class="headerlink" title="for循环打印带空格字符串"></a>for循环打印带空格字符串</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需求, 提供个数组，循环数组中的元素， 如果元素存在于某个文件中，则不追加，如果不存在,则追加</span></span><br><span class="line">K8S_CLUSTER=(<span class="string">&quot;172.1.52.250 k8s-master-250&quot;</span> <span class="string">&quot;172.1.52.50 k8s-master-50&quot;</span>)</span><br><span class="line"><span class="comment"># TARGET=/etc/hosts</span></span><br><span class="line"><span class="comment"># 错误的写法</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable">$&#123;K8S_CLUSTER[@]&#125;</span>; <span class="keyword">do</span> grep <span class="string">&quot;<span class="variable">$x</span>&quot;</span> /etc/hosts &gt; /dev/null || <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$x</span>&quot;</span> &gt;&gt; /etc/hosts;<span class="keyword">done</span></span><br><span class="line"><span class="comment"># 会发现cat /etc/hosts输出以下格式, 正是因为元素中存在空格, 而echo的时候会以空格进行换行，不符合预期</span></span><br><span class="line">172.1.52.250 </span><br><span class="line">k8s-master-250</span><br><span class="line">172.1.52.50</span><br><span class="line">k8s-master-50</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确的写法</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;K8S_CLUSTER[@]&#125;</span>&quot;</span>; <span class="keyword">do</span> grep <span class="string">&quot;<span class="variable">$x</span>&quot;</span> /etc/hosts &gt; /dev/null || <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$x</span>&quot;</span> &gt;&gt; /etc/hosts;<span class="keyword">done</span></span><br><span class="line"><span class="comment"># 将数组用引号做为一个整体进行for循环就没问题, cat /etc/hosts</span></span><br><span class="line">172.1.52.250 k8s-master-250</span><br><span class="line">172.1.52.50 k8s-master-50</span><br></pre></td></tr></table></figure>



<h3 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h3><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a>参考文章:</h3><blockquote>
<ul>
<li><a href="https://stackoverflow.com/questions/18620153/find-matching-text-and-replace-next-line">https://stackoverflow.com/questions/18620153/find-matching-text-and-replace-next-line</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>supervisor使用自定义邮件报警模板</title>
    <url>/2019/09/10/supervisor-alert-mail-template-customize/</url>
    <content><![CDATA[<p>生产中由于历史遗留问题，有些程序还是二进制部署, 对于这类程序需要使用守护进程进行托管，常用的能实现对程序进行守护的用的多的有如systemd, supervisor,monit等工具, monit其实更好用一些，拓展性也很强，但是因为线上已经有一些使用了supervisor，为了工具统一化，就都使用了supervisor了，这次讲一讲supervisor使用<strong>自定义邮件模板</strong>实现报警的实现</p>
<span id="more"></span>

<h3 id="supervisor"><a href="#supervisor" class="headerlink" title="supervisor"></a><strong>supervisor</strong></h3><p>需要说明一下,  <strong>supervisor只能管理在前台运行的程序, 所以如果应用事先是以nohup &amp;的方式启动的,则需要先停止该应用,然后使用supervisor守护启动</strong></p>
<p>supervisor的安装及使用很简单，大家可直接参考supervisor的<a href="http://supervisord.org/">官网</a></p>
<h3 id="superlance"><a href="#superlance" class="headerlink" title="superlance"></a><strong>superlance</strong></h3><p>supervisor本身是没有邮件报警的, 好在它有上拓展插件可以直接使用, 但是它自带的邮件没有格式，非常难看，因此需要修改源码来使用我们自己的邮件模板</p>
<p>superlane的gitlab在<a href="https://github.com/Supervisor/superlance">这里</a></p>
<p>superlance中提供了多种报警场景, 这里使用<strong>crashmail</strong></p>
<p>直接通过<code>python setup.py install</code></p>
<p>这里安装的是已经被我修改了源码，集成了自定义邮件模板后的包, 修改后的包在<a href="https://github.com/zhoushuke/superlanceX">这里</a></p>
<p>自定义邮件模块在<a href="https://github.com/zhoushuke/superlanceX/blob/master/superlance/sendxmail.py">这个文件</a></p>
<h3 id="启用crashmail"><a href="#启用crashmail" class="headerlink" title="启用crashmail"></a><strong>启用crashmail</strong></h3><p>在supervisor中加入一个配置文件,以开启crashmail</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[eventlistener:crashmail-exited]</span><br><span class="line"><span class="built_in">command</span>=crashmail -a -f <span class="string">&quot;http://your.mail.domain&quot;</span> -t <span class="string">&quot;xx@your.email.com&quot;</span></span><br><span class="line">events=PROCESS_STATE_EXITED</span><br><span class="line">redirect_stderr=<span class="literal">false</span></span><br><span class="line">stdout_logfile=/etc/supervisor/logs/crashmail-exited-stdout.log</span><br><span class="line">stderr_logfile=/etc/supervisor/logs/crashmail-exited-stderr.log</span><br></pre></td></tr></table></figure>

<p><code>http://your.mail.domain</code>是一个用golang写的一个邮件接口,通过restfulapi暴露出来后,可直接调用.大家也可以使用自己的邮件服务.</p>
<p><code>events=PROCESS_STATE_EXITED</code>指定当程序异常退出时产生事件.</p>
<p>Crashmail的参数大家可以参考<a href="https://github.com/zhoushuke/superlanceX/blob/master/superlance/crashmail.py">github</a></p>
<h3 id="安装守护进程"><a href="#安装守护进程" class="headerlink" title="安装守护进程"></a><strong>安装守护进程</strong></h3><p>假设要守护的进程为influxdb, 增加以下配置文件至 &#x2F;etc&#x2F;supervisor&#x2F;conf.d&#x2F;influxdb.conf</p>
<p><code>新配置文件的配置文件一定要以conf结尾</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[program:influxdb]</span><br><span class="line">directory = /usr/bin</span><br><span class="line"><span class="built_in">command</span> = /usr/bin/influxd -config /etc/influxdb/influxdb.conf</span><br><span class="line">startsecs = 5</span><br><span class="line">autostart = <span class="literal">true</span></span><br><span class="line">autorestart = <span class="literal">true</span></span><br><span class="line">stdout_logfile = /etc/supervisor/logs/supervisor-influxdb-stdout.log</span><br><span class="line">stderr_logfile = /etc/supervisor/logs/supervisor-influxdb-stderr.log</span><br></pre></td></tr></table></figure>

<p>说明: 某些软件如果直接是从apt-get上下载安装的,可能会存在下载安装完之后已经被systemd托管了,这种情况下supervisor无法接管,可先从systemd中把该应用disable掉, 再使用supervisor托管</p>
<p>查看是否被systemd托管</p>
<p><code>systemctl status influxd.service</code></p>
<p>如果状态为active则说明已被托管.</p>
<p>从systemd去除托管状态</p>
<p><code>systemctl stop influxd.service</code></p>
<p><code>systemctl disable influxd.service</code></p>
<p>最后使用以下命令将新增的配置文件加入到supervisor中托管</p>
<p><code>supervisorctl update</code></p>
<p>可使用<code>supervisorctl status</code>查看influxdb是否启动成功.</p>
<h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a><strong>最终效果</strong></h3><p>supervisor检测到程序发生异常退出后，即会通过邮件发送报警</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200121115221.png"></p>
<p>superlance自定义模块在<a href="https://github.com/zhoushuke/superlanceX">这里</a>可以找到</p>
<p>自定义邮件模块在<a href="https://github.com/zhoushuke/superlanceX/blob/master/superlance/sendxmail.py">这个文件</a></p>
<h3 id="supervisor常用命令"><a href="#supervisor常用命令" class="headerlink" title="supervisor常用命令"></a><strong>supervisor常用命令</strong></h3><p>#重启supervisor </p>
<p>supervisorctl reload </p>
<p>#查看当前守护的进程 </p>
<p>supervisorctl status </p>
<p>#加入新增加的配置文件 </p>
<p>supervisorctl update </p>
<p>#只会读取有更新的配置文件，不会启动新增加的配置文件 </p>
<p>supervisorctl reread </p>
<p>#关闭某一进程守护 </p>
<p>supervisorctl stop cassandra </p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://supervisord.org/">supervisor</a></li>
<li><a href="https://github.com/Supervisor/superlance">superlance</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>volcano如何应对大规模任务系列之volcano关键对象</title>
    <url>/2023/11/12/volcano-key-resources/</url>
    <content><![CDATA[<p>volcano做为CNCF目前唯一一款应对大规模资源批调度工具被大家熟知.<br>作者负责的kubernetes集群每天都有大量的任务需要运行, GPU任务、短任务、长任务等等，同时还存在多租户场景、复杂的调度策略等, 依托volcano的高度<strong>可插拔</strong>能力, 同时结合业务场景进行相应的优化,极大提高了资源使用效率，<strong>结果导向明显</strong><br>在此也分享一下整个落地过程,也做为现阶段的一个工作总结, 工作之余尽量更新.</p>
<p>注: 业务各有不同, 作者的选型及观点可能并不适用其它人<br>此篇为: volcano如何应对大规模任务系列之volcano关键对象</p>
<span id="more"></span>

<p>本系列总体分为以下几块内容:<br><a href="https://izsk.me/2023/08/31/volcano-introduction/">volcano如何应对大规模任务系列之volcano开篇介绍</a><br>volcano如何应对大规模任务系列之volcano关键对象<br>volcano如何应对大规模任务系列之volcano插件系统<br>volcano如何应对大规模任务系列之volcano源码解析<br>volcano如何应对大规模任务系列之volcano优化之道<br>volcano如何应对大规模任务系列之volcano生产实践<br>volcano如何应对大规模任务系列之volcano总结建议</p>
<p><strong>本系列的所用volcano版本基于v1.7.0</strong></p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20231007170014.png"></p>
<p>Volcano由scheduler、controllermanager、admission这三个核心组件及一个命令行工具vcctl组成:</p>
<ol>
<li><p>Scheduler: Volcano scheduler通过一系列的action和plugin调度Job，并为它找到一个最适合的节点。与Kubernetes default-scheduler相比，Volcano与众不同的 地方是它支持针对Job的多种调度算法。</p>
</li>
<li><p>Controllermanager: Volcano controllermanager管理CRD资源的生命周期。它主要由Queue ControllerManager、 PodGroupControllerManager、 VCJob ControllerManager构成。</p>
</li>
<li><p>Admission: Volcano admission负责对CRD API资源进行校验。</p>
</li>
<li><p>Vcctl: Volcano vcctl是Volcano的命令行客户端工具。</p>
</li>
</ol>
<h3 id="session"><a href="#session" class="headerlink" title="session"></a>session</h3><p>Session模块其实也是调度周期，默认是1s, 可配置,它的作用是承上启下。volcano在每个调度周期开始时，都会新建一个Session对象，这个Session的初始化时，会做以下操作：</p>
<ul>
<li>调用Cache.Snapshot接口，将Cache中节点、任务和队列的信息拷贝一份副本，之后在这个调度周期中使用这份副本进行调度。因为Cache的数据会不断变化，为了保持同个调度周期中的数据一致性，在一开始就拷贝了一份副本。</li>
<li>将配置中的各个Plugin初始化，然后调用plugin的OnSessionOpen接口。Plugin在OnSessionOpen中，会初始化自己需要的数据，并将一些回调函数注册到session中。Plugin可以向Session中注册的函数是,常用的如下：<ol>
<li>jobOrderFns： 决定哪个任务优先被处理</li>
<li>queueOrderFns：决定哪个队列优先被处理</li>
<li>taskOrderFns：决定任务中哪个容器优先被处理</li>
<li>predicateFns： 判断某个节点是否满足容器的基本调度要求。比如容器中指定的节点的标签</li>
<li>nodeOrderFns： 当多个节点满足容器的调度要求时，优先选择哪个节点</li>
<li>preemptableFns： 决定某个容器是否可以被抢占</li>
<li>reclaimableFns ：决定某个容器是否可以被回收</li>
<li>overusedFns： 决定某个队列使用的资源是否超过限额，是的话不再调度队列中的任务</li>
<li>jobReadyFns：判断某个任务是否已经准备好，可以调用API Server的接口将任务的容器调度到节点</li>
<li>jobPipelinedFns ： 判断某个任务是否处于Pipelined状态，后下详解</li>
<li>jobValidFns： 判断某个任务是否有效</li>
</ol>
</li>
</ul>
<p>关于plugin与各个funcion的引用关系，关注volcano如何应对大规模任务系列之volcano插件系统一节</p>
<p>注意Plugin不需要注册上面所有的函数，而是可以根据自己的需要，注册某几个函数。比如Predict plugin就只注册了predicateFns这个函数到Session中。</p>
<p>初始化成功后，volcano会依次调用不同的Action的Execute方法，并将Session对象作为参数传入。在Execute中，会调用Session的各种方法。这些方法，有些最终会调用到Cache的方法， 有些是调用Plugin注册的方法。</p>
<h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>schedulercache模块封装了对API Server的节点、容器等对象的数据同步逻辑。Kubernetes的数据保存在分布式存储etcd中，所有对数据的查询和操作都通过调用API Server的接口，而非直接操作etcd。在调度时，需要集群中的节点和容器的使用资源和状态等信息。Cache模块通过调用Kubernetes的SDK，通过watch机制监听集群中的节点、容器的状态变化，将信息同步到自己的数据结构中。<br>SchedulerCache会持有很多informer， 初始化的informer注册各个eventHandler，然后pod&#x2F;podgroup等变动会被同步在Jobs, Nodes, Queues, PriorityClasses等几个map中。<strong>podgroup加入jobInfo，pod 加入taskInfo</strong></p>
<h3 id="action"><a href="#action" class="headerlink" title="action"></a>action</h3><p>有几个比较重要的action, 下面一一介绍</p>
<h4 id="enqueue"><a href="#enqueue" class="headerlink" title="enqueue"></a>enqueue</h4><p>Enqueue action筛选符合要求的作业进入待调度队列。当一个Job下的最小资源申请量不能得到满足时，即使为Job下的Pod执行调度动作，Pod也会因为gang约束没有达到而无法进行调度；只有当job的最小资源量得到满足，状态由”Pending”刷新为”Inqueue”才可以进行。一般来说Enqueue action是调度器配置必不可少的action(v1.7+版本后可disable)</p>
<p><strong>在volcano中, 只有通过了enqueue后的任务(即由pending状态变成inqueue状态)才有资格进行真正资源的allocate</strong>, 因此,对enqueue action的逻辑需要理解, 源码解析一章中也会有这方面的分析</p>
<p>Enqueue action是调度流程中的准备阶段</p>
<h4 id="allocate"><a href="#allocate" class="headerlink" title="allocate"></a>allocate</h4><p>Allocate action是调度流程中的正常分配步骤，用于处理在待调度Pod列表中<strong>具有资源申请量的Pod调度</strong>，是调度过程必不可少的action。这个过程包括作业的predicate(预选)和prioritize(优选)。使用predicateFn预选，过滤掉不能分配作业的node；使用NodeOrderFn打分来找到最适合的分配节点。</p>
<p>Allocate action遵循commit机制，当一个Pod的调度请求得到满足后，最终并不一定会为该Pod执行绑定动作，这一步骤还取决于Pod所在Job的gang约束是否得到满足。只有Pod所在Job的gang约束得到满足，Pod才可以被调度，否则，Pod不能够被调度</p>
<h4 id="reclaim"><a href="#reclaim" class="headerlink" title="reclaim"></a>reclaim</h4><p>volcano支持弹性的资源使用，当queue中的资源没有被使用时，可暂被其它queue占用，reclaim action则是用于回收那些超格使用了queue上限的额外资源</p>
<h4 id="preempt"><a href="#preempt" class="headerlink" title="preempt"></a>preempt</h4><p>Preempt action是调度流程中的抢占步骤，用于处理高优先级调度问题。<strong>Preempt用于同一个Queue中job之间的抢占，或同一Job下Task之间的抢占</strong></p>
<h4 id="backfill"><a href="#backfill" class="headerlink" title="backfill"></a>backfill</h4><p>Backfill action是调度流程中的回填步骤，处理待调度Pod列表中<strong>没有指明资源申请量的Pod调度</strong>，在对单个Pod执行调度动作的时候，遍历所有的节点，只要节点满足了Pod的调度请求，就将Pod调度到这个节点上。</p>
<h3 id="plugins"><a href="#plugins" class="headerlink" title="plugins"></a>plugins</h3><p>volcano中包含了很多的plugin, 主要作用是<strong>在action下实现特定的业务逻辑</strong>, 现有的大部分plugin作者都在生产环境应用过，因此有一些生产经验可谈，关注volcano如何应对大规模任务系列之volcano插件系统一节,不过，还是先把所有的plugin简单介绍一下</p>
<h4 id="gang"><a href="#gang" class="headerlink" title="gang"></a>gang</h4><p>Gang调度策略是volcano-scheduler的核心调度算法之一，它满足了调度过程中的<strong>“All or nothing”</strong>的调度需求，避免Pod的任意调度导致集群资源的浪费。具体算法是，观察Job下的Pod已调度数量是否满足了最小运行数量，当Job的最小运行数量得到满足时，为Job下的所有Pod执行调度动作，否则，不执行。<br><strong>kube-scheduler本质上是个串行的调度器，而gang则可实现并行的效果</strong></p>
<h4 id="Binpack"><a href="#Binpack" class="headerlink" title="Binpack"></a>Binpack</h4><p>binpack调度算法的目标是<strong>尽量把已有的节点填满（尽量不往空白节点分配）</strong>。具体实现上，binpack调度算法是给可以投递的节点打分，分数越高表示节点的资源利用率越高。binpack算法能够尽可能填满节点，将应用负载靠拢在部分节点</p>
<p>Binpack算法以插件的形式，注入到volcano-scheduler调度过程中，将应用在Pod优选节点的阶段。Volcano-scheduler在计算binpack算法时，会考虑Pod请求的各种资源，并根据各种资源所配置的权重做平均。每种资源在节点分值计算过程中的权重并不一样，这取决于管理员为每种资源配置的权重值。同时不同的插件在计算节点分数时，也需要分配不同的权重，scheduler也为binpack插件设置了分数权重。</p>
<p><strong>工作节点上的应用拥塞程度跟binpack的设定的参数有很大关系</strong></p>
<h4 id="Priority"><a href="#Priority" class="headerlink" title="Priority"></a>Priority</h4><p>Priority plugin提供了job、task排序的实现，以及计算牺牲作业的函数preemptableFn，job的排序根据priorityClassName，task的排序依次根据priorityClassName、createTime、id。</p>
<h4 id="DRF"><a href="#DRF" class="headerlink" title="DRF"></a>DRF</h4><p>DRF调度算法的全称是Dominant Resource Fairness，是基于容器组Domaint Resource的调度算法。volcano-scheduler观察每个Job请求的主导资源，并将其作为对集群资源使用的一种度量，根据Job的主导资源，计算Job的share值，在调度的过程中，具有较低share值的Job将具有更高的调度优先级。这样能够满足更多的作业，不会因为一个胖业务，饿死大批小业务。DRF调度算法能够确保在多种类型资源共存的环境下,尽可能满足分配的公平原则。</p>
<h4 id="Proportion"><a href="#Proportion" class="headerlink" title="Proportion"></a>Proportion</h4><p>Proportion调度算法是使用queue的概念，用来控制集群总资源的分配比例。每一个queue分配到的集群资源比例是一定的</p>
<p><strong>proportion在queue中很重要，弹性的资源分配就是由proportion实现</strong></p>
<h4 id="Predicates"><a href="#Predicates" class="headerlink" title="Predicates"></a>Predicates</h4><p>Predicate plugin通过pod、nodeInfo作为参数，调用predicateGPU，根据计算结果对作业进行评估预选</p>
<h4 id="Task-topology"><a href="#Task-topology" class="headerlink" title="Task-topology"></a>Task-topology</h4><p>Task-topology算法是一种根据Job内task之间亲和性和反亲和性配置计算task优先级和Node优先级的算法。通过在Job内配置task之间的亲和性和反亲和性策略，并使用task-topology算法，可优先将具有亲和性配置的task调度到同一个节点上，将具有反亲和性配置的Pod调度到不同的节点上</p>
<h4 id="Nodeorder"><a href="#Nodeorder" class="headerlink" title="Nodeorder"></a>Nodeorder</h4><p>Nodeorder plugin是一种调度优选策略：通过模拟分配从各个维度为node打分，找到最适合当前作业的node。打分参数由用户来配置。参数包含了Affinity、reqResource，、LeastReqResource、MostReqResource、balanceReqResouce</p>
<p><strong>跟binpack一样, nodeorder的不同参数，对节点的调度结果有很大的不同</strong></p>
<h4 id="SLA"><a href="#SLA" class="headerlink" title="SLA"></a>SLA</h4><p>SLA的全称是Service Level agreement。用户向volcano提交job的时候，可能会给job增加特殊的约束，例如最长等待时间(JobWaitingTime)。这些约束条件可以视为用户与volcano之间的服务协议。SLA plugin可以为单个作业&#x2F;整个集群接收或者发送SLA参数</p>
<h4 id="Tdm"><a href="#Tdm" class="headerlink" title="Tdm"></a>Tdm</h4><p>Tdm的全称是Time Division Multiplexing。在一些场景中，一些节点既属于Kubernetes集群也属于Yarn集群。Tdm plugin 需要管理员为这些节点标记为<code>revocable node</code>。Tdm plugin会在该类节点可被撤销的时间段内尝试把<code>preemptable task</code>调度给<code>revocable node</code>，并在该时间段之外清除<code>revocable node</code>上的<code>preemptable task</code>。<strong>Tdm plugin提高了volcano在调度过程中节点资源的分时复用能力</strong></p>
<h4 id="Numa-aware"><a href="#Numa-aware" class="headerlink" title="Numa-aware"></a>Numa-aware</h4><p>当节点运行多个cpu密集的pod。基于pod是否可以迁移cpu已经调度周期cpu资源状况，工作负载可以迁移到不同的cpu核心下。许多工作负载对cpu资源迁移并不敏感。然而，有一些cpu的缓存亲和度以及调度延迟显著影响性能的工作负载，kubelet允许可选的cpu编排策略(cpu management)来确定节点上cpu资源的绑定分配</p>
<h3 id="CRD"><a href="#CRD" class="headerlink" title="CRD"></a>CRD</h3><p>volcano中有3个很重要的cr资源，queue、podgroup、 vcjob</p>
<h4 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h4><p>queue就是队列, <strong>在多租户场景下是个很重要的对象</strong></p>
<p>上一个最简单地例子:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.volcano.sh/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Queue</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">reclaimable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">capability:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;4&quot;</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">&quot;4096Mi&quot;</span></span><br></pre></td></tr></table></figure>

<ol>
<li>reclaimable: 表示该queue在资源使用量超过该queue所应得的资源份额时，是否允许其他queue回收该queue使用超额的资源,默认值为true</li>
<li>weight: 为 (weight&#x2F;total-weight) * total-resource(集群中所有资源的总和), weight是一个软约束</li>
<li>capability(可选): 表示该queue内所有podgroup使用资源量之和的上限，它是一个硬约束</li>
</ol>
<p>注意: queue capability需要与proportion插件一起使用，要不然不生效，即<strong>queue下的所有资源不可以超过capability总量</strong></p>
<p>那queue的weight计算逻辑是怎样的呢？<br>根据queue的weight为queue分配集群资源的逻辑大致如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每个queue deserved 初始为0</span></span><br><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> 每个queue &#123;</span><br><span class="line">        deserved = deserved + wrr</span><br><span class="line">        1. 如果 deserved 大于 capability，则取  deserved/capability/request 较小值，不管资源够不够用，meet了，不再继续分配资源了</span><br><span class="line">        2. 如果 deserved 小于 capability 大于 request，也是meet 了 退出循环</span><br><span class="line">        3. 如果 deserved 小于 request ，那就先给其它queue 分配资源，若还有剩余remaining 再来一轮</span><br><span class="line">    &#125;</span><br><span class="line">    更新remaining</span><br><span class="line">    如果上一次remaining 和这一次 没有变化或者没有remaining了，说明queue 都分配好了或没有资源了，退出分配过程</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>具体的计算方法将在volcano如何应对大规模任务系列之volcano源码解析一节中展开</p>
<h4 id="podgroup"><a href="#podgroup" class="headerlink" title="podgroup"></a>podgroup</h4><p>podgroup是一组强关联pod的集合, 例子如下:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.volcano.sh/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodGroup</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">ownerReferences:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">batch.volcano.sh/v1alpha1</span></span><br><span class="line">    <span class="attr">blockOwnerDeletion:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">controller:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">uid:</span> <span class="string">028ecfe8-0ff9-477d-836c-ac5676491a38</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minMember:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">minResources:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">&quot;2048Mi&quot;</span></span><br><span class="line">  <span class="attr">priorityClassName:</span> <span class="string">high-prority</span></span><br><span class="line">  <span class="attr">queue:</span> <span class="string">default</span></span><br></pre></td></tr></table></figure>
<p>关键字段</p>
<ul>
<li>minMember</li>
</ul>
<p>minMember表示该podgroup下<strong>最少</strong>需要运行的pod或任务数量。如果集群资源不满足miniMember数量任务的运行需求，调度器将不会调度任何一个该podgroup 内的任务。</p>
<ul>
<li>queue</li>
</ul>
<p>queue表示该podgroup所属的queue。queue必须提前已创建且状态为open。</p>
<ul>
<li>priorityClassName</li>
</ul>
<p>priorityClassName表示该podgroup的优先级，用于调度器为该queue中所有podgroup进行调度时进行排序。<strong>system-node-critical</strong>和<strong>system-cluster-critical</strong> 是2个预留的值，表示最高优先级。不特别指定时，默认使用default优先级或zero优先级。</p>
<ul>
<li>minResources</li>
</ul>
<p>minResources表示运行该podgroup所需要的最少资源。当集群可分配资源不满足minResources时，调度器将不会调度任何一个该podgroup内的任务。</p>
<h4 id="vcjob"><a href="#vcjob" class="headerlink" title="vcjob"></a>vcjob</h4><p>Volcano Job，简称vcjob，是Volcano自定义的Job资源类型。区别于Kubernetes Job，vcjob提供了更多高级功能，如可指定调度器、支持最小运行pod数、 支持task、支持生命周期管理、支持指定队列、支持优先级调度等。Volcano Job更加适用于机器学习、大数据、科学计算等高性能计算场景。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apiVersion: batch.volcano.sh/v1alpha1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: test-job</span><br><span class="line">spec:</span><br><span class="line">  minAvailable: 3</span><br><span class="line">  schedulerName: volcano</span><br><span class="line">  priorityClassName: high-priority</span><br><span class="line">  policies:</span><br><span class="line">    - event: PodEvicted</span><br><span class="line">      action: RestartJob</span><br><span class="line">  plugins:</span><br><span class="line">    ssh: []</span><br><span class="line">    env: []</span><br><span class="line">    svc: []</span><br><span class="line">  maxRetry: 5</span><br><span class="line">  queue: default</span><br><span class="line">  volumes:</span><br><span class="line">    - mountPath: &quot;/myinput&quot;</span><br><span class="line">    - mountPath: &quot;/myoutput&quot;</span><br><span class="line">      volumeClaimName: &quot;testvolumeclaimname&quot;</span><br><span class="line">      volumeClaim:</span><br><span class="line">        accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">        storageClassName: &quot;my-storage-class&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 1Gi</span><br><span class="line">  tasks:</span><br><span class="line">    - replicas: 6</span><br><span class="line">      name: &quot;default-nginx&quot;</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          name: web</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">            - image: nginx</span><br><span class="line">              imagePullPolicy: IfNotPresent</span><br><span class="line">              name: nginx</span><br><span class="line">              resources:</span><br><span class="line">                requests:</span><br><span class="line">                  cpu: &quot;1&quot;</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure>

<p> 关键字段</p>
<ul>
<li>schedulerName</li>
</ul>
<p>schedulerName表示该job的pod所使用的调度器，默认值为volcano，也可指定为default-scheduler。它也是tasks.template.spec.schedulerName的默认值。</p>
<ul>
<li>minAvailable</li>
</ul>
<p>minAvailable表示运行该job所要运行的<strong>最少</strong>pod数量。只有当job中处于running状态的pod数量不小于minAvailable时，才认为该job运行正常。</p>
<ul>
<li>volumes</li>
</ul>
<p>volumes表示该job的挂卷配置。volumes配置遵从kubernetes volumes配置要求。</p>
<ul>
<li>tasks.replicas</li>
</ul>
<p>tasks.replicas表示某个task pod的副本数。</p>
<ul>
<li>tasks.template</li>
</ul>
<p>tasks.template表示某个task pod的具体配置定义。</p>
<ul>
<li>tasks.policies</li>
</ul>
<p>tasks.policies表示某个task的生命周期策略。</p>
<ul>
<li>policies</li>
</ul>
<p>policies表示job中所有task的默认生命周期策略，在tasks.policies不配置时使用该策略。</p>
<ul>
<li>plugins</li>
</ul>
<p>plugins表示该job在调度过程中使用的插件。</p>
<ul>
<li>queue</li>
</ul>
<p>queue表示该job所属的队列。</p>
<ul>
<li>priorityClassName</li>
</ul>
<p>priorityClassName表示该job优先级，在抢占调度和优先级排序中生效。</p>
<ul>
<li>maxRetry</li>
</ul>
<p>maxRetry表示当该job可以进行的最大重启次数</p>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20231007170305.png"></p>
<h3 id="状态转换"><a href="#状态转换" class="headerlink" title="状态转换"></a>状态转换</h3><p>这里列一下podgroup对象的状态转换, 只有理解了这个状态转换后才能对生产上出现的意料之外的问题进行快速排查,在后文中也会多次出现这个转换过程</p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20231007170553.png"></p>
<p>本节大体介绍了volcano中比较重要的一些名词概念，volcano支持的东西比较多，所以不同的配置可能会产生截然不同的效果，接下来会结合生产情况来说明这些action&#x2F;plugin是如何协调工作的以及在哪些场景下怎么配置效果最佳</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://volcano.sh/">https://volcano.sh</a></li>
<li><a href="https://izsk.me/2023/08/31/volcano-introduction/">https://izsk.me/2023/08/31/volcano-introduction/</a></li>
<li><a href="https://qiankunli.github.io/2021/09/30/volcano.html">https://qiankunli.github.io/2021/09/30/volcano.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/349695188">https://zhuanlan.zhihu.com/p/349695188</a></li>
<li><a href="http://yost.top/2020/08/04/volcano-code-review">http://yost.top/2020/08/04/volcano-code-review</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>volcano如何应对大规模任务系列之volcano开篇介绍</title>
    <url>/2023/08/31/volcano-introduction/</url>
    <content><![CDATA[<p>volcano做为CNCF目前唯一一款应对大规模资源批调度工具被大家熟知.<br>作者负责的kubernetes集群每天都有大量的任务需要运行, GPU任务、短任务、长任务等等，同时还存在多租户场景、复杂的调度策略等, 依托volcano的高度可插拔能力, 同时结合业务场景进行相应的优化,极大提高了资源使用效率，<strong>结果导向明显</strong><br>在此也分享一下整个落地过程,也做为现阶段的一个工作总结, 工作之余尽量更新.</p>
<p>注: 业务各有不同, 作者的选型及观点可能并不适用其它人<br>此篇为: volcano如何应对大规模任务系列之volcano开篇介绍</p>
<span id="more"></span>

<p>本系列总体分为以下几块内容:<br>volcano如何应对大规模任务系列之volcano开篇介绍<br>volcano如何应对大规模任务系列之volcano关键对象<br>volcano如何应对大规模任务系列之volcano插件系统<br>volcano如何应对大规模任务系列之volcano源码解析<br>volcano如何应对大规模任务系列之volcano优化之道<br>volcano如何应对大规模任务系列之volcano生产实践<br>volcano如何应对大规模任务系列之volcano总结建议</p>
<h3 id="kube-batch"><a href="#kube-batch" class="headerlink" title="kube-batch"></a>kube-batch</h3><p>volcano是由另一款工具: kube-batch发展而来, kube-batch的创建者后来直接加入了华为创建了volcano项目 kube-batch的功能虽没有volcano那么复杂且已不再维护，但很多核心思想在volcano中沿用了下来，<br>建议大家可以看看它的源码,对volcano的入门还是很方便的。<br>感兴趣的可以看看作者之前对<a href="https://izsk.me/2021/03/15/Kubernetes-kubebatch-knows/">kube-batch的生产实践</a></p>
<h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>为什么会有volcano这款工具?<br>当然新版本的kube-scheduler是可以运行extend-resource机制来对kube-scheduler的在限能力进行自定义扩展，<br>但是在有这个能力之前, kube-scheduler的功能其实很单一，<strong>本质上它是个串行调度</strong>,不能很好地处理如HPC场景下的资源浪费问题, 因此kube-batch项目诞生，最有名的则是它实现了gang scheduler, 在迭代了多个版本且有很多生产实践后volcano诞生，同时加入了更加丰富的插件以应对更加复杂的业务场景</p>
<h3 id="When"><a href="#When" class="headerlink" title="When"></a>When</h3><p>什么时候可能会需要volcano这个工具?<br>虽然最新版volcano(v1.8.0)可以兼容kubernetes原生的所有类型对象，可以完全取代kube-scheduler调度器，但volcano毕竟还是更专注于批调度这个领域，因此，如果只是运行一些deployment、sts这类long-run的资源对象，引入volcano可能还是带来比较高的学习成本及加大系统的复杂性，并不会带来多大的收益<br>但如果kubernetes集群每天都面临需要调度大量的JOB, 对计算资源有比较高的使用率时, 还是有很大帮助, 可能有如下几个场景:</p>
<ol>
<li>Quota的弹性</li>
<li>多租户隔离</li>
<li>更复杂的算力环境、在混部、优先级等</li>
</ol>
<h3 id="How"><a href="#How" class="headerlink" title="How"></a>How</h3><p>如何落地volcano?<br><strong>首先当然是考虑业务场景，无论何时都不可能脱离业务谈技术</strong>, 作者做的最明智(后来被证实是很正确的)的一个选择就是: <strong>在打造基础设施早期就说服老板需要引入批调度器来应对未来可能出现的资源紧张问题</strong><br>作者推荐一种结果导向判断: 在分析业务模型及调研技术选型后, 如果可以得出<strong>算力不可能无限被满足</strong>这个结论，那么就可以考虑落地volcano,退一步讲, kubernetes集群本可以很好地支持多类型的调度器且volcano可以取代kube-scheduler.<br>但作者强烈建议: <strong>在落地之前，一定一定要充分理解volcano的插件系统, 不同的插件组合将产生不一样的调度结果，有时候结果可能完全不一样, 如果不能很好地理解, 那么总会出现<code>各类惊喜</code></strong></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://volcano.sh/">https://volcano.sh</a></li>
<li><a href="https://izsk.me/2021/03/15/Kubernetes-kubebatch-knows/">https://izsk.me/2021/03/15/Kubernetes-kubebatch-knows/</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>volcano如何应对大规模任务系列之volcano插件系统</title>
    <url>/2024/05/31/volcano-actions-plugins/</url>
    <content><![CDATA[<p>volcano做为CNCF目前唯一一款应对大规模资源批调度工具被大家熟知.<br>作者负责的kubernetes集群每天都有大量的任务需要运行, GPU任务、短任务、长任务等等，同时还存在多租户场景、复杂的调度策略等, 依托volcano的高度<strong>可插拔</strong>能力, 同时结合业务场景进行相应的优化,极大提高了资源使用效率，<strong>结果导向明显</strong><br>在此也分享一下整个落地过程,也做为现阶段的一个工作总结, 工作之余尽量更新.</p>
<p>注: 业务各有不同, 作者的选型及观点可能并不适用其它人<br>此篇为: volcano如何应对大规模任务系列之volcano插件系统</p>
<span id="more"></span>

<p>总体分为以下几块内容:<br><a href="https://izsk.me/2023/08/31/volcano-introduction/">volcano如何应对大规模任务系列之volcano开篇介绍</a><br><a href="https://izsk.me/2023/11/12/volcano-key-resources/">volcano如何应对大规模任务系列之volcano关键对象</a><br>volcano如何应对大规模任务系列之volcano插件系统<br>volcano如何应对大规模任务系列之volcano源码解析<br>volcano如何应对大规模任务系列之volcano优化之道<br>volcano如何应对大规模任务系列之volcano生产实践<br>volcano如何应对大规模任务系列之volcano总结建议</p>
<p>本系列的所用volcano版本基于v1.7.0</p>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20231007170305.png"></p>
<p>再来简单说一下volcano的调度流程: </p>
<p>每隔一个调度周期(默认是1s)开启一个Session对象，这个Session的初始化将集群中的节点、任务和队列的信息都会拷贝一份做为副本cache起来，然后依次执行action, action中会引用各plugin插件的实现逻辑</p>
<p>现在支持的插件列表主要可归为以下几类:</p>
<blockquote>
<ul>
<li><p>容量相关: 如proportion、capacity、resourcequota等</p>
</li>
<li><p>节点相关:  nodegroup、usage、numaaware等</p>
</li>
<li><p>任务相关: 剩下的基本就跟任务相关，比如priority、gang等</p>
</li>
</ul>
</blockquote>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><p>这里以一个典型的volcano配置为例展开</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># default configuration for scheduler</span></span><br><span class="line"><span class="attr">actions:</span> <span class="string">&quot;enqueue,allocate,backfill,preempt,reclaim&quot;</span></span><br><span class="line"><span class="attr">tiers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">priority</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">gang</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">conformance</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">drf</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">predicates</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proportion</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">resourcequota</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nodeorder</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">binpack</span></span><br></pre></td></tr></table></figure>

<p>首先解释一下这里为什么要分为多层的tiers?</p>
<p>在网上看到这个<a href="https://qiankunli.github.io/2021/09/30/volcano.html">解释</a>我觉得讲的清晰</p>
<p>每个plugin 注册了一堆函数，action 会在会在适当的实际调用<code>Session.函数()</code>执行。<code>Session.函数()</code>的大体逻辑都是遍历plugin 注册的所有函数并执行，<strong>每个plugin 只注册了跟自己逻辑有关的函数。</strong></p>
<p><code>Session.函数()</code>核心逻辑是两层循环，分为三种情况:</p>
<ol>
<li><p>“一言不合”直接返回的</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ssn *Session)</span></span> xx() xx&#123;</span><br><span class="line">    <span class="keyword">for</span> _, tier := <span class="keyword">range</span> ssn.Tiers &#123;</span><br><span class="line">        <span class="keyword">for</span> _, plugin := <span class="keyword">range</span> tier.Plugins &#123;</span><br><span class="line">            <span class="keyword">if</span>(xx)&#123;</span><br><span class="line">                <span class="keyword">return</span> xx</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> xx</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>所有plugin 一起配合计算的，比如给某个node 打分</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ssn *Session)</span></span> xx() xx&#123;</span><br><span class="line">    <span class="keyword">for</span> _, tier := <span class="keyword">range</span> ssn.Tiers &#123;</span><br><span class="line">        <span class="keyword">for</span> _, plugin := <span class="keyword">range</span> tier.Plugins &#123;</span><br><span class="line">            sum += xx</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>tier内 的所有plugin 参与计算。比如 Reclaimable 决定回收哪些正在运行的pod，即寻找victim。如果在第一层tier 中可以找到牺牲者 就直接返回了，毕竟能牺牲少点就牺牲少点，实在不行，才会计算第二层tier。</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ssn *Session)</span></span> xx() xx&#123;</span><br><span class="line">    <span class="keyword">var</span> victims []*api.TaskInfo</span><br><span class="line">    <span class="keyword">for</span> _, tier := <span class="keyword">range</span> ssn.Tiers &#123;</span><br><span class="line">        <span class="keyword">for</span> _, plugin := <span class="keyword">range</span> tier.Plugins &#123;</span><br><span class="line">            <span class="comment">// 寻找victim</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> victims != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> victims</span><br><span class="line">        &#125;         &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br></pre></td></tr></table></figure>

<p>前两种情况，是不需要区分两层的，此时所有的plugin 先后顺序是重要的，是不是在一个tier 里不重要，即要么立即结束要么全局聚合。第三种情况， tier内 的plugin<strong>局部聚合</strong>，两层for 之间做判断，如果有数据则 return。以默认的配置文件<code>scheudler.conf</code>来说，第一个tier 更多是基于用户设置，第二个tier 是基于task 和集群的实际情况，以用户设置为优先。</p>
</li>
</ol>
<p>要注意一点的是:如果config里配置的语法有误,比如当格式不对时，volcano-scheuler是不会panic退出的，它会直接使用内置的默认配置进行启动，我就在某一次配置变更时多写了一个冒号未及时发现，导致所有的调度策略都以意料之外的方式进行，跑了几天后才发现问题</p>
<h3 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h3><p>在<a href="https://izsk.me/2023/11/12/volcano-key-resources/">volcano如何应对大规模任务系列之volcano关键对象</a>一节中已经把现有的action对象都介绍了一遍, 这里不再赘述</p>
<p><code>action在volcano中就像流水线</code>,从上面的工作流图中可以看出，在一个session开始时，会将cache中拿到的所有相关的信息依次经过configuration中配置的action列表, 然后action中调用各插件的逻辑。<br>actions列表也是可选择的, 这里以enqueue为例。</p>
<p>enqueue校验pending job是否满足容量相关的条件, 如果满足，则可以从pending状态转到inqueue状态，那容量条件都有哪些呢? 从<code>pkg/scheduler/actions/enqueue.go</code>中可以看到</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 省略 代码其它 </span></span><br><span class="line"><span class="keyword">if</span> job.PodGroup.Spec.MinResources == <span class="literal">nil</span> || ssn.JobEnqueueable(job) &#123;</span><br><span class="line">   <span class="comment">// 目前只有overcommit中注册</span></span><br><span class="line">   ssn.JobEnqueued(job)</span><br><span class="line">   job.PodGroup.Status.Phase = scheduling.PodGroupInqueue</span><br><span class="line">   ssn.Jobs[job.UID] = job</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>enqueue.go中存在<code>ssn.JobEnqueueable(job)</code>, JobEnqueueable目前只在proportion、capacity、resourcequota三个plugin中有相关实现(如何知道???),从上面的配置文件来看，第一层的plugin并没有与容量相关的插件(如何知道???)，因此第一层会直接弃权(或者叫通过)，来到第二层，第二层里注册了proportion、resourcequota二个与容量相关的插件，那么这2个插件中一定会有<code>JobEnqueueable</code>的相关实现，这里以proportion为例</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/plugins/proportion/proportions.go</span></span><br><span class="line">ssn.AddJobEnqueueableFn(pp.Name(), <span class="function"><span class="keyword">func</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">    <span class="comment">// 省略相关代码</span></span><br><span class="line">    <span class="keyword">if</span> 容量检测通过 &#123;</span><br><span class="line">      <span class="keyword">return</span> util.Permit</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        util.Reject </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同样，在resourcequota plugin中也同样会有相同的逻辑,这两个插件需要同时满足，<code>ssn.JobEnqueueable(job)</code>才会满足,<br>这也很好理解, proportion是与queue的容量相关, 如果queue的容量无法满足这个job,那它自然需要继续pending,同理resourcequota是与namespacequota的容量相关，如果namespace的resourcequota资源无法满足这个job，那它还是需要pending。</p>
<p>当然也可以不配置enqueue这个action，那么所有的待调度的pending状态的job默认都直接到达inqueue状态</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/actions/allocate.go</span></span><br><span class="line"><span class="keyword">for</span> _, job := <span class="keyword">range</span> ssn.Jobs &#123;</span><br><span class="line">	<span class="comment">// If not config enqueue action, change Pending pg into Inqueue statue to avoid blocking job scheduling.</span></span><br><span class="line">	<span class="keyword">if</span> !conf.EnabledActionMap[<span class="string">&quot;enqueue&quot;</span>] &#123;</span><br><span class="line">		job.PodGroup.Status.Phase = scheduling.PodGroupInqueue</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>篇幅有限，源码以及上下文并没有说的很清楚，后续会有更详细的代码走读</p>
<p>由于actions相对来说比较稳定，其中的很多功能都是由plugin实现的，<code>plugins才是volcano中自由扩展的利器。</code></p>
<h3 id="plugins"><a href="#plugins" class="headerlink" title="plugins"></a>plugins</h3><p>在<code>pkg/scheduler/actions/plugins</code>下, 每一个目录实现了一种插件, 这些插件在调度过程会提供特定逻辑, 这里以priority为例来说明插件是什么时候起作用.<br>先试想一种典型的场景: 在集群中, 一般会存在紧急程度不一的各类job(这里的job是一种统称,可以是batchjob&#x2F;pytorchjob&#x2F;volcanojob&#x2F;deployment等等吧), 但是资源是永远不够的,那么我们期望的是在有限的资源下, 优先让紧急&#x2F;重要的job先运行起来, 不紧急的job可以排队<br>那priority如何发挥作用? 通过上面我们得知, plugins只在actions中”调用”，因此，当进入到enqueue action时<br>对于所有的jobs, enqueue会通过jobOrderFn这类的函数对所有的job进行排序, 排序的方法就在<code>pkg/schedule/plugins/priority/priority.go</code>中实现，方法可以是多样的，比如可以按PriorityClass、按创建时间等等，还自定义排序逻辑，只要能比对即可，最终生成一个heap map来存放所有job排序之后的结果提供到下一环节</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/actions/enqueue.go</span></span><br><span class="line"><span class="keyword">if</span> job.IsPending() &#123;</span><br><span class="line">    <span class="comment">// 如果在jobsMap中没有该job所属的queue信息</span></span><br><span class="line">    <span class="comment">// 则先将该queue加入到jobsMap中</span></span><br><span class="line">    <span class="keyword">if</span> _, found := jobsMap[job.Queue]; !found &#123;</span><br><span class="line">    jobsMap[job.Queue] = util.NewPriorityQueue(ssn.JobOrderFn)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>来看看<code>pkg/schedule/plugins/priority/priority.go</code>对jobOrderFn的实现</p>
<figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 其它代码</span></span><br><span class="line">jobOrderFn := <span class="function"><span class="keyword">func</span><span class="params">(l, r <span class="keyword">interface</span>&#123;&#125;)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">    lv := l.(*api.JobInfo)</span><br><span class="line">    rv := r.(*api.JobInfo)</span><br><span class="line"></span><br><span class="line">    klog.V(<span class="number">4</span>).Infof(<span class="string">&quot;Priority JobOrderFn: &lt;%v/%v&gt; priority: %d, &lt;%v/%v&gt; priority: %d&quot;</span>,</span><br><span class="line">        lv.Namespace, lv.Name, lv.Priority, rv.Namespace, rv.Name, rv.Priority)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> lv.Priority &gt; rv.Priority &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> lv.Priority &lt; rv.Priority &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ssn.AddJobOrderFn(pp.Name(), jobOrderFn)</span><br></pre></td></tr></table></figure>

<p>很简单，将jobOrderFn add到ssn的AddJobOrderFn中，jobOrderFn接受两上jobinfo的对象，通过比对这两个对象的Priority大小返回<br>当然, jobOrderFn可以在多个action中进行注册. enqueue是一种场景，preempt是另外一种场景, 按优先级进行回收, 即先回收优先级低的job, 那么在preempt中就会需要通过<code>ssn.JobOrderFn</code>来引用</p>
<p>说的可能会有些乱，比如上面的疑问，我怎么知道一个action中会调用哪些plugin, 或者说一个plugin都在哪些action中起作用以及在volcano system中如何将job封装成volcano中的job, pod如何封装成volcano中的task, 这些确实需要对照代码才能更加清晰, 好在volcano的代码风格还是比较统一, 有很多约定俗成的写法, 有些疑问可以先暂放，等走读代码后会有一个更直观的体验</p>
<p><code>如何将volcano结合进业务中发挥最大的价值,这个需要结合业务特性才能得出最符合业务架构的生产实践</code></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://volcano.sh/">https://volcano.sh</a></li>
<li><a href="https://qiankunli.github.io/2021/09/30/volcano.html">https://qiankunli.github.io/2021/09/30/volcano.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/349695188">https://zhuanlan.zhihu.com/p/349695188</a></li>
<li><a href="http://yost.top/2020/08/04/volcano-code-review">http://yost.top/2020/08/04/volcano-code-review</a></li>
<li><a href="https://izsk.me/2023/11/12/volcano-key-resources/">volcano如何应对大规模任务系列之volcano关键对象 | Z.S.K.’s Records</a></li>
<li><a href="https://izsk.me/2023/08/31/volcano-introduction/">volcano如何应对大规模任务系列之volcano开篇介绍 | Z.S.K.’s Records</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>HPC</category>
      </categories>
      <tags>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>访问https时为何会出现x509 certificate signed by unknown authority</title>
    <url>/2020/06/18/why-x509-error-when-curl-https/</url>
    <content><![CDATA[<p>今天排查了一个HTTPS证书的问题, 虽然很快的就解决了, 但里面涉及到的东西学是蛮多啊的，学习一下</p>
<span id="more"></span>

<p>问题是这样, 一个运行在容器中的服务给一个https地址发送POST请求时提示<code>x509: certificate signed by unknown authority</code></p>
<p>大家都知道, 一般https都需要通过ca认证，问题很显然, 证书认证不过, 但是为何会出这个问题呢?</p>
<p>要回答这个问题, 自然就引出另一个问题:<code>当我们访问https的问题, 一般不会带上跟证书相关的参数，那又是如何验证网站身份的呢</code></p>
<p>这里使用curl来模拟，效果一样</p>
<h3 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h3><p>在ubuntu上16.04， 内核<code>4.4.0-130-generic</code>上运行curl</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl https://baidu.com -v</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">* Rebuilt URL to: https://baidu.com/</span><br><span class="line">*   Trying 39.156.69.79...</span><br><span class="line">* Connected to baidu.com (39.156.69.79) port 443 (<span class="comment">#0)</span></span><br><span class="line">* found 148 certificates <span class="keyword">in</span> /etc/ssl/certs/ca-certificates.crt</span><br><span class="line">* found 592 certificates <span class="keyword">in</span> /etc/ssl/certs</span><br><span class="line">* ALPN, offering http/1.1</span><br><span class="line">* SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256</span><br><span class="line">* 	 server certificate verification OK</span><br><span class="line">* 	 server certificate status verification SKIPPED</span><br><span class="line">* 	 common name: www.baidu.cn (matched)</span><br><span class="line">* 	 server certificate expiration <span class="built_in">date</span> OK</span><br><span class="line">* 	 server certificate activation <span class="built_in">date</span> OK</span><br><span class="line">* 	 certificate public key: RSA</span><br><span class="line">* 	 certificate version: <span class="comment">#3</span></span><br><span class="line">* 	 subject: C=CN,ST=Beijing,O=BeiJing Baidu Netcom Science Technology Co.\, Ltd,OU=service operation department,CN=www.baidu.cn</span><br><span class="line">* 	 start <span class="built_in">date</span>: Thu, 27 Feb 2020 00:00:00 GMT</span><br><span class="line">* 	 expire <span class="built_in">date</span>: Fri, 26 Feb 2021 12:00:00 GMT</span><br><span class="line">* 	 issuer: C=US,O=DigiCert Inc,CN=DigiCert SHA2 Secure Server CA</span><br><span class="line">* 	 compression: NULL</span><br><span class="line">* ALPN, server accepted to use http/1.1</span><br><span class="line"><span class="comment"># ....</span></span><br></pre></td></tr></table></figure>

<p>关注重点信息:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">* found 148 certificates <span class="keyword">in</span> /etc/ssl/certs/ca-certificates.crt</span><br><span class="line">* found 592 certificates <span class="keyword">in</span> /etc/ssl/certs</span><br></pre></td></tr></table></figure>



<h3 id="x2F-etc-x2F-ssl"><a href="#x2F-etc-x2F-ssl" class="headerlink" title="&#x2F;etc&#x2F;ssl"></a>&#x2F;etc&#x2F;ssl</h3><p>从上面可以看出, 在<code>/etc/ssl</code>中发现大量的<code>certificates</code>, 可以来看看<code>/etc/ssl</code>,</p>
<p>这个目录在使用命令<code>apt install ca-certificates</code>后生成</p>
<p><code>/etc/ssl</code> 该目录下只有<code>certs  openssl.cnf  private</code></p>
<p>这几个文件,有用的为certs目录，这个目录下有大量跟证书相关的pem文件, 其中就包含<code>ca-certificates.crt</code>文件.</p>
<p>pem跟crt都是证书相关的文件，不同的格式罢了，这个不是重点</p>
<p>重点在于,在使用curl的时候，如果不带证书相关的参数,则会引用默认的证书路径(依操作系统不同而不同)</p>
<p>这个默认值怎么来确定呢? 可以确认是curl底层的代码根据环境因素定义的默认值，可以通过<code>strace</code>方式来查看</p>
<p><code>strace curl https://www.baidu.com |&amp; grep open</code></p>
<p><img src="https://raw.gitmirror.com/zhoushuke/BlogPhoto/master/githuboss/20200618211710.png"></p>
<p>从上面可以看到, curl如果不指定ca参数的话,则会到<code>/etc/ssl</code>目录下查找</p>
<p>同时, curl也是支持传递参数来实现https的访问</p>
<p>我们可以通过 <code>man curl</code>来查看几个重要的跟参数相关的参数,写的很详细，这里就不翻译了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--cacert &lt;CA certificate&gt;</span><br><span class="line">              (SSL)  Tells  curl  to use the specified certificate file to verify the peer. The file may contain multiple CA certificates. The cer‐</span><br><span class="line">              tificate(s) must be <span class="keyword">in</span> PEM format. Normally curl is built to use a default file <span class="keyword">for</span> this, so this option is typically used  to  alter</span><br><span class="line">              that default file.</span><br><span class="line">              curl  recognizes the environment variable named <span class="string">&#x27;CURL_CA_BUNDLE&#x27;</span> <span class="keyword">if</span> it is <span class="built_in">set</span>, and uses the given path as a path to a CA cert bundle.</span><br><span class="line">              This option overrides that variable.</span><br><span class="line">              The windows version of curl will automatically look <span class="keyword">for</span> a CA certs file named ´curl-ca-bundle.crt´, either <span class="keyword">in</span> the same  directory  as</span><br><span class="line">              curl.exe, or <span class="keyword">in</span> the Current Working Directory, or <span class="keyword">in</span> any folder along your PATH.</span><br><span class="line">              If curl is built against the NSS SSL library, the NSS PEM PKCS<span class="comment">#11 module (libnsspem.so) needs to be available for this option to work</span></span><br><span class="line">              properly.</span><br><span class="line">              If this option is used several <span class="built_in">times</span>, the last one will be used.</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--capath &lt;CA certificate directory&gt;</span><br><span class="line">              (SSL) Tells curl to use the specified certificate directory to verify the peer. Multiple paths can be  provided  by  separating  them</span><br><span class="line">              with  <span class="string">&quot;:&quot;</span>  (e.g.   <span class="string">&quot;path1:path2:path3&quot;</span>).  The certificates must be <span class="keyword">in</span> PEM format, and <span class="keyword">if</span> curl is built against OpenSSL, the directory</span><br><span class="line">              must have been processed using the c_rehash utility supplied with OpenSSL. Using --capath can allow OpenSSL-powered curl to make SSL-</span><br><span class="line">              connections much more efficiently than using --cacert <span class="keyword">if</span> the --cacert file contains many CA certificates.</span><br><span class="line">              If this option is <span class="built_in">set</span>, the default capath value will be ignored, and <span class="keyword">if</span> it is used several <span class="built_in">times</span>, the last one will be used.</span><br></pre></td></tr></table></figure>



<h3 id="ca-certificates"><a href="#ca-certificates" class="headerlink" title="ca-certificates"></a>ca-certificates</h3><p>ca-certificates则是一个包, 用于维护根证书库, 所有的 CA 根证书实际上是由 Mozilla 维护的</p>
<p>可以通过<code>dpkg  -L ca-certificates</code>来查看证书相关信息</p>
<p>也可以通过<code>apt-cache show ca-certificates</code> 查看相关信息，当然太多，看不出什么来,就是一堆证书</p>
<p>那么可能有人会问, 一般情况下, 系统安装好之后基本就不会再做操作了，那如何更新根证书呢?</p>
<p>有一个工具<code>update-ca-certificates</code>可以手动更新根证书信息, 可以使用<code>man </code>update-ca-certificates&#96;, 说的非常清楚</p>
<p>因此可以手工执行以下命令来更新根证书列表	</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">update-ca-certificates</span><br><span class="line"><span class="comment"># 全输出</span></span><br><span class="line">Updating certificates <span class="keyword">in</span> /etc/ssl/certs...</span><br><span class="line">0 added, 0 removed; <span class="keyword">done</span>.</span><br><span class="line">Running hooks <span class="keyword">in</span> /etc/ca-certificates/update.d...</span><br><span class="line"><span class="keyword">done</span>.</span><br></pre></td></tr></table></figure>



<h3 id="Resolve"><a href="#Resolve" class="headerlink" title="Resolve"></a>Resolve</h3><p>通过curl可以知道, 访问https的时候默认会到&#x2F;etc&#x2F;ssl目录下查找根证书，通过根证书验证对端网站的身份,因为对端的证书一般也是由这些根证书签名我, 因此可以难通过</p>
<p>那么出现开头的问题的原因在于, <strong>使用的镜像本身没有包含<code>/etc/ssl</code>目录，同时在Dockerfile中也没有使用<code>apt install ca-certificates</code>来安装, 因此在所有请求https时都会出现问题</strong>.</p>
<p>这里提一下， 在安装<code>curl</code>的时候，默认会安装<code>ca-certificates</code>，实际上这个包由 OpenSSL 安装的</p>
<p>Curl 是通过 OpenSSL 实现客户端 HTTPS 协议的，就是说在 Curl&#x2F;OpenSSL 平台下，Curl 使用的根证书库都是由 ca-certificates 包处理</p>
<p>curl也是一种上层应用, 对于其它的比如golang程序出现这个问题，原理都是一样的.</p>
<p>修改镜像，问题解决</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://manpages.ubuntu.com/manpages/bionic/man8/update-ca-certificates.8.html">http://manpages.ubuntu.com/manpages/bionic/man8/update-ca-certificates.8.html</a></li>
<li><a href="https://www.jianshu.com/p/abcee3270e9a">https://www.jianshu.com/p/abcee3270e9a</a></li>
<li><a href="https://curl.haxx.se/docs/sslcerts.html">https://curl.haxx.se/docs/sslcerts.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Http-Tcp-Ip</category>
      </categories>
      <tags>
        <tag>Http-Tcp-Ip</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix学习(zabbix介绍)</title>
    <url>/2016/10/27/zabbix%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80(zabbix%E4%BB%8B%E7%BB%8D)/</url>
    <content><![CDATA[<h3 id="zabbix介绍"><a href="#zabbix介绍" class="headerlink" title="zabbix介绍"></a><strong>zabbix介绍</strong></h3><p>引自网络: zabbix（音同 zæbix）是一个基于<strong>WEB界面的提供分布式系统</strong>监视以及网络监视功能的企业级的开源解决方案，zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位&#x2F;解决存在的各种问题。</p>
<span id="more"></span>

<p>zabbix构成:<strong>zabbix server与可选组件zabbix agent、zabbix proxy,当然还有java_gateway</strong>。zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器&#x2F;网络状态的监视，数据收集等功能，它可以运行在Linux，Solaris，HP-UX，AIX，Free BSD，Open BSD，OS X等平台上</p>
<p>agent端：主机通过安装agent方式采集数据。</p>
<p>server端：通过收集agent发送的数据，写入数据库（MySQL，ORACLE等）再通过php+apache在web前端展示</p>
<p>proxy端: 如果需要的监控主机很多,则可以使用proxy代理端来接收agent发送来的数据暂存在数据库中,然后由proxy来把这些数据发送给server端,减少server端的压力</p>
<p>zabbix可监控对象:</p>
<blockquote>
<ul>
<li>设备：服务器，路由器，交换机</li>
<li>软件：OS，网络，应用程序</li>
<li>主机性能指标监控</li>
<li>故障监控： down机，服务不可用，主机不可达</li>
<li>网络流量</li>
</ul>
</blockquote>
<h3 id="zabbix原理"><a href="#zabbix原理" class="headerlink" title="zabbix原理"></a><strong>zabbix原理</strong></h3><p>大概流程: <strong>zabbix agent需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端(或者proxy端)，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agent收集数据分为主动和被动两种模式:</strong></p>
<blockquote>
<ul>
<li>主动: agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server&#x2F;proxy</li>
<li>被动: server向agent请求获取监控项的数据，agent返回数据</li>
</ul>
</blockquote>
<p>下图是从网络找到的比较直观的zabbix流程图:</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-11.png" alt="zabbix-python-oracle-11"> </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-12.png" alt="zabbix-python-oracle-12"> </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-13.png" alt="zabbix-python-oracle-13"></p>
<h3 id="zabbix主要组件及进程"><a href="#zabbix主要组件及进程" class="headerlink" title="zabbix主要组件及进程"></a><strong>zabbix主要组件及进程</strong></h3><h4 id="zabbix主要组件"><a href="#zabbix主要组件" class="headerlink" title="zabbix主要组件"></a><strong>zabbix主要组件</strong></h4><blockquote>
<ol>
<li>Zabbix Server:负责接收agent发送的报告信息的核心组件，所有配置，统计数据及操作数据均由其组织进行</li>
<li>Agent:部署在被监控主机上，负责收集本地数据并发往Server端或Proxy端</li>
<li>Proxy: 可选组件，常用于分布监控环境中，代理Server收集部分被监控端的监控数据并统一发往Server端</li>
<li>Web interface: zabbix的GUI接口，通常与Server运行在同一台主机上</li>
<li>Database Storage: 专用于存储所有配置信息，以及由zabbix收集的数据,基本支持所有常见数据库</li>
</ol>
</blockquote>
<h4 id="zabbix进程"><a href="#zabbix进程" class="headerlink" title="zabbix进程"></a><strong>zabbix进程</strong></h4><blockquote>
<ol>
<li>zabbix_server: zabbix服务端守护进程。zabbix_agentd、zabbix_get、zabbix_sender、zabbix_proxy、zabbix_java_gateway的数据最终都是提交(主动)&#x2F;被提交(被动)到server</li>
<li>zabbix_agentd: 客户端守护进程，此进程收集客户端数据，例如cpu负载、内存、硬盘使用情况等</li>
<li>zabbix_proxy: zabbix代理守护进程。功能类似server，唯一不同的是它只是一个中转站，它需要把收集到的数据提交&#x2F;被提交到server里,主要是为了减少server端压力</li>
<li>zabbix_get: 通常在server或者proxy端执行获取远程客户端信息的命令</li>
<li>zabbix_sender: 用于发送数据给server或者proxy，通常用于耗时比较长的检查,使用sender主动提交数据</li>
<li>java_gateway: 使用JMX协议来获取java开发的程序相关数据,通常用来监控weblogic&#x2F;apache&#x2F;tomcat中间件,需要特别注意的是，它只能主动去获取数据，而不能被动获取数据。它的数据最终会给到server或者proxy</li>
</ol>
</blockquote>
<h3 id="zabbix常用架构"><a href="#zabbix常用架构" class="headerlink" title="zabbix常用架构"></a><strong>zabbix常用架构</strong></h3><h4 id="server"><a href="#server" class="headerlink" title="server"></a><strong>server</strong></h4><p>这是最简单的架构,只有一个server端,然后使用其它简单协议如ICMP,Ping,ODBC等获取数据,这适合出于安全原因无法在主机安装agent的情况下</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-introduce1.png" alt="zabbix-introduce1"></p>
<h4 id="server-agent"><a href="#server-agent" class="headerlink" title="server-agent"></a><strong>server-agent</strong></h4><p>这算是标配的架构了</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-introduce4.png" alt="zabbix-introduce4"></p>
<h4 id="server-proxy-agent"><a href="#server-proxy-agent" class="headerlink" title="server-proxy-agent"></a><strong>server-proxy-agent</strong></h4><p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-introduce2.png" alt="zabbix-introduce2"></p>
<h4 id="server-node-agent"><a href="#server-node-agent" class="headerlink" title="server-node-agent"></a><strong>server-node-agent</strong></h4><p>这个最复杂,本人也没使用过</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-introduce3.png" alt="zabbix-introduce3"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://www.cnblogs.com/irockcode/p/6752215.html">zabbix监控的基础概念、工作原理及架构</a></li>
<li><a href="www.zabbix.com">zabbix官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix学习(zabbix-agentd安装)</title>
    <url>/2016/11/03/zabbix%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%89(zabbix%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85)/</url>
    <content><![CDATA[<p>上一篇写了zabbix服务端的安装,这次是客户端,相对简单,其它的话就不多说了, 上来就是干吧</p>
<p>环境说明:</p>
<blockquote>
<ul>
<li>zabbix-3.2.6</li>
<li>centos-6.4&#x2F;windows2008</li>
</ul>
</blockquote>
<span id="more"></span>

<h3 id="Linux环境下"><a href="#Linux环境下" class="headerlink" title="Linux环境下"></a><strong>Linux环境下</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、新建zabbix用户并把zabbix用户密码修改为 zabbix</span></span><br><span class="line">[root@www ~]#groupadd zabbix</span><br><span class="line">[root@www ~]#useradd -g zabbix -m zabbix</span><br><span class="line">[root@www ~]#passwd zabbix</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、编译zabbix</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">zabbix-3.2.6.tar.gz放在/usr/local/src目录下</span></span><br><span class="line">[root@www ~]#cd /usr/local/src/</span><br><span class="line">[root@www ~]#chmod –R 775 zabbix-3.2.6.tar.gz</span><br><span class="line">[root@www ~]#tar xzvf zabbix-3.2.6.tar.gz</span><br><span class="line">[root@www ~]#cd zabbix-3.2.6</span><br><span class="line">[root@www ~]#./configure --prefix=/usr/local/zabbix --enable-agent</span><br><span class="line">[root@www ~]#make &amp;&amp; make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3、按实际情况修改Agentd配置文件以下内容,配置文件位于/usr/local/zabbix/etc/zabbix_agentd.conf</span></span><br><span class="line">[root@www ~]# vim /usr/local/zabbix/etc/zabbix_agentd.conf</span><br><span class="line">PidFile=/home/zabbix/zabbix_agentd.pid</span><br><span class="line">LogFile=/home/zabbix/zabbix_agentd.log</span><br><span class="line">EnableRemoteCommands=1       #允许zabbix-server远程执行命令</span><br><span class="line">Server=100.12.255.160        #zabbix-server/proxy服务器地址</span><br><span class="line">ListenPort=10050</span><br><span class="line">StartAgents=10               #开启获取数据进程个数</span><br><span class="line">ServerActive=100.12.255.160  #zabbix-server/proxy服务器地址</span><br><span class="line">Hostname=100.12.255.142      #可随意填写,但最好方便记忆,如没有填写,则直接取的是主机名</span><br><span class="line">Timeout=30                   #zabbix-agentd与zabbix-server通信的超时时间,单位为秒,最长为30s</span><br><span class="line">RefreshActiveChecks=300      #主动模式下多久去zabbix-server获取监控项列表</span><br><span class="line">UnsafeUserParameters=1       #可使用用户自定义para</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">注意:Hostname这里填写的内容必须跟zabbix前台配置主机时填写的Hostname保持一致(如下图),不然无法收集到数据</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4、修改/usr/local/zabbix目录所属用户为zabbix用户</span></span><br><span class="line">[root@www ~]#chown -R zabbix:zabbix /usr/local/zabbix</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">5、把zabbix_agentd加入开机启动服务</span></span><br><span class="line"><span class="meta prompt_">[root@www~]#</span><span class="language-bash"><span class="built_in">cd</span> /usr/local/src/zabbix-3.2.6/misc/init.d/</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">请根据操作系统进入相应的文件夹中,如在init.d文件夹中没有对应的操作系统,则统一选择tru64,本实验中为centos6.4,所以选择tru64</span></span><br><span class="line">[root@www ~]#cp tru64/zabbix_agentd /etc/init.d/zabbix_agentd</span><br><span class="line">[root@www ~]#chmod a+x /etc/init.d/zabbix_agentd</span><br><span class="line">[root@www ~]#chown zabbix:zabbix /etc/init.d/zabbix_agentd</span><br><span class="line">[root@www ~]#vim /etc/init.d/zabbix_agentd</span><br><span class="line"></span><br><span class="line">SERVICE=&quot;Zabbix agent&quot;</span><br><span class="line">DAEMON=/usr/local/zabbix/sbin/zabbix_agentd</span><br><span class="line">PIDFILE=/home/zabbix/zabbix_agentd.pid</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">6、启动zabbix_agentd</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">切换到zabbix用户启动</span></span><br><span class="line">[root@www ~]#su – zabbix</span><br><span class="line">[zabbix@www ~]$ service zabbix_agentd start</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">7、运行结果显示：Zabbix agent started 为正常启动，如果启动异常,可查看/home/zabbix/zabbix_agentd.log中查看报错信息</span></span><br></pre></td></tr></table></figure>

<h3 id="Windows环境下"><a href="#Windows环境下" class="headerlink" title="Windows环境下"></a><strong>Windows环境下</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、把zabbix_agents_3.2.0.win.zip解压到C盘根目录并重命名为zabbix</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、进入zabbix目录中,新建名为logs的文件夹</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3、进入conf文件夹,修改zabbix_agentd.win.conf以下部分如下所示</span></span><br><span class="line">LogFile=c:\zabbix\logs\zabbix_agentd.log</span><br><span class="line">EnableRemoteCommands=1</span><br><span class="line">Server=100.12.255.160</span><br><span class="line">ListenPort=10050</span><br><span class="line">StartAgents=10</span><br><span class="line">ServerActive=100.12.255.160</span><br><span class="line">Hostname=100.12.255.101</span><br><span class="line">RefreshActiveChecks=300</span><br><span class="line">Timeout=30</span><br><span class="line">UnsafeUserParameters=1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">说明: Server与ServerActive请修改为zabbix_server服务器的ip,Hostname修改为本机的ip地址便于记忆</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4、打开命令行CMD运行以下命令,根据操作系统位数执行相应的exe</span></span><br><span class="line">C:\Users\pc091&gt;C:\zabbix\bin\win64\zabbix_agentd.exe –c c:\zabbix\conf\zabbix_agentd.win.conf –i</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">5、再执行</span></span><br><span class="line">C:\Users\pc091&gt;C:\zabbix\bin\win64\zabbix_agentd.exe</span><br><span class="line">–c c:\zabbix\conf\zabbix_agentd.win.conf –s</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">6、可在windows任务管理器中看到zabbix_agentd进程已在进行</span></span><br></pre></td></tr></table></figure>

<p>到此基本客户端基本安装完了</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.zabbix.com/manuals">zabbix</a></li>
<li><a href="http://www.361way.com/zabbix-summarize/3335.html">zabbix概述</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix学习(zabbix-server安装)</title>
    <url>/2016/10/30/zabbix%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%BA%8C(zabbix%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%AE%89%E8%A3%85)/</url>
    <content><![CDATA[<p>其它的话就不多说了, 上来说是干吧</p>
<p>环境说明:</p>
<blockquote>
<ul>
<li>zabbix-3.2.6</li>
<li>centos-6.4</li>
<li>mysql-5.7.18</li>
<li>nginx-1.8</li>
<li>php-5.6</li>
</ul>
</blockquote>
<span id="more"></span>

<h3 id="NLMP环境安装"><a href="#NLMP环境安装" class="headerlink" title="NLMP环境安装"></a><strong>NLMP环境安装</strong></h3><h4 id="mysql-5-7-18安装"><a href="#mysql-5-7-18安装" class="headerlink" title="mysql-5.7.18安装"></a><strong>mysql-5.7.18安装</strong></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、安装mysql依赖</span></span><br><span class="line">[root@www ~]# yum install gcc gcc-c++ zlib-devel libtool ncurses-devel openssh-clients cmake ncurses bison</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、新建mysql用户并将mysql系统用户的密码修改为mysql</span></span><br><span class="line">[root@www ~]# groupadd mysql</span><br><span class="line">[root@www ~]# useradd -g mysql mysql</span><br><span class="line">[root@www ~]# passwd mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3、mysql-5.7需要安装boost_1_59_0.tar.gz,且编译器使用cmake</span></span><br><span class="line">[root@www ~]# tar zxvf boost_1_59_0.tar.gz</span><br><span class="line">[root@www ~]# tar mysql5.7.tar.gz</span><br><span class="line">[root@www ~]# cd mysql5.7</span><br><span class="line">[root@www ~]# cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql \</span><br><span class="line">-DMYSQL_DATADIR=/data/mysql \</span><br><span class="line">-DWITH_BOOST=../boost_1_59_0 \</span><br><span class="line">-DMYSQL_UNIX_ADDR=/tmp/mysql.sock \</span><br><span class="line">-DSYSCONFDIR=/etc \</span><br><span class="line">-DWITH_INNOBASE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_PARTITION_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_FEDERATED_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_MYISAM_STORAGE_ENGINE=1 \</span><br><span class="line">-DMYSQL_TCP_PORT=3306 \</span><br><span class="line">-DEXTRA_CHARSETS=all \</span><br><span class="line">-DDEFAULT_CHARSET=utf8 \</span><br><span class="line">-DDEFAULT_COLLATION=utf8_general_ci \</span><br><span class="line">-DWITH_DEBUG=0</span><br><span class="line">[root@www ~]# make &amp;&amp; make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4、初始化mysql</span></span><br><span class="line">[root@www ~]# cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld</span><br><span class="line">[root@www ~]# chmod a+x /etc/init.d/mysqld </span><br><span class="line">[root@www ~]# chown -R mysql:mysql /etc/init.d/mysqld</span><br><span class="line">[root@www ~]# chown -R mysql:mysql /usr/local/mysql</span><br><span class="line">[root@www ~]# chown -R mysql:mysql /data/mysql</span><br><span class="line">[root@www ~]# chown -R mysql:mysql /etc/my.cnf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">5、修改mysql配置文件/etc/my.cnf</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">6、加入开机启动</span></span><br><span class="line">[root@www ~]# chkconfig --add mysqld</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">7、把mysql路径添加到/etc/ld.so.conf</span></span><br><span class="line">[root@www ~]# echo &quot;/usr/local/mysql/include/&quot; &gt;&gt;/etc/ld.so.conf</span><br><span class="line">[root@www ~]# echo &quot;/usr/local/mysql/bin/&quot; &gt;&gt;/etc/ld.so.conf</span><br><span class="line">[root@www ~]# ldconfig</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">8、修改/etc/profile在最后添加</span></span><br><span class="line">[root@www ~]# vi /etc/profile</span><br><span class="line">export PATH=/usr/local/mysql/bin:$PATH</span><br><span class="line">[root@www ~]# source /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">9、执行</span></span><br><span class="line">[root@www ~]#mysqld –tmpdir=/home/mysql/mysqltmp --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">10、使用mysql用户启动mysql</span></span><br><span class="line">[mysql@www ~]$ service mysqld start</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">11、切换回root,修改mysql数据库中root用户的登陆密码</span></span><br><span class="line">[root@www ~]# mysql_secure_installation</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">一路输入Y即可,其中会询问设置密码的强度,请根据自身需要设置,如果选择密码强度为高,则密码需要符合强度要求才能继续,不然会提示密码强度不够,这里密码修改为Mysql0000<span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">12、给root用户授权</span></span><br><span class="line">[root@www ~]# mysql -uroot -pMysql0000#</span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">use mysql;</span></span><br><span class="line">mysql &gt; grant all privileges on *.* to &#x27;root&#x27;@localhost;</span><br><span class="line">mysql &gt; grant all privileges on *.* to &#x27;root&#x27;@’100.12.255.160’;</span><br><span class="line">mysql &gt; flush privileges;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">根据实际情况改为服务器的ip地址</span></span><br></pre></td></tr></table></figure>

<h4 id="PHP安装"><a href="#PHP安装" class="headerlink" title="PHP安装"></a><strong>PHP安装</strong></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、php依赖关系</span></span><br><span class="line">[root@www ~]# yum -y install gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers gd gd2 gd-devel gd2-devel perl-CPAN pcre-devel</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、注意：centos源不能安装libmcrypt-devel,由于版权的原因没有自带mcrypt的包</span></span><br><span class="line">使用php mcrypt 前必须先安装Libmcrypt 这个需要源码编译安装,且版本要大于2.5.6</span><br><span class="line">[root@www ~]# tar zxvf libmcrypt.tar.gz</span><br><span class="line">[root@www ~]# ./configure</span><br><span class="line">[root@www ~]# make &amp;&amp; make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3、对php的编译要求如下,不然的话在最后前端生成zabbix前台的时候模块会检验不过:</span></span><br><span class="line">[root@www ~]#./configure --prefix=/usr/local/php --with-config-file-path=/etc/php/ --with-bz2 --with-curl --enable-mbstring --enable-calendar --enable-gd-native-ttf --enable-ftp --enable-sockets --enable-dom --enable-shmop --enable-fpm --enable-ctype --enable-session --enable-xmlwriter --enable-xmlreader --enable-bcmath --enable-xml --enable-opcache --enable-sysvmsg --enable-sysvsem --enable-zip --disable-ipv6 --disable-rpath --disable-debug --disable-fileinfo --with-libdir=lib64 --with-gd --with-jpeg-dir=/usr/local --with-png-dir=/usr/local --with-freetype-dir=/usr/local --with-iconv-dir=/usr/local --with-gettext --with-libxml-dir=/usr/local --with-zlib --with-mysql --with-curl --with-pdo-mysql=mysqlnd --with-mysqli=mysqlnd --with-mysql=mysqlnd --with-openssl --with-mhash --with-xmlrpc --with-mcrypt</span><br><span class="line">[root@www ~]# make &amp;&amp; make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4、编译安装完成之后 在/usr/local/php/etc/或是/etc/php下都没有php.ini文件 所以需要<span class="built_in">cp</span>一个模板文件到/etc/php/下</span></span><br><span class="line">[root@www ~]# cp php.ini-production /etc/php/php.ini</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">打开php.ini配置文件，找到如下参数,修改为如下值，否则zabbix安装不了</span></span><br><span class="line"></span><br><span class="line">upload_max_filesize = 2M</span><br><span class="line">max_execution_time = 300</span><br><span class="line">max_input_time = 300</span><br><span class="line">memory_limit = 128M</span><br><span class="line">post_max_size = 32M</span><br><span class="line">date.timezone = Asia/Shanghai</span><br><span class="line">mbstring.func_overload = 0</span><br><span class="line">session.auto_start = 0</span><br><span class="line">always_populate_raw_post_data = -1</span><br><span class="line"></span><br><span class="line">[root@www ~]# cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf</span><br><span class="line">[root@www ~]# cp /usr/local/src/php-5.6.30/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm</span><br><span class="line">[root@www ~]# chmod a+x /etc/init.d/php-fpm</span><br><span class="line">[root@www ~]# chkconfig --add php-fpm</span><br><span class="line">[root@www ~]# /etc/init.d/php-fpm start</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">若访问前台报502 bad gateway时,修改 /usr/local/php/etc/php-fpm.ini里global中的pid=run/php-fpm.pid前的注释去掉 重启php-fpm</span></span><br></pre></td></tr></table></figure>

<h4 id="Nginx1-8安装"><a href="#Nginx1-8安装" class="headerlink" title="Nginx1.8安装"></a><strong>Nginx1.8安装</strong></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、新建www用户</span></span><br><span class="line">[root@www ~]# groupadd www </span><br><span class="line">[root@www ~]# useradd -g www www</span><br><span class="line">[root@www ~]# passwd www</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">密码修改为www</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、把nginx-1.8.0.tar.gz放到/usr/local/src</span></span><br><span class="line">[root@www ~]# tar zxvf nginx-1.8.0.tar.gz</span><br><span class="line">[root@www ~]# cd nginx-1.8.0</span><br><span class="line">[root@www ~]# ./configure --user=www --group=www --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module</span><br><span class="line">[root@www ~]# cd /usr/local/nginx/conf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3. 修改nginx的配置文件nginx.conf</span></span><br></pre></td></tr></table></figure>

<h3 id="Zabbix3安装"><a href="#Zabbix3安装" class="headerlink" title="Zabbix3安装"></a><strong>Zabbix3安装</strong></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、zabbix依赖</span></span><br><span class="line">[root@www ~]# yum install gcc mysql-devel net-snmp-devel net-snmp curl-devel perl-DBI php-gd php-ctype php-mysql php-bcmath php-mbstring  php-xml php-session php-sockets php-gettext php-xmlreader php-xmlwriter libxml2-devel libcurl-devel unixODBC unixODBC-devel libssh2-devel libssh2 OpenIPMI-devel OpenIPMI OpenIPMI-libs</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、新建系统用户</span></span><br><span class="line">[root@www ~]#groupadd zabbix</span><br><span class="line">[root@www ~]#useradd -g zabbix -m zabbix</span><br><span class="line">[root@www ~]#passwd zabbix</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">密码修改为 zabbix</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3、创建zabbix数据库,新建的mysql数据库用户zabbix,密码也设为zabbix</span></span><br><span class="line">[root@www ~]#mysql -uroot -pMysql0000#</span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">use mysql;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">create database zabbix character <span class="built_in">set</span> utf8;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">grant all privileges on zabbix.* to zabbix@<span class="string">&#x27;100.12.255.160&#x27;</span> identified by <span class="string">&#x27;Zabbix0000#&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">grant all privileges on zabbix.* to zabbix@<span class="string">&#x27;localhost&#x27;</span> identified by <span class="string">&#x27;Zabbix0000#&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">grant all privileges on zabbix.* to zabbix@<span class="string">&#x27;127.0.0.1&#x27;</span> identified by <span class="string">&#x27;Zabbix0000#&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash">flush privileges;</span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span><span class="language-bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">红色标注的为zabbix服务器的地址,zabbix0000<span class="comment">#为数据库用户zabbix设置的密码</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4、从zabbix.tar.gz源文件中导入zabbix数据库文件</span></span><br><span class="line">[root@www ~]# tar -zxvf zabbix-3.2.6.tar.gz</span><br><span class="line">[root@www ~]# cd zabbix-3.2.6/database/mysql</span><br><span class="line">[root@www ~]# mysql -uroot -pMysql0000# zabbix &lt; schema.sql</span><br><span class="line">[root@www ~]# mysql -uroot -pMysql0000# zabbix &lt; images.sql </span><br><span class="line">[root@www ~]# mysql -uroot -pMysql0000# zabbix &lt; data.sql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">5、服务器端编译</span></span><br><span class="line">[root@www ~]# ./configure --prefix=/usr/local/zabbix --enable-server --enable-agent --enable-java --with-mysql --with-net-snmp --with-libcurl --with-libxml2 --with-openssl --with-unixodbc --with-ssh2 --with-openipmi</span><br><span class="line">[root@www ~]# make &amp;&amp; make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">6、添加服务端口：(可能这个配置文件中已有如下内容则不需要添加)</span></span><br><span class="line">[root@www ~]# vim /etc/services</span><br><span class="line">zabbix-agent 10050/tcp   # Zabbix Agent</span><br><span class="line">zabbix-agent 10050/udp   # Zabbix Agent</span><br><span class="line">zabbix-trapper 10051/tcp # Zabbix Trapper</span><br><span class="line">zabbix-trapper 10051/udp # Zabbix Trapper</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">7、服务端copy源码包中的前端文件</span></span><br><span class="line">[root@www ~]# mkdir -p /usr/local/nginx/html/zabbix </span><br><span class="line">[root@www ~]# cp -rf /usr/local/src/zabbix-3.2.6/frontends/php/* /usr/local/nginx/html/zabbix/   #虚拟主机目录</span><br><span class="line">[root@www ~]# chown -R zabbix:zabbix /usr/local/nginx/html/zabbix</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">8、修改zabbix的GUI配置文件：</span></span><br><span class="line"><span class="meta prompt_">[root@www~]#</span><span class="language-bash"><span class="built_in">cp</span> /usr/local/nginx/html/zabbix/conf/zabbix.conf.php.default /usr/local/nginx/html/zabbix/conf/zabbix.conf.php</span></span><br><span class="line">[root@www ~]# vim /usr/local/nginx/html/zabbix/conf/zabbix.conf.php</span><br><span class="line">&lt;?php</span><br><span class="line">// Zabbix GUI configuration file.</span><br><span class="line">global $DB;</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;TYPE&#x27;</span>]     = <span class="string">&#x27;MYSQL&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;SERVER&#x27;</span>]   = <span class="string">&#x27;192.168.150.160&#x27;</span>;  <span class="comment">#这里必须填写zabbix_server真实地址,默认的#127.0.0.1在启动zabbix_server时web会报is not running错误</span></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;PORT&#x27;</span>]     = <span class="string">&#x27;3306&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;DATABASE&#x27;</span>] = <span class="string">&#x27;zabbix&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;USER&#x27;</span>]     = <span class="string">&#x27;zabbix&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;PASSWORD&#x27;</span>] = <span class="string">&#x27;Zabbix0000#&#x27;</span>;</span></span><br><span class="line">// Schema name. Used for IBM DB2 and PostgreSQL.</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">DB[<span class="string">&#x27;SCHEMA&#x27;</span>] = <span class="string">&#x27;&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">ZBX_SERVER      = <span class="string">&#x27;192.168.150.160&#x27;</span>;  <span class="comment">#这里必须填写zabbix_server真实地址,默认的#127.0.0.1在启动zabbix_server时web会报is not running错误</span></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">ZBX_SERVER_PORT = <span class="string">&#x27;10051&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">ZBX_SERVER_NAME = <span class="string">&#x27;&#x27;</span>;</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">IMAGE_FORMAT_DEFAULT = IMAGE_FORMAT_PNG;</span></span><br><span class="line">?&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">9、修改zabbix_server.conf</span></span><br><span class="line">[root@www ~]# vi /usr/local/zabbix/etc/zabbix_server.conf</span><br><span class="line">LogFile=/tmp/zabbix_server.log</span><br><span class="line">PidFile=/tmp/zabbix_server.pid</span><br><span class="line">DBSocket=/tmp/mysql.sock</span><br><span class="line">DBName=zabbix                #指定zabbix数据库</span><br><span class="line">DBUser=zabbix                #指定zabbix数据库用户</span><br><span class="line">DBPassword=Zabbix0000#       #指定zabbix数据库密码</span><br><span class="line">DBPort=3306                  #指定zabbix数据库端口</span><br><span class="line">ListenIP=192.168.150.160     #服务器IP地址</span><br><span class="line">Timeout=30</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">10、[root@www ~]<span class="comment"># chown -R zabbix:zabbix /usr/local/zabbix</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">11、把zabbix_server加入开机启动服务</span></span><br><span class="line">[root@www ~]# cp /usr/local/src/zabbix-3.2.6/misc/init.d/tru64/zabbix_server</span><br><span class="line">[root@www ~]# chmod a+x /etc/init.d/zabbix_agentd</span><br><span class="line">[root@www ~]# chown zabbix:zabbix /etc/init.d/zabbix_server</span><br><span class="line">[root@www ~]# vi /etc/init.d/zabbix_agentd</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">按照实际情况修改以下三行</span></span><br><span class="line">SERVICE=&quot;Zabbix server&quot;</span><br><span class="line">DAEMON=/usr/local/zabbix/sbin/zabbix_server</span><br><span class="line">PIDFILE=/tmp/zabbix_server.pid</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">12、若启动zabbix_server 提示如下错误</span></span><br><span class="line">./zabbix_server: error while loading shared libraries: libmysqlclient.so.20: cannot open shared object file: No such file or directory</span><br><span class="line">[root@www ~]#vim /etc/ld.so.conf</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在最后一行添加 /usr/local/mysql/lib</span></span><br><span class="line">[root@www ~]#ldconfig</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://www.zabbix.com/manuals">zabbix</a></li>
<li><a href="http://www.361way.com/zabbix-summarize/3335.html">zabbix概述</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix学习(批量监控指定网页)</title>
    <url>/2017/08/28/zabbix%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%BA%94(%E6%89%B9%E9%87%8F%E7%9B%91%E6%8E%A7%E6%8C%87%E5%AE%9A%E7%BD%91%E9%A1%B5)/</url>
    <content><![CDATA[<p>有时需要监控业务系统特定的入口页面及页面的响应速度,节点数少的话还能写个小脚本使用curl处理,数量一多的话就有点力不从心了,而且需要自动报警功能,使用脚本不太容易实现了,特别是处于内网环境,使用zabbix的http模板则很容易解决这类问题,思路也非常简单:脚本(shell&#x2F;python均可)+zabbix的simple check即能实现</p>
<span id="more"></span>

<h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a><strong>脚本</strong></h3><p>这里还是使用curl一个网址,得到返回值,一般情况下正常能够访问返回值为200,维护一个WEB.txt,shell脚本从该文件中读取url,需要注意一点的是,<strong>这里使用到了zabbix的自动发现功能,所以返回格式严格为json格式</strong>,shell脚本如下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat web_site_code_status.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">function</span>:monitor tcp connect status from zabbix</span> </span><br><span class="line"></span><br><span class="line">source /etc/bashrc &gt;/dev/null 2&gt;&amp;1 </span><br><span class="line">source /etc/profile  &gt;/dev/null 2&gt;&amp;1 </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">/usr/bin/curl -o /dev/null -s -w %&#123;http_code&#125; <span class="variable">$1</span></span></span><br><span class="line"></span><br><span class="line">web_site_discovery () &#123; </span><br><span class="line">WEB_SITE=($(cat /usr/local/zabbix/sbin/WEB.txt|grep -v &quot;^#&quot;)) </span><br><span class="line">        printf &#x27;&#123;\n&#x27; </span><br><span class="line">        printf &#x27;\t&quot;data&quot;:[\n&#x27; </span><br><span class="line">for((i=0;i&lt;$&#123;#WEB_SITE[@]&#125;;++i)) </span><br><span class="line">&#123; </span><br><span class="line">num=$(echo $(($&#123;#WEB_SITE[@]&#125;-1))) </span><br><span class="line">        if [ &quot;$i&quot; != $&#123;num&#125; ]; </span><br><span class="line">                then </span><br><span class="line">        printf &quot;\t\t&#123; \n&quot; </span><br><span class="line">        printf &quot;\t\t\t\&quot;&#123;#SITENAME&#125;\&quot;:\&quot;$&#123;WEB_SITE[$i]&#125;\&quot;&#125;,\n&quot; </span><br><span class="line">                else </span><br><span class="line">                        printf  &quot;\t\t&#123; \n&quot; </span><br><span class="line">                        printf  &quot;\t\t\t\&quot;&#123;#SITENAME&#125;\&quot;:\&quot;$&#123;WEB_SITE[$num]&#125;\&quot;&#125;]&#125;\n&quot; </span><br><span class="line">        fi </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">web_site_code () &#123; </span><br><span class="line">/usr/bin/curl -o /dev/null -s -w %&#123;http_code&#125; $1 </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">case &quot;$1&quot; in </span><br><span class="line">web_site_discovery) </span><br><span class="line">web_site_discovery </span><br><span class="line">;;</span><br><span class="line">web_site_code) </span><br><span class="line">web_site_code $2 </span><br><span class="line">;; </span><br><span class="line">*) </span><br><span class="line"></span><br><span class="line">echo &quot;Usage:$0 &#123;web_site_discovery|web_site_code [URL]&#125;&quot; </span><br><span class="line">;; </span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>WEB.txt的格式如下,一个url对应一行:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://127.0.0.1/index.html</span><br><span class="line">http://127.0.0.1:7002/</span><br><span class="line">http://baidu.com/</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>把脚本加入crontab每分钟执行一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*/1 * * * * root /usr/local/zabbix/script/web_site_code_status.sh</span><br></pre></td></tr></table></figure>

<p>觉得shell脚本的格式比较费劲的等方面,也可以使用python,脚本如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment">#这个函数主要是构造出一个特定格式的字典，用于zabbix</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">web_site_discovery</span>():</span><br><span class="line">    web_list=[]</span><br><span class="line">    web_dict=&#123;<span class="string">&quot;data&quot;</span>:<span class="literal">None</span>&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;WEB.txt&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f: </span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> f:</span><br><span class="line">            url_dict=&#123;&#125;</span><br><span class="line">            url_dict[<span class="string">&quot;&#123;#SITENAME&#125;&quot;</span>]=url.strip()</span><br><span class="line">            web_list.append(url_dict)</span><br><span class="line"></span><br><span class="line">    web_dict[<span class="string">&quot;data&quot;</span>]=web_list</span><br><span class="line">    jsonStr = json.dumps(web_dict, sort_keys=<span class="literal">True</span>, indent=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> jsonStr</span><br><span class="line"></span><br><span class="line"><span class="comment">#这个函数主要是用于测试站点返回的状态码，注意在cmd命令中如果有%&#123;&#125;这种字符要使用占位符代替,否则会报错</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">web_site_code</span>():</span><br><span class="line">       cmd=<span class="string">&#x27;curl -o /dev/null -s -w %s %s&#x27;</span> %(<span class="string">&quot;%&#123;http_code&#125;&quot;</span>,sys.argv[<span class="number">2</span>])</span><br><span class="line">       reply_code=os.popen(cmd).readlines()[<span class="number">0</span>]</span><br><span class="line">       <span class="keyword">return</span> reply_code</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">if</span> sys.argv[<span class="number">1</span>] == <span class="string">&quot;web_site_discovery&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span> web_site_discovery()</span><br><span class="line">    <span class="keyword">elif</span> sys.argv[<span class="number">1</span>] == <span class="string">&quot;web_site_code&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span> web_site_code()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Pls sys.argv[0] web_site_discovery | web_site_code [URL]&quot;</span></span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> msg:</span><br><span class="line">        <span class="built_in">print</span> msg</span><br><span class="line"><span class="comment">#这里对脚本传递进来的第一个参数做判断去执行不同的函数，为什么要这样，因为通过一个脚本写了两个功能</span></span><br></pre></td></tr></table></figure>

<h3 id="修改zabbix-agentd-conf"><a href="#修改zabbix-agentd-conf" class="headerlink" title="修改zabbix_agentd.conf"></a><strong>修改zabbix_agentd.conf</strong></h3><p>在客户端的zabbix_agentd.conf中增加自定义的key</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">UnsafeUserParameters=<span class="number">1</span></span><br><span class="line">UserParameter=web_site_discovery,/etc/zabbix/scripts/web_site_moniter.py web_site_discovery</span><br><span class="line">UserParameter=web_site_code[*],/etc/zabbix/scripts/web_site_moniter.py web_site_code $<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>在服务器端测试返回值,返回数据如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试url返回值</span></span><br><span class="line">zabbix_get -s <span class="number">100.12</span><span class="number">.255</span><span class="number">.160</span> -p <span class="number">10050</span> -k web_site_code[http://baidu.com/]</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="comment">#测试zabbix自动发现</span></span><br><span class="line">zabbix_get -s <span class="number">100.12</span><span class="number">.255</span><span class="number">.160</span> -p <span class="number">10050</span> -k web_site_discovery </span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;data&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;&#123;#SITENAME&#125;&quot;</span>: <span class="string">&quot;http://127.0.0.1/index.html&quot;</span></span><br><span class="line">        &#125;, </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;&#123;#SITENAME&#125;&quot;</span>: <span class="string">&quot;http://127.0.0.1:7001/&quot;</span></span><br><span class="line">        &#125;, </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;&#123;#SITENAME&#125;&quot;</span>: <span class="string">&quot;http://www.baidu.com/&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="zabbix前端配置"><a href="#zabbix前端配置" class="headerlink" title="zabbix前端配置"></a><strong>zabbix前端配置</strong></h3><p>大家直接看图吧</p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url1.png" alt="zabbix-web-url1"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url2.png" alt="zabbix-web-url2"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url3.png" alt="zabbix-web-url3"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url4.png" alt="zabbix-web-url4"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url5.png" alt="zabbix-web-url5"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url6.png" alt="zabbix-web-url6"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url7.png" alt="zabbix-web-url7"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url8.png" alt="zabbix-web-url8"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url9.png" alt="zabbix-web-url9"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url10.png" alt="zabbix-web-url10"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url11.png" alt="zabbix-web-url11"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url12.png" alt="zabbix-web-url12"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url13.png" alt="zabbix-web-url13"></p>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-web-url14.png" alt="zabbix-web-url14"></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://crazy123.blog.51cto.com/1029610/1711148">python实现zabbix批量监控WEB网站和批量监控主机端口</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix学习(利用python实现对oracle实时监控)</title>
    <url>/2016/12/25/zabbix%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E5%9B%9B(%E5%88%A9%E7%94%A8python%E5%AE%9E%E7%8E%B0%E5%AF%B9oracle%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7)/</url>
    <content><![CDATA[<h3 id="Pyora介绍"><a href="#Pyora介绍" class="headerlink" title="Pyora介绍"></a><strong>Pyora介绍</strong></h3><p>首先要介绍下：cx_Oracle ，这是一个使用 python 开发的  python连接Oracle数据库的驱动，有了这个驱动，python 就可以与Oracle 交互了 </p>
<p>Pyora 就是利用 cx_Oracle 完成与Oracle的连接， 然后利用 sql 来获取 Oracle 数据库的状态，达到监控目的的 </p>
<span id="more"></span>

<p>所以要安装 Pyora ，首先就得安装 数据库驱动： cx_Oracle,然后在安装 Pyora ，最后Zabbix_agentd 利用Userparameter，调用Pyora 脚本来 获取监控数据 </p>
<p>有了Pyora后，监控就变成了这样了，我们不需要在数据库上 安装任何软件，只需要找一个能部署abbix_agent的节点，而这个节点最好啥都不要运行，专用来搜集监控 数据库，从而这个节点就成了 监控数据库的 中心节点. </p>
<h3 id="Oracleclient11g-安装"><a href="#Oracleclient11g-安装" class="headerlink" title="Oracleclient11g 安装"></a><strong>Oracleclient11g 安装</strong></h3><blockquote>
<ul>
<li>oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>oracle-instantclient11.2-devel-11.2.0.3.0-1.x86_64.rpm</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>oracle-instantclient11.2-jdbc-11.2.0.3.0-1.x86_64.rpm</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>oracle-instantclient11.2-sqlplus-11.2.0.3.0-1.x86_64.rpm</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm</span><br><span class="line">#其它的安装类似如上命令...</span><br></pre></td></tr></table></figure>

<h4 id="填加tnsanme信息"><a href="#填加tnsanme信息" class="headerlink" title="填加tnsanme信息"></a><strong>填加tnsanme信息</strong></h4><p>创建文件夹nework&#x2F;admin，整个路径如：&#x2F;usr&#x2F;lib&#x2F;oracle&#x2F;12.1&#x2F;client64&#x2F;network&#x2F;admin 加入tnsnames.ora </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">shell</span><br><span class="line">#tnames.ora Network Configuration File: /opt/oracle/app/product/11.2.0/dbhome_1/network/admin/tnsnames.ora </span><br><span class="line">#Generated by Oracle configuration tools. </span><br><span class="line">ORCL = </span><br><span class="line">(DESCRIPTION = </span><br><span class="line">(ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.14.31)(PORT = 1521)) </span><br><span class="line">(CONNECT_DATA = </span><br><span class="line">(SERVER = DEDICATED) </span><br><span class="line">(SERVICE_NAME = orcl) </span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<h4 id="配置环境信息"><a href="#配置环境信息" class="headerlink" title="配置环境信息"></a><strong>配置环境信息</strong></h4><p>请打开 &#x2F;etc&#x2F;profile 文件，在这个文件里面追加如下内容： </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export ORACLE_BASE=/usr/lib/oracle/11.2 </span><br><span class="line">export ORACLE_HOME=$ORACLE_BASE/client64 </span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH </span><br><span class="line">export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 </span><br><span class="line">export SQLPATH=/usr/lib/oracle/11.2/client64/network/admin </span><br><span class="line">export TNS_ADMIN=/home/oracle/network/admin </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">运行 <span class="comment">#&gt; source /etc/profile 命令使该profile立即生效</span></span> </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">再次运行vi .bash_profile ，加入下面这个：</span> </span><br><span class="line">export PATH=$PATH:$HOME/bin:$ORACLE_HOME/bin </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/lib:$ORACLE_HOME/lib:/usr/local/python27/lib:$LD_LIBRARY_PATH </span><br></pre></td></tr></table></figure>

<h4 id="验证是否登陆成功"><a href="#验证是否登陆成功" class="headerlink" title="验证是否登陆成功"></a><strong>验证是否登陆成功</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sqlplus xxx/xxx@192.168.14.31:1521/orcl </span><br></pre></td></tr></table></figure>

<h3 id="Python安装"><a href="#Python安装" class="headerlink" title="Python安装"></a><strong>Python安装</strong></h3><p>系统自带的Python 2.6.6 对 Pyora 支持不好 编译python27时加上-enable-shared 否则不会生成libpython2.7.so.1.0的动态链接库，不加默认生成libpython2.7.so.a的静态链接库 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure --prefix=/usr/local/python27 -enable-shared </span><br><span class="line"> make &amp;&amp; make install </span><br></pre></td></tr></table></figure>

<p>没有报错Python安装成功<br>如果启动python时报以下错误: </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-1.png" alt="zabbix-python-oracle-1"> </p>
<p>解决如下: </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-2.png" alt="zabbix-python-oracle-2"> </p>
<h3 id="cx-Oracle安装"><a href="#cx-Oracle安装" class="headerlink" title="cx_Oracle安装"></a><strong>cx_Oracle安装</strong></h3><p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-3.png" alt="zabbix-python-oracle-3"> </p>
<h3 id="Pyora-安装"><a href="#Pyora-安装" class="headerlink" title="Pyora 安装"></a><strong>Pyora 安装</strong></h3><p>离线安装Pyora时需要提前安装好setuptools及 argparse模块 </p>
<p>写入下面的参数到 &#x2F;usr&#x2F;local&#x2F;zabbix&#x2F;etc&#x2F;zabbix_agentd.conf 并重启 zabbix_agent 读取新配置 ​</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">UserParameter=pyora[*],/usr/local/python27 /home/zabbix/scripts/pyora.py --username $1 --password $2 --address $3 --database $4 $5 $6 $7 $8</span><br></pre></td></tr></table></figure>

<p>在Pyora 的包里，别人已经为我们做好了模板，直接导入模板：zabbix-template&#x2F;Pyora.xml即可 </p>
<p>导入模板后，需要稍作修改，以适用于我们 自己的监控需求，你监控那个Oracle 数据库，只需在 模板级别的宏里面，知道其相应信息，让后将模板 套到 该zabbix_agent上 ，这样zabbix_agent 就可以调用脚本来完成监控 远程的数据库服务器了 </p>
<p>填入宏前,要手动调试,看看 Python脚本是否能 连接数据库正常 获取数据,以及zabbix server 能否正常通信。 </p>
<p>zbbix_agent 端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/python27/bin/python /usr/local/zabbix/shared/exectscripts/pyora.py --username $1 --password $2 --address $3 --database $4 $5 $6 $7 $8</span><br></pre></td></tr></table></figure>

<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-5.png" alt="zabbix-python-oracle-5"> </p>
<p>zabbix_server端： </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-4.png" alt="zabbix-python-oracle-4"> </p>
<p>这些内容 填在 模板 宏变量出：例如 </p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-6.png" alt="zabbix-python-oracle-6"> </p>
<p>监控的结果：</p>
<p><img src="https://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/zabbix-python-oracle-7.png" alt="zabbix-python-oracle-7"> </p>
<p>最后， Oracle 表空间的 自动发现，大概需要1个小时 左右，所以 请耐心等待 ，而 脚本 也能 根据 自己需求 ，自定义一些 SQL 调用 </p>
<p>不错一个工具 ，至少比 那些zabbix 监控 Oracle 插件  方便多了. </p>
<h3 id="zabbix使用pyora的工作原理"><a href="#zabbix使用pyora的工作原理" class="headerlink" title="zabbix使用pyora的工作原理"></a><strong>zabbix使用pyora的工作原理</strong></h3><p>通过在zabbix_agentd.conf中加入UserParameter&#x3D;<key>,<command> ,key必须整个系统唯一，配置好之后，重启客户端 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">UserParameter=pyora[*],/usr/local/python27/python /home/zabbix/scripts/pyora.py --username $1 --password $2 --address $3 --database $4 $5 $6 $7 $8 </span><br></pre></td></tr></table></figure>

<p>在zabbix GUI中设定主机宏来指定zabbix_agentd调用pyora时运行时需要的参数信息,zabbix基于宏保存预设的文本模式,并在调用时将其替换为其中的文本,可理解为全局变量,最大可返回512k数据, </p>
<p>带*参数的UserParameter可批量监控多个同类监控项 </p>
<p>如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">UserParameter=memory.usage[*]，/bin/cat  /proc/meminfo | awk &#x27;/^$1:/&#123;print $2&#125;&#x27; </span><br></pre></td></tr></table></figure>

<p>server端的调用的key为memory.usage[MemFree] ，获得被监控主机剩余内存的数值 </p>
<p>server端的调用的key为memory.usage[MemTotal} ，获得被监控主机总内存的数值 </p>
<p>UserParameter 详情请<a href="http://gzsamlee.blog.51cto.com/9976612/1866779">参考这里</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://gzsamlee.blog.51cto.com/9976612/1866779">http://gzsamlee.blog.51cto.com/9976612/1866779</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><a href="https://github.com/bicofino/Pyora">https://github.com/bicofino/Pyora</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>分布式架构</category>
      </categories>
      <tags>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title>使用python库pycurl来检测web服务质量</title>
    <url>/2017/12/27/%E4%BD%BF%E7%94%A8python%E5%BA%93pycurl%E6%9D%A5%E6%A3%80%E6%B5%8Bweb%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/</url>
    <content><![CDATA[<p><a href="http://pycurl.io/">pycurl</a>是一个用C语言写的libcurl Python实现,速度很快,比urllib和httplib都快功能非常强大,支持的操作协议有FTP、HTTP、HTTPS、TELNET等,可以理解成Linux下curl命令功能的Python封装,简单易用.在最近参与的自动化运维平台上使用它检测web应用节点的性能参数,利用libcurl包提供的这些常量值来达到探测Web服务质量的目的.</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># close()：对应libcurl包中的curl_easy_cleanup方法，无参数，实现关闭、回收Curl对象。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform(): 对应libcurl包中的curl_easy_perform方法，无参数，实现Curl对象请求的提交。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#setopt(option, value): 对应libcurl包中的curl_easy_setopt方法，参数option是通过libcurl的常量来指定的，参数value的值会依赖option，可以是一个字符串、整型、长整型、文件对象、列表或函数等。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面列举常用的常量列表：</span></span><br><span class="line"></span><br><span class="line">c = pycurl.Curl()    <span class="comment">#创建一个curl对象</span></span><br><span class="line">c.setopt(pycurl.CONNECTTIMEOUT, <span class="number">5</span>)    <span class="comment">#连接的等待时间，设置为0则不等待</span></span><br><span class="line">c.setopt(pycurl.TIMEOUT, <span class="number">5</span>)    <span class="comment">#请求超时时间</span></span><br><span class="line">c.setopt(pycurl.NOPROGRESS, <span class="number">0</span>)    <span class="comment">#是否屏蔽下载进度条，非0则屏蔽</span></span><br><span class="line">c.setopt(pycurl.MAXREDIRS, <span class="number">5</span>)    <span class="comment">#指定HTTP重定向的最大数</span></span><br><span class="line">c.setopt(pycurl.FORBID_REUSE, <span class="number">1</span>)    <span class="comment">#完成交互后强制断开连接，不重用</span></span><br><span class="line">c.setopt(pycurl.FRESH_CONNECT,<span class="number">1</span>)    <span class="comment">#强制获取新的连接，即替代缓存中的连接</span></span><br><span class="line">c.setopt(pycurl.DNS_CACHE_TIMEOUT,<span class="number">60</span>)    <span class="comment">#设置保存DNS信息的时间，默认为120秒</span></span><br><span class="line">c.setopt(pycurl.URL,<span class="string">&quot;http://www.baidu.com&quot;</span>)    <span class="comment">#指定请求的URL</span></span><br><span class="line">c.setopt(pycurl.USERAGENT,<span class="string">&quot;Mozilla/5.2 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50324)&quot;</span>)    <span class="comment">#配置请求HTTP头的User-Agent</span></span><br><span class="line">c.setopt(pycurl.HEADERFUNCTION, getheader)  <span class="comment">#将返回的HTTP HEADER定向到回调函数getheader</span></span><br><span class="line">c.setopt(pycurl.WRITEFUNCTION, getbody)    <span class="comment">#将返回的内容定向到回调函数getbody</span></span><br><span class="line">c.setopt(pycurl.WRITEHEADER, fileobj)     <span class="comment">#将返回的HTTP HEADER定向到fileobj文件对象</span></span><br><span class="line">c.setopt(pycurl.WRITEDATA, fileobj)    <span class="comment">#将返回的HTML内容定向到fileobj文件对象</span></span><br><span class="line">getinfo(option)     <span class="comment">#对应libcurl包中的curl_easy_getinfo方法，参数option是通过libcurl的常量来指定的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面列举常用的常量列表：</span></span><br><span class="line">c = pycurl.Curl()    <span class="comment">#创建一个curl对象</span></span><br><span class="line">c.getinfo(pycurl.HTTP_CODE)    <span class="comment">#返回的HTTP状态码</span></span><br><span class="line">c.getinfo(pycurl.TOTAL_TIME)    <span class="comment">#传输结束所消耗的总时间</span></span><br><span class="line">c.getinfo(pycurl.NAMELOOKUP_TIME)    <span class="comment">#DNS解析所消耗的时间</span></span><br><span class="line">c.getinfo(pycurl.CONNECT_TIME)    <span class="comment">#建立连接所消耗的时间</span></span><br><span class="line">c.getinfo(pycurl.PRETRANSFER_TIME)    <span class="comment">#从建立连接到准备传输所消耗的时间</span></span><br><span class="line">c.getinfo(pycurl.STARTTRANSFER_TIME)    <span class="comment">#从建立连接到传输开始消耗的时间</span></span><br><span class="line">c.getinfo(pycurl.REDIRECT_TIME)    <span class="comment">#重定向所消耗的时间</span></span><br><span class="line">c.getinfo(pycurl.SIZE_UPLOAD)    <span class="comment">#上传数据包大小</span></span><br><span class="line">c.getinfo(pycurl.SIZE_DOWNLOAD)    <span class="comment">#下载数据包大小</span></span><br><span class="line">c.getinfo(pycurl.SPEED_DOWNLOAD)    <span class="comment">#平均下载速度</span></span><br><span class="line">c.getinfo(pycurl.SPEED_UPLOAD)    <span class="comment">#平均上传速度</span></span><br><span class="line">c.getinfo(pycurl.HEADER_SIZE)    <span class="comment">#HTTP头部大小</span></span><br></pre></td></tr></table></figure>

<p>脚本如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pycurl</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    探测web服务质量</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">URL = <span class="string">&#x27;http://www.XXX.com&#x27;</span>           <span class="comment">#探测的目标URL</span></span><br><span class="line">c = pycurl.Curl()                    <span class="comment">#创建一个Curl对象</span></span><br><span class="line">c.setopt(pycurl.URL,URL)             <span class="comment">#定义请求的URL常量</span></span><br><span class="line">c.setopt(pycurl.CONNECTTIMEOUT,<span class="number">10</span>)   <span class="comment">#定义请求的等待连接时间</span></span><br><span class="line">c.setopt(pycurl.TIMEOUT,<span class="number">20</span>)          <span class="comment">#定义请求的超时时间</span></span><br><span class="line">c.setopt(pycurl.NOPROGRESS,<span class="number">1</span>)        <span class="comment">#屏蔽下载进度条</span></span><br><span class="line">c.setopt(pycurl.FORBID_REUSE,<span class="number">1</span>)      <span class="comment">#完成交互后强制断开连接,不重用</span></span><br><span class="line">c.setopt(pycurl.MAXREDIRS,<span class="number">1</span>)         <span class="comment">#指定HTTP重定向的最大数为1</span></span><br><span class="line">c.setopt(pycurl.DNS_CACHE_TIMEOUT,<span class="number">30</span>)<span class="comment">#设置保存DNS信息的时间为30秒</span></span><br><span class="line">c.setopt(c.USERAGENT, <span class="string">&#x27;Foo&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个文件对象,以‘wb’方式打开,用来存储返回的http头部及页面内容</span></span><br><span class="line">indexfile = <span class="built_in">open</span>(os.path.dirname(os.path.realpath(__file__))+<span class="string">&#x27;/content&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line"><span class="comment">#os.path.realpath(__file__) 返回指定的文件名真实路径,不使用任何符号链接。os.path.dirname返回文件路径</span></span><br><span class="line"></span><br><span class="line">c.setopt(pycurl.WRITEHEADER,indexfile)  <span class="comment">#将返回的HTTP HEADER定向到indefile文件对象</span></span><br><span class="line">c.setopt(pycurl.WRITEDATA,indexfile)    <span class="comment">#将返回的HTML内容定向到indexfile文件对象</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    c.perform()                     <span class="comment">#提交请求</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;connection error:&#x27;</span> + <span class="built_in">str</span>(e))</span><br><span class="line">    indexfile.close()</span><br><span class="line">    c.close()</span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line">NAMELOOKUP_TIME = c.getinfo(c.NAMELOOKUP_TIME)      <span class="comment">#获取DNS解析时间    </span></span><br><span class="line">CONNECT_TIME = c.getinfo(c.CONNECT_TIME)            <span class="comment">#获取连接建立的时间</span></span><br><span class="line">PRETRANSFER_TIME = c.getinfo(c.PRETRANSFER_TIME)    <span class="comment">#获取从连接建立到准备传输所消耗的时间</span></span><br><span class="line">STARTTRANSFER_TIME = c.getinfo(c.STARTTRANSFER_TIME)<span class="comment">#获取从连接建立到传输开始所消耗的时间</span></span><br><span class="line">TOTAL_TIME = c.getinfo(c.TOTAL_TIME)                <span class="comment">#获取传输的总时间</span></span><br><span class="line">HTTP_CODE = c.getinfo(c.HTTP_CODE)                  <span class="comment">#获取 HTTP 状态码</span></span><br><span class="line">SIZE_DOWNLOAD = c.getinfo(c.SIZE_DOWNLOAD)          <span class="comment">#获取下载数据包大小</span></span><br><span class="line">HEADER_SIZE = c.getinfo(c.HEADER_SIZE)              <span class="comment">#获取 HTTP 头部大小</span></span><br><span class="line">SPEED_DOWNLOAD = c.getinfo(c.SPEED_DOWNLOAD)        <span class="comment">#获取平均下载速度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印输出相关数据</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HTTP状态码: %s&#x27;</span> %(HTTP_CODE))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;DNS解析状态：%.2f ms&#x27;</span> %(NAMELOOKUP_TIME*<span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;建立连接时间: %.2f ms&#x27;</span> %(CONNECT_TIME*<span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;准备传输时间：%.2f ms&#x27;</span> %(PRETRANSFER_TIME*<span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;传输开始时间：%.2f ms&#x27;</span> %(STARTTRANSFER_TIME*<span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;传输结束总时间：%.2f ms&#x27;</span> %(TOTAL_TIME*<span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;下载数据包大小：%d bytes/s&#x27;</span> %(SIZE_DOWNLOAD))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HTTP 头部大小：%d bytes/s&#x27;</span> %(HEADER_SIZE))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平均下载速度：%d bytes/s&#x27;</span> %(SPEED_DOWNLOAD))</span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭文件及curl对象</span></span><br><span class="line">indexfile.close()</span><br><span class="line">c.close()</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python web_check.py</span></span><br><span class="line"><span class="comment">#HTTP状态码: 200</span></span><br><span class="line"><span class="comment">#DNS解析状态：5055.88 ms</span></span><br><span class="line"><span class="comment">#建立连接时间: 5100.66 ms</span></span><br><span class="line"><span class="comment">#准备传输时间：5100.67 ms</span></span><br><span class="line"><span class="comment">#传输开始时间：5581.72 ms</span></span><br><span class="line"><span class="comment">#传输结束总时间：5627.53 ms</span></span><br><span class="line"><span class="comment">#下载数据包大小：32662 bytes/s</span></span><br><span class="line"><span class="comment">#HTTP头部大小：248 bytes/s</span></span><br><span class="line"><span class="comment">#平均下载速度：5803 bytes/s</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看http头部及页面内容</span></span><br><span class="line"><span class="comment"># cat content </span></span><br><span class="line">HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">Date: Thu, <span class="number">19</span> Nov <span class="number">2015</span> <span class="number">10</span>:<span class="number">28</span>:<span class="number">24</span> GMT</span><br><span class="line">Server: Apache/<span class="number">2.4</span><span class="number">.3</span> (Unix) OpenSSL/<span class="number">1.0</span><span class="number">.1</span>e-fips</span><br><span class="line">X-Powered-By: PHP/<span class="number">5.4</span><span class="number">.26</span></span><br><span class="line">X-Pingback: http://www.tianfeiyu.com/xmlrpc.php</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Content-<span class="type">Type</span>: text/html; charset=UTF-<span class="number">8</span></span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="http://pycurl.io/">http://pycurl.io/</a></li>
<li><a href="http://python.jqlinux.com/pythonchang-yong-di-san-fang-3001-nei-zhi-mo-kuai/pythonzhi-pycurl.html">http://python.jqlinux.com/pythonchang-yong-di-san-fang-3001-nei-zhi-mo-kuai/pythonzhi-pycurl.html</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>合并K个有序数组(python)</title>
    <url>/2019/03/02/%E5%90%88%E5%B9%B6K%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84(python)/</url>
    <content><![CDATA[<p>题目是: 给定K个有序数组,将它们合并成一个数组并且有序, 需要考虑时间复杂度.</p>
<p>这也是leetcode上的第23道题,由合并两个数组延生而来. </p>
<span id="more"></span>

<h3 id="K个数组合并"><a href="#K个数组合并" class="headerlink" title="K个数组合并"></a><strong>K个数组合并</strong></h3><p>这里做个假设, 有序数组的个数为 L</p>
<p>所有数组中, 最长数组的长度为 N, 每个数组中的元素个数可以不等</p>
<p>很正常的会想到写二层for循环相邻两两进行合并,但时间复杂度肯定过不去</p>
<p>可以使用最小堆, python中很少情况会想到使用堆, 开始还以为python中没有, 顺便研究了下<a href="https://docs.python.org/3/library/heapq.html">heap</a>,具体思路为: </p>
<blockquote>
<ol>
<li>维护一个最小堆, 堆的个数等于链表个数, 为L</li>
<li>维护一个新链表,为D</li>
<li>初始化堆中的元素为每个链表的第一个元素</li>
<li>由最小堆的特性,堆的最顶层元素为最小值,每次取该最小值并记录该最小值所在有序链表的索引</li>
<li>将该最小值添加到D中</li>
<li>再把该最小值所在的链表的下一个元素入堆(如果存在的话)</li>
<li>循环4-6直到所有元素都已添加到D中</li>
</ol>
</blockquote>
<p><img src="http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/merge_list_python.jpg" alt="merge_list_python"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">list_merge</span>(<span class="params">*lists</span>):</span><br><span class="line">    <span class="comment">#入参判断, 这里直接pass</span></span><br><span class="line">    <span class="comment">#将所有链表转化为deque,方便使用popleft获取链表的最左元素及根据索引返回该索引对应的剩余链表</span></span><br><span class="line">    queues = [queue <span class="keyword">for</span> queue <span class="keyword">in</span> <span class="built_in">map</span>(deque, lists)]</span><br><span class="line">    heap = []</span><br><span class="line">    <span class="comment">#初始化链表,该链表中的元素为元组, 各个链表的第一个元素及链表所在索引</span></span><br><span class="line">    <span class="keyword">for</span> i, lst <span class="keyword">in</span> <span class="built_in">enumerate</span>(queues):</span><br><span class="line">        heap.append((lst.popleft(), i))</span><br><span class="line">    <span class="comment">#将链表转换成最小堆</span></span><br><span class="line">    heapq.heapify(heap)</span><br><span class="line">    <span class="comment">#链表: 用于存放每次获取的堆顶层元素</span></span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> heap:</span><br><span class="line">        <span class="comment">#将堆顶层元素出堆</span></span><br><span class="line">        value, index = heapq.heappop(heap)</span><br><span class="line">        <span class="comment">#将顶层元素追加</span></span><br><span class="line">        result.append(value)</span><br><span class="line">        <span class="comment">#根据索引获取对应链表的剩余元素</span></span><br><span class="line">        <span class="keyword">if</span> queues[index]:</span><br><span class="line">             <span class="comment">#如果存在下一个元素,则将该元素及索引入堆</span></span><br><span class="line">            heapq.heappush(heap, (queues[index].popleft(), index))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> list_merge(*[[<span class="number">4</span>, <span class="number">8</span>, <span class="number">20</span>], [<span class="number">100</span>, <span class="number">200</span>, <span class="number">350</span>, <span class="number">370</span>], [<span class="number">5</span>, <span class="number">8</span>, <span class="number">350</span>, <span class="number">500</span>, <span class="number">1000</span>]])</span><br></pre></td></tr></table></figure>

<p>通过索引来获取对应数组的下一个元素, 使用的非常巧妙</p>
<p>这里主要使用到了heapq跟deque,大家可以到官网查看这两个的用法</p>
<h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a><strong>复杂度分析</strong></h3><blockquote>
<ul>
<li>时间复杂度: 由于每个元素都需要读取一次, 即最大次数为L*N, 将每一个元素插入最小堆中的复杂度为O(logL),即总的复杂度为O(L*NlogL)</li>
<li>空间复杂度为: 维护最小堆的大小,即 O(L)</li>
</ul>
</blockquote>
<h3 id="在实际中的运用"><a href="#在实际中的运用" class="headerlink" title="在实际中的运用"></a><strong>在实际中的运用</strong></h3><p>在分布式任务聚合后需要按照时间顺序统一排序的场景下会有这个需要,linux下也有一些实现,可参照<a href="https://blog.csdn.net/dog250/article/details/80234049">这篇文章</a></p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://docs.python.org/3/library/heapq.html">https://docs.python.org/3/library/heapq.html</a></li>
<li><a href="https://docs.python.org/3/library/collections.html#deque-objects">https://docs.python.org/3/library/collections.html#deque-objects</a></li>
<li><a href="https://leetcode.com/problems/merge-k-sorted-lists/solution/">https://leetcode.com/problems/merge-k-sorted-lists/solution/</a></li>
<li><a href="https://blog.csdn.net/dog250/article/details/80234049">合并N个有序链表与FQ公平调度</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>寻找重复数(python)</title>
    <url>/2019/04/21/%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0(python)/</url>
    <content><![CDATA[<p>题目是: 给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数</p>
<p>这也是leetcode上的第287道题. </p>
<span id="more"></span>

<h3 id="寻找重复数"><a href="#寻找重复数" class="headerlink" title="寻找重复数"></a><strong>寻找重复数</strong></h3><p>这里有前置条件, 数组中所有的元素都是在1到n之间! </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pysnooper</span><br><span class="line"></span><br><span class="line"><span class="comment">#解法一</span></span><br><span class="line"><span class="comment">#这里有个比较有意思的地方在于, 完全利用了数组中所有元素都在1跟n之间.</span></span><br><span class="line"><span class="comment">#所以对数组求中位数,因为一定存在重复元素, 所以如果比中位数大的元素的个数大于中位数</span></span><br><span class="line"><span class="comment">#则说明重复元素存在在中位素的右边，反之在左边.</span></span><br><span class="line"><span class="meta">@pysnooper.snoop()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findDuplicate</span>(<span class="params">nums</span>):</span><br><span class="line">    low, high = <span class="number">1</span>, <span class="built_in">len</span>(nums)-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = (high + low) // <span class="number">2</span></span><br><span class="line">        count = <span class="built_in">sum</span>(num &lt;= mid <span class="keyword">for</span> num <span class="keyword">in</span> nums)</span><br><span class="line">        <span class="keyword">if</span> count &lt;= mid:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> low</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决二</span></span><br><span class="line"><span class="comment">#使用数组中的值作为索引下标进行遍历，遍历的结果肯定是一个环（有一个重复元素） 检测重复元素问题转换成检测环的入口 为了找到环的入口，可以进行如下步骤：</span></span><br><span class="line"><span class="comment"># 设置两个快慢指针， fast每次走两步，slow每次走一步，最终走了slow走了n步与fast相遇，fast走了2*n，fast可能比slow多饶了环的i圈，得到环的周长为n/i</span></span><br><span class="line"><span class="comment"># slow指针继续走, 且另设第三个指针每次走一步，两个指针必定在入口处相遇</span></span><br><span class="line"><span class="comment"># 假设环的入口和起点的距离时m</span></span><br><span class="line"><span class="comment"># 当第三个指针走了m步到环的入口时</span></span><br><span class="line"><span class="comment"># slow刚好走了n + m步，换句话说时饶了环i圈（环的周长为n/i）加m步（起点到入口的距离）</span></span><br><span class="line"><span class="comment">#得到相遇的是环的入口，入口元素即为重复元素</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@pysnooper.snoop()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findDuplicate2</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(nums) &gt; <span class="number">1</span>:</span><br><span class="line">        slow = nums[<span class="number">0</span>]</span><br><span class="line">        fast = nums[nums[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">while</span> slow != fast:</span><br><span class="line">            slow = nums[slow]</span><br><span class="line">            fast = nums[nums[fast]]</span><br><span class="line">        <span class="comment">#如果些是 快慢指针相遇, 则说明相遇的点为环内的点</span></span><br><span class="line">        entry = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> entry != slow:</span><br><span class="line">            entry = nums[entry]</span><br><span class="line">            slow = nums[slow]</span><br><span class="line">        <span class="keyword">return</span> entry</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>博客新功能2025</title>
    <url>/2024/12/31/%E5%8D%9A%E5%AE%A2%E6%96%B0%E5%8A%9F%E8%83%BD2025/</url>
    <content><![CDATA[<p>最近花了一点时间重新整理了一下博客:</p>
<span id="more"></span>

<ol>
<li>版本升级, hexo&#x2F;NexT版本实在是有点太老，统一升级到了最新版本, 跨版本升级还是花了不少时间处理</li>
<li>修改了文章内标题不能跳转的问题</li>
<li>首页icon无法显示问题</li>
<li>开放了本站pv&#x2F;uv&#x2F;文章pv&#x2F;uv&#x2F;字数等数据， 之前一直没有配置这部分, 历史数据也无从查起, 干脆就2025前从新统计</li>
<li>对SEO&#x2F;CDN做了一些配置,速度会有些提升</li>
<li>依旧想保持简约风格，其它花里胡哨的就先不弄了</li>
</ol>
<p>就这样.</p>
]]></content>
      <categories>
        <category>捣鼓手册</category>
      </categories>
      <tags>
        <tag>捣鼓手册</tag>
      </tags>
  </entry>
  <entry>
    <title>用python将html页面转换成pdf文件,顺便解决中文乱码</title>
    <url>/2017/09/13/%E7%94%A8python%E5%B0%86html%E9%A1%B5%E9%9D%A2%E8%BD%AC%E6%8D%A2%E6%88%90pdf%E6%96%87%E4%BB%B6,%E9%A1%BA%E4%BE%BF%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<p>现在很多主流的浏览器都直接支持把html转换成pdf,比如谷歌大神器,Ctrl+P就能直接完美的转换,再复杂的css样式都没毛病,但我们不能奢求所有客户都使用谷歌浏览器,也不是所有客户都知道可以Ctrl+P,所以不得不在项目中提供一个[另存为PDF]的功能,而且还需要能自动转换为PDF文件,这对于定期生成报表功能尤其重要.</p>
<span id="more"></span>

<p>python环境下, 对于将html转换成pdf,比较常用的有以下几个开源库:</p>
<blockquote>
<ul>
<li>reportlab</li>
<li>xhtml2pdf</li>
<li>weasyprint</li>
<li>pdfkit</li>
<li>wkhtmltopdf</li>
</ul>
</blockquote>
<p>在实现这个需求之前也网上对比了这几个库,个人感觉weasyprint是最理想的方案,而且该项目的开发者非常活跃,但由于当时在机器上其它原因导致weasyprint依赖包没有装上,也花了点时间reportlab研究了它的API,因为已经有了报表的html模板,感觉没有必使用它的API从头到尾再生成一个了,所有放弃了,再来说说xhmt2pdf,查看xhtml2pdf的源码,里面引用了reportlab库,而且xhtml2pdf貌似没怎么更新,2017年也偶尔有几个github的push,最后使用xhmt2pdf的原因一方面是xhtml2pdf可以t很直接地实现我需要的功能,另一方面很大程度上是因为当时网上查看方案的时候看的最多的就是xhtml2pdf的例子,所以选定了xhtml2pdf.更多深入的探究大家可以上各项目的github摸索下吧.</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a><strong>环境</strong></h3><blockquote>
<ul>
<li>flask</li>
<li>python3.4</li>
<li>windows</li>
</ul>
</blockquote>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h3><p>html2pdf转换成pdf,主要碰到以下几个问题:</p>
<blockquote>
<ol>
<li>中文转换后乱码问题</li>
<li>中文的自动换行问题</li>
<li>BytesIO&#x2F;StringIO问题</li>
<li>资源引用路径问题</li>
</ol>
</blockquote>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a><strong>解决</strong></h3><p><strong>第一个问题</strong>一般都是默认的引用字体不支持中文,故可在模板文件中直接引用对应的支持中文的字体文件,这里使用微软雅黑字体,去网上下一个,把该字体文体放到reportlab库的安装路径下&#x2F;font文件夹下,然后在模板文件中如下引用即可:</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">@font-face &#123;</span><br><span class="line">   font-family: msyh;</span><br><span class="line">   src: url(&quot;msyh.ttf&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当然上面也可以不把字体文件放到font目录下,反正只需要上面代码src.url里能找到字体文件即可</p>
<p><strong>第二个问题</strong>中文的自动换行问题,网上通用的做法是在.py文件中引用以下几句代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> reportlab.pdfbase <span class="keyword">import</span> pdfmetrics</span><br><span class="line"><span class="keyword">from</span> reportlab.pdfbase.ttfonts <span class="keyword">import</span> TTFont </span><br><span class="line">pdfmetrics.registerFont(TTFont(<span class="string">&#x27;msyh&#x27;</span>, <span class="string">&#x27;msyh.ttf&#x27;</span>))</span><br><span class="line"><span class="keyword">import</span> reportlab.lib.styles</span><br><span class="line">reportlab.lib.styles.ParagraphStyle.defaults[<span class="string">&#x27;wordWrap&#x27;</span>] = <span class="string">&#x27;CJK&#x27;</span></span><br></pre></td></tr></table></figure>

<p>最重要的是最后一句话,CJK指的是中日韩文字,这句话的意思是按照中文韩文字的规则进行换行判断,查看reportlab_paragraph.py源码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> style.wordWrap == <span class="string">&#x27;CJK&#x27;</span>:</span><br><span class="line">            <span class="comment">#use Asian text wrap algorithm to break characters</span></span><br><span class="line">            blPara = self.breakLinesCJK([first_line_width, later_widths])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blPara = self.breakLines([first_line_width, later_widths])</span><br><span class="line">        self.blPara = blPara</span><br><span class="line">        autoLeading = <span class="built_in">getattr</span>(self, <span class="string">&#x27;autoLeading&#x27;</span>, <span class="built_in">getattr</span>(style, <span class="string">&#x27;autoLeading&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">        leading = style.leading</span><br><span class="line">        <span class="keyword">if</span> blPara.kind == <span class="number">1</span> <span class="keyword">and</span> autoLeading <span class="keyword">not</span> <span class="keyword">in</span> (<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;off&#x27;</span>):</span><br><span class="line">            height = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> autoLeading == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                <span class="keyword">for</span> l <span class="keyword">in</span> blPara.lines:</span><br><span class="line">                    height += <span class="built_in">max</span>(l.ascent - l.descent, leading)</span><br><span class="line">            <span class="keyword">elif</span> autoLeading == <span class="string">&#x27;min&#x27;</span>:</span><br><span class="line">                <span class="keyword">for</span> l <span class="keyword">in</span> blPara.lines:</span><br><span class="line">                    height += l.ascent - l.descent</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&#x27;invalid autoLeading value %r&#x27;</span> % autoLeading)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> autoLeading == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                leading = <span class="built_in">max</span>(leading, LEADING_FACTOR * style.fontSize)</span><br><span class="line">            <span class="keyword">elif</span> autoLeading == <span class="string">&#x27;min&#x27;</span>:</span><br><span class="line">                leading = LEADING_FACTOR * style.fontSize</span><br><span class="line">            height = <span class="built_in">len</span>(blPara.lines) * leading</span><br><span class="line">        self.height = height</span><br></pre></td></tr></table></figure>

<p>但是很可惜,本人各种尝试之后追了源码之后也没有实现中文自动换行,后来没有办法只能使用手工br的方式,好在模板html文件不是很大,工作量不是很大也就忍了</p>
<p><strong>第三个问题</strong>其实是版本的问题,网上很多教程都是基于python2.x环境下,python2.x环境下,StringIO是from cStringIO import StringIO,而在python3.x下,StringIO在io下了,要from io import StringIO,还有就是pisa.CreatePDF(StringIO(pdf_data.encode(‘utf-8’)), pdf)在python3.x下StringIO无法使用,只能使用BytesIO,查看源码document.py里也改成了out &#x3D; io.BytesIO(),故应该改成</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pdf = pisa.CreatePDF(BytesIO(html.encode(<span class="string">&#x27;utf8&#x27;</span>)),result,encoding=<span class="string">&#x27;utf8&#x27;</span>,link_callback=fetch_resources)</span><br></pre></td></tr></table></figure>

<p><strong>第四个问题</strong>则是flask运行,默认的根目录为app.py所有目录,如果模板中使用了资源引用,如</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;&#123;&#123; url_for(&#x27;static&#x27;,filename=&#x27;img/pdf/0.png&#x27;) &#125;&#125;&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>

<p>则生成pdf的时候应用会提示 **”need a valid filename”**错误,其它就是找不到该资源,因为它需要按绝对路径去引用,所以需要pisa.CreatePDF指定link_callback函数,</p>
<p>flask环境如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fetch_resources</span>(<span class="params">uri,rel</span>):</span><br><span class="line">    path = os.getcwd() + uri</span><br><span class="line">    <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure>

<p>而如果是Djang则如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">link_callback</span>(<span class="params">uri, rel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convert HTML URIs to absolute system paths so xhtml2pdf can access those</span></span><br><span class="line"><span class="string">    resources</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># use short variable names</span></span><br><span class="line">    sUrl = settings.STATIC_URL      <span class="comment"># Typically /static/</span></span><br><span class="line">    sRoot = settings.STATIC_ROOT    <span class="comment"># Typically /home/userX/project_static/</span></span><br><span class="line">    mUrl = settings.MEDIA_URL       <span class="comment"># Typically /static/media/</span></span><br><span class="line">    mRoot = settings.MEDIA_ROOT     <span class="comment"># Typically /home/userX/project_static/media/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert URIs to absolute system paths</span></span><br><span class="line">    <span class="keyword">if</span> uri.startswith(mUrl):</span><br><span class="line">        path = os.path.join(mRoot, uri.replace(mUrl, <span class="string">&quot;&quot;</span>))</span><br><span class="line">    <span class="keyword">elif</span> uri.startswith(sUrl):</span><br><span class="line">        path = os.path.join(sRoot, uri.replace(sUrl, <span class="string">&quot;&quot;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> uri  <span class="comment"># handle absolute uri (ie: http://some.tld/foo.png)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># make sure that file exists</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(path):</span><br><span class="line">            <span class="keyword">raise</span> Exception(</span><br><span class="line">                <span class="string">&#x27;media URI must start with %s or %s&#x27;</span> % (sUrl, mUrl)</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure>

<p>生成PDF代码如下,这里当把PDF当做附件,下载的时候浏览器会提示另存为下载框:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成pdf</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/pdfdownload/&#x27;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>,<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdfdownload</span>():</span><br><span class="line">    <span class="comment">#其它业务代码</span></span><br><span class="line">    <span class="comment">#填充pdf模板文件</span></span><br><span class="line">    html = render_template(<span class="string">&#x27;pdfdownload.html&#x27;</span>)</span><br><span class="line">    result = BytesIO()</span><br><span class="line">    <span class="comment">#生成pdf</span></span><br><span class="line">    pdf = pisa.CreatePDF(BytesIO(html.encode(<span class="string">&#x27;utf-8&#x27;</span>)),result,encoding=<span class="string">&#x27;utf-8&#x27;</span>,link_callback=fetch_resources)</span><br><span class="line">    resp = make_response(result.getvalue())</span><br><span class="line">    result.close()</span><br><span class="line">    resp.headers[<span class="string">&quot;Content-Disposition&quot;</span>] = (<span class="string">&quot;attachment; filename=&#x27;&#123;0&#125;&#x27;&quot;</span>.<span class="built_in">format</span>(<span class="string">&#x27;skxt-jkxj.pdf&#x27;</span>))</span><br><span class="line">    resp.headers[<span class="string">&#x27;Content-Type&#x27;</span>] = <span class="string">&#x27;application/pdf&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure>

<p>如果是自动生成PDF文件,官方代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> xhtml2pdf <span class="keyword">import</span> pisa</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your data</span></span><br><span class="line">sourceHtml = <span class="string">&quot;&lt;html&gt;&lt;body&gt;&lt;p&gt;To PDF or not to PDF&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&quot;</span></span><br><span class="line">outputFilename = <span class="string">&quot;test.pdf&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convertHtmlToPdf</span>(<span class="params">sourceHtml, outputFilename</span>):</span><br><span class="line">    <span class="comment"># open output file for writing (truncated binary)</span></span><br><span class="line">    resultFile = <span class="built_in">open</span>(outputFilename, <span class="string">&quot;w+b&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert HTML to PDF</span></span><br><span class="line">    pisaStatus = pisa.CreatePDF(</span><br><span class="line">            sourceHtml,                <span class="comment"># the HTML to convert</span></span><br><span class="line">            dest=resultFile)           <span class="comment"># file handle to recieve result</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># close output file</span></span><br><span class="line">    resultFile.close()                 <span class="comment"># close output file</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># return True on success and False on errors</span></span><br><span class="line">    <span class="keyword">return</span> pisaStatus.err</span><br><span class="line"></span><br><span class="line"><span class="comment"># Main program</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    pisa.showLogging()</span><br><span class="line">    convertHtmlToPdf(sourceHtml, outputFilename)</span><br></pre></td></tr></table></figure>

<p>官方的usage.rst其它很有用,还有些example也很值得借鉴,有时候遇到问题不防也看看issue,你碰到的问题,其它人肯定也碰到过.</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章:"></a><strong>参考文章:</strong></h3><blockquote>
<ul>
<li><a href="https://github.com/xhtml2pdf">xhtml2pdf项目github</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
